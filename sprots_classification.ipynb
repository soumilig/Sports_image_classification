{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size=32\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sports/sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df[df['data set']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df[df['data set']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class id', 'filepaths', 'labels', 'data set'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'sports/'+df_train['filepaths'][0]\n",
    "im = Image.open(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n",
      "(224, 224)\n",
      "<built-in method size of Tensor object at 0x000001A19FE83B50>\n"
     ]
    }
   ],
   "source": [
    "print(im.size)\n",
    "im_rgb = im.convert('RGB')\n",
    "print(im_rgb.size)\n",
    "tens = transforms.ToTensor()\n",
    "im_tensor = tens(im_rgb)\n",
    "print(im_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(im_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_24828\\713196039.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_train[df['labels']=='high jump']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class id</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/001.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5464</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/002.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/003.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/004.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/005.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/155.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/156.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/157.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/158.jpg</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>40</td>\n",
       "      <td>train/high jump/159.lnk</td>\n",
       "      <td>high jump</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class id                filepaths     labels data set\n",
       "5463        40  train/high jump/001.jpg  high jump    train\n",
       "5464        40  train/high jump/002.jpg  high jump    train\n",
       "5465        40  train/high jump/003.jpg  high jump    train\n",
       "5466        40  train/high jump/004.jpg  high jump    train\n",
       "5467        40  train/high jump/005.jpg  high jump    train\n",
       "...        ...                      ...        ...      ...\n",
       "5617        40  train/high jump/155.jpg  high jump    train\n",
       "5618        40  train/high jump/156.jpg  high jump    train\n",
       "5619        40  train/high jump/157.jpg  high jump    train\n",
       "5620        40  train/high jump/158.jpg  high jump    train\n",
       "5621        40  train/high jump/159.lnk  high jump    train\n",
       "\n",
       "[159 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df['labels']=='high jump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_op = df_train.drop(index=5621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class id                filepaths     labels data set\n",
      "5463        40  train/high jump/001.jpg  high jump    train\n",
      "5464        40  train/high jump/002.jpg  high jump    train\n",
      "5465        40  train/high jump/003.jpg  high jump    train\n",
      "5466        40  train/high jump/004.jpg  high jump    train\n",
      "5467        40  train/high jump/005.jpg  high jump    train\n",
      "...        ...                      ...        ...      ...\n",
      "5616        40  train/high jump/154.jpg  high jump    train\n",
      "5617        40  train/high jump/155.jpg  high jump    train\n",
      "5618        40  train/high jump/156.jpg  high jump    train\n",
      "5619        40  train/high jump/157.jpg  high jump    train\n",
      "5620        40  train/high jump/158.jpg  high jump    train\n",
      "\n",
      "[158 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train_op[df_train_op['labels']=='high jump'])\n",
    "len(df_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SportsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_list, df_desc):\n",
    "        # image_series = pd.Series(image_list)\n",
    "\n",
    "        self.x = image_list\n",
    "        self.y = list(df_desc['enc_label'])\n",
    "\n",
    "        self.n_samples = len(df_desc)      \n",
    "         \n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 1\n",
      "Processed image 2\n",
      "Processed image 3\n",
      "Processed image 4\n",
      "Processed image 5\n",
      "Processed image 6\n",
      "Processed image 7\n",
      "Processed image 8\n",
      "Processed image 9\n",
      "Processed image 10\n",
      "Processed image 11\n",
      "Processed image 12\n",
      "Processed image 13\n",
      "Processed image 14\n",
      "Processed image 15\n",
      "Processed image 16\n",
      "Processed image 17\n",
      "Processed image 18\n",
      "Processed image 19\n",
      "Processed image 20\n",
      "Processed image 21\n",
      "Processed image 22\n",
      "Processed image 23\n",
      "Processed image 24\n",
      "Processed image 25\n",
      "Processed image 26\n",
      "Processed image 27\n",
      "Processed image 28\n",
      "Processed image 29\n",
      "Processed image 30\n",
      "Processed image 31\n",
      "Processed image 32\n",
      "Processed image 33\n",
      "Processed image 34\n",
      "Processed image 35\n",
      "Processed image 36\n",
      "Processed image 37\n",
      "Processed image 38\n",
      "Processed image 39\n",
      "Processed image 40\n",
      "Processed image 41\n",
      "Processed image 42\n",
      "Processed image 43\n",
      "Processed image 44\n",
      "Processed image 45\n",
      "Processed image 46\n",
      "Processed image 47\n",
      "Processed image 48\n",
      "Processed image 49\n",
      "Processed image 50\n",
      "Processed image 51\n",
      "Processed image 52\n",
      "Processed image 53\n",
      "Processed image 54\n",
      "Processed image 55\n",
      "Processed image 56\n",
      "Processed image 57\n",
      "Processed image 58\n",
      "Processed image 59\n",
      "Processed image 60\n",
      "Processed image 61\n",
      "Processed image 62\n",
      "Processed image 63\n",
      "Processed image 64\n",
      "Processed image 65\n",
      "Processed image 66\n",
      "Processed image 67\n",
      "Processed image 68\n",
      "Processed image 69\n",
      "Processed image 70\n",
      "Processed image 71\n",
      "Processed image 72\n",
      "Processed image 73\n",
      "Processed image 74\n",
      "Processed image 75\n",
      "Processed image 76\n",
      "Processed image 77\n",
      "Processed image 78\n",
      "Processed image 79\n",
      "Processed image 80\n",
      "Processed image 81\n",
      "Processed image 82\n",
      "Processed image 83\n",
      "Processed image 84\n",
      "Processed image 85\n",
      "Processed image 86\n",
      "Processed image 87\n",
      "Processed image 88\n",
      "Processed image 89\n",
      "Processed image 90\n",
      "Processed image 91\n",
      "Processed image 92\n",
      "Processed image 93\n",
      "Processed image 94\n",
      "Processed image 95\n",
      "Processed image 96\n",
      "Processed image 97\n",
      "Processed image 98\n",
      "Processed image 99\n",
      "Processed image 100\n",
      "Processed image 101\n",
      "Processed image 102\n",
      "Processed image 103\n",
      "Processed image 104\n",
      "Processed image 105\n",
      "Processed image 106\n",
      "Processed image 107\n",
      "Processed image 108\n",
      "Processed image 109\n",
      "Processed image 110\n",
      "Processed image 111\n",
      "Processed image 112\n",
      "Processed image 113\n",
      "Processed image 114\n",
      "Processed image 115\n",
      "Processed image 116\n",
      "Processed image 117\n",
      "Processed image 118\n",
      "Processed image 119\n",
      "Processed image 120\n",
      "Processed image 121\n",
      "Processed image 122\n",
      "Processed image 123\n",
      "Processed image 124\n",
      "Processed image 125\n",
      "Processed image 126\n",
      "Processed image 127\n",
      "Processed image 128\n",
      "Processed image 129\n",
      "Processed image 130\n",
      "Processed image 131\n",
      "Processed image 132\n",
      "Processed image 133\n",
      "Processed image 134\n",
      "Processed image 135\n",
      "Processed image 136\n",
      "Processed image 137\n",
      "Processed image 138\n",
      "Processed image 139\n",
      "Processed image 140\n",
      "Processed image 141\n",
      "Processed image 142\n",
      "Processed image 143\n",
      "Processed image 144\n",
      "Processed image 145\n",
      "Processed image 146\n",
      "Processed image 147\n",
      "Processed image 148\n",
      "Processed image 149\n",
      "Processed image 150\n",
      "Processed image 151\n",
      "Processed image 152\n",
      "Processed image 153\n",
      "Processed image 154\n",
      "Processed image 155\n",
      "Processed image 156\n",
      "Processed image 157\n",
      "Processed image 158\n",
      "Processed image 159\n",
      "Processed image 160\n",
      "Processed image 161\n",
      "Processed image 162\n",
      "Processed image 163\n",
      "Processed image 164\n",
      "Processed image 165\n",
      "Processed image 166\n",
      "Processed image 167\n",
      "Processed image 168\n",
      "Processed image 169\n",
      "Processed image 170\n",
      "Processed image 171\n",
      "Processed image 172\n",
      "Processed image 173\n",
      "Processed image 174\n",
      "Processed image 175\n",
      "Processed image 176\n",
      "Processed image 177\n",
      "Processed image 178\n",
      "Processed image 179\n",
      "Processed image 180\n",
      "Processed image 181\n",
      "Processed image 182\n",
      "Processed image 183\n",
      "Processed image 184\n",
      "Processed image 185\n",
      "Processed image 186\n",
      "Processed image 187\n",
      "Processed image 188\n",
      "Processed image 189\n",
      "Processed image 190\n",
      "Processed image 191\n",
      "Processed image 192\n",
      "Processed image 193\n",
      "Processed image 194\n",
      "Processed image 195\n",
      "Processed image 196\n",
      "Processed image 197\n",
      "Processed image 198\n",
      "Processed image 199\n",
      "Processed image 200\n",
      "Processed image 201\n",
      "Processed image 202\n",
      "Processed image 203\n",
      "Processed image 204\n",
      "Processed image 205\n",
      "Processed image 206\n",
      "Processed image 207\n",
      "Processed image 208\n",
      "Processed image 209\n",
      "Processed image 210\n",
      "Processed image 211\n",
      "Processed image 212\n",
      "Processed image 213\n",
      "Processed image 214\n",
      "Processed image 215\n",
      "Processed image 216\n",
      "Processed image 217\n",
      "Processed image 218\n",
      "Processed image 219\n",
      "Processed image 220\n",
      "Processed image 221\n",
      "Processed image 222\n",
      "Processed image 223\n",
      "Processed image 224\n",
      "Processed image 225\n",
      "Processed image 226\n",
      "Processed image 227\n",
      "Processed image 228\n",
      "Processed image 229\n",
      "Processed image 230\n",
      "Processed image 231\n",
      "Processed image 232\n",
      "Processed image 233\n",
      "Processed image 234\n",
      "Processed image 235\n",
      "Processed image 236\n",
      "Processed image 237\n",
      "Processed image 238\n",
      "Processed image 239\n",
      "Processed image 240\n",
      "Processed image 241\n",
      "Processed image 242\n",
      "Processed image 243\n",
      "Processed image 244\n",
      "Processed image 245\n",
      "Processed image 246\n",
      "Processed image 247\n",
      "Processed image 248\n",
      "Processed image 249\n",
      "Processed image 250\n",
      "Processed image 251\n",
      "Processed image 252\n",
      "Processed image 253\n",
      "Processed image 254\n",
      "Processed image 255\n",
      "Processed image 256\n",
      "Processed image 257\n",
      "Processed image 258\n",
      "Processed image 259\n",
      "Processed image 260\n",
      "Processed image 261\n",
      "Processed image 262\n",
      "Processed image 263\n",
      "Processed image 264\n",
      "Processed image 265\n",
      "Processed image 266\n",
      "Processed image 267\n",
      "Processed image 268\n",
      "Processed image 269\n",
      "Processed image 270\n",
      "Processed image 271\n",
      "Processed image 272\n",
      "Processed image 273\n",
      "Processed image 274\n",
      "Processed image 275\n",
      "Processed image 276\n",
      "Processed image 277\n",
      "Processed image 278\n",
      "Processed image 279\n",
      "Processed image 280\n",
      "Processed image 281\n",
      "Processed image 282\n",
      "Processed image 283\n",
      "Processed image 284\n",
      "Processed image 285\n",
      "Processed image 286\n",
      "Processed image 287\n",
      "Processed image 288\n",
      "Processed image 289\n",
      "Processed image 290\n",
      "Processed image 291\n",
      "Processed image 292\n",
      "Processed image 293\n",
      "Processed image 294\n",
      "Processed image 295\n",
      "Processed image 296\n",
      "Processed image 297\n",
      "Processed image 298\n",
      "Processed image 299\n",
      "Processed image 300\n",
      "Processed image 301\n",
      "Processed image 302\n",
      "Processed image 303\n",
      "Processed image 304\n",
      "Processed image 305\n",
      "Processed image 306\n",
      "Processed image 307\n",
      "Processed image 308\n",
      "Processed image 309\n",
      "Processed image 310\n",
      "Processed image 311\n",
      "Processed image 312\n",
      "Processed image 313\n",
      "Processed image 314\n",
      "Processed image 315\n",
      "Processed image 316\n",
      "Processed image 317\n",
      "Processed image 318\n",
      "Processed image 319\n",
      "Processed image 320\n",
      "Processed image 321\n",
      "Processed image 322\n",
      "Processed image 323\n",
      "Processed image 324\n",
      "Processed image 325\n",
      "Processed image 326\n",
      "Processed image 327\n",
      "Processed image 328\n",
      "Processed image 329\n",
      "Processed image 330\n",
      "Processed image 331\n",
      "Processed image 332\n",
      "Processed image 333\n",
      "Processed image 334\n",
      "Processed image 335\n",
      "Processed image 336\n",
      "Processed image 337\n",
      "Processed image 338\n",
      "Processed image 339\n",
      "Processed image 340\n",
      "Processed image 341\n",
      "Processed image 342\n",
      "Processed image 343\n",
      "Processed image 344\n",
      "Processed image 345\n",
      "Processed image 346\n",
      "Processed image 347\n",
      "Processed image 348\n",
      "Processed image 349\n",
      "Processed image 350\n",
      "Processed image 351\n",
      "Processed image 352\n",
      "Processed image 353\n",
      "Processed image 354\n",
      "Processed image 355\n",
      "Processed image 356\n",
      "Processed image 357\n",
      "Processed image 358\n",
      "Processed image 359\n",
      "Processed image 360\n",
      "Processed image 361\n",
      "Processed image 362\n",
      "Processed image 363\n",
      "Processed image 364\n",
      "Processed image 365\n",
      "Processed image 366\n",
      "Processed image 367\n",
      "Processed image 368\n",
      "Processed image 369\n",
      "Processed image 370\n",
      "Processed image 371\n",
      "Processed image 372\n",
      "Processed image 373\n",
      "Processed image 374\n",
      "Processed image 375\n",
      "Processed image 376\n",
      "Processed image 377\n",
      "Processed image 378\n",
      "Processed image 379\n",
      "Processed image 380\n",
      "Processed image 381\n",
      "Processed image 382\n",
      "Processed image 383\n",
      "Processed image 384\n",
      "Processed image 385\n",
      "Processed image 386\n",
      "Processed image 387\n",
      "Processed image 388\n",
      "Processed image 389\n",
      "Processed image 390\n",
      "Processed image 391\n",
      "Processed image 392\n",
      "Processed image 393\n",
      "Processed image 394\n",
      "Processed image 395\n",
      "Processed image 396\n",
      "Processed image 397\n",
      "Processed image 398\n",
      "Processed image 399\n",
      "Processed image 400\n",
      "Processed image 401\n",
      "Processed image 402\n",
      "Processed image 403\n",
      "Processed image 404\n",
      "Processed image 405\n",
      "Processed image 406\n",
      "Processed image 407\n",
      "Processed image 408\n",
      "Processed image 409\n",
      "Processed image 410\n",
      "Processed image 411\n",
      "Processed image 412\n",
      "Processed image 413\n",
      "Processed image 414\n",
      "Processed image 415\n",
      "Processed image 416\n",
      "Processed image 417\n",
      "Processed image 418\n",
      "Processed image 419\n",
      "Processed image 420\n",
      "Processed image 421\n",
      "Processed image 422\n",
      "Processed image 423\n",
      "Processed image 424\n",
      "Processed image 425\n",
      "Processed image 426\n",
      "Processed image 427\n",
      "Processed image 428\n",
      "Processed image 429\n",
      "Processed image 430\n",
      "Processed image 431\n",
      "Processed image 432\n",
      "Processed image 433\n",
      "Processed image 434\n",
      "Processed image 435\n",
      "Processed image 436\n",
      "Processed image 437\n",
      "Processed image 438\n",
      "Processed image 439\n",
      "Processed image 440\n",
      "Processed image 441\n",
      "Processed image 442\n",
      "Processed image 443\n",
      "Processed image 444\n",
      "Processed image 445\n",
      "Processed image 446\n",
      "Processed image 447\n",
      "Processed image 448\n",
      "Processed image 449\n",
      "Processed image 450\n",
      "Processed image 451\n",
      "Processed image 452\n",
      "Processed image 453\n",
      "Processed image 454\n",
      "Processed image 455\n",
      "Processed image 456\n",
      "Processed image 457\n",
      "Processed image 458\n",
      "Processed image 459\n",
      "Processed image 460\n",
      "Processed image 461\n",
      "Processed image 462\n",
      "Processed image 463\n",
      "Processed image 464\n",
      "Processed image 465\n",
      "Processed image 466\n",
      "Processed image 467\n",
      "Processed image 468\n",
      "Processed image 469\n",
      "Processed image 470\n",
      "Processed image 471\n",
      "Processed image 472\n",
      "Processed image 473\n",
      "Processed image 474\n",
      "Processed image 475\n",
      "Processed image 476\n",
      "Processed image 477\n",
      "Processed image 478\n",
      "Processed image 479\n",
      "Processed image 480\n",
      "Processed image 481\n",
      "Processed image 482\n",
      "Processed image 483\n",
      "Processed image 484\n",
      "Processed image 485\n",
      "Processed image 486\n",
      "Processed image 487\n",
      "Processed image 488\n",
      "Processed image 489\n",
      "Processed image 490\n",
      "Processed image 491\n",
      "Processed image 492\n",
      "Processed image 493\n",
      "Processed image 494\n",
      "Processed image 495\n",
      "Processed image 496\n",
      "Processed image 497\n",
      "Processed image 498\n",
      "Processed image 499\n",
      "Processed image 500\n",
      "Processed image 501\n",
      "Processed image 502\n",
      "Processed image 503\n",
      "Processed image 504\n",
      "Processed image 505\n",
      "Processed image 506\n",
      "Processed image 507\n",
      "Processed image 508\n",
      "Processed image 509\n",
      "Processed image 510\n",
      "Processed image 511\n",
      "Processed image 512\n",
      "Processed image 513\n",
      "Processed image 514\n",
      "Processed image 515\n",
      "Processed image 516\n",
      "Processed image 517\n",
      "Processed image 518\n",
      "Processed image 519\n",
      "Processed image 520\n",
      "Processed image 521\n",
      "Processed image 522\n",
      "Processed image 523\n",
      "Processed image 524\n",
      "Processed image 525\n",
      "Processed image 526\n",
      "Processed image 527\n",
      "Processed image 528\n",
      "Processed image 529\n",
      "Processed image 530\n",
      "Processed image 531\n",
      "Processed image 532\n",
      "Processed image 533\n",
      "Processed image 534\n",
      "Processed image 535\n",
      "Processed image 536\n",
      "Processed image 537\n",
      "Processed image 538\n",
      "Processed image 539\n",
      "Processed image 540\n",
      "Processed image 541\n",
      "Processed image 542\n",
      "Processed image 543\n",
      "Processed image 544\n",
      "Processed image 545\n",
      "Processed image 546\n",
      "Processed image 547\n",
      "Processed image 548\n",
      "Processed image 549\n",
      "Processed image 550\n",
      "Processed image 551\n",
      "Processed image 552\n",
      "Processed image 553\n",
      "Processed image 554\n",
      "Processed image 555\n",
      "Processed image 556\n",
      "Processed image 557\n",
      "Processed image 558\n",
      "Processed image 559\n",
      "Processed image 560\n",
      "Processed image 561\n",
      "Processed image 562\n",
      "Processed image 563\n",
      "Processed image 564\n",
      "Processed image 565\n",
      "Processed image 566\n",
      "Processed image 567\n",
      "Processed image 568\n",
      "Processed image 569\n",
      "Processed image 570\n",
      "Processed image 571\n",
      "Processed image 572\n",
      "Processed image 573\n",
      "Processed image 574\n",
      "Processed image 575\n",
      "Processed image 576\n",
      "Processed image 577\n",
      "Processed image 578\n",
      "Processed image 579\n",
      "Processed image 580\n",
      "Processed image 581\n",
      "Processed image 582\n",
      "Processed image 583\n",
      "Processed image 584\n",
      "Processed image 585\n",
      "Processed image 586\n",
      "Processed image 587\n",
      "Processed image 588\n",
      "Processed image 589\n",
      "Processed image 590\n",
      "Processed image 591\n",
      "Processed image 592\n",
      "Processed image 593\n",
      "Processed image 594\n",
      "Processed image 595\n",
      "Processed image 596\n",
      "Processed image 597\n",
      "Processed image 598\n",
      "Processed image 599\n",
      "Processed image 600\n",
      "Processed image 601\n",
      "Processed image 602\n",
      "Processed image 603\n",
      "Processed image 604\n",
      "Processed image 605\n",
      "Processed image 606\n",
      "Processed image 607\n",
      "Processed image 608\n",
      "Processed image 609\n",
      "Processed image 610\n",
      "Processed image 611\n",
      "Processed image 612\n",
      "Processed image 613\n",
      "Processed image 614\n",
      "Processed image 615\n",
      "Processed image 616\n",
      "Processed image 617\n",
      "Processed image 618\n",
      "Processed image 619\n",
      "Processed image 620\n",
      "Processed image 621\n",
      "Processed image 622\n",
      "Processed image 623\n",
      "Processed image 624\n",
      "Processed image 625\n",
      "Processed image 626\n",
      "Processed image 627\n",
      "Processed image 628\n",
      "Processed image 629\n",
      "Processed image 630\n",
      "Processed image 631\n",
      "Processed image 632\n",
      "Processed image 633\n",
      "Processed image 634\n",
      "Processed image 635\n",
      "Processed image 636\n",
      "Processed image 637\n",
      "Processed image 638\n",
      "Processed image 639\n",
      "Processed image 640\n",
      "Processed image 641\n",
      "Processed image 642\n",
      "Processed image 643\n",
      "Processed image 644\n",
      "Processed image 645\n",
      "Processed image 646\n",
      "Processed image 647\n",
      "Processed image 648\n",
      "Processed image 649\n",
      "Processed image 650\n",
      "Processed image 651\n",
      "Processed image 652\n",
      "Processed image 653\n",
      "Processed image 654\n",
      "Processed image 655\n",
      "Processed image 656\n",
      "Processed image 657\n",
      "Processed image 658\n",
      "Processed image 659\n",
      "Processed image 660\n",
      "Processed image 661\n",
      "Processed image 662\n",
      "Processed image 663\n",
      "Processed image 664\n",
      "Processed image 665\n",
      "Processed image 666\n",
      "Processed image 667\n",
      "Processed image 668\n",
      "Processed image 669\n",
      "Processed image 670\n",
      "Processed image 671\n",
      "Processed image 672\n",
      "Processed image 673\n",
      "Processed image 674\n",
      "Processed image 675\n",
      "Processed image 676\n",
      "Processed image 677\n",
      "Processed image 678\n",
      "Processed image 679\n",
      "Processed image 680\n",
      "Processed image 681\n",
      "Processed image 682\n",
      "Processed image 683\n",
      "Processed image 684\n",
      "Processed image 685\n",
      "Processed image 686\n",
      "Processed image 687\n",
      "Processed image 688\n",
      "Processed image 689\n",
      "Processed image 690\n",
      "Processed image 691\n",
      "Processed image 692\n",
      "Processed image 693\n",
      "Processed image 694\n",
      "Processed image 695\n",
      "Processed image 696\n",
      "Processed image 697\n",
      "Processed image 698\n",
      "Processed image 699\n",
      "Processed image 700\n",
      "Processed image 701\n",
      "Processed image 702\n",
      "Processed image 703\n",
      "Processed image 704\n",
      "Processed image 705\n",
      "Processed image 706\n",
      "Processed image 707\n",
      "Processed image 708\n",
      "Processed image 709\n",
      "Processed image 710\n",
      "Processed image 711\n",
      "Processed image 712\n",
      "Processed image 713\n",
      "Processed image 714\n",
      "Processed image 715\n",
      "Processed image 716\n",
      "Processed image 717\n",
      "Processed image 718\n",
      "Processed image 719\n",
      "Processed image 720\n",
      "Processed image 721\n",
      "Processed image 722\n",
      "Processed image 723\n",
      "Processed image 724\n",
      "Processed image 725\n",
      "Processed image 726\n",
      "Processed image 727\n",
      "Processed image 728\n",
      "Processed image 729\n",
      "Processed image 730\n",
      "Processed image 731\n",
      "Processed image 732\n",
      "Processed image 733\n",
      "Processed image 734\n",
      "Processed image 735\n",
      "Processed image 736\n",
      "Processed image 737\n",
      "Processed image 738\n",
      "Processed image 739\n",
      "Processed image 740\n",
      "Processed image 741\n",
      "Processed image 742\n",
      "Processed image 743\n",
      "Processed image 744\n",
      "Processed image 745\n",
      "Processed image 746\n",
      "Processed image 747\n",
      "Processed image 748\n",
      "Processed image 749\n",
      "Processed image 750\n",
      "Processed image 751\n",
      "Processed image 752\n",
      "Processed image 753\n",
      "Processed image 754\n",
      "Processed image 755\n",
      "Processed image 756\n",
      "Processed image 757\n",
      "Processed image 758\n",
      "Processed image 759\n",
      "Processed image 760\n",
      "Processed image 761\n",
      "Processed image 762\n",
      "Processed image 763\n",
      "Processed image 764\n",
      "Processed image 765\n",
      "Processed image 766\n",
      "Processed image 767\n",
      "Processed image 768\n",
      "Processed image 769\n",
      "Processed image 770\n",
      "Processed image 771\n",
      "Processed image 772\n",
      "Processed image 773\n",
      "Processed image 774\n",
      "Processed image 775\n",
      "Processed image 776\n",
      "Processed image 777\n",
      "Processed image 778\n",
      "Processed image 779\n",
      "Processed image 780\n",
      "Processed image 781\n",
      "Processed image 782\n",
      "Processed image 783\n",
      "Processed image 784\n",
      "Processed image 785\n",
      "Processed image 786\n",
      "Processed image 787\n",
      "Processed image 788\n",
      "Processed image 789\n",
      "Processed image 790\n",
      "Processed image 791\n",
      "Processed image 792\n",
      "Processed image 793\n",
      "Processed image 794\n",
      "Processed image 795\n",
      "Processed image 796\n",
      "Processed image 797\n",
      "Processed image 798\n",
      "Processed image 799\n",
      "Processed image 800\n",
      "Processed image 801\n",
      "Processed image 802\n",
      "Processed image 803\n",
      "Processed image 804\n",
      "Processed image 805\n",
      "Processed image 806\n",
      "Processed image 807\n",
      "Processed image 808\n",
      "Processed image 809\n",
      "Processed image 810\n",
      "Processed image 811\n",
      "Processed image 812\n",
      "Processed image 813\n",
      "Processed image 814\n",
      "Processed image 815\n",
      "Processed image 816\n",
      "Processed image 817\n",
      "Processed image 818\n",
      "Processed image 819\n",
      "Processed image 820\n",
      "Processed image 821\n",
      "Processed image 822\n",
      "Processed image 823\n",
      "Processed image 824\n",
      "Processed image 825\n",
      "Processed image 826\n",
      "Processed image 827\n",
      "Processed image 828\n",
      "Processed image 829\n",
      "Processed image 830\n",
      "Processed image 831\n",
      "Processed image 832\n",
      "Processed image 833\n",
      "Processed image 834\n",
      "Processed image 835\n",
      "Processed image 836\n",
      "Processed image 837\n",
      "Processed image 838\n",
      "Processed image 839\n",
      "Processed image 840\n",
      "Processed image 841\n",
      "Processed image 842\n",
      "Processed image 843\n",
      "Processed image 844\n",
      "Processed image 845\n",
      "Processed image 846\n",
      "Processed image 847\n",
      "Processed image 848\n",
      "Processed image 849\n",
      "Processed image 850\n",
      "Processed image 851\n",
      "Processed image 852\n",
      "Processed image 853\n",
      "Processed image 854\n",
      "Processed image 855\n",
      "Processed image 856\n",
      "Processed image 857\n",
      "Processed image 858\n",
      "Processed image 859\n",
      "Processed image 860\n",
      "Processed image 861\n",
      "Processed image 862\n",
      "Processed image 863\n",
      "Processed image 864\n",
      "Processed image 865\n",
      "Processed image 866\n",
      "Processed image 867\n",
      "Processed image 868\n",
      "Processed image 869\n",
      "Processed image 870\n",
      "Processed image 871\n",
      "Processed image 872\n",
      "Processed image 873\n",
      "Processed image 874\n",
      "Processed image 875\n",
      "Processed image 876\n",
      "Processed image 877\n",
      "Processed image 878\n",
      "Processed image 879\n",
      "Processed image 880\n",
      "Processed image 881\n",
      "Processed image 882\n",
      "Processed image 883\n",
      "Processed image 884\n",
      "Processed image 885\n",
      "Processed image 886\n",
      "Processed image 887\n",
      "Processed image 888\n",
      "Processed image 889\n",
      "Processed image 890\n",
      "Processed image 891\n",
      "Processed image 892\n",
      "Processed image 893\n",
      "Processed image 894\n",
      "Processed image 895\n",
      "Processed image 896\n",
      "Processed image 897\n",
      "Processed image 898\n",
      "Processed image 899\n",
      "Processed image 900\n",
      "Processed image 901\n",
      "Processed image 902\n",
      "Processed image 903\n",
      "Processed image 904\n",
      "Processed image 905\n",
      "Processed image 906\n",
      "Processed image 907\n",
      "Processed image 908\n",
      "Processed image 909\n",
      "Processed image 910\n",
      "Processed image 911\n",
      "Processed image 912\n",
      "Processed image 913\n",
      "Processed image 914\n",
      "Processed image 915\n",
      "Processed image 916\n",
      "Processed image 917\n",
      "Processed image 918\n",
      "Processed image 919\n",
      "Processed image 920\n",
      "Processed image 921\n",
      "Processed image 922\n",
      "Processed image 923\n",
      "Processed image 924\n",
      "Processed image 925\n",
      "Processed image 926\n",
      "Processed image 927\n",
      "Processed image 928\n",
      "Processed image 929\n",
      "Processed image 930\n",
      "Processed image 931\n",
      "Processed image 932\n",
      "Processed image 933\n",
      "Processed image 934\n",
      "Processed image 935\n",
      "Processed image 936\n",
      "Processed image 937\n",
      "Processed image 938\n",
      "Processed image 939\n",
      "Processed image 940\n",
      "Processed image 941\n",
      "Processed image 942\n",
      "Processed image 943\n",
      "Processed image 944\n",
      "Processed image 945\n",
      "Processed image 946\n",
      "Processed image 947\n",
      "Processed image 948\n",
      "Processed image 949\n",
      "Processed image 950\n",
      "Processed image 951\n",
      "Processed image 952\n",
      "Processed image 953\n",
      "Processed image 954\n",
      "Processed image 955\n",
      "Processed image 956\n",
      "Processed image 957\n",
      "Processed image 958\n",
      "Processed image 959\n",
      "Processed image 960\n",
      "Processed image 961\n",
      "Processed image 962\n",
      "Processed image 963\n",
      "Processed image 964\n",
      "Processed image 965\n",
      "Processed image 966\n",
      "Processed image 967\n",
      "Processed image 968\n",
      "Processed image 969\n",
      "Processed image 970\n",
      "Processed image 971\n",
      "Processed image 972\n",
      "Processed image 973\n",
      "Processed image 974\n",
      "Processed image 975\n",
      "Processed image 976\n",
      "Processed image 977\n",
      "Processed image 978\n",
      "Processed image 979\n",
      "Processed image 980\n",
      "Processed image 981\n",
      "Processed image 982\n",
      "Processed image 983\n",
      "Processed image 984\n",
      "Processed image 985\n",
      "Processed image 986\n",
      "Processed image 987\n",
      "Processed image 988\n",
      "Processed image 989\n",
      "Processed image 990\n",
      "Processed image 991\n",
      "Processed image 992\n",
      "Processed image 993\n",
      "Processed image 994\n",
      "Processed image 995\n",
      "Processed image 996\n",
      "Processed image 997\n",
      "Processed image 998\n",
      "Processed image 999\n",
      "Processed image 1000\n",
      "Processed image 1001\n",
      "Processed image 1002\n",
      "Processed image 1003\n",
      "Processed image 1004\n",
      "Processed image 1005\n",
      "Processed image 1006\n",
      "Processed image 1007\n",
      "Processed image 1008\n",
      "Processed image 1009\n",
      "Processed image 1010\n",
      "Processed image 1011\n",
      "Processed image 1012\n",
      "Processed image 1013\n",
      "Processed image 1014\n",
      "Processed image 1015\n",
      "Processed image 1016\n",
      "Processed image 1017\n",
      "Processed image 1018\n",
      "Processed image 1019\n",
      "Processed image 1020\n",
      "Processed image 1021\n",
      "Processed image 1022\n",
      "Processed image 1023\n",
      "Processed image 1024\n",
      "Processed image 1025\n",
      "Processed image 1026\n",
      "Processed image 1027\n",
      "Processed image 1028\n",
      "Processed image 1029\n",
      "Processed image 1030\n",
      "Processed image 1031\n",
      "Processed image 1032\n",
      "Processed image 1033\n",
      "Processed image 1034\n",
      "Processed image 1035\n",
      "Processed image 1036\n",
      "Processed image 1037\n",
      "Processed image 1038\n",
      "Processed image 1039\n",
      "Processed image 1040\n",
      "Processed image 1041\n",
      "Processed image 1042\n",
      "Processed image 1043\n",
      "Processed image 1044\n",
      "Processed image 1045\n",
      "Processed image 1046\n",
      "Processed image 1047\n",
      "Processed image 1048\n",
      "Processed image 1049\n",
      "Processed image 1050\n",
      "Processed image 1051\n",
      "Processed image 1052\n",
      "Processed image 1053\n",
      "Processed image 1054\n",
      "Processed image 1055\n",
      "Processed image 1056\n",
      "Processed image 1057\n",
      "Processed image 1058\n",
      "Processed image 1059\n",
      "Processed image 1060\n",
      "Processed image 1061\n",
      "Processed image 1062\n",
      "Processed image 1063\n",
      "Processed image 1064\n",
      "Processed image 1065\n",
      "Processed image 1066\n",
      "Processed image 1067\n",
      "Processed image 1068\n",
      "Processed image 1069\n",
      "Processed image 1070\n",
      "Processed image 1071\n",
      "Processed image 1072\n",
      "Processed image 1073\n",
      "Processed image 1074\n",
      "Processed image 1075\n",
      "Processed image 1076\n",
      "Processed image 1077\n",
      "Processed image 1078\n",
      "Processed image 1079\n",
      "Processed image 1080\n",
      "Processed image 1081\n",
      "Processed image 1082\n",
      "Processed image 1083\n",
      "Processed image 1084\n",
      "Processed image 1085\n",
      "Processed image 1086\n",
      "Processed image 1087\n",
      "Processed image 1088\n",
      "Processed image 1089\n",
      "Processed image 1090\n",
      "Processed image 1091\n",
      "Processed image 1092\n",
      "Processed image 1093\n",
      "Processed image 1094\n",
      "Processed image 1095\n",
      "Processed image 1096\n",
      "Processed image 1097\n",
      "Processed image 1098\n",
      "Processed image 1099\n",
      "Processed image 1100\n",
      "Processed image 1101\n",
      "Processed image 1102\n",
      "Processed image 1103\n",
      "Processed image 1104\n",
      "Processed image 1105\n",
      "Processed image 1106\n",
      "Processed image 1107\n",
      "Processed image 1108\n",
      "Processed image 1109\n",
      "Processed image 1110\n",
      "Processed image 1111\n",
      "Processed image 1112\n",
      "Processed image 1113\n",
      "Processed image 1114\n",
      "Processed image 1115\n",
      "Processed image 1116\n",
      "Processed image 1117\n",
      "Processed image 1118\n",
      "Processed image 1119\n",
      "Processed image 1120\n",
      "Processed image 1121\n",
      "Processed image 1122\n",
      "Processed image 1123\n",
      "Processed image 1124\n",
      "Processed image 1125\n",
      "Processed image 1126\n",
      "Processed image 1127\n",
      "Processed image 1128\n",
      "Processed image 1129\n",
      "Processed image 1130\n",
      "Processed image 1131\n",
      "Processed image 1132\n",
      "Processed image 1133\n",
      "Processed image 1134\n",
      "Processed image 1135\n",
      "Processed image 1136\n",
      "Processed image 1137\n",
      "Processed image 1138\n",
      "Processed image 1139\n",
      "Processed image 1140\n",
      "Processed image 1141\n",
      "Processed image 1142\n",
      "Processed image 1143\n",
      "Processed image 1144\n",
      "Processed image 1145\n",
      "Processed image 1146\n",
      "Processed image 1147\n",
      "Processed image 1148\n",
      "Processed image 1149\n",
      "Processed image 1150\n",
      "Processed image 1151\n",
      "Processed image 1152\n",
      "Processed image 1153\n",
      "Processed image 1154\n",
      "Processed image 1155\n",
      "Processed image 1156\n",
      "Processed image 1157\n",
      "Processed image 1158\n",
      "Processed image 1159\n",
      "Processed image 1160\n",
      "Processed image 1161\n",
      "Processed image 1162\n",
      "Processed image 1163\n",
      "Processed image 1164\n",
      "Processed image 1165\n",
      "Processed image 1166\n",
      "Processed image 1167\n",
      "Processed image 1168\n",
      "Processed image 1169\n",
      "Processed image 1170\n",
      "Processed image 1171\n",
      "Processed image 1172\n",
      "Processed image 1173\n",
      "Processed image 1174\n",
      "Processed image 1175\n",
      "Processed image 1176\n",
      "Processed image 1177\n",
      "Processed image 1178\n",
      "Processed image 1179\n",
      "Processed image 1180\n",
      "Processed image 1181\n",
      "Processed image 1182\n",
      "Processed image 1183\n",
      "Processed image 1184\n",
      "Processed image 1185\n",
      "Processed image 1186\n",
      "Processed image 1187\n",
      "Processed image 1188\n",
      "Processed image 1189\n",
      "Processed image 1190\n",
      "Processed image 1191\n",
      "Processed image 1192\n",
      "Processed image 1193\n",
      "Processed image 1194\n",
      "Processed image 1195\n",
      "Processed image 1196\n",
      "Processed image 1197\n",
      "Processed image 1198\n",
      "Processed image 1199\n",
      "Processed image 1200\n",
      "Processed image 1201\n",
      "Processed image 1202\n",
      "Processed image 1203\n",
      "Processed image 1204\n",
      "Processed image 1205\n",
      "Processed image 1206\n",
      "Processed image 1207\n",
      "Processed image 1208\n",
      "Processed image 1209\n",
      "Processed image 1210\n",
      "Processed image 1211\n",
      "Processed image 1212\n",
      "Processed image 1213\n",
      "Processed image 1214\n",
      "Processed image 1215\n",
      "Processed image 1216\n",
      "Processed image 1217\n",
      "Processed image 1218\n",
      "Processed image 1219\n",
      "Processed image 1220\n",
      "Processed image 1221\n",
      "Processed image 1222\n",
      "Processed image 1223\n",
      "Processed image 1224\n",
      "Processed image 1225\n",
      "Processed image 1226\n",
      "Processed image 1227\n",
      "Processed image 1228\n",
      "Processed image 1229\n",
      "Processed image 1230\n",
      "Processed image 1231\n",
      "Processed image 1232\n",
      "Processed image 1233\n",
      "Processed image 1234\n",
      "Processed image 1235\n",
      "Processed image 1236\n",
      "Processed image 1237\n",
      "Processed image 1238\n",
      "Processed image 1239\n",
      "Processed image 1240\n",
      "Processed image 1241\n",
      "Processed image 1242\n",
      "Processed image 1243\n",
      "Processed image 1244\n",
      "Processed image 1245\n",
      "Processed image 1246\n",
      "Processed image 1247\n",
      "Processed image 1248\n",
      "Processed image 1249\n",
      "Processed image 1250\n",
      "Processed image 1251\n",
      "Processed image 1252\n",
      "Processed image 1253\n",
      "Processed image 1254\n",
      "Processed image 1255\n",
      "Processed image 1256\n",
      "Processed image 1257\n",
      "Processed image 1258\n",
      "Processed image 1259\n",
      "Processed image 1260\n",
      "Processed image 1261\n",
      "Processed image 1262\n",
      "Processed image 1263\n",
      "Processed image 1264\n",
      "Processed image 1265\n",
      "Processed image 1266\n",
      "Processed image 1267\n",
      "Processed image 1268\n",
      "Processed image 1269\n",
      "Processed image 1270\n",
      "Processed image 1271\n",
      "Processed image 1272\n",
      "Processed image 1273\n",
      "Processed image 1274\n",
      "Processed image 1275\n",
      "Processed image 1276\n",
      "Processed image 1277\n",
      "Processed image 1278\n",
      "Processed image 1279\n",
      "Processed image 1280\n",
      "Processed image 1281\n",
      "Processed image 1282\n",
      "Processed image 1283\n",
      "Processed image 1284\n",
      "Processed image 1285\n",
      "Processed image 1286\n",
      "Processed image 1287\n",
      "Processed image 1288\n",
      "Processed image 1289\n",
      "Processed image 1290\n",
      "Processed image 1291\n",
      "Processed image 1292\n",
      "Processed image 1293\n",
      "Processed image 1294\n",
      "Processed image 1295\n",
      "Processed image 1296\n",
      "Processed image 1297\n",
      "Processed image 1298\n",
      "Processed image 1299\n",
      "Processed image 1300\n",
      "Processed image 1301\n",
      "Processed image 1302\n",
      "Processed image 1303\n",
      "Processed image 1304\n",
      "Processed image 1305\n",
      "Processed image 1306\n",
      "Processed image 1307\n",
      "Processed image 1308\n",
      "Processed image 1309\n",
      "Processed image 1310\n",
      "Processed image 1311\n",
      "Processed image 1312\n",
      "Processed image 1313\n",
      "Processed image 1314\n",
      "Processed image 1315\n",
      "Processed image 1316\n",
      "Processed image 1317\n",
      "Processed image 1318\n",
      "Processed image 1319\n",
      "Processed image 1320\n",
      "Processed image 1321\n",
      "Processed image 1322\n",
      "Processed image 1323\n",
      "Processed image 1324\n",
      "Processed image 1325\n",
      "Processed image 1326\n",
      "Processed image 1327\n",
      "Processed image 1328\n",
      "Processed image 1329\n",
      "Processed image 1330\n",
      "Processed image 1331\n",
      "Processed image 1332\n",
      "Processed image 1333\n",
      "Processed image 1334\n",
      "Processed image 1335\n",
      "Processed image 1336\n",
      "Processed image 1337\n",
      "Processed image 1338\n",
      "Processed image 1339\n",
      "Processed image 1340\n",
      "Processed image 1341\n",
      "Processed image 1342\n",
      "Processed image 1343\n",
      "Processed image 1344\n",
      "Processed image 1345\n",
      "Processed image 1346\n",
      "Processed image 1347\n",
      "Processed image 1348\n",
      "Processed image 1349\n",
      "Processed image 1350\n",
      "Processed image 1351\n",
      "Processed image 1352\n",
      "Processed image 1353\n",
      "Processed image 1354\n",
      "Processed image 1355\n",
      "Processed image 1356\n",
      "Processed image 1357\n",
      "Processed image 1358\n",
      "Processed image 1359\n",
      "Processed image 1360\n",
      "Processed image 1361\n",
      "Processed image 1362\n",
      "Processed image 1363\n",
      "Processed image 1364\n",
      "Processed image 1365\n",
      "Processed image 1366\n",
      "Processed image 1367\n",
      "Processed image 1368\n",
      "Processed image 1369\n",
      "Processed image 1370\n",
      "Processed image 1371\n",
      "Processed image 1372\n",
      "Processed image 1373\n",
      "Processed image 1374\n",
      "Processed image 1375\n",
      "Processed image 1376\n",
      "Processed image 1377\n",
      "Processed image 1378\n",
      "Processed image 1379\n",
      "Processed image 1380\n",
      "Processed image 1381\n",
      "Processed image 1382\n",
      "Processed image 1383\n",
      "Processed image 1384\n",
      "Processed image 1385\n",
      "Processed image 1386\n",
      "Processed image 1387\n",
      "Processed image 1388\n",
      "Processed image 1389\n",
      "Processed image 1390\n",
      "Processed image 1391\n",
      "Processed image 1392\n",
      "Processed image 1393\n",
      "Processed image 1394\n",
      "Processed image 1395\n",
      "Processed image 1396\n",
      "Processed image 1397\n",
      "Processed image 1398\n",
      "Processed image 1399\n",
      "Processed image 1400\n",
      "Processed image 1401\n",
      "Processed image 1402\n",
      "Processed image 1403\n",
      "Processed image 1404\n",
      "Processed image 1405\n",
      "Processed image 1406\n",
      "Processed image 1407\n",
      "Processed image 1408\n",
      "Processed image 1409\n",
      "Processed image 1410\n",
      "Processed image 1411\n",
      "Processed image 1412\n",
      "Processed image 1413\n",
      "Processed image 1414\n",
      "Processed image 1415\n",
      "Processed image 1416\n",
      "Processed image 1417\n",
      "Processed image 1418\n",
      "Processed image 1419\n",
      "Processed image 1420\n",
      "Processed image 1421\n",
      "Processed image 1422\n",
      "Processed image 1423\n",
      "Processed image 1424\n",
      "Processed image 1425\n",
      "Processed image 1426\n",
      "Processed image 1427\n",
      "Processed image 1428\n",
      "Processed image 1429\n",
      "Processed image 1430\n",
      "Processed image 1431\n",
      "Processed image 1432\n",
      "Processed image 1433\n",
      "Processed image 1434\n",
      "Processed image 1435\n",
      "Processed image 1436\n",
      "Processed image 1437\n",
      "Processed image 1438\n",
      "Processed image 1439\n",
      "Processed image 1440\n",
      "Processed image 1441\n",
      "Processed image 1442\n",
      "Processed image 1443\n",
      "Processed image 1444\n",
      "Processed image 1445\n",
      "Processed image 1446\n",
      "Processed image 1447\n",
      "Processed image 1448\n",
      "Processed image 1449\n",
      "Processed image 1450\n",
      "Processed image 1451\n",
      "Processed image 1452\n",
      "Processed image 1453\n",
      "Processed image 1454\n",
      "Processed image 1455\n",
      "Processed image 1456\n",
      "Processed image 1457\n",
      "Processed image 1458\n",
      "Processed image 1459\n",
      "Processed image 1460\n",
      "Processed image 1461\n",
      "Processed image 1462\n",
      "Processed image 1463\n",
      "Processed image 1464\n",
      "Processed image 1465\n",
      "Processed image 1466\n",
      "Processed image 1467\n",
      "Processed image 1468\n",
      "Processed image 1469\n",
      "Processed image 1470\n",
      "Processed image 1471\n",
      "Processed image 1472\n",
      "Processed image 1473\n",
      "Processed image 1474\n",
      "Processed image 1475\n",
      "Processed image 1476\n",
      "Processed image 1477\n",
      "Processed image 1478\n",
      "Processed image 1479\n",
      "Processed image 1480\n",
      "Processed image 1481\n",
      "Processed image 1482\n",
      "Processed image 1483\n",
      "Processed image 1484\n",
      "Processed image 1485\n",
      "Processed image 1486\n",
      "Processed image 1487\n",
      "Processed image 1488\n",
      "Processed image 1489\n",
      "Processed image 1490\n",
      "Processed image 1491\n",
      "Processed image 1492\n",
      "Processed image 1493\n",
      "Processed image 1494\n",
      "Processed image 1495\n",
      "Processed image 1496\n",
      "Processed image 1497\n",
      "Processed image 1498\n",
      "Processed image 1499\n",
      "Processed image 1500\n",
      "Processed image 1501\n",
      "Processed image 1502\n",
      "Processed image 1503\n",
      "Processed image 1504\n",
      "Processed image 1505\n",
      "Processed image 1506\n",
      "Processed image 1507\n",
      "Processed image 1508\n",
      "Processed image 1509\n",
      "Processed image 1510\n",
      "Processed image 1511\n",
      "Processed image 1512\n",
      "Processed image 1513\n",
      "Processed image 1514\n",
      "Processed image 1515\n",
      "Processed image 1516\n",
      "Processed image 1517\n",
      "Processed image 1518\n",
      "Processed image 1519\n",
      "Processed image 1520\n",
      "Processed image 1521\n",
      "Processed image 1522\n",
      "Processed image 1523\n",
      "Processed image 1524\n",
      "Processed image 1525\n",
      "Processed image 1526\n",
      "Processed image 1527\n",
      "Processed image 1528\n",
      "Processed image 1529\n",
      "Processed image 1530\n",
      "Processed image 1531\n",
      "Processed image 1532\n",
      "Processed image 1533\n",
      "Processed image 1534\n",
      "Processed image 1535\n",
      "Processed image 1536\n",
      "Processed image 1537\n",
      "Processed image 1538\n",
      "Processed image 1539\n",
      "Processed image 1540\n",
      "Processed image 1541\n",
      "Processed image 1542\n",
      "Processed image 1543\n",
      "Processed image 1544\n",
      "Processed image 1545\n",
      "Processed image 1546\n",
      "Processed image 1547\n",
      "Processed image 1548\n",
      "Processed image 1549\n",
      "Processed image 1550\n",
      "Processed image 1551\n",
      "Processed image 1552\n",
      "Processed image 1553\n",
      "Processed image 1554\n",
      "Processed image 1555\n",
      "Processed image 1556\n",
      "Processed image 1557\n",
      "Processed image 1558\n",
      "Processed image 1559\n",
      "Processed image 1560\n",
      "Processed image 1561\n",
      "Processed image 1562\n",
      "Processed image 1563\n",
      "Processed image 1564\n",
      "Processed image 1565\n",
      "Processed image 1566\n",
      "Processed image 1567\n",
      "Processed image 1568\n",
      "Processed image 1569\n",
      "Processed image 1570\n",
      "Processed image 1571\n",
      "Processed image 1572\n",
      "Processed image 1573\n",
      "Processed image 1574\n",
      "Processed image 1575\n",
      "Processed image 1576\n",
      "Processed image 1577\n",
      "Processed image 1578\n",
      "Processed image 1579\n",
      "Processed image 1580\n",
      "Processed image 1581\n",
      "Processed image 1582\n",
      "Processed image 1583\n",
      "Processed image 1584\n",
      "Processed image 1585\n",
      "Processed image 1586\n",
      "Processed image 1587\n",
      "Processed image 1588\n",
      "Processed image 1589\n",
      "Processed image 1590\n",
      "Processed image 1591\n",
      "Processed image 1592\n",
      "Processed image 1593\n",
      "Processed image 1594\n",
      "Processed image 1595\n",
      "Processed image 1596\n",
      "Processed image 1597\n",
      "Processed image 1598\n",
      "Processed image 1599\n",
      "Processed image 1600\n",
      "Processed image 1601\n",
      "Processed image 1602\n",
      "Processed image 1603\n",
      "Processed image 1604\n",
      "Processed image 1605\n",
      "Processed image 1606\n",
      "Processed image 1607\n",
      "Processed image 1608\n",
      "Processed image 1609\n",
      "Processed image 1610\n",
      "Processed image 1611\n",
      "Processed image 1612\n",
      "Processed image 1613\n",
      "Processed image 1614\n",
      "Processed image 1615\n",
      "Processed image 1616\n",
      "Processed image 1617\n",
      "Processed image 1618\n",
      "Processed image 1619\n",
      "Processed image 1620\n",
      "Processed image 1621\n",
      "Processed image 1622\n",
      "Processed image 1623\n",
      "Processed image 1624\n",
      "Processed image 1625\n",
      "Processed image 1626\n",
      "Processed image 1627\n",
      "Processed image 1628\n",
      "Processed image 1629\n",
      "Processed image 1630\n",
      "Processed image 1631\n",
      "Processed image 1632\n",
      "Processed image 1633\n",
      "Processed image 1634\n",
      "Processed image 1635\n",
      "Processed image 1636\n",
      "Processed image 1637\n",
      "Processed image 1638\n",
      "Processed image 1639\n",
      "Processed image 1640\n",
      "Processed image 1641\n",
      "Processed image 1642\n",
      "Processed image 1643\n",
      "Processed image 1644\n",
      "Processed image 1645\n",
      "Processed image 1646\n",
      "Processed image 1647\n",
      "Processed image 1648\n",
      "Processed image 1649\n",
      "Processed image 1650\n",
      "Processed image 1651\n",
      "Processed image 1652\n",
      "Processed image 1653\n",
      "Processed image 1654\n",
      "Processed image 1655\n",
      "Processed image 1656\n",
      "Processed image 1657\n",
      "Processed image 1658\n",
      "Processed image 1659\n",
      "Processed image 1660\n",
      "Processed image 1661\n",
      "Processed image 1662\n",
      "Processed image 1663\n",
      "Processed image 1664\n",
      "Processed image 1665\n",
      "Processed image 1666\n",
      "Processed image 1667\n",
      "Processed image 1668\n",
      "Processed image 1669\n",
      "Processed image 1670\n",
      "Processed image 1671\n",
      "Processed image 1672\n",
      "Processed image 1673\n",
      "Processed image 1674\n",
      "Processed image 1675\n",
      "Processed image 1676\n",
      "Processed image 1677\n",
      "Processed image 1678\n",
      "Processed image 1679\n",
      "Processed image 1680\n",
      "Processed image 1681\n",
      "Processed image 1682\n",
      "Processed image 1683\n",
      "Processed image 1684\n",
      "Processed image 1685\n",
      "Processed image 1686\n",
      "Processed image 1687\n",
      "Processed image 1688\n",
      "Processed image 1689\n",
      "Processed image 1690\n",
      "Processed image 1691\n",
      "Processed image 1692\n",
      "Processed image 1693\n",
      "Processed image 1694\n",
      "Processed image 1695\n",
      "Processed image 1696\n",
      "Processed image 1697\n",
      "Processed image 1698\n",
      "Processed image 1699\n",
      "Processed image 1700\n",
      "Processed image 1701\n",
      "Processed image 1702\n",
      "Processed image 1703\n",
      "Processed image 1704\n",
      "Processed image 1705\n",
      "Processed image 1706\n",
      "Processed image 1707\n",
      "Processed image 1708\n",
      "Processed image 1709\n",
      "Processed image 1710\n",
      "Processed image 1711\n",
      "Processed image 1712\n",
      "Processed image 1713\n",
      "Processed image 1714\n",
      "Processed image 1715\n",
      "Processed image 1716\n",
      "Processed image 1717\n",
      "Processed image 1718\n",
      "Processed image 1719\n",
      "Processed image 1720\n",
      "Processed image 1721\n",
      "Processed image 1722\n",
      "Processed image 1723\n",
      "Processed image 1724\n",
      "Processed image 1725\n",
      "Processed image 1726\n",
      "Processed image 1727\n",
      "Processed image 1728\n",
      "Processed image 1729\n",
      "Processed image 1730\n",
      "Processed image 1731\n",
      "Processed image 1732\n",
      "Processed image 1733\n",
      "Processed image 1734\n",
      "Processed image 1735\n",
      "Processed image 1736\n",
      "Processed image 1737\n",
      "Processed image 1738\n",
      "Processed image 1739\n",
      "Processed image 1740\n",
      "Processed image 1741\n",
      "Processed image 1742\n",
      "Processed image 1743\n",
      "Processed image 1744\n",
      "Processed image 1745\n",
      "Processed image 1746\n",
      "Processed image 1747\n",
      "Processed image 1748\n",
      "Processed image 1749\n",
      "Processed image 1750\n",
      "Processed image 1751\n",
      "Processed image 1752\n",
      "Processed image 1753\n",
      "Processed image 1754\n",
      "Processed image 1755\n",
      "Processed image 1756\n",
      "Processed image 1757\n",
      "Processed image 1758\n",
      "Processed image 1759\n",
      "Processed image 1760\n",
      "Processed image 1761\n",
      "Processed image 1762\n",
      "Processed image 1763\n",
      "Processed image 1764\n",
      "Processed image 1765\n",
      "Processed image 1766\n",
      "Processed image 1767\n",
      "Processed image 1768\n",
      "Processed image 1769\n",
      "Processed image 1770\n",
      "Processed image 1771\n",
      "Processed image 1772\n",
      "Processed image 1773\n",
      "Processed image 1774\n",
      "Processed image 1775\n",
      "Processed image 1776\n",
      "Processed image 1777\n",
      "Processed image 1778\n",
      "Processed image 1779\n",
      "Processed image 1780\n",
      "Processed image 1781\n",
      "Processed image 1782\n",
      "Processed image 1783\n",
      "Processed image 1784\n",
      "Processed image 1785\n",
      "Processed image 1786\n",
      "Processed image 1787\n",
      "Processed image 1788\n",
      "Processed image 1789\n",
      "Processed image 1790\n",
      "Processed image 1791\n",
      "Processed image 1792\n",
      "Processed image 1793\n",
      "Processed image 1794\n",
      "Processed image 1795\n",
      "Processed image 1796\n",
      "Processed image 1797\n",
      "Processed image 1798\n",
      "Processed image 1799\n",
      "Processed image 1800\n",
      "Processed image 1801\n",
      "Processed image 1802\n",
      "Processed image 1803\n",
      "Processed image 1804\n",
      "Processed image 1805\n",
      "Processed image 1806\n",
      "Processed image 1807\n",
      "Processed image 1808\n",
      "Processed image 1809\n",
      "Processed image 1810\n",
      "Processed image 1811\n",
      "Processed image 1812\n",
      "Processed image 1813\n",
      "Processed image 1814\n",
      "Processed image 1815\n",
      "Processed image 1816\n",
      "Processed image 1817\n",
      "Processed image 1818\n",
      "Processed image 1819\n",
      "Processed image 1820\n",
      "Processed image 1821\n",
      "Processed image 1822\n",
      "Processed image 1823\n",
      "Processed image 1824\n",
      "Processed image 1825\n",
      "Processed image 1826\n",
      "Processed image 1827\n",
      "Processed image 1828\n",
      "Processed image 1829\n",
      "Processed image 1830\n",
      "Processed image 1831\n",
      "Processed image 1832\n",
      "Processed image 1833\n",
      "Processed image 1834\n",
      "Processed image 1835\n",
      "Processed image 1836\n",
      "Processed image 1837\n",
      "Processed image 1838\n",
      "Processed image 1839\n",
      "Processed image 1840\n",
      "Processed image 1841\n",
      "Processed image 1842\n",
      "Processed image 1843\n",
      "Processed image 1844\n",
      "Processed image 1845\n",
      "Processed image 1846\n",
      "Processed image 1847\n",
      "Processed image 1848\n",
      "Processed image 1849\n",
      "Processed image 1850\n",
      "Processed image 1851\n",
      "Processed image 1852\n",
      "Processed image 1853\n",
      "Processed image 1854\n",
      "Processed image 1855\n",
      "Processed image 1856\n",
      "Processed image 1857\n",
      "Processed image 1858\n",
      "Processed image 1859\n",
      "Processed image 1860\n",
      "Processed image 1861\n",
      "Processed image 1862\n",
      "Processed image 1863\n",
      "Processed image 1864\n",
      "Processed image 1865\n",
      "Processed image 1866\n",
      "Processed image 1867\n",
      "Processed image 1868\n",
      "Processed image 1869\n",
      "Processed image 1870\n",
      "Processed image 1871\n",
      "Processed image 1872\n",
      "Processed image 1873\n",
      "Processed image 1874\n",
      "Processed image 1875\n",
      "Processed image 1876\n",
      "Processed image 1877\n",
      "Processed image 1878\n",
      "Processed image 1879\n",
      "Processed image 1880\n",
      "Processed image 1881\n",
      "Processed image 1882\n",
      "Processed image 1883\n",
      "Processed image 1884\n",
      "Processed image 1885\n",
      "Processed image 1886\n",
      "Processed image 1887\n",
      "Processed image 1888\n",
      "Processed image 1889\n",
      "Processed image 1890\n",
      "Processed image 1891\n",
      "Processed image 1892\n",
      "Processed image 1893\n",
      "Processed image 1894\n",
      "Processed image 1895\n",
      "Processed image 1896\n",
      "Processed image 1897\n",
      "Processed image 1898\n",
      "Processed image 1899\n",
      "Processed image 1900\n",
      "Processed image 1901\n",
      "Processed image 1902\n",
      "Processed image 1903\n",
      "Processed image 1904\n",
      "Processed image 1905\n",
      "Processed image 1906\n",
      "Processed image 1907\n",
      "Processed image 1908\n",
      "Processed image 1909\n",
      "Processed image 1910\n",
      "Processed image 1911\n",
      "Processed image 1912\n",
      "Processed image 1913\n",
      "Processed image 1914\n",
      "Processed image 1915\n",
      "Processed image 1916\n",
      "Processed image 1917\n",
      "Processed image 1918\n",
      "Processed image 1919\n",
      "Processed image 1920\n",
      "Processed image 1921\n",
      "Processed image 1922\n",
      "Processed image 1923\n",
      "Processed image 1924\n",
      "Processed image 1925\n",
      "Processed image 1926\n",
      "Processed image 1927\n",
      "Processed image 1928\n",
      "Processed image 1929\n",
      "Processed image 1930\n",
      "Processed image 1931\n",
      "Processed image 1932\n",
      "Processed image 1933\n",
      "Processed image 1934\n",
      "Processed image 1935\n",
      "Processed image 1936\n",
      "Processed image 1937\n",
      "Processed image 1938\n",
      "Processed image 1939\n",
      "Processed image 1940\n",
      "Processed image 1941\n",
      "Processed image 1942\n",
      "Processed image 1943\n",
      "Processed image 1944\n",
      "Processed image 1945\n",
      "Processed image 1946\n",
      "Processed image 1947\n",
      "Processed image 1948\n",
      "Processed image 1949\n",
      "Processed image 1950\n",
      "Processed image 1951\n",
      "Processed image 1952\n",
      "Processed image 1953\n",
      "Processed image 1954\n",
      "Processed image 1955\n",
      "Processed image 1956\n",
      "Processed image 1957\n",
      "Processed image 1958\n",
      "Processed image 1959\n",
      "Processed image 1960\n",
      "Processed image 1961\n",
      "Processed image 1962\n",
      "Processed image 1963\n",
      "Processed image 1964\n",
      "Processed image 1965\n",
      "Processed image 1966\n",
      "Processed image 1967\n",
      "Processed image 1968\n",
      "Processed image 1969\n",
      "Processed image 1970\n",
      "Processed image 1971\n",
      "Processed image 1972\n",
      "Processed image 1973\n",
      "Processed image 1974\n",
      "Processed image 1975\n",
      "Processed image 1976\n",
      "Processed image 1977\n",
      "Processed image 1978\n",
      "Processed image 1979\n",
      "Processed image 1980\n",
      "Processed image 1981\n",
      "Processed image 1982\n",
      "Processed image 1983\n",
      "Processed image 1984\n",
      "Processed image 1985\n",
      "Processed image 1986\n",
      "Processed image 1987\n",
      "Processed image 1988\n",
      "Processed image 1989\n",
      "Processed image 1990\n",
      "Processed image 1991\n",
      "Processed image 1992\n",
      "Processed image 1993\n",
      "Processed image 1994\n",
      "Processed image 1995\n",
      "Processed image 1996\n",
      "Processed image 1997\n",
      "Processed image 1998\n",
      "Processed image 1999\n",
      "Processed image 2000\n",
      "Processed image 2001\n",
      "Processed image 2002\n",
      "Processed image 2003\n",
      "Processed image 2004\n",
      "Processed image 2005\n",
      "Processed image 2006\n",
      "Processed image 2007\n",
      "Processed image 2008\n",
      "Processed image 2009\n",
      "Processed image 2010\n",
      "Processed image 2011\n",
      "Processed image 2012\n",
      "Processed image 2013\n",
      "Processed image 2014\n",
      "Processed image 2015\n",
      "Processed image 2016\n",
      "Processed image 2017\n",
      "Processed image 2018\n",
      "Processed image 2019\n",
      "Processed image 2020\n",
      "Processed image 2021\n",
      "Processed image 2022\n",
      "Processed image 2023\n",
      "Processed image 2024\n",
      "Processed image 2025\n",
      "Processed image 2026\n",
      "Processed image 2027\n",
      "Processed image 2028\n",
      "Processed image 2029\n",
      "Processed image 2030\n",
      "Processed image 2031\n",
      "Processed image 2032\n",
      "Processed image 2033\n",
      "Processed image 2034\n",
      "Processed image 2035\n",
      "Processed image 2036\n",
      "Processed image 2037\n",
      "Processed image 2038\n",
      "Processed image 2039\n",
      "Processed image 2040\n",
      "Processed image 2041\n",
      "Processed image 2042\n",
      "Processed image 2043\n",
      "Processed image 2044\n",
      "Processed image 2045\n",
      "Processed image 2046\n",
      "Processed image 2047\n",
      "Processed image 2048\n",
      "Processed image 2049\n",
      "Processed image 2050\n",
      "Processed image 2051\n",
      "Processed image 2052\n",
      "Processed image 2053\n",
      "Processed image 2054\n",
      "Processed image 2055\n",
      "Processed image 2056\n",
      "Processed image 2057\n",
      "Processed image 2058\n",
      "Processed image 2059\n",
      "Processed image 2060\n",
      "Processed image 2061\n",
      "Processed image 2062\n",
      "Processed image 2063\n",
      "Processed image 2064\n",
      "Processed image 2065\n",
      "Processed image 2066\n",
      "Processed image 2067\n",
      "Processed image 2068\n",
      "Processed image 2069\n",
      "Processed image 2070\n",
      "Processed image 2071\n",
      "Processed image 2072\n",
      "Processed image 2073\n",
      "Processed image 2074\n",
      "Processed image 2075\n",
      "Processed image 2076\n",
      "Processed image 2077\n",
      "Processed image 2078\n",
      "Processed image 2079\n",
      "Processed image 2080\n",
      "Processed image 2081\n",
      "Processed image 2082\n",
      "Processed image 2083\n",
      "Processed image 2084\n",
      "Processed image 2085\n",
      "Processed image 2086\n",
      "Processed image 2087\n",
      "Processed image 2088\n",
      "Processed image 2089\n",
      "Processed image 2090\n",
      "Processed image 2091\n",
      "Processed image 2092\n",
      "Processed image 2093\n",
      "Processed image 2094\n",
      "Processed image 2095\n",
      "Processed image 2096\n",
      "Processed image 2097\n",
      "Processed image 2098\n",
      "Processed image 2099\n",
      "Processed image 2100\n",
      "Processed image 2101\n",
      "Processed image 2102\n",
      "Processed image 2103\n",
      "Processed image 2104\n",
      "Processed image 2105\n",
      "Processed image 2106\n",
      "Processed image 2107\n",
      "Processed image 2108\n",
      "Processed image 2109\n",
      "Processed image 2110\n",
      "Processed image 2111\n",
      "Processed image 2112\n",
      "Processed image 2113\n",
      "Processed image 2114\n",
      "Processed image 2115\n",
      "Processed image 2116\n",
      "Processed image 2117\n",
      "Processed image 2118\n",
      "Processed image 2119\n",
      "Processed image 2120\n",
      "Processed image 2121\n",
      "Processed image 2122\n",
      "Processed image 2123\n",
      "Processed image 2124\n",
      "Processed image 2125\n",
      "Processed image 2126\n",
      "Processed image 2127\n",
      "Processed image 2128\n",
      "Processed image 2129\n",
      "Processed image 2130\n",
      "Processed image 2131\n",
      "Processed image 2132\n",
      "Processed image 2133\n",
      "Processed image 2134\n",
      "Processed image 2135\n",
      "Processed image 2136\n",
      "Processed image 2137\n",
      "Processed image 2138\n",
      "Processed image 2139\n",
      "Processed image 2140\n",
      "Processed image 2141\n",
      "Processed image 2142\n",
      "Processed image 2143\n",
      "Processed image 2144\n",
      "Processed image 2145\n",
      "Processed image 2146\n",
      "Processed image 2147\n",
      "Processed image 2148\n",
      "Processed image 2149\n",
      "Processed image 2150\n",
      "Processed image 2151\n",
      "Processed image 2152\n",
      "Processed image 2153\n",
      "Processed image 2154\n",
      "Processed image 2155\n",
      "Processed image 2156\n",
      "Processed image 2157\n",
      "Processed image 2158\n",
      "Processed image 2159\n",
      "Processed image 2160\n",
      "Processed image 2161\n",
      "Processed image 2162\n",
      "Processed image 2163\n",
      "Processed image 2164\n",
      "Processed image 2165\n",
      "Processed image 2166\n",
      "Processed image 2167\n",
      "Processed image 2168\n",
      "Processed image 2169\n",
      "Processed image 2170\n",
      "Processed image 2171\n",
      "Processed image 2172\n",
      "Processed image 2173\n",
      "Processed image 2174\n",
      "Processed image 2175\n",
      "Processed image 2176\n",
      "Processed image 2177\n",
      "Processed image 2178\n",
      "Processed image 2179\n",
      "Processed image 2180\n",
      "Processed image 2181\n",
      "Processed image 2182\n",
      "Processed image 2183\n",
      "Processed image 2184\n",
      "Processed image 2185\n",
      "Processed image 2186\n",
      "Processed image 2187\n",
      "Processed image 2188\n",
      "Processed image 2189\n",
      "Processed image 2190\n",
      "Processed image 2191\n",
      "Processed image 2192\n",
      "Processed image 2193\n",
      "Processed image 2194\n",
      "Processed image 2195\n",
      "Processed image 2196\n",
      "Processed image 2197\n",
      "Processed image 2198\n",
      "Processed image 2199\n",
      "Processed image 2200\n",
      "Processed image 2201\n",
      "Processed image 2202\n",
      "Processed image 2203\n",
      "Processed image 2204\n",
      "Processed image 2205\n",
      "Processed image 2206\n",
      "Processed image 2207\n",
      "Processed image 2208\n",
      "Processed image 2209\n",
      "Processed image 2210\n",
      "Processed image 2211\n",
      "Processed image 2212\n",
      "Processed image 2213\n",
      "Processed image 2214\n",
      "Processed image 2215\n",
      "Processed image 2216\n",
      "Processed image 2217\n",
      "Processed image 2218\n",
      "Processed image 2219\n",
      "Processed image 2220\n",
      "Processed image 2221\n",
      "Processed image 2222\n",
      "Processed image 2223\n",
      "Processed image 2224\n",
      "Processed image 2225\n",
      "Processed image 2226\n",
      "Processed image 2227\n",
      "Processed image 2228\n",
      "Processed image 2229\n",
      "Processed image 2230\n",
      "Processed image 2231\n",
      "Processed image 2232\n",
      "Processed image 2233\n",
      "Processed image 2234\n",
      "Processed image 2235\n",
      "Processed image 2236\n",
      "Processed image 2237\n",
      "Processed image 2238\n",
      "Processed image 2239\n",
      "Processed image 2240\n",
      "Processed image 2241\n",
      "Processed image 2242\n",
      "Processed image 2243\n",
      "Processed image 2244\n",
      "Processed image 2245\n",
      "Processed image 2246\n",
      "Processed image 2247\n",
      "Processed image 2248\n",
      "Processed image 2249\n",
      "Processed image 2250\n",
      "Processed image 2251\n",
      "Processed image 2252\n",
      "Processed image 2253\n",
      "Processed image 2254\n",
      "Processed image 2255\n",
      "Processed image 2256\n",
      "Processed image 2257\n",
      "Processed image 2258\n",
      "Processed image 2259\n",
      "Processed image 2260\n",
      "Processed image 2261\n",
      "Processed image 2262\n",
      "Processed image 2263\n",
      "Processed image 2264\n",
      "Processed image 2265\n",
      "Processed image 2266\n",
      "Processed image 2267\n",
      "Processed image 2268\n",
      "Processed image 2269\n",
      "Processed image 2270\n",
      "Processed image 2271\n",
      "Processed image 2272\n",
      "Processed image 2273\n",
      "Processed image 2274\n",
      "Processed image 2275\n",
      "Processed image 2276\n",
      "Processed image 2277\n",
      "Processed image 2278\n",
      "Processed image 2279\n",
      "Processed image 2280\n",
      "Processed image 2281\n",
      "Processed image 2282\n",
      "Processed image 2283\n",
      "Processed image 2284\n",
      "Processed image 2285\n",
      "Processed image 2286\n",
      "Processed image 2287\n",
      "Processed image 2288\n",
      "Processed image 2289\n",
      "Processed image 2290\n",
      "Processed image 2291\n",
      "Processed image 2292\n",
      "Processed image 2293\n",
      "Processed image 2294\n",
      "Processed image 2295\n",
      "Processed image 2296\n",
      "Processed image 2297\n",
      "Processed image 2298\n",
      "Processed image 2299\n",
      "Processed image 2300\n",
      "Processed image 2301\n",
      "Processed image 2302\n",
      "Processed image 2303\n",
      "Processed image 2304\n",
      "Processed image 2305\n",
      "Processed image 2306\n",
      "Processed image 2307\n",
      "Processed image 2308\n",
      "Processed image 2309\n",
      "Processed image 2310\n",
      "Processed image 2311\n",
      "Processed image 2312\n",
      "Processed image 2313\n",
      "Processed image 2314\n",
      "Processed image 2315\n",
      "Processed image 2316\n",
      "Processed image 2317\n",
      "Processed image 2318\n",
      "Processed image 2319\n",
      "Processed image 2320\n",
      "Processed image 2321\n",
      "Processed image 2322\n",
      "Processed image 2323\n",
      "Processed image 2324\n",
      "Processed image 2325\n",
      "Processed image 2326\n",
      "Processed image 2327\n",
      "Processed image 2328\n",
      "Processed image 2329\n",
      "Processed image 2330\n",
      "Processed image 2331\n",
      "Processed image 2332\n",
      "Processed image 2333\n",
      "Processed image 2334\n",
      "Processed image 2335\n",
      "Processed image 2336\n",
      "Processed image 2337\n",
      "Processed image 2338\n",
      "Processed image 2339\n",
      "Processed image 2340\n",
      "Processed image 2341\n",
      "Processed image 2342\n",
      "Processed image 2343\n",
      "Processed image 2344\n",
      "Processed image 2345\n",
      "Processed image 2346\n",
      "Processed image 2347\n",
      "Processed image 2348\n",
      "Processed image 2349\n",
      "Processed image 2350\n",
      "Processed image 2351\n",
      "Processed image 2352\n",
      "Processed image 2353\n",
      "Processed image 2354\n",
      "Processed image 2355\n",
      "Processed image 2356\n",
      "Processed image 2357\n",
      "Processed image 2358\n",
      "Processed image 2359\n",
      "Processed image 2360\n",
      "Processed image 2361\n",
      "Processed image 2362\n",
      "Processed image 2363\n",
      "Processed image 2364\n",
      "Processed image 2365\n",
      "Processed image 2366\n",
      "Processed image 2367\n",
      "Processed image 2368\n",
      "Processed image 2369\n",
      "Processed image 2370\n",
      "Processed image 2371\n",
      "Processed image 2372\n",
      "Processed image 2373\n",
      "Processed image 2374\n",
      "Processed image 2375\n",
      "Processed image 2376\n",
      "Processed image 2377\n",
      "Processed image 2378\n",
      "Processed image 2379\n",
      "Processed image 2380\n",
      "Processed image 2381\n",
      "Processed image 2382\n",
      "Processed image 2383\n",
      "Processed image 2384\n",
      "Processed image 2385\n",
      "Processed image 2386\n",
      "Processed image 2387\n",
      "Processed image 2388\n",
      "Processed image 2389\n",
      "Processed image 2390\n",
      "Processed image 2391\n",
      "Processed image 2392\n",
      "Processed image 2393\n",
      "Processed image 2394\n",
      "Processed image 2395\n",
      "Processed image 2396\n",
      "Processed image 2397\n",
      "Processed image 2398\n",
      "Processed image 2399\n",
      "Processed image 2400\n",
      "Processed image 2401\n",
      "Processed image 2402\n",
      "Processed image 2403\n",
      "Processed image 2404\n",
      "Processed image 2405\n",
      "Processed image 2406\n",
      "Processed image 2407\n",
      "Processed image 2408\n",
      "Processed image 2409\n",
      "Processed image 2410\n",
      "Processed image 2411\n",
      "Processed image 2412\n",
      "Processed image 2413\n",
      "Processed image 2414\n",
      "Processed image 2415\n",
      "Processed image 2416\n",
      "Processed image 2417\n",
      "Processed image 2418\n",
      "Processed image 2419\n",
      "Processed image 2420\n",
      "Processed image 2421\n",
      "Processed image 2422\n",
      "Processed image 2423\n",
      "Processed image 2424\n",
      "Processed image 2425\n",
      "Processed image 2426\n",
      "Processed image 2427\n",
      "Processed image 2428\n",
      "Processed image 2429\n",
      "Processed image 2430\n",
      "Processed image 2431\n",
      "Processed image 2432\n",
      "Processed image 2433\n",
      "Processed image 2434\n",
      "Processed image 2435\n",
      "Processed image 2436\n",
      "Processed image 2437\n",
      "Processed image 2438\n",
      "Processed image 2439\n",
      "Processed image 2440\n",
      "Processed image 2441\n",
      "Processed image 2442\n",
      "Processed image 2443\n",
      "Processed image 2444\n",
      "Processed image 2445\n",
      "Processed image 2446\n",
      "Processed image 2447\n",
      "Processed image 2448\n",
      "Processed image 2449\n",
      "Processed image 2450\n",
      "Processed image 2451\n",
      "Processed image 2452\n",
      "Processed image 2453\n",
      "Processed image 2454\n",
      "Processed image 2455\n",
      "Processed image 2456\n",
      "Processed image 2457\n",
      "Processed image 2458\n",
      "Processed image 2459\n",
      "Processed image 2460\n",
      "Processed image 2461\n",
      "Processed image 2462\n",
      "Processed image 2463\n",
      "Processed image 2464\n",
      "Processed image 2465\n",
      "Processed image 2466\n",
      "Processed image 2467\n",
      "Processed image 2468\n",
      "Processed image 2469\n",
      "Processed image 2470\n",
      "Processed image 2471\n",
      "Processed image 2472\n",
      "Processed image 2473\n",
      "Processed image 2474\n",
      "Processed image 2475\n",
      "Processed image 2476\n",
      "Processed image 2477\n",
      "Processed image 2478\n",
      "Processed image 2479\n",
      "Processed image 2480\n",
      "Processed image 2481\n",
      "Processed image 2482\n",
      "Processed image 2483\n",
      "Processed image 2484\n",
      "Processed image 2485\n",
      "Processed image 2486\n",
      "Processed image 2487\n",
      "Processed image 2488\n",
      "Processed image 2489\n",
      "Processed image 2490\n",
      "Processed image 2491\n",
      "Processed image 2492\n",
      "Processed image 2493\n",
      "Processed image 2494\n",
      "Processed image 2495\n",
      "Processed image 2496\n",
      "Processed image 2497\n",
      "Processed image 2498\n",
      "Processed image 2499\n",
      "Processed image 2500\n",
      "Processed image 2501\n",
      "Processed image 2502\n",
      "Processed image 2503\n",
      "Processed image 2504\n",
      "Processed image 2505\n",
      "Processed image 2506\n",
      "Processed image 2507\n",
      "Processed image 2508\n",
      "Processed image 2509\n",
      "Processed image 2510\n",
      "Processed image 2511\n",
      "Processed image 2512\n",
      "Processed image 2513\n",
      "Processed image 2514\n",
      "Processed image 2515\n",
      "Processed image 2516\n",
      "Processed image 2517\n",
      "Processed image 2518\n",
      "Processed image 2519\n",
      "Processed image 2520\n",
      "Processed image 2521\n",
      "Processed image 2522\n",
      "Processed image 2523\n",
      "Processed image 2524\n",
      "Processed image 2525\n",
      "Processed image 2526\n",
      "Processed image 2527\n",
      "Processed image 2528\n",
      "Processed image 2529\n",
      "Processed image 2530\n",
      "Processed image 2531\n",
      "Processed image 2532\n",
      "Processed image 2533\n",
      "Processed image 2534\n",
      "Processed image 2535\n",
      "Processed image 2536\n",
      "Processed image 2537\n",
      "Processed image 2538\n",
      "Processed image 2539\n",
      "Processed image 2540\n",
      "Processed image 2541\n",
      "Processed image 2542\n",
      "Processed image 2543\n",
      "Processed image 2544\n",
      "Processed image 2545\n",
      "Processed image 2546\n",
      "Processed image 2547\n",
      "Processed image 2548\n",
      "Processed image 2549\n",
      "Processed image 2550\n",
      "Processed image 2551\n",
      "Processed image 2552\n",
      "Processed image 2553\n",
      "Processed image 2554\n",
      "Processed image 2555\n",
      "Processed image 2556\n",
      "Processed image 2557\n",
      "Processed image 2558\n",
      "Processed image 2559\n",
      "Processed image 2560\n",
      "Processed image 2561\n",
      "Processed image 2562\n",
      "Processed image 2563\n",
      "Processed image 2564\n",
      "Processed image 2565\n",
      "Processed image 2566\n",
      "Processed image 2567\n",
      "Processed image 2568\n",
      "Processed image 2569\n",
      "Processed image 2570\n",
      "Processed image 2571\n",
      "Processed image 2572\n",
      "Processed image 2573\n",
      "Processed image 2574\n",
      "Processed image 2575\n",
      "Processed image 2576\n",
      "Processed image 2577\n",
      "Processed image 2578\n",
      "Processed image 2579\n",
      "Processed image 2580\n",
      "Processed image 2581\n",
      "Processed image 2582\n",
      "Processed image 2583\n",
      "Processed image 2584\n",
      "Processed image 2585\n",
      "Processed image 2586\n",
      "Processed image 2587\n",
      "Processed image 2588\n",
      "Processed image 2589\n",
      "Processed image 2590\n",
      "Processed image 2591\n",
      "Processed image 2592\n",
      "Processed image 2593\n",
      "Processed image 2594\n",
      "Processed image 2595\n",
      "Processed image 2596\n",
      "Processed image 2597\n",
      "Processed image 2598\n",
      "Processed image 2599\n",
      "Processed image 2600\n",
      "Processed image 2601\n",
      "Processed image 2602\n",
      "Processed image 2603\n",
      "Processed image 2604\n",
      "Processed image 2605\n",
      "Processed image 2606\n",
      "Processed image 2607\n",
      "Processed image 2608\n",
      "Processed image 2609\n",
      "Processed image 2610\n",
      "Processed image 2611\n",
      "Processed image 2612\n",
      "Processed image 2613\n",
      "Processed image 2614\n",
      "Processed image 2615\n",
      "Processed image 2616\n",
      "Processed image 2617\n",
      "Processed image 2618\n",
      "Processed image 2619\n",
      "Processed image 2620\n",
      "Processed image 2621\n",
      "Processed image 2622\n",
      "Processed image 2623\n",
      "Processed image 2624\n",
      "Processed image 2625\n",
      "Processed image 2626\n",
      "Processed image 2627\n",
      "Processed image 2628\n",
      "Processed image 2629\n",
      "Processed image 2630\n",
      "Processed image 2631\n",
      "Processed image 2632\n",
      "Processed image 2633\n",
      "Processed image 2634\n",
      "Processed image 2635\n",
      "Processed image 2636\n",
      "Processed image 2637\n",
      "Processed image 2638\n",
      "Processed image 2639\n",
      "Processed image 2640\n",
      "Processed image 2641\n",
      "Processed image 2642\n",
      "Processed image 2643\n",
      "Processed image 2644\n",
      "Processed image 2645\n",
      "Processed image 2646\n",
      "Processed image 2647\n",
      "Processed image 2648\n",
      "Processed image 2649\n",
      "Processed image 2650\n",
      "Processed image 2651\n",
      "Processed image 2652\n",
      "Processed image 2653\n",
      "Processed image 2654\n",
      "Processed image 2655\n",
      "Processed image 2656\n",
      "Processed image 2657\n",
      "Processed image 2658\n",
      "Processed image 2659\n",
      "Processed image 2660\n",
      "Processed image 2661\n",
      "Processed image 2662\n",
      "Processed image 2663\n",
      "Processed image 2664\n",
      "Processed image 2665\n",
      "Processed image 2666\n",
      "Processed image 2667\n",
      "Processed image 2668\n",
      "Processed image 2669\n",
      "Processed image 2670\n",
      "Processed image 2671\n",
      "Processed image 2672\n",
      "Processed image 2673\n",
      "Processed image 2674\n",
      "Processed image 2675\n",
      "Processed image 2676\n",
      "Processed image 2677\n",
      "Processed image 2678\n",
      "Processed image 2679\n",
      "Processed image 2680\n",
      "Processed image 2681\n",
      "Processed image 2682\n",
      "Processed image 2683\n",
      "Processed image 2684\n",
      "Processed image 2685\n",
      "Processed image 2686\n",
      "Processed image 2687\n",
      "Processed image 2688\n",
      "Processed image 2689\n",
      "Processed image 2690\n",
      "Processed image 2691\n",
      "Processed image 2692\n",
      "Processed image 2693\n",
      "Processed image 2694\n",
      "Processed image 2695\n",
      "Processed image 2696\n",
      "Processed image 2697\n",
      "Processed image 2698\n",
      "Processed image 2699\n",
      "Processed image 2700\n",
      "Processed image 2701\n",
      "Processed image 2702\n",
      "Processed image 2703\n",
      "Processed image 2704\n",
      "Processed image 2705\n",
      "Processed image 2706\n",
      "Processed image 2707\n",
      "Processed image 2708\n",
      "Processed image 2709\n",
      "Processed image 2710\n",
      "Processed image 2711\n",
      "Processed image 2712\n",
      "Processed image 2713\n",
      "Processed image 2714\n",
      "Processed image 2715\n",
      "Processed image 2716\n",
      "Processed image 2717\n",
      "Processed image 2718\n",
      "Processed image 2719\n",
      "Processed image 2720\n",
      "Processed image 2721\n",
      "Processed image 2722\n",
      "Processed image 2723\n",
      "Processed image 2724\n",
      "Processed image 2725\n",
      "Processed image 2726\n",
      "Processed image 2727\n",
      "Processed image 2728\n",
      "Processed image 2729\n",
      "Processed image 2730\n",
      "Processed image 2731\n",
      "Processed image 2732\n",
      "Processed image 2733\n",
      "Processed image 2734\n",
      "Processed image 2735\n",
      "Processed image 2736\n",
      "Processed image 2737\n",
      "Processed image 2738\n",
      "Processed image 2739\n",
      "Processed image 2740\n",
      "Processed image 2741\n",
      "Processed image 2742\n",
      "Processed image 2743\n",
      "Processed image 2744\n",
      "Processed image 2745\n",
      "Processed image 2746\n",
      "Processed image 2747\n",
      "Processed image 2748\n",
      "Processed image 2749\n",
      "Processed image 2750\n",
      "Processed image 2751\n",
      "Processed image 2752\n",
      "Processed image 2753\n",
      "Processed image 2754\n",
      "Processed image 2755\n",
      "Processed image 2756\n",
      "Processed image 2757\n",
      "Processed image 2758\n",
      "Processed image 2759\n",
      "Processed image 2760\n",
      "Processed image 2761\n",
      "Processed image 2762\n",
      "Processed image 2763\n",
      "Processed image 2764\n",
      "Processed image 2765\n",
      "Processed image 2766\n",
      "Processed image 2767\n",
      "Processed image 2768\n",
      "Processed image 2769\n",
      "Processed image 2770\n",
      "Processed image 2771\n",
      "Processed image 2772\n",
      "Processed image 2773\n",
      "Processed image 2774\n",
      "Processed image 2775\n",
      "Processed image 2776\n",
      "Processed image 2777\n",
      "Processed image 2778\n",
      "Processed image 2779\n",
      "Processed image 2780\n",
      "Processed image 2781\n",
      "Processed image 2782\n",
      "Processed image 2783\n",
      "Processed image 2784\n",
      "Processed image 2785\n",
      "Processed image 2786\n",
      "Processed image 2787\n",
      "Processed image 2788\n",
      "Processed image 2789\n",
      "Processed image 2790\n",
      "Processed image 2791\n",
      "Processed image 2792\n",
      "Processed image 2793\n",
      "Processed image 2794\n",
      "Processed image 2795\n",
      "Processed image 2796\n",
      "Processed image 2797\n",
      "Processed image 2798\n",
      "Processed image 2799\n",
      "Processed image 2800\n",
      "Processed image 2801\n",
      "Processed image 2802\n",
      "Processed image 2803\n",
      "Processed image 2804\n",
      "Processed image 2805\n",
      "Processed image 2806\n",
      "Processed image 2807\n",
      "Processed image 2808\n",
      "Processed image 2809\n",
      "Processed image 2810\n",
      "Processed image 2811\n",
      "Processed image 2812\n",
      "Processed image 2813\n",
      "Processed image 2814\n",
      "Processed image 2815\n",
      "Processed image 2816\n",
      "Processed image 2817\n",
      "Processed image 2818\n",
      "Processed image 2819\n",
      "Processed image 2820\n",
      "Processed image 2821\n",
      "Processed image 2822\n",
      "Processed image 2823\n",
      "Processed image 2824\n",
      "Processed image 2825\n",
      "Processed image 2826\n",
      "Processed image 2827\n",
      "Processed image 2828\n",
      "Processed image 2829\n",
      "Processed image 2830\n",
      "Processed image 2831\n",
      "Processed image 2832\n",
      "Processed image 2833\n",
      "Processed image 2834\n",
      "Processed image 2835\n",
      "Processed image 2836\n",
      "Processed image 2837\n",
      "Processed image 2838\n",
      "Processed image 2839\n",
      "Processed image 2840\n",
      "Processed image 2841\n",
      "Processed image 2842\n",
      "Processed image 2843\n",
      "Processed image 2844\n",
      "Processed image 2845\n",
      "Processed image 2846\n",
      "Processed image 2847\n",
      "Processed image 2848\n",
      "Processed image 2849\n",
      "Processed image 2850\n",
      "Processed image 2851\n",
      "Processed image 2852\n",
      "Processed image 2853\n",
      "Processed image 2854\n",
      "Processed image 2855\n",
      "Processed image 2856\n",
      "Processed image 2857\n",
      "Processed image 2858\n",
      "Processed image 2859\n",
      "Processed image 2860\n",
      "Processed image 2861\n",
      "Processed image 2862\n",
      "Processed image 2863\n",
      "Processed image 2864\n",
      "Processed image 2865\n",
      "Processed image 2866\n",
      "Processed image 2867\n",
      "Processed image 2868\n",
      "Processed image 2869\n",
      "Processed image 2870\n",
      "Processed image 2871\n",
      "Processed image 2872\n",
      "Processed image 2873\n",
      "Processed image 2874\n",
      "Processed image 2875\n",
      "Processed image 2876\n",
      "Processed image 2877\n",
      "Processed image 2878\n",
      "Processed image 2879\n",
      "Processed image 2880\n",
      "Processed image 2881\n",
      "Processed image 2882\n",
      "Processed image 2883\n",
      "Processed image 2884\n",
      "Processed image 2885\n",
      "Processed image 2886\n",
      "Processed image 2887\n",
      "Processed image 2888\n",
      "Processed image 2889\n",
      "Processed image 2890\n",
      "Processed image 2891\n",
      "Processed image 2892\n",
      "Processed image 2893\n",
      "Processed image 2894\n",
      "Processed image 2895\n",
      "Processed image 2896\n",
      "Processed image 2897\n",
      "Processed image 2898\n",
      "Processed image 2899\n",
      "Processed image 2900\n",
      "Processed image 2901\n",
      "Processed image 2902\n",
      "Processed image 2903\n",
      "Processed image 2904\n",
      "Processed image 2905\n",
      "Processed image 2906\n",
      "Processed image 2907\n",
      "Processed image 2908\n",
      "Processed image 2909\n",
      "Processed image 2910\n",
      "Processed image 2911\n",
      "Processed image 2912\n",
      "Processed image 2913\n",
      "Processed image 2914\n",
      "Processed image 2915\n",
      "Processed image 2916\n",
      "Processed image 2917\n",
      "Processed image 2918\n",
      "Processed image 2919\n",
      "Processed image 2920\n",
      "Processed image 2921\n",
      "Processed image 2922\n",
      "Processed image 2923\n",
      "Processed image 2924\n",
      "Processed image 2925\n",
      "Processed image 2926\n",
      "Processed image 2927\n",
      "Processed image 2928\n",
      "Processed image 2929\n",
      "Processed image 2930\n",
      "Processed image 2931\n",
      "Processed image 2932\n",
      "Processed image 2933\n",
      "Processed image 2934\n",
      "Processed image 2935\n",
      "Processed image 2936\n",
      "Processed image 2937\n",
      "Processed image 2938\n",
      "Processed image 2939\n",
      "Processed image 2940\n",
      "Processed image 2941\n",
      "Processed image 2942\n",
      "Processed image 2943\n",
      "Processed image 2944\n",
      "Processed image 2945\n",
      "Processed image 2946\n",
      "Processed image 2947\n",
      "Processed image 2948\n",
      "Processed image 2949\n",
      "Processed image 2950\n",
      "Processed image 2951\n",
      "Processed image 2952\n",
      "Processed image 2953\n",
      "Processed image 2954\n",
      "Processed image 2955\n",
      "Processed image 2956\n",
      "Processed image 2957\n",
      "Processed image 2958\n",
      "Processed image 2959\n",
      "Processed image 2960\n",
      "Processed image 2961\n",
      "Processed image 2962\n",
      "Processed image 2963\n",
      "Processed image 2964\n",
      "Processed image 2965\n",
      "Processed image 2966\n",
      "Processed image 2967\n",
      "Processed image 2968\n",
      "Processed image 2969\n",
      "Processed image 2970\n",
      "Processed image 2971\n",
      "Processed image 2972\n",
      "Processed image 2973\n",
      "Processed image 2974\n",
      "Processed image 2975\n",
      "Processed image 2976\n",
      "Processed image 2977\n",
      "Processed image 2978\n",
      "Processed image 2979\n",
      "Processed image 2980\n",
      "Processed image 2981\n",
      "Processed image 2982\n",
      "Processed image 2983\n",
      "Processed image 2984\n",
      "Processed image 2985\n",
      "Processed image 2986\n",
      "Processed image 2987\n",
      "Processed image 2988\n",
      "Processed image 2989\n",
      "Processed image 2990\n",
      "Processed image 2991\n",
      "Processed image 2992\n",
      "Processed image 2993\n",
      "Processed image 2994\n",
      "Processed image 2995\n",
      "Processed image 2996\n",
      "Processed image 2997\n",
      "Processed image 2998\n",
      "Processed image 2999\n",
      "Processed image 3000\n",
      "Processed image 3001\n",
      "Processed image 3002\n",
      "Processed image 3003\n",
      "Processed image 3004\n",
      "Processed image 3005\n",
      "Processed image 3006\n",
      "Processed image 3007\n",
      "Processed image 3008\n",
      "Processed image 3009\n",
      "Processed image 3010\n",
      "Processed image 3011\n",
      "Processed image 3012\n",
      "Processed image 3013\n",
      "Processed image 3014\n",
      "Processed image 3015\n",
      "Processed image 3016\n",
      "Processed image 3017\n",
      "Processed image 3018\n",
      "Processed image 3019\n",
      "Processed image 3020\n",
      "Processed image 3021\n",
      "Processed image 3022\n",
      "Processed image 3023\n",
      "Processed image 3024\n",
      "Processed image 3025\n",
      "Processed image 3026\n",
      "Processed image 3027\n",
      "Processed image 3028\n",
      "Processed image 3029\n",
      "Processed image 3030\n",
      "Processed image 3031\n",
      "Processed image 3032\n",
      "Processed image 3033\n",
      "Processed image 3034\n",
      "Processed image 3035\n",
      "Processed image 3036\n",
      "Processed image 3037\n",
      "Processed image 3038\n",
      "Processed image 3039\n",
      "Processed image 3040\n",
      "Processed image 3041\n",
      "Processed image 3042\n",
      "Processed image 3043\n",
      "Processed image 3044\n",
      "Processed image 3045\n",
      "Processed image 3046\n",
      "Processed image 3047\n",
      "Processed image 3048\n",
      "Processed image 3049\n",
      "Processed image 3050\n",
      "Processed image 3051\n",
      "Processed image 3052\n",
      "Processed image 3053\n",
      "Processed image 3054\n",
      "Processed image 3055\n",
      "Processed image 3056\n",
      "Processed image 3057\n",
      "Processed image 3058\n",
      "Processed image 3059\n",
      "Processed image 3060\n",
      "Processed image 3061\n",
      "Processed image 3062\n",
      "Processed image 3063\n",
      "Processed image 3064\n",
      "Processed image 3065\n",
      "Processed image 3066\n",
      "Processed image 3067\n",
      "Processed image 3068\n",
      "Processed image 3069\n",
      "Processed image 3070\n",
      "Processed image 3071\n",
      "Processed image 3072\n",
      "Processed image 3073\n",
      "Processed image 3074\n",
      "Processed image 3075\n",
      "Processed image 3076\n",
      "Processed image 3077\n",
      "Processed image 3078\n",
      "Processed image 3079\n",
      "Processed image 3080\n",
      "Processed image 3081\n",
      "Processed image 3082\n",
      "Processed image 3083\n",
      "Processed image 3084\n",
      "Processed image 3085\n",
      "Processed image 3086\n",
      "Processed image 3087\n",
      "Processed image 3088\n",
      "Processed image 3089\n",
      "Processed image 3090\n",
      "Processed image 3091\n",
      "Processed image 3092\n",
      "Processed image 3093\n",
      "Processed image 3094\n",
      "Processed image 3095\n",
      "Processed image 3096\n",
      "Processed image 3097\n",
      "Processed image 3098\n",
      "Processed image 3099\n",
      "Processed image 3100\n",
      "Processed image 3101\n",
      "Processed image 3102\n",
      "Processed image 3103\n",
      "Processed image 3104\n",
      "Processed image 3105\n",
      "Processed image 3106\n",
      "Processed image 3107\n",
      "Processed image 3108\n",
      "Processed image 3109\n",
      "Processed image 3110\n",
      "Processed image 3111\n",
      "Processed image 3112\n",
      "Processed image 3113\n",
      "Processed image 3114\n",
      "Processed image 3115\n",
      "Processed image 3116\n",
      "Processed image 3117\n",
      "Processed image 3118\n",
      "Processed image 3119\n",
      "Processed image 3120\n",
      "Processed image 3121\n",
      "Processed image 3122\n",
      "Processed image 3123\n",
      "Processed image 3124\n",
      "Processed image 3125\n",
      "Processed image 3126\n",
      "Processed image 3127\n",
      "Processed image 3128\n",
      "Processed image 3129\n",
      "Processed image 3130\n",
      "Processed image 3131\n",
      "Processed image 3132\n",
      "Processed image 3133\n",
      "Processed image 3134\n",
      "Processed image 3135\n",
      "Processed image 3136\n",
      "Processed image 3137\n",
      "Processed image 3138\n",
      "Processed image 3139\n",
      "Processed image 3140\n",
      "Processed image 3141\n",
      "Processed image 3142\n",
      "Processed image 3143\n",
      "Processed image 3144\n",
      "Processed image 3145\n",
      "Processed image 3146\n",
      "Processed image 3147\n",
      "Processed image 3148\n",
      "Processed image 3149\n",
      "Processed image 3150\n",
      "Processed image 3151\n",
      "Processed image 3152\n",
      "Processed image 3153\n",
      "Processed image 3154\n",
      "Processed image 3155\n",
      "Processed image 3156\n",
      "Processed image 3157\n",
      "Processed image 3158\n",
      "Processed image 3159\n",
      "Processed image 3160\n",
      "Processed image 3161\n",
      "Processed image 3162\n",
      "Processed image 3163\n",
      "Processed image 3164\n",
      "Processed image 3165\n",
      "Processed image 3166\n",
      "Processed image 3167\n",
      "Processed image 3168\n",
      "Processed image 3169\n",
      "Processed image 3170\n",
      "Processed image 3171\n",
      "Processed image 3172\n",
      "Processed image 3173\n",
      "Processed image 3174\n",
      "Processed image 3175\n",
      "Processed image 3176\n",
      "Processed image 3177\n",
      "Processed image 3178\n",
      "Processed image 3179\n",
      "Processed image 3180\n",
      "Processed image 3181\n",
      "Processed image 3182\n",
      "Processed image 3183\n",
      "Processed image 3184\n",
      "Processed image 3185\n",
      "Processed image 3186\n",
      "Processed image 3187\n",
      "Processed image 3188\n",
      "Processed image 3189\n",
      "Processed image 3190\n",
      "Processed image 3191\n",
      "Processed image 3192\n",
      "Processed image 3193\n",
      "Processed image 3194\n",
      "Processed image 3195\n",
      "Processed image 3196\n",
      "Processed image 3197\n",
      "Processed image 3198\n",
      "Processed image 3199\n",
      "Processed image 3200\n",
      "Processed image 3201\n",
      "Processed image 3202\n",
      "Processed image 3203\n",
      "Processed image 3204\n",
      "Processed image 3205\n",
      "Processed image 3206\n",
      "Processed image 3207\n",
      "Processed image 3208\n",
      "Processed image 3209\n",
      "Processed image 3210\n",
      "Processed image 3211\n",
      "Processed image 3212\n",
      "Processed image 3213\n",
      "Processed image 3214\n",
      "Processed image 3215\n",
      "Processed image 3216\n",
      "Processed image 3217\n",
      "Processed image 3218\n",
      "Processed image 3219\n",
      "Processed image 3220\n",
      "Processed image 3221\n",
      "Processed image 3222\n",
      "Processed image 3223\n",
      "Processed image 3224\n",
      "Processed image 3225\n",
      "Processed image 3226\n",
      "Processed image 3227\n",
      "Processed image 3228\n",
      "Processed image 3229\n",
      "Processed image 3230\n",
      "Processed image 3231\n",
      "Processed image 3232\n",
      "Processed image 3233\n",
      "Processed image 3234\n",
      "Processed image 3235\n",
      "Processed image 3236\n",
      "Processed image 3237\n",
      "Processed image 3238\n",
      "Processed image 3239\n",
      "Processed image 3240\n",
      "Processed image 3241\n",
      "Processed image 3242\n",
      "Processed image 3243\n",
      "Processed image 3244\n",
      "Processed image 3245\n",
      "Processed image 3246\n",
      "Processed image 3247\n",
      "Processed image 3248\n",
      "Processed image 3249\n",
      "Processed image 3250\n",
      "Processed image 3251\n",
      "Processed image 3252\n",
      "Processed image 3253\n",
      "Processed image 3254\n",
      "Processed image 3255\n",
      "Processed image 3256\n",
      "Processed image 3257\n",
      "Processed image 3258\n",
      "Processed image 3259\n",
      "Processed image 3260\n",
      "Processed image 3261\n",
      "Processed image 3262\n",
      "Processed image 3263\n",
      "Processed image 3264\n",
      "Processed image 3265\n",
      "Processed image 3266\n",
      "Processed image 3267\n",
      "Processed image 3268\n",
      "Processed image 3269\n",
      "Processed image 3270\n",
      "Processed image 3271\n",
      "Processed image 3272\n",
      "Processed image 3273\n",
      "Processed image 3274\n",
      "Processed image 3275\n",
      "Processed image 3276\n",
      "Processed image 3277\n",
      "Processed image 3278\n",
      "Processed image 3279\n",
      "Processed image 3280\n",
      "Processed image 3281\n",
      "Processed image 3282\n",
      "Processed image 3283\n",
      "Processed image 3284\n",
      "Processed image 3285\n",
      "Processed image 3286\n",
      "Processed image 3287\n",
      "Processed image 3288\n",
      "Processed image 3289\n",
      "Processed image 3290\n",
      "Processed image 3291\n",
      "Processed image 3292\n",
      "Processed image 3293\n",
      "Processed image 3294\n",
      "Processed image 3295\n",
      "Processed image 3296\n",
      "Processed image 3297\n",
      "Processed image 3298\n",
      "Processed image 3299\n",
      "Processed image 3300\n",
      "Processed image 3301\n",
      "Processed image 3302\n",
      "Processed image 3303\n",
      "Processed image 3304\n",
      "Processed image 3305\n",
      "Processed image 3306\n",
      "Processed image 3307\n",
      "Processed image 3308\n",
      "Processed image 3309\n",
      "Processed image 3310\n",
      "Processed image 3311\n",
      "Processed image 3312\n",
      "Processed image 3313\n",
      "Processed image 3314\n",
      "Processed image 3315\n",
      "Processed image 3316\n",
      "Processed image 3317\n",
      "Processed image 3318\n",
      "Processed image 3319\n",
      "Processed image 3320\n",
      "Processed image 3321\n",
      "Processed image 3322\n",
      "Processed image 3323\n",
      "Processed image 3324\n",
      "Processed image 3325\n",
      "Processed image 3326\n",
      "Processed image 3327\n",
      "Processed image 3328\n",
      "Processed image 3329\n",
      "Processed image 3330\n",
      "Processed image 3331\n",
      "Processed image 3332\n",
      "Processed image 3333\n",
      "Processed image 3334\n",
      "Processed image 3335\n",
      "Processed image 3336\n",
      "Processed image 3337\n",
      "Processed image 3338\n",
      "Processed image 3339\n",
      "Processed image 3340\n",
      "Processed image 3341\n",
      "Processed image 3342\n",
      "Processed image 3343\n",
      "Processed image 3344\n",
      "Processed image 3345\n",
      "Processed image 3346\n",
      "Processed image 3347\n",
      "Processed image 3348\n",
      "Processed image 3349\n",
      "Processed image 3350\n",
      "Processed image 3351\n",
      "Processed image 3352\n",
      "Processed image 3353\n",
      "Processed image 3354\n",
      "Processed image 3355\n",
      "Processed image 3356\n",
      "Processed image 3357\n",
      "Processed image 3358\n",
      "Processed image 3359\n",
      "Processed image 3360\n",
      "Processed image 3361\n",
      "Processed image 3362\n",
      "Processed image 3363\n",
      "Processed image 3364\n",
      "Processed image 3365\n",
      "Processed image 3366\n",
      "Processed image 3367\n",
      "Processed image 3368\n",
      "Processed image 3369\n",
      "Processed image 3370\n",
      "Processed image 3371\n",
      "Processed image 3372\n",
      "Processed image 3373\n",
      "Processed image 3374\n",
      "Processed image 3375\n",
      "Processed image 3376\n",
      "Processed image 3377\n",
      "Processed image 3378\n",
      "Processed image 3379\n",
      "Processed image 3380\n",
      "Processed image 3381\n",
      "Processed image 3382\n",
      "Processed image 3383\n",
      "Processed image 3384\n",
      "Processed image 3385\n",
      "Processed image 3386\n",
      "Processed image 3387\n",
      "Processed image 3388\n",
      "Processed image 3389\n",
      "Processed image 3390\n",
      "Processed image 3391\n",
      "Processed image 3392\n",
      "Processed image 3393\n",
      "Processed image 3394\n",
      "Processed image 3395\n",
      "Processed image 3396\n",
      "Processed image 3397\n",
      "Processed image 3398\n",
      "Processed image 3399\n",
      "Processed image 3400\n",
      "Processed image 3401\n",
      "Processed image 3402\n",
      "Processed image 3403\n",
      "Processed image 3404\n",
      "Processed image 3405\n",
      "Processed image 3406\n",
      "Processed image 3407\n",
      "Processed image 3408\n",
      "Processed image 3409\n",
      "Processed image 3410\n",
      "Processed image 3411\n",
      "Processed image 3412\n",
      "Processed image 3413\n",
      "Processed image 3414\n",
      "Processed image 3415\n",
      "Processed image 3416\n",
      "Processed image 3417\n",
      "Processed image 3418\n",
      "Processed image 3419\n",
      "Processed image 3420\n",
      "Processed image 3421\n",
      "Processed image 3422\n",
      "Processed image 3423\n",
      "Processed image 3424\n",
      "Processed image 3425\n",
      "Processed image 3426\n",
      "Processed image 3427\n",
      "Processed image 3428\n",
      "Processed image 3429\n",
      "Processed image 3430\n",
      "Processed image 3431\n",
      "Processed image 3432\n",
      "Processed image 3433\n",
      "Processed image 3434\n",
      "Processed image 3435\n",
      "Processed image 3436\n",
      "Processed image 3437\n",
      "Processed image 3438\n",
      "Processed image 3439\n",
      "Processed image 3440\n",
      "Processed image 3441\n",
      "Processed image 3442\n",
      "Processed image 3443\n",
      "Processed image 3444\n",
      "Processed image 3445\n",
      "Processed image 3446\n",
      "Processed image 3447\n",
      "Processed image 3448\n",
      "Processed image 3449\n",
      "Processed image 3450\n",
      "Processed image 3451\n",
      "Processed image 3452\n",
      "Processed image 3453\n",
      "Processed image 3454\n",
      "Processed image 3455\n",
      "Processed image 3456\n",
      "Processed image 3457\n",
      "Processed image 3458\n",
      "Processed image 3459\n",
      "Processed image 3460\n",
      "Processed image 3461\n",
      "Processed image 3462\n",
      "Processed image 3463\n",
      "Processed image 3464\n",
      "Processed image 3465\n",
      "Processed image 3466\n",
      "Processed image 3467\n",
      "Processed image 3468\n",
      "Processed image 3469\n",
      "Processed image 3470\n",
      "Processed image 3471\n",
      "Processed image 3472\n",
      "Processed image 3473\n",
      "Processed image 3474\n",
      "Processed image 3475\n",
      "Processed image 3476\n",
      "Processed image 3477\n",
      "Processed image 3478\n",
      "Processed image 3479\n",
      "Processed image 3480\n",
      "Processed image 3481\n",
      "Processed image 3482\n",
      "Processed image 3483\n",
      "Processed image 3484\n",
      "Processed image 3485\n",
      "Processed image 3486\n",
      "Processed image 3487\n",
      "Processed image 3488\n",
      "Processed image 3489\n",
      "Processed image 3490\n",
      "Processed image 3491\n",
      "Processed image 3492\n",
      "Processed image 3493\n",
      "Processed image 3494\n",
      "Processed image 3495\n",
      "Processed image 3496\n",
      "Processed image 3497\n",
      "Processed image 3498\n",
      "Processed image 3499\n",
      "Processed image 3500\n",
      "Processed image 3501\n",
      "Processed image 3502\n",
      "Processed image 3503\n",
      "Processed image 3504\n",
      "Processed image 3505\n",
      "Processed image 3506\n",
      "Processed image 3507\n",
      "Processed image 3508\n",
      "Processed image 3509\n",
      "Processed image 3510\n",
      "Processed image 3511\n",
      "Processed image 3512\n",
      "Processed image 3513\n",
      "Processed image 3514\n",
      "Processed image 3515\n",
      "Processed image 3516\n",
      "Processed image 3517\n",
      "Processed image 3518\n",
      "Processed image 3519\n",
      "Processed image 3520\n",
      "Processed image 3521\n",
      "Processed image 3522\n",
      "Processed image 3523\n",
      "Processed image 3524\n",
      "Processed image 3525\n",
      "Processed image 3526\n",
      "Processed image 3527\n",
      "Processed image 3528\n",
      "Processed image 3529\n",
      "Processed image 3530\n",
      "Processed image 3531\n",
      "Processed image 3532\n",
      "Processed image 3533\n",
      "Processed image 3534\n",
      "Processed image 3535\n",
      "Processed image 3536\n",
      "Processed image 3537\n",
      "Processed image 3538\n",
      "Processed image 3539\n",
      "Processed image 3540\n",
      "Processed image 3541\n",
      "Processed image 3542\n",
      "Processed image 3543\n",
      "Processed image 3544\n",
      "Processed image 3545\n",
      "Processed image 3546\n",
      "Processed image 3547\n",
      "Processed image 3548\n",
      "Processed image 3549\n",
      "Processed image 3550\n",
      "Processed image 3551\n",
      "Processed image 3552\n",
      "Processed image 3553\n",
      "Processed image 3554\n",
      "Processed image 3555\n",
      "Processed image 3556\n",
      "Processed image 3557\n",
      "Processed image 3558\n",
      "Processed image 3559\n",
      "Processed image 3560\n",
      "Processed image 3561\n",
      "Processed image 3562\n",
      "Processed image 3563\n",
      "Processed image 3564\n",
      "Processed image 3565\n",
      "Processed image 3566\n",
      "Processed image 3567\n",
      "Processed image 3568\n",
      "Processed image 3569\n",
      "Processed image 3570\n",
      "Processed image 3571\n",
      "Processed image 3572\n",
      "Processed image 3573\n",
      "Processed image 3574\n",
      "Processed image 3575\n",
      "Processed image 3576\n",
      "Processed image 3577\n",
      "Processed image 3578\n",
      "Processed image 3579\n",
      "Processed image 3580\n",
      "Processed image 3581\n",
      "Processed image 3582\n",
      "Processed image 3583\n",
      "Processed image 3584\n",
      "Processed image 3585\n",
      "Processed image 3586\n",
      "Processed image 3587\n",
      "Processed image 3588\n",
      "Processed image 3589\n",
      "Processed image 3590\n",
      "Processed image 3591\n",
      "Processed image 3592\n",
      "Processed image 3593\n",
      "Processed image 3594\n",
      "Processed image 3595\n",
      "Processed image 3596\n",
      "Processed image 3597\n",
      "Processed image 3598\n",
      "Processed image 3599\n",
      "Processed image 3600\n",
      "Processed image 3601\n",
      "Processed image 3602\n",
      "Processed image 3603\n",
      "Processed image 3604\n",
      "Processed image 3605\n",
      "Processed image 3606\n",
      "Processed image 3607\n",
      "Processed image 3608\n",
      "Processed image 3609\n",
      "Processed image 3610\n",
      "Processed image 3611\n",
      "Processed image 3612\n",
      "Processed image 3613\n",
      "Processed image 3614\n",
      "Processed image 3615\n",
      "Processed image 3616\n",
      "Processed image 3617\n",
      "Processed image 3618\n",
      "Processed image 3619\n",
      "Processed image 3620\n",
      "Processed image 3621\n",
      "Processed image 3622\n",
      "Processed image 3623\n",
      "Processed image 3624\n",
      "Processed image 3625\n",
      "Processed image 3626\n",
      "Processed image 3627\n",
      "Processed image 3628\n",
      "Processed image 3629\n",
      "Processed image 3630\n",
      "Processed image 3631\n",
      "Processed image 3632\n",
      "Processed image 3633\n",
      "Processed image 3634\n",
      "Processed image 3635\n",
      "Processed image 3636\n",
      "Processed image 3637\n",
      "Processed image 3638\n",
      "Processed image 3639\n",
      "Processed image 3640\n",
      "Processed image 3641\n",
      "Processed image 3642\n",
      "Processed image 3643\n",
      "Processed image 3644\n",
      "Processed image 3645\n",
      "Processed image 3646\n",
      "Processed image 3647\n",
      "Processed image 3648\n",
      "Processed image 3649\n",
      "Processed image 3650\n",
      "Processed image 3651\n",
      "Processed image 3652\n",
      "Processed image 3653\n",
      "Processed image 3654\n",
      "Processed image 3655\n",
      "Processed image 3656\n",
      "Processed image 3657\n",
      "Processed image 3658\n",
      "Processed image 3659\n",
      "Processed image 3660\n",
      "Processed image 3661\n",
      "Processed image 3662\n",
      "Processed image 3663\n",
      "Processed image 3664\n",
      "Processed image 3665\n",
      "Processed image 3666\n",
      "Processed image 3667\n",
      "Processed image 3668\n",
      "Processed image 3669\n",
      "Processed image 3670\n",
      "Processed image 3671\n",
      "Processed image 3672\n",
      "Processed image 3673\n",
      "Processed image 3674\n",
      "Processed image 3675\n",
      "Processed image 3676\n",
      "Processed image 3677\n",
      "Processed image 3678\n",
      "Processed image 3679\n",
      "Processed image 3680\n",
      "Processed image 3681\n",
      "Processed image 3682\n",
      "Processed image 3683\n",
      "Processed image 3684\n",
      "Processed image 3685\n",
      "Processed image 3686\n",
      "Processed image 3687\n",
      "Processed image 3688\n",
      "Processed image 3689\n",
      "Processed image 3690\n",
      "Processed image 3691\n",
      "Processed image 3692\n",
      "Processed image 3693\n",
      "Processed image 3694\n",
      "Processed image 3695\n",
      "Processed image 3696\n",
      "Processed image 3697\n",
      "Processed image 3698\n",
      "Processed image 3699\n",
      "Processed image 3700\n",
      "Processed image 3701\n",
      "Processed image 3702\n",
      "Processed image 3703\n",
      "Processed image 3704\n",
      "Processed image 3705\n",
      "Processed image 3706\n",
      "Processed image 3707\n",
      "Processed image 3708\n",
      "Processed image 3709\n",
      "Processed image 3710\n",
      "Processed image 3711\n",
      "Processed image 3712\n",
      "Processed image 3713\n",
      "Processed image 3714\n",
      "Processed image 3715\n",
      "Processed image 3716\n",
      "Processed image 3717\n",
      "Processed image 3718\n",
      "Processed image 3719\n",
      "Processed image 3720\n",
      "Processed image 3721\n",
      "Processed image 3722\n",
      "Processed image 3723\n",
      "Processed image 3724\n",
      "Processed image 3725\n",
      "Processed image 3726\n",
      "Processed image 3727\n",
      "Processed image 3728\n",
      "Processed image 3729\n",
      "Processed image 3730\n",
      "Processed image 3731\n",
      "Processed image 3732\n",
      "Processed image 3733\n",
      "Processed image 3734\n",
      "Processed image 3735\n",
      "Processed image 3736\n",
      "Processed image 3737\n",
      "Processed image 3738\n",
      "Processed image 3739\n",
      "Processed image 3740\n",
      "Processed image 3741\n",
      "Processed image 3742\n",
      "Processed image 3743\n",
      "Processed image 3744\n",
      "Processed image 3745\n",
      "Processed image 3746\n",
      "Processed image 3747\n",
      "Processed image 3748\n",
      "Processed image 3749\n",
      "Processed image 3750\n",
      "Processed image 3751\n",
      "Processed image 3752\n",
      "Processed image 3753\n",
      "Processed image 3754\n",
      "Processed image 3755\n",
      "Processed image 3756\n",
      "Processed image 3757\n",
      "Processed image 3758\n",
      "Processed image 3759\n",
      "Processed image 3760\n",
      "Processed image 3761\n",
      "Processed image 3762\n",
      "Processed image 3763\n",
      "Processed image 3764\n",
      "Processed image 3765\n",
      "Processed image 3766\n",
      "Processed image 3767\n",
      "Processed image 3768\n",
      "Processed image 3769\n",
      "Processed image 3770\n",
      "Processed image 3771\n",
      "Processed image 3772\n",
      "Processed image 3773\n",
      "Processed image 3774\n",
      "Processed image 3775\n",
      "Processed image 3776\n",
      "Processed image 3777\n",
      "Processed image 3778\n",
      "Processed image 3779\n",
      "Processed image 3780\n",
      "Processed image 3781\n",
      "Processed image 3782\n",
      "Processed image 3783\n",
      "Processed image 3784\n",
      "Processed image 3785\n",
      "Processed image 3786\n",
      "Processed image 3787\n",
      "Processed image 3788\n",
      "Processed image 3789\n",
      "Processed image 3790\n",
      "Processed image 3791\n",
      "Processed image 3792\n",
      "Processed image 3793\n",
      "Processed image 3794\n",
      "Processed image 3795\n",
      "Processed image 3796\n",
      "Processed image 3797\n",
      "Processed image 3798\n",
      "Processed image 3799\n",
      "Processed image 3800\n",
      "Processed image 3801\n",
      "Processed image 3802\n",
      "Processed image 3803\n",
      "Processed image 3804\n",
      "Processed image 3805\n",
      "Processed image 3806\n",
      "Processed image 3807\n",
      "Processed image 3808\n",
      "Processed image 3809\n",
      "Processed image 3810\n",
      "Processed image 3811\n",
      "Processed image 3812\n",
      "Processed image 3813\n",
      "Processed image 3814\n",
      "Processed image 3815\n",
      "Processed image 3816\n",
      "Processed image 3817\n",
      "Processed image 3818\n",
      "Processed image 3819\n",
      "Processed image 3820\n",
      "Processed image 3821\n",
      "Processed image 3822\n",
      "Processed image 3823\n",
      "Processed image 3824\n",
      "Processed image 3825\n",
      "Processed image 3826\n",
      "Processed image 3827\n",
      "Processed image 3828\n",
      "Processed image 3829\n",
      "Processed image 3830\n",
      "Processed image 3831\n",
      "Processed image 3832\n",
      "Processed image 3833\n",
      "Processed image 3834\n",
      "Processed image 3835\n",
      "Processed image 3836\n",
      "Processed image 3837\n",
      "Processed image 3838\n",
      "Processed image 3839\n",
      "Processed image 3840\n",
      "Processed image 3841\n",
      "Processed image 3842\n",
      "Processed image 3843\n",
      "Processed image 3844\n",
      "Processed image 3845\n",
      "Processed image 3846\n",
      "Processed image 3847\n",
      "Processed image 3848\n",
      "Processed image 3849\n",
      "Processed image 3850\n",
      "Processed image 3851\n",
      "Processed image 3852\n",
      "Processed image 3853\n",
      "Processed image 3854\n",
      "Processed image 3855\n",
      "Processed image 3856\n",
      "Processed image 3857\n",
      "Processed image 3858\n",
      "Processed image 3859\n",
      "Processed image 3860\n",
      "Processed image 3861\n",
      "Processed image 3862\n",
      "Processed image 3863\n",
      "Processed image 3864\n",
      "Processed image 3865\n",
      "Processed image 3866\n",
      "Processed image 3867\n",
      "Processed image 3868\n",
      "Processed image 3869\n",
      "Processed image 3870\n",
      "Processed image 3871\n",
      "Processed image 3872\n",
      "Processed image 3873\n",
      "Processed image 3874\n",
      "Processed image 3875\n",
      "Processed image 3876\n",
      "Processed image 3877\n",
      "Processed image 3878\n",
      "Processed image 3879\n",
      "Processed image 3880\n",
      "Processed image 3881\n",
      "Processed image 3882\n",
      "Processed image 3883\n",
      "Processed image 3884\n",
      "Processed image 3885\n",
      "Processed image 3886\n",
      "Processed image 3887\n",
      "Processed image 3888\n",
      "Processed image 3889\n",
      "Processed image 3890\n",
      "Processed image 3891\n",
      "Processed image 3892\n",
      "Processed image 3893\n",
      "Processed image 3894\n",
      "Processed image 3895\n",
      "Processed image 3896\n",
      "Processed image 3897\n",
      "Processed image 3898\n",
      "Processed image 3899\n",
      "Processed image 3900\n",
      "Processed image 3901\n",
      "Processed image 3902\n",
      "Processed image 3903\n",
      "Processed image 3904\n",
      "Processed image 3905\n",
      "Processed image 3906\n",
      "Processed image 3907\n",
      "Processed image 3908\n",
      "Processed image 3909\n",
      "Processed image 3910\n",
      "Processed image 3911\n",
      "Processed image 3912\n",
      "Processed image 3913\n",
      "Processed image 3914\n",
      "Processed image 3915\n",
      "Processed image 3916\n",
      "Processed image 3917\n",
      "Processed image 3918\n",
      "Processed image 3919\n",
      "Processed image 3920\n",
      "Processed image 3921\n",
      "Processed image 3922\n",
      "Processed image 3923\n",
      "Processed image 3924\n",
      "Processed image 3925\n",
      "Processed image 3926\n",
      "Processed image 3927\n",
      "Processed image 3928\n",
      "Processed image 3929\n",
      "Processed image 3930\n",
      "Processed image 3931\n",
      "Processed image 3932\n",
      "Processed image 3933\n",
      "Processed image 3934\n",
      "Processed image 3935\n",
      "Processed image 3936\n",
      "Processed image 3937\n",
      "Processed image 3938\n",
      "Processed image 3939\n",
      "Processed image 3940\n",
      "Processed image 3941\n",
      "Processed image 3942\n",
      "Processed image 3943\n",
      "Processed image 3944\n",
      "Processed image 3945\n",
      "Processed image 3946\n",
      "Processed image 3947\n",
      "Processed image 3948\n",
      "Processed image 3949\n",
      "Processed image 3950\n",
      "Processed image 3951\n",
      "Processed image 3952\n",
      "Processed image 3953\n",
      "Processed image 3954\n",
      "Processed image 3955\n",
      "Processed image 3956\n",
      "Processed image 3957\n",
      "Processed image 3958\n",
      "Processed image 3959\n",
      "Processed image 3960\n",
      "Processed image 3961\n",
      "Processed image 3962\n",
      "Processed image 3963\n",
      "Processed image 3964\n",
      "Processed image 3965\n",
      "Processed image 3966\n",
      "Processed image 3967\n",
      "Processed image 3968\n",
      "Processed image 3969\n",
      "Processed image 3970\n",
      "Processed image 3971\n",
      "Processed image 3972\n",
      "Processed image 3973\n",
      "Processed image 3974\n",
      "Processed image 3975\n",
      "Processed image 3976\n",
      "Processed image 3977\n",
      "Processed image 3978\n",
      "Processed image 3979\n",
      "Processed image 3980\n",
      "Processed image 3981\n",
      "Processed image 3982\n",
      "Processed image 3983\n",
      "Processed image 3984\n",
      "Processed image 3985\n",
      "Processed image 3986\n",
      "Processed image 3987\n",
      "Processed image 3988\n",
      "Processed image 3989\n",
      "Processed image 3990\n",
      "Processed image 3991\n",
      "Processed image 3992\n",
      "Processed image 3993\n",
      "Processed image 3994\n",
      "Processed image 3995\n",
      "Processed image 3996\n",
      "Processed image 3997\n",
      "Processed image 3998\n",
      "Processed image 3999\n",
      "Processed image 4000\n",
      "Processed image 4001\n",
      "Processed image 4002\n",
      "Processed image 4003\n",
      "Processed image 4004\n",
      "Processed image 4005\n",
      "Processed image 4006\n",
      "Processed image 4007\n",
      "Processed image 4008\n",
      "Processed image 4009\n",
      "Processed image 4010\n",
      "Processed image 4011\n",
      "Processed image 4012\n",
      "Processed image 4013\n",
      "Processed image 4014\n",
      "Processed image 4015\n",
      "Processed image 4016\n",
      "Processed image 4017\n",
      "Processed image 4018\n",
      "Processed image 4019\n",
      "Processed image 4020\n",
      "Processed image 4021\n",
      "Processed image 4022\n",
      "Processed image 4023\n",
      "Processed image 4024\n",
      "Processed image 4025\n",
      "Processed image 4026\n",
      "Processed image 4027\n",
      "Processed image 4028\n",
      "Processed image 4029\n",
      "Processed image 4030\n",
      "Processed image 4031\n",
      "Processed image 4032\n",
      "Processed image 4033\n",
      "Processed image 4034\n",
      "Processed image 4035\n",
      "Processed image 4036\n",
      "Processed image 4037\n",
      "Processed image 4038\n",
      "Processed image 4039\n",
      "Processed image 4040\n",
      "Processed image 4041\n",
      "Processed image 4042\n",
      "Processed image 4043\n",
      "Processed image 4044\n",
      "Processed image 4045\n",
      "Processed image 4046\n",
      "Processed image 4047\n",
      "Processed image 4048\n",
      "Processed image 4049\n",
      "Processed image 4050\n",
      "Processed image 4051\n",
      "Processed image 4052\n",
      "Processed image 4053\n",
      "Processed image 4054\n",
      "Processed image 4055\n",
      "Processed image 4056\n",
      "Processed image 4057\n",
      "Processed image 4058\n",
      "Processed image 4059\n",
      "Processed image 4060\n",
      "Processed image 4061\n",
      "Processed image 4062\n",
      "Processed image 4063\n",
      "Processed image 4064\n",
      "Processed image 4065\n",
      "Processed image 4066\n",
      "Processed image 4067\n",
      "Processed image 4068\n",
      "Processed image 4069\n",
      "Processed image 4070\n",
      "Processed image 4071\n",
      "Processed image 4072\n",
      "Processed image 4073\n",
      "Processed image 4074\n",
      "Processed image 4075\n",
      "Processed image 4076\n",
      "Processed image 4077\n",
      "Processed image 4078\n",
      "Processed image 4079\n",
      "Processed image 4080\n",
      "Processed image 4081\n",
      "Processed image 4082\n",
      "Processed image 4083\n",
      "Processed image 4084\n",
      "Processed image 4085\n",
      "Processed image 4086\n",
      "Processed image 4087\n",
      "Processed image 4088\n",
      "Processed image 4089\n",
      "Processed image 4090\n",
      "Processed image 4091\n",
      "Processed image 4092\n",
      "Processed image 4093\n",
      "Processed image 4094\n",
      "Processed image 4095\n",
      "Processed image 4096\n",
      "Processed image 4097\n",
      "Processed image 4098\n",
      "Processed image 4099\n",
      "Processed image 4100\n",
      "Processed image 4101\n",
      "Processed image 4102\n",
      "Processed image 4103\n",
      "Processed image 4104\n",
      "Processed image 4105\n",
      "Processed image 4106\n",
      "Processed image 4107\n",
      "Processed image 4108\n",
      "Processed image 4109\n",
      "Processed image 4110\n",
      "Processed image 4111\n",
      "Processed image 4112\n",
      "Processed image 4113\n",
      "Processed image 4114\n",
      "Processed image 4115\n",
      "Processed image 4116\n",
      "Processed image 4117\n",
      "Processed image 4118\n",
      "Processed image 4119\n",
      "Processed image 4120\n",
      "Processed image 4121\n",
      "Processed image 4122\n",
      "Processed image 4123\n",
      "Processed image 4124\n",
      "Processed image 4125\n",
      "Processed image 4126\n",
      "Processed image 4127\n",
      "Processed image 4128\n",
      "Processed image 4129\n",
      "Processed image 4130\n",
      "Processed image 4131\n",
      "Processed image 4132\n",
      "Processed image 4133\n",
      "Processed image 4134\n",
      "Processed image 4135\n",
      "Processed image 4136\n",
      "Processed image 4137\n",
      "Processed image 4138\n",
      "Processed image 4139\n",
      "Processed image 4140\n",
      "Processed image 4141\n",
      "Processed image 4142\n",
      "Processed image 4143\n",
      "Processed image 4144\n",
      "Processed image 4145\n",
      "Processed image 4146\n",
      "Processed image 4147\n",
      "Processed image 4148\n",
      "Processed image 4149\n",
      "Processed image 4150\n",
      "Processed image 4151\n",
      "Processed image 4152\n",
      "Processed image 4153\n",
      "Processed image 4154\n",
      "Processed image 4155\n",
      "Processed image 4156\n",
      "Processed image 4157\n",
      "Processed image 4158\n",
      "Processed image 4159\n",
      "Processed image 4160\n",
      "Processed image 4161\n",
      "Processed image 4162\n",
      "Processed image 4163\n",
      "Processed image 4164\n",
      "Processed image 4165\n",
      "Processed image 4166\n",
      "Processed image 4167\n",
      "Processed image 4168\n",
      "Processed image 4169\n",
      "Processed image 4170\n",
      "Processed image 4171\n",
      "Processed image 4172\n",
      "Processed image 4173\n",
      "Processed image 4174\n",
      "Processed image 4175\n",
      "Processed image 4176\n",
      "Processed image 4177\n",
      "Processed image 4178\n",
      "Processed image 4179\n",
      "Processed image 4180\n",
      "Processed image 4181\n",
      "Processed image 4182\n",
      "Processed image 4183\n",
      "Processed image 4184\n",
      "Processed image 4185\n",
      "Processed image 4186\n",
      "Processed image 4187\n",
      "Processed image 4188\n",
      "Processed image 4189\n",
      "Processed image 4190\n",
      "Processed image 4191\n",
      "Processed image 4192\n",
      "Processed image 4193\n",
      "Processed image 4194\n",
      "Processed image 4195\n",
      "Processed image 4196\n",
      "Processed image 4197\n",
      "Processed image 4198\n",
      "Processed image 4199\n",
      "Processed image 4200\n",
      "Processed image 4201\n",
      "Processed image 4202\n",
      "Processed image 4203\n",
      "Processed image 4204\n",
      "Processed image 4205\n",
      "Processed image 4206\n",
      "Processed image 4207\n",
      "Processed image 4208\n",
      "Processed image 4209\n",
      "Processed image 4210\n",
      "Processed image 4211\n",
      "Processed image 4212\n",
      "Processed image 4213\n",
      "Processed image 4214\n",
      "Processed image 4215\n",
      "Processed image 4216\n",
      "Processed image 4217\n",
      "Processed image 4218\n",
      "Processed image 4219\n",
      "Processed image 4220\n",
      "Processed image 4221\n",
      "Processed image 4222\n",
      "Processed image 4223\n",
      "Processed image 4224\n",
      "Processed image 4225\n",
      "Processed image 4226\n",
      "Processed image 4227\n",
      "Processed image 4228\n",
      "Processed image 4229\n",
      "Processed image 4230\n",
      "Processed image 4231\n",
      "Processed image 4232\n",
      "Processed image 4233\n",
      "Processed image 4234\n",
      "Processed image 4235\n",
      "Processed image 4236\n",
      "Processed image 4237\n",
      "Processed image 4238\n",
      "Processed image 4239\n",
      "Processed image 4240\n",
      "Processed image 4241\n",
      "Processed image 4242\n",
      "Processed image 4243\n",
      "Processed image 4244\n",
      "Processed image 4245\n",
      "Processed image 4246\n",
      "Processed image 4247\n",
      "Processed image 4248\n",
      "Processed image 4249\n",
      "Processed image 4250\n",
      "Processed image 4251\n",
      "Processed image 4252\n",
      "Processed image 4253\n",
      "Processed image 4254\n",
      "Processed image 4255\n",
      "Processed image 4256\n",
      "Processed image 4257\n",
      "Processed image 4258\n",
      "Processed image 4259\n",
      "Processed image 4260\n",
      "Processed image 4261\n",
      "Processed image 4262\n",
      "Processed image 4263\n",
      "Processed image 4264\n",
      "Processed image 4265\n",
      "Processed image 4266\n",
      "Processed image 4267\n",
      "Processed image 4268\n",
      "Processed image 4269\n",
      "Processed image 4270\n",
      "Processed image 4271\n",
      "Processed image 4272\n",
      "Processed image 4273\n",
      "Processed image 4274\n",
      "Processed image 4275\n",
      "Processed image 4276\n",
      "Processed image 4277\n",
      "Processed image 4278\n",
      "Processed image 4279\n",
      "Processed image 4280\n",
      "Processed image 4281\n",
      "Processed image 4282\n",
      "Processed image 4283\n",
      "Processed image 4284\n",
      "Processed image 4285\n",
      "Processed image 4286\n",
      "Processed image 4287\n",
      "Processed image 4288\n",
      "Processed image 4289\n",
      "Processed image 4290\n",
      "Processed image 4291\n",
      "Processed image 4292\n",
      "Processed image 4293\n",
      "Processed image 4294\n",
      "Processed image 4295\n",
      "Processed image 4296\n",
      "Processed image 4297\n",
      "Processed image 4298\n",
      "Processed image 4299\n",
      "Processed image 4300\n",
      "Processed image 4301\n",
      "Processed image 4302\n",
      "Processed image 4303\n",
      "Processed image 4304\n",
      "Processed image 4305\n",
      "Processed image 4306\n",
      "Processed image 4307\n",
      "Processed image 4308\n",
      "Processed image 4309\n",
      "Processed image 4310\n",
      "Processed image 4311\n",
      "Processed image 4312\n",
      "Processed image 4313\n",
      "Processed image 4314\n",
      "Processed image 4315\n",
      "Processed image 4316\n",
      "Processed image 4317\n",
      "Processed image 4318\n",
      "Processed image 4319\n",
      "Processed image 4320\n",
      "Processed image 4321\n",
      "Processed image 4322\n",
      "Processed image 4323\n",
      "Processed image 4324\n",
      "Processed image 4325\n",
      "Processed image 4326\n",
      "Processed image 4327\n",
      "Processed image 4328\n",
      "Processed image 4329\n",
      "Processed image 4330\n",
      "Processed image 4331\n",
      "Processed image 4332\n",
      "Processed image 4333\n",
      "Processed image 4334\n",
      "Processed image 4335\n",
      "Processed image 4336\n",
      "Processed image 4337\n",
      "Processed image 4338\n",
      "Processed image 4339\n",
      "Processed image 4340\n",
      "Processed image 4341\n",
      "Processed image 4342\n",
      "Processed image 4343\n",
      "Processed image 4344\n",
      "Processed image 4345\n",
      "Processed image 4346\n",
      "Processed image 4347\n",
      "Processed image 4348\n",
      "Processed image 4349\n",
      "Processed image 4350\n",
      "Processed image 4351\n",
      "Processed image 4352\n",
      "Processed image 4353\n",
      "Processed image 4354\n",
      "Processed image 4355\n",
      "Processed image 4356\n",
      "Processed image 4357\n",
      "Processed image 4358\n",
      "Processed image 4359\n",
      "Processed image 4360\n",
      "Processed image 4361\n",
      "Processed image 4362\n",
      "Processed image 4363\n",
      "Processed image 4364\n",
      "Processed image 4365\n",
      "Processed image 4366\n",
      "Processed image 4367\n",
      "Processed image 4368\n",
      "Processed image 4369\n",
      "Processed image 4370\n",
      "Processed image 4371\n",
      "Processed image 4372\n",
      "Processed image 4373\n",
      "Processed image 4374\n",
      "Processed image 4375\n",
      "Processed image 4376\n",
      "Processed image 4377\n",
      "Processed image 4378\n",
      "Processed image 4379\n",
      "Processed image 4380\n",
      "Processed image 4381\n",
      "Processed image 4382\n",
      "Processed image 4383\n",
      "Processed image 4384\n",
      "Processed image 4385\n",
      "Processed image 4386\n",
      "Processed image 4387\n",
      "Processed image 4388\n",
      "Processed image 4389\n",
      "Processed image 4390\n",
      "Processed image 4391\n",
      "Processed image 4392\n",
      "Processed image 4393\n",
      "Processed image 4394\n",
      "Processed image 4395\n",
      "Processed image 4396\n",
      "Processed image 4397\n",
      "Processed image 4398\n",
      "Processed image 4399\n",
      "Processed image 4400\n",
      "Processed image 4401\n",
      "Processed image 4402\n",
      "Processed image 4403\n",
      "Processed image 4404\n",
      "Processed image 4405\n",
      "Processed image 4406\n",
      "Processed image 4407\n",
      "Processed image 4408\n",
      "Processed image 4409\n",
      "Processed image 4410\n",
      "Processed image 4411\n",
      "Processed image 4412\n",
      "Processed image 4413\n",
      "Processed image 4414\n",
      "Processed image 4415\n",
      "Processed image 4416\n",
      "Processed image 4417\n",
      "Processed image 4418\n",
      "Processed image 4419\n",
      "Processed image 4420\n",
      "Processed image 4421\n",
      "Processed image 4422\n",
      "Processed image 4423\n",
      "Processed image 4424\n",
      "Processed image 4425\n",
      "Processed image 4426\n",
      "Processed image 4427\n",
      "Processed image 4428\n",
      "Processed image 4429\n",
      "Processed image 4430\n",
      "Processed image 4431\n",
      "Processed image 4432\n",
      "Processed image 4433\n",
      "Processed image 4434\n",
      "Processed image 4435\n",
      "Processed image 4436\n",
      "Processed image 4437\n",
      "Processed image 4438\n",
      "Processed image 4439\n",
      "Processed image 4440\n",
      "Processed image 4441\n",
      "Processed image 4442\n",
      "Processed image 4443\n",
      "Processed image 4444\n",
      "Processed image 4445\n",
      "Processed image 4446\n",
      "Processed image 4447\n",
      "Processed image 4448\n",
      "Processed image 4449\n",
      "Processed image 4450\n",
      "Processed image 4451\n",
      "Processed image 4452\n",
      "Processed image 4453\n",
      "Processed image 4454\n",
      "Processed image 4455\n",
      "Processed image 4456\n",
      "Processed image 4457\n",
      "Processed image 4458\n",
      "Processed image 4459\n",
      "Processed image 4460\n",
      "Processed image 4461\n",
      "Processed image 4462\n",
      "Processed image 4463\n",
      "Processed image 4464\n",
      "Processed image 4465\n",
      "Processed image 4466\n",
      "Processed image 4467\n",
      "Processed image 4468\n",
      "Processed image 4469\n",
      "Processed image 4470\n",
      "Processed image 4471\n",
      "Processed image 4472\n",
      "Processed image 4473\n",
      "Processed image 4474\n",
      "Processed image 4475\n",
      "Processed image 4476\n",
      "Processed image 4477\n",
      "Processed image 4478\n",
      "Processed image 4479\n",
      "Processed image 4480\n",
      "Processed image 4481\n",
      "Processed image 4482\n",
      "Processed image 4483\n",
      "Processed image 4484\n",
      "Processed image 4485\n",
      "Processed image 4486\n",
      "Processed image 4487\n",
      "Processed image 4488\n",
      "Processed image 4489\n",
      "Processed image 4490\n",
      "Processed image 4491\n",
      "Processed image 4492\n",
      "Processed image 4493\n",
      "Processed image 4494\n",
      "Processed image 4495\n",
      "Processed image 4496\n",
      "Processed image 4497\n",
      "Processed image 4498\n",
      "Processed image 4499\n",
      "Processed image 4500\n",
      "Processed image 4501\n",
      "Processed image 4502\n",
      "Processed image 4503\n",
      "Processed image 4504\n",
      "Processed image 4505\n",
      "Processed image 4506\n",
      "Processed image 4507\n",
      "Processed image 4508\n",
      "Processed image 4509\n",
      "Processed image 4510\n",
      "Processed image 4511\n",
      "Processed image 4512\n",
      "Processed image 4513\n",
      "Processed image 4514\n",
      "Processed image 4515\n",
      "Processed image 4516\n",
      "Processed image 4517\n",
      "Processed image 4518\n",
      "Processed image 4519\n",
      "Processed image 4520\n",
      "Processed image 4521\n",
      "Processed image 4522\n",
      "Processed image 4523\n",
      "Processed image 4524\n",
      "Processed image 4525\n",
      "Processed image 4526\n",
      "Processed image 4527\n",
      "Processed image 4528\n",
      "Processed image 4529\n",
      "Processed image 4530\n",
      "Processed image 4531\n",
      "Processed image 4532\n",
      "Processed image 4533\n",
      "Processed image 4534\n",
      "Processed image 4535\n",
      "Processed image 4536\n",
      "Processed image 4537\n",
      "Processed image 4538\n",
      "Processed image 4539\n",
      "Processed image 4540\n",
      "Processed image 4541\n",
      "Processed image 4542\n",
      "Processed image 4543\n",
      "Processed image 4544\n",
      "Processed image 4545\n",
      "Processed image 4546\n",
      "Processed image 4547\n",
      "Processed image 4548\n",
      "Processed image 4549\n",
      "Processed image 4550\n",
      "Processed image 4551\n",
      "Processed image 4552\n",
      "Processed image 4553\n",
      "Processed image 4554\n",
      "Processed image 4555\n",
      "Processed image 4556\n",
      "Processed image 4557\n",
      "Processed image 4558\n",
      "Processed image 4559\n",
      "Processed image 4560\n",
      "Processed image 4561\n",
      "Processed image 4562\n",
      "Processed image 4563\n",
      "Processed image 4564\n",
      "Processed image 4565\n",
      "Processed image 4566\n",
      "Processed image 4567\n",
      "Processed image 4568\n",
      "Processed image 4569\n",
      "Processed image 4570\n",
      "Processed image 4571\n",
      "Processed image 4572\n",
      "Processed image 4573\n",
      "Processed image 4574\n",
      "Processed image 4575\n",
      "Processed image 4576\n",
      "Processed image 4577\n",
      "Processed image 4578\n",
      "Processed image 4579\n",
      "Processed image 4580\n",
      "Processed image 4581\n",
      "Processed image 4582\n",
      "Processed image 4583\n",
      "Processed image 4584\n",
      "Processed image 4585\n",
      "Processed image 4586\n",
      "Processed image 4587\n",
      "Processed image 4588\n",
      "Processed image 4589\n",
      "Processed image 4590\n",
      "Processed image 4591\n",
      "Processed image 4592\n",
      "Processed image 4593\n",
      "Processed image 4594\n",
      "Processed image 4595\n",
      "Processed image 4596\n",
      "Processed image 4597\n",
      "Processed image 4598\n",
      "Processed image 4599\n",
      "Processed image 4600\n",
      "Processed image 4601\n",
      "Processed image 4602\n",
      "Processed image 4603\n",
      "Processed image 4604\n",
      "Processed image 4605\n",
      "Processed image 4606\n",
      "Processed image 4607\n",
      "Processed image 4608\n",
      "Processed image 4609\n",
      "Processed image 4610\n",
      "Processed image 4611\n",
      "Processed image 4612\n",
      "Processed image 4613\n",
      "Processed image 4614\n",
      "Processed image 4615\n",
      "Processed image 4616\n",
      "Processed image 4617\n",
      "Processed image 4618\n",
      "Processed image 4619\n",
      "Processed image 4620\n",
      "Processed image 4621\n",
      "Processed image 4622\n",
      "Processed image 4623\n",
      "Processed image 4624\n",
      "Processed image 4625\n",
      "Processed image 4626\n",
      "Processed image 4627\n",
      "Processed image 4628\n",
      "Processed image 4629\n",
      "Processed image 4630\n",
      "Processed image 4631\n",
      "Processed image 4632\n",
      "Processed image 4633\n",
      "Processed image 4634\n",
      "Processed image 4635\n",
      "Processed image 4636\n",
      "Processed image 4637\n",
      "Processed image 4638\n",
      "Processed image 4639\n",
      "Processed image 4640\n",
      "Processed image 4641\n",
      "Processed image 4642\n",
      "Processed image 4643\n",
      "Processed image 4644\n",
      "Processed image 4645\n",
      "Processed image 4646\n",
      "Processed image 4647\n",
      "Processed image 4648\n",
      "Processed image 4649\n",
      "Processed image 4650\n",
      "Processed image 4651\n",
      "Processed image 4652\n",
      "Processed image 4653\n",
      "Processed image 4654\n",
      "Processed image 4655\n",
      "Processed image 4656\n",
      "Processed image 4657\n",
      "Processed image 4658\n",
      "Processed image 4659\n",
      "Processed image 4660\n",
      "Processed image 4661\n",
      "Processed image 4662\n",
      "Processed image 4663\n",
      "Processed image 4664\n",
      "Processed image 4665\n",
      "Processed image 4666\n",
      "Processed image 4667\n",
      "Processed image 4668\n",
      "Processed image 4669\n",
      "Processed image 4670\n",
      "Processed image 4671\n",
      "Processed image 4672\n",
      "Processed image 4673\n",
      "Processed image 4674\n",
      "Processed image 4675\n",
      "Processed image 4676\n",
      "Processed image 4677\n",
      "Processed image 4678\n",
      "Processed image 4679\n",
      "Processed image 4680\n",
      "Processed image 4681\n",
      "Processed image 4682\n",
      "Processed image 4683\n",
      "Processed image 4684\n",
      "Processed image 4685\n",
      "Processed image 4686\n",
      "Processed image 4687\n",
      "Processed image 4688\n",
      "Processed image 4689\n",
      "Processed image 4690\n",
      "Processed image 4691\n",
      "Processed image 4692\n",
      "Processed image 4693\n",
      "Processed image 4694\n",
      "Processed image 4695\n",
      "Processed image 4696\n",
      "Processed image 4697\n",
      "Processed image 4698\n",
      "Processed image 4699\n",
      "Processed image 4700\n",
      "Processed image 4701\n",
      "Processed image 4702\n",
      "Processed image 4703\n",
      "Processed image 4704\n",
      "Processed image 4705\n",
      "Processed image 4706\n",
      "Processed image 4707\n",
      "Processed image 4708\n",
      "Processed image 4709\n",
      "Processed image 4710\n",
      "Processed image 4711\n",
      "Processed image 4712\n",
      "Processed image 4713\n",
      "Processed image 4714\n",
      "Processed image 4715\n",
      "Processed image 4716\n",
      "Processed image 4717\n",
      "Processed image 4718\n",
      "Processed image 4719\n",
      "Processed image 4720\n",
      "Processed image 4721\n",
      "Processed image 4722\n",
      "Processed image 4723\n",
      "Processed image 4724\n",
      "Processed image 4725\n",
      "Processed image 4726\n",
      "Processed image 4727\n",
      "Processed image 4728\n",
      "Processed image 4729\n",
      "Processed image 4730\n",
      "Processed image 4731\n",
      "Processed image 4732\n",
      "Processed image 4733\n",
      "Processed image 4734\n",
      "Processed image 4735\n",
      "Processed image 4736\n",
      "Processed image 4737\n",
      "Processed image 4738\n",
      "Processed image 4739\n",
      "Processed image 4740\n",
      "Processed image 4741\n",
      "Processed image 4742\n",
      "Processed image 4743\n",
      "Processed image 4744\n",
      "Processed image 4745\n",
      "Processed image 4746\n",
      "Processed image 4747\n",
      "Processed image 4748\n",
      "Processed image 4749\n",
      "Processed image 4750\n",
      "Processed image 4751\n",
      "Processed image 4752\n",
      "Processed image 4753\n",
      "Processed image 4754\n",
      "Processed image 4755\n",
      "Processed image 4756\n",
      "Processed image 4757\n",
      "Processed image 4758\n",
      "Processed image 4759\n",
      "Processed image 4760\n",
      "Processed image 4761\n",
      "Processed image 4762\n",
      "Processed image 4763\n",
      "Processed image 4764\n",
      "Processed image 4765\n",
      "Processed image 4766\n",
      "Processed image 4767\n",
      "Processed image 4768\n",
      "Processed image 4769\n",
      "Processed image 4770\n",
      "Processed image 4771\n",
      "Processed image 4772\n",
      "Processed image 4773\n",
      "Processed image 4774\n",
      "Processed image 4775\n",
      "Processed image 4776\n",
      "Processed image 4777\n",
      "Processed image 4778\n",
      "Processed image 4779\n",
      "Processed image 4780\n",
      "Processed image 4781\n",
      "Processed image 4782\n",
      "Processed image 4783\n",
      "Processed image 4784\n",
      "Processed image 4785\n",
      "Processed image 4786\n",
      "Processed image 4787\n",
      "Processed image 4788\n",
      "Processed image 4789\n",
      "Processed image 4790\n",
      "Processed image 4791\n",
      "Processed image 4792\n",
      "Processed image 4793\n",
      "Processed image 4794\n",
      "Processed image 4795\n",
      "Processed image 4796\n",
      "Processed image 4797\n",
      "Processed image 4798\n",
      "Processed image 4799\n",
      "Processed image 4800\n",
      "Processed image 4801\n",
      "Processed image 4802\n",
      "Processed image 4803\n",
      "Processed image 4804\n",
      "Processed image 4805\n",
      "Processed image 4806\n",
      "Processed image 4807\n",
      "Processed image 4808\n",
      "Processed image 4809\n",
      "Processed image 4810\n",
      "Processed image 4811\n",
      "Processed image 4812\n",
      "Processed image 4813\n",
      "Processed image 4814\n",
      "Processed image 4815\n",
      "Processed image 4816\n",
      "Processed image 4817\n",
      "Processed image 4818\n",
      "Processed image 4819\n",
      "Processed image 4820\n",
      "Processed image 4821\n",
      "Processed image 4822\n",
      "Processed image 4823\n",
      "Processed image 4824\n",
      "Processed image 4825\n",
      "Processed image 4826\n",
      "Processed image 4827\n",
      "Processed image 4828\n",
      "Processed image 4829\n",
      "Processed image 4830\n",
      "Processed image 4831\n",
      "Processed image 4832\n",
      "Processed image 4833\n",
      "Processed image 4834\n",
      "Processed image 4835\n",
      "Processed image 4836\n",
      "Processed image 4837\n",
      "Processed image 4838\n",
      "Processed image 4839\n",
      "Processed image 4840\n",
      "Processed image 4841\n",
      "Processed image 4842\n",
      "Processed image 4843\n",
      "Processed image 4844\n",
      "Processed image 4845\n",
      "Processed image 4846\n",
      "Processed image 4847\n",
      "Processed image 4848\n",
      "Processed image 4849\n",
      "Processed image 4850\n",
      "Processed image 4851\n",
      "Processed image 4852\n",
      "Processed image 4853\n",
      "Processed image 4854\n",
      "Processed image 4855\n",
      "Processed image 4856\n",
      "Processed image 4857\n",
      "Processed image 4858\n",
      "Processed image 4859\n",
      "Processed image 4860\n",
      "Processed image 4861\n",
      "Processed image 4862\n",
      "Processed image 4863\n",
      "Processed image 4864\n",
      "Processed image 4865\n",
      "Processed image 4866\n",
      "Processed image 4867\n",
      "Processed image 4868\n",
      "Processed image 4869\n",
      "Processed image 4870\n",
      "Processed image 4871\n",
      "Processed image 4872\n",
      "Processed image 4873\n",
      "Processed image 4874\n",
      "Processed image 4875\n",
      "Processed image 4876\n",
      "Processed image 4877\n",
      "Processed image 4878\n",
      "Processed image 4879\n",
      "Processed image 4880\n",
      "Processed image 4881\n",
      "Processed image 4882\n",
      "Processed image 4883\n",
      "Processed image 4884\n",
      "Processed image 4885\n",
      "Processed image 4886\n",
      "Processed image 4887\n",
      "Processed image 4888\n",
      "Processed image 4889\n",
      "Processed image 4890\n",
      "Processed image 4891\n",
      "Processed image 4892\n",
      "Processed image 4893\n",
      "Processed image 4894\n",
      "Processed image 4895\n",
      "Processed image 4896\n",
      "Processed image 4897\n",
      "Processed image 4898\n",
      "Processed image 4899\n",
      "Processed image 4900\n",
      "Processed image 4901\n",
      "Processed image 4902\n",
      "Processed image 4903\n",
      "Processed image 4904\n",
      "Processed image 4905\n",
      "Processed image 4906\n",
      "Processed image 4907\n",
      "Processed image 4908\n",
      "Processed image 4909\n",
      "Processed image 4910\n",
      "Processed image 4911\n",
      "Processed image 4912\n",
      "Processed image 4913\n",
      "Processed image 4914\n",
      "Processed image 4915\n",
      "Processed image 4916\n",
      "Processed image 4917\n",
      "Processed image 4918\n",
      "Processed image 4919\n",
      "Processed image 4920\n",
      "Processed image 4921\n",
      "Processed image 4922\n",
      "Processed image 4923\n",
      "Processed image 4924\n",
      "Processed image 4925\n",
      "Processed image 4926\n",
      "Processed image 4927\n",
      "Processed image 4928\n",
      "Processed image 4929\n",
      "Processed image 4930\n",
      "Processed image 4931\n",
      "Processed image 4932\n",
      "Processed image 4933\n",
      "Processed image 4934\n",
      "Processed image 4935\n",
      "Processed image 4936\n",
      "Processed image 4937\n",
      "Processed image 4938\n",
      "Processed image 4939\n",
      "Processed image 4940\n",
      "Processed image 4941\n",
      "Processed image 4942\n",
      "Processed image 4943\n",
      "Processed image 4944\n",
      "Processed image 4945\n",
      "Processed image 4946\n",
      "Processed image 4947\n",
      "Processed image 4948\n",
      "Processed image 4949\n",
      "Processed image 4950\n",
      "Processed image 4951\n",
      "Processed image 4952\n",
      "Processed image 4953\n",
      "Processed image 4954\n",
      "Processed image 4955\n",
      "Processed image 4956\n",
      "Processed image 4957\n",
      "Processed image 4958\n",
      "Processed image 4959\n",
      "Processed image 4960\n",
      "Processed image 4961\n",
      "Processed image 4962\n",
      "Processed image 4963\n",
      "Processed image 4964\n",
      "Processed image 4965\n",
      "Processed image 4966\n",
      "Processed image 4967\n",
      "Processed image 4968\n",
      "Processed image 4969\n",
      "Processed image 4970\n",
      "Processed image 4971\n",
      "Processed image 4972\n",
      "Processed image 4973\n",
      "Processed image 4974\n",
      "Processed image 4975\n",
      "Processed image 4976\n",
      "Processed image 4977\n",
      "Processed image 4978\n",
      "Processed image 4979\n",
      "Processed image 4980\n",
      "Processed image 4981\n",
      "Processed image 4982\n",
      "Processed image 4983\n",
      "Processed image 4984\n",
      "Processed image 4985\n",
      "Processed image 4986\n",
      "Processed image 4987\n",
      "Processed image 4988\n",
      "Processed image 4989\n",
      "Processed image 4990\n",
      "Processed image 4991\n",
      "Processed image 4992\n",
      "Processed image 4993\n",
      "Processed image 4994\n",
      "Processed image 4995\n",
      "Processed image 4996\n",
      "Processed image 4997\n",
      "Processed image 4998\n",
      "Processed image 4999\n",
      "Processed image 5000\n",
      "Processed image 5001\n",
      "Processed image 5002\n",
      "Processed image 5003\n",
      "Processed image 5004\n",
      "Processed image 5005\n",
      "Processed image 5006\n",
      "Processed image 5007\n",
      "Processed image 5008\n",
      "Processed image 5009\n",
      "Processed image 5010\n",
      "Processed image 5011\n",
      "Processed image 5012\n",
      "Processed image 5013\n",
      "Processed image 5014\n",
      "Processed image 5015\n",
      "Processed image 5016\n",
      "Processed image 5017\n",
      "Processed image 5018\n",
      "Processed image 5019\n",
      "Processed image 5020\n",
      "Processed image 5021\n",
      "Processed image 5022\n",
      "Processed image 5023\n",
      "Processed image 5024\n",
      "Processed image 5025\n",
      "Processed image 5026\n",
      "Processed image 5027\n",
      "Processed image 5028\n",
      "Processed image 5029\n",
      "Processed image 5030\n",
      "Processed image 5031\n",
      "Processed image 5032\n",
      "Processed image 5033\n",
      "Processed image 5034\n",
      "Processed image 5035\n",
      "Processed image 5036\n",
      "Processed image 5037\n",
      "Processed image 5038\n",
      "Processed image 5039\n",
      "Processed image 5040\n",
      "Processed image 5041\n",
      "Processed image 5042\n",
      "Processed image 5043\n",
      "Processed image 5044\n",
      "Processed image 5045\n",
      "Processed image 5046\n",
      "Processed image 5047\n",
      "Processed image 5048\n",
      "Processed image 5049\n",
      "Processed image 5050\n",
      "Processed image 5051\n",
      "Processed image 5052\n",
      "Processed image 5053\n",
      "Processed image 5054\n",
      "Processed image 5055\n",
      "Processed image 5056\n",
      "Processed image 5057\n",
      "Processed image 5058\n",
      "Processed image 5059\n",
      "Processed image 5060\n",
      "Processed image 5061\n",
      "Processed image 5062\n",
      "Processed image 5063\n",
      "Processed image 5064\n",
      "Processed image 5065\n",
      "Processed image 5066\n",
      "Processed image 5067\n",
      "Processed image 5068\n",
      "Processed image 5069\n",
      "Processed image 5070\n",
      "Processed image 5071\n",
      "Processed image 5072\n",
      "Processed image 5073\n",
      "Processed image 5074\n",
      "Processed image 5075\n",
      "Processed image 5076\n",
      "Processed image 5077\n",
      "Processed image 5078\n",
      "Processed image 5079\n",
      "Processed image 5080\n",
      "Processed image 5081\n",
      "Processed image 5082\n",
      "Processed image 5083\n",
      "Processed image 5084\n",
      "Processed image 5085\n",
      "Processed image 5086\n",
      "Processed image 5087\n",
      "Processed image 5088\n",
      "Processed image 5089\n",
      "Processed image 5090\n",
      "Processed image 5091\n",
      "Processed image 5092\n",
      "Processed image 5093\n",
      "Processed image 5094\n",
      "Processed image 5095\n",
      "Processed image 5096\n",
      "Processed image 5097\n",
      "Processed image 5098\n",
      "Processed image 5099\n",
      "Processed image 5100\n",
      "Processed image 5101\n",
      "Processed image 5102\n",
      "Processed image 5103\n",
      "Processed image 5104\n",
      "Processed image 5105\n",
      "Processed image 5106\n",
      "Processed image 5107\n",
      "Processed image 5108\n",
      "Processed image 5109\n",
      "Processed image 5110\n",
      "Processed image 5111\n",
      "Processed image 5112\n",
      "Processed image 5113\n",
      "Processed image 5114\n",
      "Processed image 5115\n",
      "Processed image 5116\n",
      "Processed image 5117\n",
      "Processed image 5118\n",
      "Processed image 5119\n",
      "Processed image 5120\n",
      "Processed image 5121\n",
      "Processed image 5122\n",
      "Processed image 5123\n",
      "Processed image 5124\n",
      "Processed image 5125\n",
      "Processed image 5126\n",
      "Processed image 5127\n",
      "Processed image 5128\n",
      "Processed image 5129\n",
      "Processed image 5130\n",
      "Processed image 5131\n",
      "Processed image 5132\n",
      "Processed image 5133\n",
      "Processed image 5134\n",
      "Processed image 5135\n",
      "Processed image 5136\n",
      "Processed image 5137\n",
      "Processed image 5138\n",
      "Processed image 5139\n",
      "Processed image 5140\n",
      "Processed image 5141\n",
      "Processed image 5142\n",
      "Processed image 5143\n",
      "Processed image 5144\n",
      "Processed image 5145\n",
      "Processed image 5146\n",
      "Processed image 5147\n",
      "Processed image 5148\n",
      "Processed image 5149\n",
      "Processed image 5150\n",
      "Processed image 5151\n",
      "Processed image 5152\n",
      "Processed image 5153\n",
      "Processed image 5154\n",
      "Processed image 5155\n",
      "Processed image 5156\n",
      "Processed image 5157\n",
      "Processed image 5158\n",
      "Processed image 5159\n",
      "Processed image 5160\n",
      "Processed image 5161\n",
      "Processed image 5162\n",
      "Processed image 5163\n",
      "Processed image 5164\n",
      "Processed image 5165\n",
      "Processed image 5166\n",
      "Processed image 5167\n",
      "Processed image 5168\n",
      "Processed image 5169\n",
      "Processed image 5170\n",
      "Processed image 5171\n",
      "Processed image 5172\n",
      "Processed image 5173\n",
      "Processed image 5174\n",
      "Processed image 5175\n",
      "Processed image 5176\n",
      "Processed image 5177\n",
      "Processed image 5178\n",
      "Processed image 5179\n",
      "Processed image 5180\n",
      "Processed image 5181\n",
      "Processed image 5182\n",
      "Processed image 5183\n",
      "Processed image 5184\n",
      "Processed image 5185\n",
      "Processed image 5186\n",
      "Processed image 5187\n",
      "Processed image 5188\n",
      "Processed image 5189\n",
      "Processed image 5190\n",
      "Processed image 5191\n",
      "Processed image 5192\n",
      "Processed image 5193\n",
      "Processed image 5194\n",
      "Processed image 5195\n",
      "Processed image 5196\n",
      "Processed image 5197\n",
      "Processed image 5198\n",
      "Processed image 5199\n",
      "Processed image 5200\n",
      "Processed image 5201\n",
      "Processed image 5202\n",
      "Processed image 5203\n",
      "Processed image 5204\n",
      "Processed image 5205\n",
      "Processed image 5206\n",
      "Processed image 5207\n",
      "Processed image 5208\n",
      "Processed image 5209\n",
      "Processed image 5210\n",
      "Processed image 5211\n",
      "Processed image 5212\n",
      "Processed image 5213\n",
      "Processed image 5214\n",
      "Processed image 5215\n",
      "Processed image 5216\n",
      "Processed image 5217\n",
      "Processed image 5218\n",
      "Processed image 5219\n",
      "Processed image 5220\n",
      "Processed image 5221\n",
      "Processed image 5222\n",
      "Processed image 5223\n",
      "Processed image 5224\n",
      "Processed image 5225\n",
      "Processed image 5226\n",
      "Processed image 5227\n",
      "Processed image 5228\n",
      "Processed image 5229\n",
      "Processed image 5230\n",
      "Processed image 5231\n",
      "Processed image 5232\n",
      "Processed image 5233\n",
      "Processed image 5234\n",
      "Processed image 5235\n",
      "Processed image 5236\n",
      "Processed image 5237\n",
      "Processed image 5238\n",
      "Processed image 5239\n",
      "Processed image 5240\n",
      "Processed image 5241\n",
      "Processed image 5242\n",
      "Processed image 5243\n",
      "Processed image 5244\n",
      "Processed image 5245\n",
      "Processed image 5246\n",
      "Processed image 5247\n",
      "Processed image 5248\n",
      "Processed image 5249\n",
      "Processed image 5250\n",
      "Processed image 5251\n",
      "Processed image 5252\n",
      "Processed image 5253\n",
      "Processed image 5254\n",
      "Processed image 5255\n",
      "Processed image 5256\n",
      "Processed image 5257\n",
      "Processed image 5258\n",
      "Processed image 5259\n",
      "Processed image 5260\n",
      "Processed image 5261\n",
      "Processed image 5262\n",
      "Processed image 5263\n",
      "Processed image 5264\n",
      "Processed image 5265\n",
      "Processed image 5266\n",
      "Processed image 5267\n",
      "Processed image 5268\n",
      "Processed image 5269\n",
      "Processed image 5270\n",
      "Processed image 5271\n",
      "Processed image 5272\n",
      "Processed image 5273\n",
      "Processed image 5274\n",
      "Processed image 5275\n",
      "Processed image 5276\n",
      "Processed image 5277\n",
      "Processed image 5278\n",
      "Processed image 5279\n",
      "Processed image 5280\n",
      "Processed image 5281\n",
      "Processed image 5282\n",
      "Processed image 5283\n",
      "Processed image 5284\n",
      "Processed image 5285\n",
      "Processed image 5286\n",
      "Processed image 5287\n",
      "Processed image 5288\n",
      "Processed image 5289\n",
      "Processed image 5290\n",
      "Processed image 5291\n",
      "Processed image 5292\n",
      "Processed image 5293\n",
      "Processed image 5294\n",
      "Processed image 5295\n",
      "Processed image 5296\n",
      "Processed image 5297\n",
      "Processed image 5298\n",
      "Processed image 5299\n",
      "Processed image 5300\n",
      "Processed image 5301\n",
      "Processed image 5302\n",
      "Processed image 5303\n",
      "Processed image 5304\n",
      "Processed image 5305\n",
      "Processed image 5306\n",
      "Processed image 5307\n",
      "Processed image 5308\n",
      "Processed image 5309\n",
      "Processed image 5310\n",
      "Processed image 5311\n",
      "Processed image 5312\n",
      "Processed image 5313\n",
      "Processed image 5314\n",
      "Processed image 5315\n",
      "Processed image 5316\n",
      "Processed image 5317\n",
      "Processed image 5318\n",
      "Processed image 5319\n",
      "Processed image 5320\n",
      "Processed image 5321\n",
      "Processed image 5322\n",
      "Processed image 5323\n",
      "Processed image 5324\n",
      "Processed image 5325\n",
      "Processed image 5326\n",
      "Processed image 5327\n",
      "Processed image 5328\n",
      "Processed image 5329\n",
      "Processed image 5330\n",
      "Processed image 5331\n",
      "Processed image 5332\n",
      "Processed image 5333\n",
      "Processed image 5334\n",
      "Processed image 5335\n",
      "Processed image 5336\n",
      "Processed image 5337\n",
      "Processed image 5338\n",
      "Processed image 5339\n",
      "Processed image 5340\n",
      "Processed image 5341\n",
      "Processed image 5342\n",
      "Processed image 5343\n",
      "Processed image 5344\n",
      "Processed image 5345\n",
      "Processed image 5346\n",
      "Processed image 5347\n",
      "Processed image 5348\n",
      "Processed image 5349\n",
      "Processed image 5350\n",
      "Processed image 5351\n",
      "Processed image 5352\n",
      "Processed image 5353\n",
      "Processed image 5354\n",
      "Processed image 5355\n",
      "Processed image 5356\n",
      "Processed image 5357\n",
      "Processed image 5358\n",
      "Processed image 5359\n",
      "Processed image 5360\n",
      "Processed image 5361\n",
      "Processed image 5362\n",
      "Processed image 5363\n",
      "Processed image 5364\n",
      "Processed image 5365\n",
      "Processed image 5366\n",
      "Processed image 5367\n",
      "Processed image 5368\n",
      "Processed image 5369\n",
      "Processed image 5370\n",
      "Processed image 5371\n",
      "Processed image 5372\n",
      "Processed image 5373\n",
      "Processed image 5374\n",
      "Processed image 5375\n",
      "Processed image 5376\n",
      "Processed image 5377\n",
      "Processed image 5378\n",
      "Processed image 5379\n",
      "Processed image 5380\n",
      "Processed image 5381\n",
      "Processed image 5382\n",
      "Processed image 5383\n",
      "Processed image 5384\n",
      "Processed image 5385\n",
      "Processed image 5386\n",
      "Processed image 5387\n",
      "Processed image 5388\n",
      "Processed image 5389\n",
      "Processed image 5390\n",
      "Processed image 5391\n",
      "Processed image 5392\n",
      "Processed image 5393\n",
      "Processed image 5394\n",
      "Processed image 5395\n",
      "Processed image 5396\n",
      "Processed image 5397\n",
      "Processed image 5398\n",
      "Processed image 5399\n",
      "Processed image 5400\n",
      "Processed image 5401\n",
      "Processed image 5402\n",
      "Processed image 5403\n",
      "Processed image 5404\n",
      "Processed image 5405\n",
      "Processed image 5406\n",
      "Processed image 5407\n",
      "Processed image 5408\n",
      "Processed image 5409\n",
      "Processed image 5410\n",
      "Processed image 5411\n",
      "Processed image 5412\n",
      "Processed image 5413\n",
      "Processed image 5414\n",
      "Processed image 5415\n",
      "Processed image 5416\n",
      "Processed image 5417\n",
      "Processed image 5418\n",
      "Processed image 5419\n",
      "Processed image 5420\n",
      "Processed image 5421\n",
      "Processed image 5422\n",
      "Processed image 5423\n",
      "Processed image 5424\n",
      "Processed image 5425\n",
      "Processed image 5426\n",
      "Processed image 5427\n",
      "Processed image 5428\n",
      "Processed image 5429\n",
      "Processed image 5430\n",
      "Processed image 5431\n",
      "Processed image 5432\n",
      "Processed image 5433\n",
      "Processed image 5434\n",
      "Processed image 5435\n",
      "Processed image 5436\n",
      "Processed image 5437\n",
      "Processed image 5438\n",
      "Processed image 5439\n",
      "Processed image 5440\n",
      "Processed image 5441\n",
      "Processed image 5442\n",
      "Processed image 5443\n",
      "Processed image 5444\n",
      "Processed image 5445\n",
      "Processed image 5446\n",
      "Processed image 5447\n",
      "Processed image 5448\n",
      "Processed image 5449\n",
      "Processed image 5450\n",
      "Processed image 5451\n",
      "Processed image 5452\n",
      "Processed image 5453\n",
      "Processed image 5454\n",
      "Processed image 5455\n",
      "Processed image 5456\n",
      "Processed image 5457\n",
      "Processed image 5458\n",
      "Processed image 5459\n",
      "Processed image 5460\n",
      "Processed image 5461\n",
      "Processed image 5462\n",
      "Processed image 5463\n",
      "Processed image 5464\n",
      "Processed image 5465\n",
      "Processed image 5466\n",
      "Processed image 5467\n",
      "Processed image 5468\n",
      "Processed image 5469\n",
      "Processed image 5470\n",
      "Processed image 5471\n",
      "Processed image 5472\n",
      "Processed image 5473\n",
      "Processed image 5474\n",
      "Processed image 5475\n",
      "Processed image 5476\n",
      "Processed image 5477\n",
      "Processed image 5478\n",
      "Processed image 5479\n",
      "Processed image 5480\n",
      "Processed image 5481\n",
      "Processed image 5482\n",
      "Processed image 5483\n",
      "Processed image 5484\n",
      "Processed image 5485\n",
      "Processed image 5486\n",
      "Processed image 5487\n",
      "Processed image 5488\n",
      "Processed image 5489\n",
      "Processed image 5490\n",
      "Processed image 5491\n",
      "Processed image 5492\n",
      "Processed image 5493\n",
      "Processed image 5494\n",
      "Processed image 5495\n",
      "Processed image 5496\n",
      "Processed image 5497\n",
      "Processed image 5498\n",
      "Processed image 5499\n",
      "Processed image 5500\n",
      "Processed image 5501\n",
      "Processed image 5502\n",
      "Processed image 5503\n",
      "Processed image 5504\n",
      "Processed image 5505\n",
      "Processed image 5506\n",
      "Processed image 5507\n",
      "Processed image 5508\n",
      "Processed image 5509\n",
      "Processed image 5510\n",
      "Processed image 5511\n",
      "Processed image 5512\n",
      "Processed image 5513\n",
      "Processed image 5514\n",
      "Processed image 5515\n",
      "Processed image 5516\n",
      "Processed image 5517\n",
      "Processed image 5518\n",
      "Processed image 5519\n",
      "Processed image 5520\n",
      "Processed image 5521\n",
      "Processed image 5522\n",
      "Processed image 5523\n",
      "Processed image 5524\n",
      "Processed image 5525\n",
      "Processed image 5526\n",
      "Processed image 5527\n",
      "Processed image 5528\n",
      "Processed image 5529\n",
      "Processed image 5530\n",
      "Processed image 5531\n",
      "Processed image 5532\n",
      "Processed image 5533\n",
      "Processed image 5534\n",
      "Processed image 5535\n",
      "Processed image 5536\n",
      "Processed image 5537\n",
      "Processed image 5538\n",
      "Processed image 5539\n",
      "Processed image 5540\n",
      "Processed image 5541\n",
      "Processed image 5542\n",
      "Processed image 5543\n",
      "Processed image 5544\n",
      "Processed image 5545\n",
      "Processed image 5546\n",
      "Processed image 5547\n",
      "Processed image 5548\n",
      "Processed image 5549\n",
      "Processed image 5550\n",
      "Processed image 5551\n",
      "Processed image 5552\n",
      "Processed image 5553\n",
      "Processed image 5554\n",
      "Processed image 5555\n",
      "Processed image 5556\n",
      "Processed image 5557\n",
      "Processed image 5558\n",
      "Processed image 5559\n",
      "Processed image 5560\n",
      "Processed image 5561\n",
      "Processed image 5562\n",
      "Processed image 5563\n",
      "Processed image 5564\n",
      "Processed image 5565\n",
      "Processed image 5566\n",
      "Processed image 5567\n",
      "Processed image 5568\n",
      "Processed image 5569\n",
      "Processed image 5570\n",
      "Processed image 5571\n",
      "Processed image 5572\n",
      "Processed image 5573\n",
      "Processed image 5574\n",
      "Processed image 5575\n",
      "Processed image 5576\n",
      "Processed image 5577\n",
      "Processed image 5578\n",
      "Processed image 5579\n",
      "Processed image 5580\n",
      "Processed image 5581\n",
      "Processed image 5582\n",
      "Processed image 5583\n",
      "Processed image 5584\n",
      "Processed image 5585\n",
      "Processed image 5586\n",
      "Processed image 5587\n",
      "Processed image 5588\n",
      "Processed image 5589\n",
      "Processed image 5590\n",
      "Processed image 5591\n",
      "Processed image 5592\n",
      "Processed image 5593\n",
      "Processed image 5594\n",
      "Processed image 5595\n",
      "Processed image 5596\n",
      "Processed image 5597\n",
      "Processed image 5598\n",
      "Processed image 5599\n",
      "Processed image 5600\n",
      "Processed image 5601\n",
      "Processed image 5602\n",
      "Processed image 5603\n",
      "Processed image 5604\n",
      "Processed image 5605\n",
      "Processed image 5606\n",
      "Processed image 5607\n",
      "Processed image 5608\n",
      "Processed image 5609\n",
      "Processed image 5610\n",
      "Processed image 5611\n",
      "Processed image 5612\n",
      "Processed image 5613\n",
      "Processed image 5614\n",
      "Processed image 5615\n",
      "Processed image 5616\n",
      "Processed image 5617\n",
      "Processed image 5618\n",
      "Processed image 5619\n",
      "Processed image 5620\n",
      "Processed image 5621\n",
      "Processed image 5623\n",
      "Processed image 5624\n",
      "Processed image 5625\n",
      "Processed image 5626\n",
      "Processed image 5627\n",
      "Processed image 5628\n",
      "Processed image 5629\n",
      "Processed image 5630\n",
      "Processed image 5631\n",
      "Processed image 5632\n",
      "Processed image 5633\n",
      "Processed image 5634\n",
      "Processed image 5635\n",
      "Processed image 5636\n",
      "Processed image 5637\n",
      "Processed image 5638\n",
      "Processed image 5639\n",
      "Processed image 5640\n",
      "Processed image 5641\n",
      "Processed image 5642\n",
      "Processed image 5643\n",
      "Processed image 5644\n",
      "Processed image 5645\n",
      "Processed image 5646\n",
      "Processed image 5647\n",
      "Processed image 5648\n",
      "Processed image 5649\n",
      "Processed image 5650\n",
      "Processed image 5651\n",
      "Processed image 5652\n",
      "Processed image 5653\n",
      "Processed image 5654\n",
      "Processed image 5655\n",
      "Processed image 5656\n",
      "Processed image 5657\n",
      "Processed image 5658\n",
      "Processed image 5659\n",
      "Processed image 5660\n",
      "Processed image 5661\n",
      "Processed image 5662\n",
      "Processed image 5663\n",
      "Processed image 5664\n",
      "Processed image 5665\n",
      "Processed image 5666\n",
      "Processed image 5667\n",
      "Processed image 5668\n",
      "Processed image 5669\n",
      "Processed image 5670\n",
      "Processed image 5671\n",
      "Processed image 5672\n",
      "Processed image 5673\n",
      "Processed image 5674\n",
      "Processed image 5675\n",
      "Processed image 5676\n",
      "Processed image 5677\n",
      "Processed image 5678\n",
      "Processed image 5679\n",
      "Processed image 5680\n",
      "Processed image 5681\n",
      "Processed image 5682\n",
      "Processed image 5683\n",
      "Processed image 5684\n",
      "Processed image 5685\n",
      "Processed image 5686\n",
      "Processed image 5687\n",
      "Processed image 5688\n",
      "Processed image 5689\n",
      "Processed image 5690\n",
      "Processed image 5691\n",
      "Processed image 5692\n",
      "Processed image 5693\n",
      "Processed image 5694\n",
      "Processed image 5695\n",
      "Processed image 5696\n",
      "Processed image 5697\n",
      "Processed image 5698\n",
      "Processed image 5699\n",
      "Processed image 5700\n",
      "Processed image 5701\n",
      "Processed image 5702\n",
      "Processed image 5703\n",
      "Processed image 5704\n",
      "Processed image 5705\n",
      "Processed image 5706\n",
      "Processed image 5707\n",
      "Processed image 5708\n",
      "Processed image 5709\n",
      "Processed image 5710\n",
      "Processed image 5711\n",
      "Processed image 5712\n",
      "Processed image 5713\n",
      "Processed image 5714\n",
      "Processed image 5715\n",
      "Processed image 5716\n",
      "Processed image 5717\n",
      "Processed image 5718\n",
      "Processed image 5719\n",
      "Processed image 5720\n",
      "Processed image 5721\n",
      "Processed image 5722\n",
      "Processed image 5723\n",
      "Processed image 5724\n",
      "Processed image 5725\n",
      "Processed image 5726\n",
      "Processed image 5727\n",
      "Processed image 5728\n",
      "Processed image 5729\n",
      "Processed image 5730\n",
      "Processed image 5731\n",
      "Processed image 5732\n",
      "Processed image 5733\n",
      "Processed image 5734\n",
      "Processed image 5735\n",
      "Processed image 5736\n",
      "Processed image 5737\n",
      "Processed image 5738\n",
      "Processed image 5739\n",
      "Processed image 5740\n",
      "Processed image 5741\n",
      "Processed image 5742\n",
      "Processed image 5743\n",
      "Processed image 5744\n",
      "Processed image 5745\n",
      "Processed image 5746\n",
      "Processed image 5747\n",
      "Processed image 5748\n",
      "Processed image 5749\n",
      "Processed image 5750\n",
      "Processed image 5751\n",
      "Processed image 5752\n",
      "Processed image 5753\n",
      "Processed image 5754\n",
      "Processed image 5755\n",
      "Processed image 5756\n",
      "Processed image 5757\n",
      "Processed image 5758\n",
      "Processed image 5759\n",
      "Processed image 5760\n",
      "Processed image 5761\n",
      "Processed image 5762\n",
      "Processed image 5763\n",
      "Processed image 5764\n",
      "Processed image 5765\n",
      "Processed image 5766\n",
      "Processed image 5767\n",
      "Processed image 5768\n",
      "Processed image 5769\n",
      "Processed image 5770\n",
      "Processed image 5771\n",
      "Processed image 5772\n",
      "Processed image 5773\n",
      "Processed image 5774\n",
      "Processed image 5775\n",
      "Processed image 5776\n",
      "Processed image 5777\n",
      "Processed image 5778\n",
      "Processed image 5779\n",
      "Processed image 5780\n",
      "Processed image 5781\n",
      "Processed image 5782\n",
      "Processed image 5783\n",
      "Processed image 5784\n",
      "Processed image 5785\n",
      "Processed image 5786\n",
      "Processed image 5787\n",
      "Processed image 5788\n",
      "Processed image 5789\n",
      "Processed image 5790\n",
      "Processed image 5791\n",
      "Processed image 5792\n",
      "Processed image 5793\n",
      "Processed image 5794\n",
      "Processed image 5795\n",
      "Processed image 5796\n",
      "Processed image 5797\n",
      "Processed image 5798\n",
      "Processed image 5799\n",
      "Processed image 5800\n",
      "Processed image 5801\n",
      "Processed image 5802\n",
      "Processed image 5803\n",
      "Processed image 5804\n",
      "Processed image 5805\n",
      "Processed image 5806\n",
      "Processed image 5807\n",
      "Processed image 5808\n",
      "Processed image 5809\n",
      "Processed image 5810\n",
      "Processed image 5811\n",
      "Processed image 5812\n",
      "Processed image 5813\n",
      "Processed image 5814\n",
      "Processed image 5815\n",
      "Processed image 5816\n",
      "Processed image 5817\n",
      "Processed image 5818\n",
      "Processed image 5819\n",
      "Processed image 5820\n",
      "Processed image 5821\n",
      "Processed image 5822\n",
      "Processed image 5823\n",
      "Processed image 5824\n",
      "Processed image 5825\n",
      "Processed image 5826\n",
      "Processed image 5827\n",
      "Processed image 5828\n",
      "Processed image 5829\n",
      "Processed image 5830\n",
      "Processed image 5831\n",
      "Processed image 5832\n",
      "Processed image 5833\n",
      "Processed image 5834\n",
      "Processed image 5835\n",
      "Processed image 5836\n",
      "Processed image 5837\n",
      "Processed image 5838\n",
      "Processed image 5839\n",
      "Processed image 5840\n",
      "Processed image 5841\n",
      "Processed image 5842\n",
      "Processed image 5843\n",
      "Processed image 5844\n",
      "Processed image 5845\n",
      "Processed image 5846\n",
      "Processed image 5847\n",
      "Processed image 5848\n",
      "Processed image 5849\n",
      "Processed image 5850\n",
      "Processed image 5851\n",
      "Processed image 5852\n",
      "Processed image 5853\n",
      "Processed image 5854\n",
      "Processed image 5855\n",
      "Processed image 5856\n",
      "Processed image 5857\n",
      "Processed image 5858\n",
      "Processed image 5859\n",
      "Processed image 5860\n",
      "Processed image 5861\n",
      "Processed image 5862\n",
      "Processed image 5863\n",
      "Processed image 5864\n",
      "Processed image 5865\n",
      "Processed image 5866\n",
      "Processed image 5867\n",
      "Processed image 5868\n",
      "Processed image 5869\n",
      "Processed image 5870\n",
      "Processed image 5871\n",
      "Processed image 5872\n",
      "Processed image 5873\n",
      "Processed image 5874\n",
      "Processed image 5875\n",
      "Processed image 5876\n",
      "Processed image 5877\n",
      "Processed image 5878\n",
      "Processed image 5879\n",
      "Processed image 5880\n",
      "Processed image 5881\n",
      "Processed image 5882\n",
      "Processed image 5883\n",
      "Processed image 5884\n",
      "Processed image 5885\n",
      "Processed image 5886\n",
      "Processed image 5887\n",
      "Processed image 5888\n",
      "Processed image 5889\n",
      "Processed image 5890\n",
      "Processed image 5891\n",
      "Processed image 5892\n",
      "Processed image 5893\n",
      "Processed image 5894\n",
      "Processed image 5895\n",
      "Processed image 5896\n",
      "Processed image 5897\n",
      "Processed image 5898\n",
      "Processed image 5899\n",
      "Processed image 5900\n",
      "Processed image 5901\n",
      "Processed image 5902\n",
      "Processed image 5903\n",
      "Processed image 5904\n",
      "Processed image 5905\n",
      "Processed image 5906\n",
      "Processed image 5907\n",
      "Processed image 5908\n",
      "Processed image 5909\n",
      "Processed image 5910\n",
      "Processed image 5911\n",
      "Processed image 5912\n",
      "Processed image 5913\n",
      "Processed image 5914\n",
      "Processed image 5915\n",
      "Processed image 5916\n",
      "Processed image 5917\n",
      "Processed image 5918\n",
      "Processed image 5919\n",
      "Processed image 5920\n",
      "Processed image 5921\n",
      "Processed image 5922\n",
      "Processed image 5923\n",
      "Processed image 5924\n",
      "Processed image 5925\n",
      "Processed image 5926\n",
      "Processed image 5927\n",
      "Processed image 5928\n",
      "Processed image 5929\n",
      "Processed image 5930\n",
      "Processed image 5931\n",
      "Processed image 5932\n",
      "Processed image 5933\n",
      "Processed image 5934\n",
      "Processed image 5935\n",
      "Processed image 5936\n",
      "Processed image 5937\n",
      "Processed image 5938\n",
      "Processed image 5939\n",
      "Processed image 5940\n",
      "Processed image 5941\n",
      "Processed image 5942\n",
      "Processed image 5943\n",
      "Processed image 5944\n",
      "Processed image 5945\n",
      "Processed image 5946\n",
      "Processed image 5947\n",
      "Processed image 5948\n",
      "Processed image 5949\n",
      "Processed image 5950\n",
      "Processed image 5951\n",
      "Processed image 5952\n",
      "Processed image 5953\n",
      "Processed image 5954\n",
      "Processed image 5955\n",
      "Processed image 5956\n",
      "Processed image 5957\n",
      "Processed image 5958\n",
      "Processed image 5959\n",
      "Processed image 5960\n",
      "Processed image 5961\n",
      "Processed image 5962\n",
      "Processed image 5963\n",
      "Processed image 5964\n",
      "Processed image 5965\n",
      "Processed image 5966\n",
      "Processed image 5967\n",
      "Processed image 5968\n",
      "Processed image 5969\n",
      "Processed image 5970\n",
      "Processed image 5971\n",
      "Processed image 5972\n",
      "Processed image 5973\n",
      "Processed image 5974\n",
      "Processed image 5975\n",
      "Processed image 5976\n",
      "Processed image 5977\n",
      "Processed image 5978\n",
      "Processed image 5979\n",
      "Processed image 5980\n",
      "Processed image 5981\n",
      "Processed image 5982\n",
      "Processed image 5983\n",
      "Processed image 5984\n",
      "Processed image 5985\n",
      "Processed image 5986\n",
      "Processed image 5987\n",
      "Processed image 5988\n",
      "Processed image 5989\n",
      "Processed image 5990\n",
      "Processed image 5991\n",
      "Processed image 5992\n",
      "Processed image 5993\n",
      "Processed image 5994\n",
      "Processed image 5995\n",
      "Processed image 5996\n",
      "Processed image 5997\n",
      "Processed image 5998\n",
      "Processed image 5999\n",
      "Processed image 6000\n",
      "Processed image 6001\n",
      "Processed image 6002\n",
      "Processed image 6003\n",
      "Processed image 6004\n",
      "Processed image 6005\n",
      "Processed image 6006\n",
      "Processed image 6007\n",
      "Processed image 6008\n",
      "Processed image 6009\n",
      "Processed image 6010\n",
      "Processed image 6011\n",
      "Processed image 6012\n",
      "Processed image 6013\n",
      "Processed image 6014\n",
      "Processed image 6015\n",
      "Processed image 6016\n",
      "Processed image 6017\n",
      "Processed image 6018\n",
      "Processed image 6019\n",
      "Processed image 6020\n",
      "Processed image 6021\n",
      "Processed image 6022\n",
      "Processed image 6023\n",
      "Processed image 6024\n",
      "Processed image 6025\n",
      "Processed image 6026\n",
      "Processed image 6027\n",
      "Processed image 6028\n",
      "Processed image 6029\n",
      "Processed image 6030\n",
      "Processed image 6031\n",
      "Processed image 6032\n",
      "Processed image 6033\n",
      "Processed image 6034\n",
      "Processed image 6035\n",
      "Processed image 6036\n",
      "Processed image 6037\n",
      "Processed image 6038\n",
      "Processed image 6039\n",
      "Processed image 6040\n",
      "Processed image 6041\n",
      "Processed image 6042\n",
      "Processed image 6043\n",
      "Processed image 6044\n",
      "Processed image 6045\n",
      "Processed image 6046\n",
      "Processed image 6047\n",
      "Processed image 6048\n",
      "Processed image 6049\n",
      "Processed image 6050\n",
      "Processed image 6051\n",
      "Processed image 6052\n",
      "Processed image 6053\n",
      "Processed image 6054\n",
      "Processed image 6055\n",
      "Processed image 6056\n",
      "Processed image 6057\n",
      "Processed image 6058\n",
      "Processed image 6059\n",
      "Processed image 6060\n",
      "Processed image 6061\n",
      "Processed image 6062\n",
      "Processed image 6063\n",
      "Processed image 6064\n",
      "Processed image 6065\n",
      "Processed image 6066\n",
      "Processed image 6067\n",
      "Processed image 6068\n",
      "Processed image 6069\n",
      "Processed image 6070\n",
      "Processed image 6071\n",
      "Processed image 6072\n",
      "Processed image 6073\n",
      "Processed image 6074\n",
      "Processed image 6075\n",
      "Processed image 6076\n",
      "Processed image 6077\n",
      "Processed image 6078\n",
      "Processed image 6079\n",
      "Processed image 6080\n",
      "Processed image 6081\n",
      "Processed image 6082\n",
      "Processed image 6083\n",
      "Processed image 6084\n",
      "Processed image 6085\n",
      "Processed image 6086\n",
      "Processed image 6087\n",
      "Processed image 6088\n",
      "Processed image 6089\n",
      "Processed image 6090\n",
      "Processed image 6091\n",
      "Processed image 6092\n",
      "Processed image 6093\n",
      "Processed image 6094\n",
      "Processed image 6095\n",
      "Processed image 6096\n",
      "Processed image 6097\n",
      "Processed image 6098\n",
      "Processed image 6099\n",
      "Processed image 6100\n",
      "Processed image 6101\n",
      "Processed image 6102\n",
      "Processed image 6103\n",
      "Processed image 6104\n",
      "Processed image 6105\n",
      "Processed image 6106\n",
      "Processed image 6107\n",
      "Processed image 6108\n",
      "Processed image 6109\n",
      "Processed image 6110\n",
      "Processed image 6111\n",
      "Processed image 6112\n",
      "Processed image 6113\n",
      "Processed image 6114\n",
      "Processed image 6115\n",
      "Processed image 6116\n",
      "Processed image 6117\n",
      "Processed image 6118\n",
      "Processed image 6119\n",
      "Processed image 6120\n",
      "Processed image 6121\n",
      "Processed image 6122\n",
      "Processed image 6123\n",
      "Processed image 6124\n",
      "Processed image 6125\n",
      "Processed image 6126\n",
      "Processed image 6127\n",
      "Processed image 6128\n",
      "Processed image 6129\n",
      "Processed image 6130\n",
      "Processed image 6131\n",
      "Processed image 6132\n",
      "Processed image 6133\n",
      "Processed image 6134\n",
      "Processed image 6135\n",
      "Processed image 6136\n",
      "Processed image 6137\n",
      "Processed image 6138\n",
      "Processed image 6139\n",
      "Processed image 6140\n",
      "Processed image 6141\n",
      "Processed image 6142\n",
      "Processed image 6143\n",
      "Processed image 6144\n",
      "Processed image 6145\n",
      "Processed image 6146\n",
      "Processed image 6147\n",
      "Processed image 6148\n",
      "Processed image 6149\n",
      "Processed image 6150\n",
      "Processed image 6151\n",
      "Processed image 6152\n",
      "Processed image 6153\n",
      "Processed image 6154\n",
      "Processed image 6155\n",
      "Processed image 6156\n",
      "Processed image 6157\n",
      "Processed image 6158\n",
      "Processed image 6159\n",
      "Processed image 6160\n",
      "Processed image 6161\n",
      "Processed image 6162\n",
      "Processed image 6163\n",
      "Processed image 6164\n",
      "Processed image 6165\n",
      "Processed image 6166\n",
      "Processed image 6167\n",
      "Processed image 6168\n",
      "Processed image 6169\n",
      "Processed image 6170\n",
      "Processed image 6171\n",
      "Processed image 6172\n",
      "Processed image 6173\n",
      "Processed image 6174\n",
      "Processed image 6175\n",
      "Processed image 6176\n",
      "Processed image 6177\n",
      "Processed image 6178\n",
      "Processed image 6179\n",
      "Processed image 6180\n",
      "Processed image 6181\n",
      "Processed image 6182\n",
      "Processed image 6183\n",
      "Processed image 6184\n",
      "Processed image 6185\n",
      "Processed image 6186\n",
      "Processed image 6187\n",
      "Processed image 6188\n",
      "Processed image 6189\n",
      "Processed image 6190\n",
      "Processed image 6191\n",
      "Processed image 6192\n",
      "Processed image 6193\n",
      "Processed image 6194\n",
      "Processed image 6195\n",
      "Processed image 6196\n",
      "Processed image 6197\n",
      "Processed image 6198\n",
      "Processed image 6199\n",
      "Processed image 6200\n",
      "Processed image 6201\n",
      "Processed image 6202\n",
      "Processed image 6203\n",
      "Processed image 6204\n",
      "Processed image 6205\n",
      "Processed image 6206\n",
      "Processed image 6207\n",
      "Processed image 6208\n",
      "Processed image 6209\n",
      "Processed image 6210\n",
      "Processed image 6211\n",
      "Processed image 6212\n",
      "Processed image 6213\n",
      "Processed image 6214\n",
      "Processed image 6215\n",
      "Processed image 6216\n",
      "Processed image 6217\n",
      "Processed image 6218\n",
      "Processed image 6219\n",
      "Processed image 6220\n",
      "Processed image 6221\n",
      "Processed image 6222\n",
      "Processed image 6223\n",
      "Processed image 6224\n",
      "Processed image 6225\n",
      "Processed image 6226\n",
      "Processed image 6227\n",
      "Processed image 6228\n",
      "Processed image 6229\n",
      "Processed image 6230\n",
      "Processed image 6231\n",
      "Processed image 6232\n",
      "Processed image 6233\n",
      "Processed image 6234\n",
      "Processed image 6235\n",
      "Processed image 6236\n",
      "Processed image 6237\n",
      "Processed image 6238\n",
      "Processed image 6239\n",
      "Processed image 6240\n",
      "Processed image 6241\n",
      "Processed image 6242\n",
      "Processed image 6243\n",
      "Processed image 6244\n",
      "Processed image 6245\n",
      "Processed image 6246\n",
      "Processed image 6247\n",
      "Processed image 6248\n",
      "Processed image 6249\n",
      "Processed image 6250\n",
      "Processed image 6251\n",
      "Processed image 6252\n",
      "Processed image 6253\n",
      "Processed image 6254\n",
      "Processed image 6255\n",
      "Processed image 6256\n",
      "Processed image 6257\n",
      "Processed image 6258\n",
      "Processed image 6259\n",
      "Processed image 6260\n",
      "Processed image 6261\n",
      "Processed image 6262\n",
      "Processed image 6263\n",
      "Processed image 6264\n",
      "Processed image 6265\n",
      "Processed image 6266\n",
      "Processed image 6267\n",
      "Processed image 6268\n",
      "Processed image 6269\n",
      "Processed image 6270\n",
      "Processed image 6271\n",
      "Processed image 6272\n",
      "Processed image 6273\n",
      "Processed image 6274\n",
      "Processed image 6275\n",
      "Processed image 6276\n",
      "Processed image 6277\n",
      "Processed image 6278\n",
      "Processed image 6279\n",
      "Processed image 6280\n",
      "Processed image 6281\n",
      "Processed image 6282\n",
      "Processed image 6283\n",
      "Processed image 6284\n",
      "Processed image 6285\n",
      "Processed image 6286\n",
      "Processed image 6287\n",
      "Processed image 6288\n",
      "Processed image 6289\n",
      "Processed image 6290\n",
      "Processed image 6291\n",
      "Processed image 6292\n",
      "Processed image 6293\n",
      "Processed image 6294\n",
      "Processed image 6295\n",
      "Processed image 6296\n",
      "Processed image 6297\n",
      "Processed image 6298\n",
      "Processed image 6299\n",
      "Processed image 6300\n",
      "Processed image 6301\n",
      "Processed image 6302\n",
      "Processed image 6303\n",
      "Processed image 6304\n",
      "Processed image 6305\n",
      "Processed image 6306\n",
      "Processed image 6307\n",
      "Processed image 6308\n",
      "Processed image 6309\n",
      "Processed image 6310\n",
      "Processed image 6311\n",
      "Processed image 6312\n",
      "Processed image 6313\n",
      "Processed image 6314\n",
      "Processed image 6315\n",
      "Processed image 6316\n",
      "Processed image 6317\n",
      "Processed image 6318\n",
      "Processed image 6319\n",
      "Processed image 6320\n",
      "Processed image 6321\n",
      "Processed image 6322\n",
      "Processed image 6323\n",
      "Processed image 6324\n",
      "Processed image 6325\n",
      "Processed image 6326\n",
      "Processed image 6327\n",
      "Processed image 6328\n",
      "Processed image 6329\n",
      "Processed image 6330\n",
      "Processed image 6331\n",
      "Processed image 6332\n",
      "Processed image 6333\n",
      "Processed image 6334\n",
      "Processed image 6335\n",
      "Processed image 6336\n",
      "Processed image 6337\n",
      "Processed image 6338\n",
      "Processed image 6339\n",
      "Processed image 6340\n",
      "Processed image 6341\n",
      "Processed image 6342\n",
      "Processed image 6343\n",
      "Processed image 6344\n",
      "Processed image 6345\n",
      "Processed image 6346\n",
      "Processed image 6347\n",
      "Processed image 6348\n",
      "Processed image 6349\n",
      "Processed image 6350\n",
      "Processed image 6351\n",
      "Processed image 6352\n",
      "Processed image 6353\n",
      "Processed image 6354\n",
      "Processed image 6355\n",
      "Processed image 6356\n",
      "Processed image 6357\n",
      "Processed image 6358\n",
      "Processed image 6359\n",
      "Processed image 6360\n",
      "Processed image 6361\n",
      "Processed image 6362\n",
      "Processed image 6363\n",
      "Processed image 6364\n",
      "Processed image 6365\n",
      "Processed image 6366\n",
      "Processed image 6367\n",
      "Processed image 6368\n",
      "Processed image 6369\n",
      "Processed image 6370\n",
      "Processed image 6371\n",
      "Processed image 6372\n",
      "Processed image 6373\n",
      "Processed image 6374\n",
      "Processed image 6375\n",
      "Processed image 6376\n",
      "Processed image 6377\n",
      "Processed image 6378\n",
      "Processed image 6379\n",
      "Processed image 6380\n",
      "Processed image 6381\n",
      "Processed image 6382\n",
      "Processed image 6383\n",
      "Processed image 6384\n",
      "Processed image 6385\n",
      "Processed image 6386\n",
      "Processed image 6387\n",
      "Processed image 6388\n",
      "Processed image 6389\n",
      "Processed image 6390\n",
      "Processed image 6391\n",
      "Processed image 6392\n",
      "Processed image 6393\n",
      "Processed image 6394\n",
      "Processed image 6395\n",
      "Processed image 6396\n",
      "Processed image 6397\n",
      "Processed image 6398\n",
      "Processed image 6399\n",
      "Processed image 6400\n",
      "Processed image 6401\n",
      "Processed image 6402\n",
      "Processed image 6403\n",
      "Processed image 6404\n",
      "Processed image 6405\n",
      "Processed image 6406\n",
      "Processed image 6407\n",
      "Processed image 6408\n",
      "Processed image 6409\n",
      "Processed image 6410\n",
      "Processed image 6411\n",
      "Processed image 6412\n",
      "Processed image 6413\n",
      "Processed image 6414\n",
      "Processed image 6415\n",
      "Processed image 6416\n",
      "Processed image 6417\n",
      "Processed image 6418\n",
      "Processed image 6419\n",
      "Processed image 6420\n",
      "Processed image 6421\n",
      "Processed image 6422\n",
      "Processed image 6423\n",
      "Processed image 6424\n",
      "Processed image 6425\n",
      "Processed image 6426\n",
      "Processed image 6427\n",
      "Processed image 6428\n",
      "Processed image 6429\n",
      "Processed image 6430\n",
      "Processed image 6431\n",
      "Processed image 6432\n",
      "Processed image 6433\n",
      "Processed image 6434\n",
      "Processed image 6435\n",
      "Processed image 6436\n",
      "Processed image 6437\n",
      "Processed image 6438\n",
      "Processed image 6439\n",
      "Processed image 6440\n",
      "Processed image 6441\n",
      "Processed image 6442\n",
      "Processed image 6443\n",
      "Processed image 6444\n",
      "Processed image 6445\n",
      "Processed image 6446\n",
      "Processed image 6447\n",
      "Processed image 6448\n",
      "Processed image 6449\n",
      "Processed image 6450\n",
      "Processed image 6451\n",
      "Processed image 6452\n",
      "Processed image 6453\n",
      "Processed image 6454\n",
      "Processed image 6455\n",
      "Processed image 6456\n",
      "Processed image 6457\n",
      "Processed image 6458\n",
      "Processed image 6459\n",
      "Processed image 6460\n",
      "Processed image 6461\n",
      "Processed image 6462\n",
      "Processed image 6463\n",
      "Processed image 6464\n",
      "Processed image 6465\n",
      "Processed image 6466\n",
      "Processed image 6467\n",
      "Processed image 6468\n",
      "Processed image 6469\n",
      "Processed image 6470\n",
      "Processed image 6471\n",
      "Processed image 6472\n",
      "Processed image 6473\n",
      "Processed image 6474\n",
      "Processed image 6475\n",
      "Processed image 6476\n",
      "Processed image 6477\n",
      "Processed image 6478\n",
      "Processed image 6479\n",
      "Processed image 6480\n",
      "Processed image 6481\n",
      "Processed image 6482\n",
      "Processed image 6483\n",
      "Processed image 6484\n",
      "Processed image 6485\n",
      "Processed image 6486\n",
      "Processed image 6487\n",
      "Processed image 6488\n",
      "Processed image 6489\n",
      "Processed image 6490\n",
      "Processed image 6491\n",
      "Processed image 6492\n",
      "Processed image 6493\n",
      "Processed image 6494\n",
      "Processed image 6495\n",
      "Processed image 6496\n",
      "Processed image 6497\n",
      "Processed image 6498\n",
      "Processed image 6499\n",
      "Processed image 6500\n",
      "Processed image 6501\n",
      "Processed image 6502\n",
      "Processed image 6503\n",
      "Processed image 6504\n",
      "Processed image 6505\n",
      "Processed image 6506\n",
      "Processed image 6507\n",
      "Processed image 6508\n",
      "Processed image 6509\n",
      "Processed image 6510\n",
      "Processed image 6511\n",
      "Processed image 6512\n",
      "Processed image 6513\n",
      "Processed image 6514\n",
      "Processed image 6515\n",
      "Processed image 6516\n",
      "Processed image 6517\n",
      "Processed image 6518\n",
      "Processed image 6519\n",
      "Processed image 6520\n",
      "Processed image 6521\n",
      "Processed image 6522\n",
      "Processed image 6523\n",
      "Processed image 6524\n",
      "Processed image 6525\n",
      "Processed image 6526\n",
      "Processed image 6527\n",
      "Processed image 6528\n",
      "Processed image 6529\n",
      "Processed image 6530\n",
      "Processed image 6531\n",
      "Processed image 6532\n",
      "Processed image 6533\n",
      "Processed image 6534\n",
      "Processed image 6535\n",
      "Processed image 6536\n",
      "Processed image 6537\n",
      "Processed image 6538\n",
      "Processed image 6539\n",
      "Processed image 6540\n",
      "Processed image 6541\n",
      "Processed image 6542\n",
      "Processed image 6543\n",
      "Processed image 6544\n",
      "Processed image 6545\n",
      "Processed image 6546\n",
      "Processed image 6547\n",
      "Processed image 6548\n",
      "Processed image 6549\n",
      "Processed image 6550\n",
      "Processed image 6551\n",
      "Processed image 6552\n",
      "Processed image 6553\n",
      "Processed image 6554\n",
      "Processed image 6555\n",
      "Processed image 6556\n",
      "Processed image 6557\n",
      "Processed image 6558\n",
      "Processed image 6559\n",
      "Processed image 6560\n",
      "Processed image 6561\n",
      "Processed image 6562\n",
      "Processed image 6563\n",
      "Processed image 6564\n",
      "Processed image 6565\n",
      "Processed image 6566\n",
      "Processed image 6567\n",
      "Processed image 6568\n",
      "Processed image 6569\n",
      "Processed image 6570\n",
      "Processed image 6571\n",
      "Processed image 6572\n",
      "Processed image 6573\n",
      "Processed image 6574\n",
      "Processed image 6575\n",
      "Processed image 6576\n",
      "Processed image 6577\n",
      "Processed image 6578\n",
      "Processed image 6579\n",
      "Processed image 6580\n",
      "Processed image 6581\n",
      "Processed image 6582\n",
      "Processed image 6583\n",
      "Processed image 6584\n",
      "Processed image 6585\n",
      "Processed image 6586\n",
      "Processed image 6587\n",
      "Processed image 6588\n",
      "Processed image 6589\n",
      "Processed image 6590\n",
      "Processed image 6591\n",
      "Processed image 6592\n",
      "Processed image 6593\n",
      "Processed image 6594\n",
      "Processed image 6595\n",
      "Processed image 6596\n",
      "Processed image 6597\n",
      "Processed image 6598\n",
      "Processed image 6599\n",
      "Processed image 6600\n",
      "Processed image 6601\n",
      "Processed image 6602\n",
      "Processed image 6603\n",
      "Processed image 6604\n",
      "Processed image 6605\n",
      "Processed image 6606\n",
      "Processed image 6607\n",
      "Processed image 6608\n",
      "Processed image 6609\n",
      "Processed image 6610\n",
      "Processed image 6611\n",
      "Processed image 6612\n",
      "Processed image 6613\n",
      "Processed image 6614\n",
      "Processed image 6615\n",
      "Processed image 6616\n",
      "Processed image 6617\n",
      "Processed image 6618\n",
      "Processed image 6619\n",
      "Processed image 6620\n",
      "Processed image 6621\n",
      "Processed image 6622\n",
      "Processed image 6623\n",
      "Processed image 6624\n",
      "Processed image 6625\n",
      "Processed image 6626\n",
      "Processed image 6627\n",
      "Processed image 6628\n",
      "Processed image 6629\n",
      "Processed image 6630\n",
      "Processed image 6631\n",
      "Processed image 6632\n",
      "Processed image 6633\n",
      "Processed image 6634\n",
      "Processed image 6635\n",
      "Processed image 6636\n",
      "Processed image 6637\n",
      "Processed image 6638\n",
      "Processed image 6639\n",
      "Processed image 6640\n",
      "Processed image 6641\n",
      "Processed image 6642\n",
      "Processed image 6643\n",
      "Processed image 6644\n",
      "Processed image 6645\n",
      "Processed image 6646\n",
      "Processed image 6647\n",
      "Processed image 6648\n",
      "Processed image 6649\n",
      "Processed image 6650\n",
      "Processed image 6651\n",
      "Processed image 6652\n",
      "Processed image 6653\n",
      "Processed image 6654\n",
      "Processed image 6655\n",
      "Processed image 6656\n",
      "Processed image 6657\n",
      "Processed image 6658\n",
      "Processed image 6659\n",
      "Processed image 6660\n",
      "Processed image 6661\n",
      "Processed image 6662\n",
      "Processed image 6663\n",
      "Processed image 6664\n",
      "Processed image 6665\n",
      "Processed image 6666\n",
      "Processed image 6667\n",
      "Processed image 6668\n",
      "Processed image 6669\n",
      "Processed image 6670\n",
      "Processed image 6671\n",
      "Processed image 6672\n",
      "Processed image 6673\n",
      "Processed image 6674\n",
      "Processed image 6675\n",
      "Processed image 6676\n",
      "Processed image 6677\n",
      "Processed image 6678\n",
      "Processed image 6679\n",
      "Processed image 6680\n",
      "Processed image 6681\n",
      "Processed image 6682\n",
      "Processed image 6683\n",
      "Processed image 6684\n",
      "Processed image 6685\n",
      "Processed image 6686\n",
      "Processed image 6687\n",
      "Processed image 6688\n",
      "Processed image 6689\n",
      "Processed image 6690\n",
      "Processed image 6691\n",
      "Processed image 6692\n",
      "Processed image 6693\n",
      "Processed image 6694\n",
      "Processed image 6695\n",
      "Processed image 6696\n",
      "Processed image 6697\n",
      "Processed image 6698\n",
      "Processed image 6699\n",
      "Processed image 6700\n",
      "Processed image 6701\n",
      "Processed image 6702\n",
      "Processed image 6703\n",
      "Processed image 6704\n",
      "Processed image 6705\n",
      "Processed image 6706\n",
      "Processed image 6707\n",
      "Processed image 6708\n",
      "Processed image 6709\n",
      "Processed image 6710\n",
      "Processed image 6711\n",
      "Processed image 6712\n",
      "Processed image 6713\n",
      "Processed image 6714\n",
      "Processed image 6715\n",
      "Processed image 6716\n",
      "Processed image 6717\n",
      "Processed image 6718\n",
      "Processed image 6719\n",
      "Processed image 6720\n",
      "Processed image 6721\n",
      "Processed image 6722\n",
      "Processed image 6723\n",
      "Processed image 6724\n",
      "Processed image 6725\n",
      "Processed image 6726\n",
      "Processed image 6727\n",
      "Processed image 6728\n",
      "Processed image 6729\n",
      "Processed image 6730\n",
      "Processed image 6731\n",
      "Processed image 6732\n",
      "Processed image 6733\n",
      "Processed image 6734\n",
      "Processed image 6735\n",
      "Processed image 6736\n",
      "Processed image 6737\n",
      "Processed image 6738\n",
      "Processed image 6739\n",
      "Processed image 6740\n",
      "Processed image 6741\n",
      "Processed image 6742\n",
      "Processed image 6743\n",
      "Processed image 6744\n",
      "Processed image 6745\n",
      "Processed image 6746\n",
      "Processed image 6747\n",
      "Processed image 6748\n",
      "Processed image 6749\n",
      "Processed image 6750\n",
      "Processed image 6751\n",
      "Processed image 6752\n",
      "Processed image 6753\n",
      "Processed image 6754\n",
      "Processed image 6755\n",
      "Processed image 6756\n",
      "Processed image 6757\n",
      "Processed image 6758\n",
      "Processed image 6759\n",
      "Processed image 6760\n",
      "Processed image 6761\n",
      "Processed image 6762\n",
      "Processed image 6763\n",
      "Processed image 6764\n",
      "Processed image 6765\n",
      "Processed image 6766\n",
      "Processed image 6767\n",
      "Processed image 6768\n",
      "Processed image 6769\n",
      "Processed image 6770\n",
      "Processed image 6771\n",
      "Processed image 6772\n",
      "Processed image 6773\n",
      "Processed image 6774\n",
      "Processed image 6775\n",
      "Processed image 6776\n",
      "Processed image 6777\n",
      "Processed image 6778\n",
      "Processed image 6779\n",
      "Processed image 6780\n",
      "Processed image 6781\n",
      "Processed image 6782\n",
      "Processed image 6783\n",
      "Processed image 6784\n",
      "Processed image 6785\n",
      "Processed image 6786\n",
      "Processed image 6787\n",
      "Processed image 6788\n",
      "Processed image 6789\n",
      "Processed image 6790\n",
      "Processed image 6791\n",
      "Processed image 6792\n",
      "Processed image 6793\n",
      "Processed image 6794\n",
      "Processed image 6795\n",
      "Processed image 6796\n",
      "Processed image 6797\n",
      "Processed image 6798\n",
      "Processed image 6799\n",
      "Processed image 6800\n",
      "Processed image 6801\n",
      "Processed image 6802\n",
      "Processed image 6803\n",
      "Processed image 6804\n",
      "Processed image 6805\n",
      "Processed image 6806\n",
      "Processed image 6807\n",
      "Processed image 6808\n",
      "Processed image 6809\n",
      "Processed image 6810\n",
      "Processed image 6811\n",
      "Processed image 6812\n",
      "Processed image 6813\n",
      "Processed image 6814\n",
      "Processed image 6815\n",
      "Processed image 6816\n",
      "Processed image 6817\n",
      "Processed image 6818\n",
      "Processed image 6819\n",
      "Processed image 6820\n",
      "Processed image 6821\n",
      "Processed image 6822\n",
      "Processed image 6823\n",
      "Processed image 6824\n",
      "Processed image 6825\n",
      "Processed image 6826\n",
      "Processed image 6827\n",
      "Processed image 6828\n",
      "Processed image 6829\n",
      "Processed image 6830\n",
      "Processed image 6831\n",
      "Processed image 6832\n",
      "Processed image 6833\n",
      "Processed image 6834\n",
      "Processed image 6835\n",
      "Processed image 6836\n",
      "Processed image 6837\n",
      "Processed image 6838\n",
      "Processed image 6839\n",
      "Processed image 6840\n",
      "Processed image 6841\n",
      "Processed image 6842\n",
      "Processed image 6843\n",
      "Processed image 6844\n",
      "Processed image 6845\n",
      "Processed image 6846\n",
      "Processed image 6847\n",
      "Processed image 6848\n",
      "Processed image 6849\n",
      "Processed image 6850\n",
      "Processed image 6851\n",
      "Processed image 6852\n",
      "Processed image 6853\n",
      "Processed image 6854\n",
      "Processed image 6855\n",
      "Processed image 6856\n",
      "Processed image 6857\n",
      "Processed image 6858\n",
      "Processed image 6859\n",
      "Processed image 6860\n",
      "Processed image 6861\n",
      "Processed image 6862\n",
      "Processed image 6863\n",
      "Processed image 6864\n",
      "Processed image 6865\n",
      "Processed image 6866\n",
      "Processed image 6867\n",
      "Processed image 6868\n",
      "Processed image 6869\n",
      "Processed image 6870\n",
      "Processed image 6871\n",
      "Processed image 6872\n",
      "Processed image 6873\n",
      "Processed image 6874\n",
      "Processed image 6875\n",
      "Processed image 6876\n",
      "Processed image 6877\n",
      "Processed image 6878\n",
      "Processed image 6879\n",
      "Processed image 6880\n",
      "Processed image 6881\n",
      "Processed image 6882\n",
      "Processed image 6883\n",
      "Processed image 6884\n",
      "Processed image 6885\n",
      "Processed image 6886\n",
      "Processed image 6887\n",
      "Processed image 6888\n",
      "Processed image 6889\n",
      "Processed image 6890\n",
      "Processed image 6891\n",
      "Processed image 6892\n",
      "Processed image 6893\n",
      "Processed image 6894\n",
      "Processed image 6895\n",
      "Processed image 6896\n",
      "Processed image 6897\n",
      "Processed image 6898\n",
      "Processed image 6899\n",
      "Processed image 6900\n",
      "Processed image 6901\n",
      "Processed image 6902\n",
      "Processed image 6903\n",
      "Processed image 6904\n",
      "Processed image 6905\n",
      "Processed image 6906\n",
      "Processed image 6907\n",
      "Processed image 6908\n",
      "Processed image 6909\n",
      "Processed image 6910\n",
      "Processed image 6911\n",
      "Processed image 6912\n",
      "Processed image 6913\n",
      "Processed image 6914\n",
      "Processed image 6915\n",
      "Processed image 6916\n",
      "Processed image 6917\n",
      "Processed image 6918\n",
      "Processed image 6919\n",
      "Processed image 6920\n",
      "Processed image 6921\n",
      "Processed image 6922\n",
      "Processed image 6923\n",
      "Processed image 6924\n",
      "Processed image 6925\n",
      "Processed image 6926\n",
      "Processed image 6927\n",
      "Processed image 6928\n",
      "Processed image 6929\n",
      "Processed image 6930\n",
      "Processed image 6931\n",
      "Processed image 6932\n",
      "Processed image 6933\n",
      "Processed image 6934\n",
      "Processed image 6935\n",
      "Processed image 6936\n",
      "Processed image 6937\n",
      "Processed image 6938\n",
      "Processed image 6939\n",
      "Processed image 6940\n",
      "Processed image 6941\n",
      "Processed image 6942\n",
      "Processed image 6943\n",
      "Processed image 6944\n",
      "Processed image 6945\n",
      "Processed image 6946\n",
      "Processed image 6947\n",
      "Processed image 6948\n",
      "Processed image 6949\n",
      "Processed image 6950\n",
      "Processed image 6951\n",
      "Processed image 6952\n",
      "Processed image 6953\n",
      "Processed image 6954\n",
      "Processed image 6955\n",
      "Processed image 6956\n",
      "Processed image 6957\n",
      "Processed image 6958\n",
      "Processed image 6959\n",
      "Processed image 6960\n",
      "Processed image 6961\n",
      "Processed image 6962\n",
      "Processed image 6963\n",
      "Processed image 6964\n",
      "Processed image 6965\n",
      "Processed image 6966\n",
      "Processed image 6967\n",
      "Processed image 6968\n",
      "Processed image 6969\n",
      "Processed image 6970\n",
      "Processed image 6971\n",
      "Processed image 6972\n",
      "Processed image 6973\n",
      "Processed image 6974\n",
      "Processed image 6975\n",
      "Processed image 6976\n",
      "Processed image 6977\n",
      "Processed image 6978\n",
      "Processed image 6979\n",
      "Processed image 6980\n",
      "Processed image 6981\n",
      "Processed image 6982\n",
      "Processed image 6983\n",
      "Processed image 6984\n",
      "Processed image 6985\n",
      "Processed image 6986\n",
      "Processed image 6987\n",
      "Processed image 6988\n",
      "Processed image 6989\n",
      "Processed image 6990\n",
      "Processed image 6991\n",
      "Processed image 6992\n",
      "Processed image 6993\n",
      "Processed image 6994\n",
      "Processed image 6995\n",
      "Processed image 6996\n",
      "Processed image 6997\n",
      "Processed image 6998\n",
      "Processed image 6999\n",
      "Processed image 7000\n",
      "Processed image 7001\n",
      "Processed image 7002\n",
      "Processed image 7003\n",
      "Processed image 7004\n",
      "Processed image 7005\n",
      "Processed image 7006\n",
      "Processed image 7007\n",
      "Processed image 7008\n",
      "Processed image 7009\n",
      "Processed image 7010\n",
      "Processed image 7011\n",
      "Processed image 7012\n",
      "Processed image 7013\n",
      "Processed image 7014\n",
      "Processed image 7015\n",
      "Processed image 7016\n",
      "Processed image 7017\n",
      "Processed image 7018\n",
      "Processed image 7019\n",
      "Processed image 7020\n",
      "Processed image 7021\n",
      "Processed image 7022\n",
      "Processed image 7023\n",
      "Processed image 7024\n",
      "Processed image 7025\n",
      "Processed image 7026\n",
      "Processed image 7027\n",
      "Processed image 7028\n",
      "Processed image 7029\n",
      "Processed image 7030\n",
      "Processed image 7031\n",
      "Processed image 7032\n",
      "Processed image 7033\n",
      "Processed image 7034\n",
      "Processed image 7035\n",
      "Processed image 7036\n",
      "Processed image 7037\n",
      "Processed image 7038\n",
      "Processed image 7039\n",
      "Processed image 7040\n",
      "Processed image 7041\n",
      "Processed image 7042\n",
      "Processed image 7043\n",
      "Processed image 7044\n",
      "Processed image 7045\n",
      "Processed image 7046\n",
      "Processed image 7047\n",
      "Processed image 7048\n",
      "Processed image 7049\n",
      "Processed image 7050\n",
      "Processed image 7051\n",
      "Processed image 7052\n",
      "Processed image 7053\n",
      "Processed image 7054\n",
      "Processed image 7055\n",
      "Processed image 7056\n",
      "Processed image 7057\n",
      "Processed image 7058\n",
      "Processed image 7059\n",
      "Processed image 7060\n",
      "Processed image 7061\n",
      "Processed image 7062\n",
      "Processed image 7063\n",
      "Processed image 7064\n",
      "Processed image 7065\n",
      "Processed image 7066\n",
      "Processed image 7067\n",
      "Processed image 7068\n",
      "Processed image 7069\n",
      "Processed image 7070\n",
      "Processed image 7071\n",
      "Processed image 7072\n",
      "Processed image 7073\n",
      "Processed image 7074\n",
      "Processed image 7075\n",
      "Processed image 7076\n",
      "Processed image 7077\n",
      "Processed image 7078\n",
      "Processed image 7079\n",
      "Processed image 7080\n",
      "Processed image 7081\n",
      "Processed image 7082\n",
      "Processed image 7083\n",
      "Processed image 7084\n",
      "Processed image 7085\n",
      "Processed image 7086\n",
      "Processed image 7087\n",
      "Processed image 7088\n",
      "Processed image 7089\n",
      "Processed image 7090\n",
      "Processed image 7091\n",
      "Processed image 7092\n",
      "Processed image 7093\n",
      "Processed image 7094\n",
      "Processed image 7095\n",
      "Processed image 7096\n",
      "Processed image 7097\n",
      "Processed image 7098\n",
      "Processed image 7099\n",
      "Processed image 7100\n",
      "Processed image 7101\n",
      "Processed image 7102\n",
      "Processed image 7103\n",
      "Processed image 7104\n",
      "Processed image 7105\n",
      "Processed image 7106\n",
      "Processed image 7107\n",
      "Processed image 7108\n",
      "Processed image 7109\n",
      "Processed image 7110\n",
      "Processed image 7111\n",
      "Processed image 7112\n",
      "Processed image 7113\n",
      "Processed image 7114\n",
      "Processed image 7115\n",
      "Processed image 7116\n",
      "Processed image 7117\n",
      "Processed image 7118\n",
      "Processed image 7119\n",
      "Processed image 7120\n",
      "Processed image 7121\n",
      "Processed image 7122\n",
      "Processed image 7123\n",
      "Processed image 7124\n",
      "Processed image 7125\n",
      "Processed image 7126\n",
      "Processed image 7127\n",
      "Processed image 7128\n",
      "Processed image 7129\n",
      "Processed image 7130\n",
      "Processed image 7131\n",
      "Processed image 7132\n",
      "Processed image 7133\n",
      "Processed image 7134\n",
      "Processed image 7135\n",
      "Processed image 7136\n",
      "Processed image 7137\n",
      "Processed image 7138\n",
      "Processed image 7139\n",
      "Processed image 7140\n",
      "Processed image 7141\n",
      "Processed image 7142\n",
      "Processed image 7143\n",
      "Processed image 7144\n",
      "Processed image 7145\n",
      "Processed image 7146\n",
      "Processed image 7147\n",
      "Processed image 7148\n",
      "Processed image 7149\n",
      "Processed image 7150\n",
      "Processed image 7151\n",
      "Processed image 7152\n",
      "Processed image 7153\n",
      "Processed image 7154\n",
      "Processed image 7155\n",
      "Processed image 7156\n",
      "Processed image 7157\n",
      "Processed image 7158\n",
      "Processed image 7159\n",
      "Processed image 7160\n",
      "Processed image 7161\n",
      "Processed image 7162\n",
      "Processed image 7163\n",
      "Processed image 7164\n",
      "Processed image 7165\n",
      "Processed image 7166\n",
      "Processed image 7167\n",
      "Processed image 7168\n",
      "Processed image 7169\n",
      "Processed image 7170\n",
      "Processed image 7171\n",
      "Processed image 7172\n",
      "Processed image 7173\n",
      "Processed image 7174\n",
      "Processed image 7175\n",
      "Processed image 7176\n",
      "Processed image 7177\n",
      "Processed image 7178\n",
      "Processed image 7179\n",
      "Processed image 7180\n",
      "Processed image 7181\n",
      "Processed image 7182\n",
      "Processed image 7183\n",
      "Processed image 7184\n",
      "Processed image 7185\n",
      "Processed image 7186\n",
      "Processed image 7187\n",
      "Processed image 7188\n",
      "Processed image 7189\n",
      "Processed image 7190\n",
      "Processed image 7191\n",
      "Processed image 7192\n",
      "Processed image 7193\n",
      "Processed image 7194\n",
      "Processed image 7195\n",
      "Processed image 7196\n",
      "Processed image 7197\n",
      "Processed image 7198\n",
      "Processed image 7199\n",
      "Processed image 7200\n",
      "Processed image 7201\n",
      "Processed image 7202\n",
      "Processed image 7203\n",
      "Processed image 7204\n",
      "Processed image 7205\n",
      "Processed image 7206\n",
      "Processed image 7207\n",
      "Processed image 7208\n",
      "Processed image 7209\n",
      "Processed image 7210\n",
      "Processed image 7211\n",
      "Processed image 7212\n",
      "Processed image 7213\n",
      "Processed image 7214\n",
      "Processed image 7215\n",
      "Processed image 7216\n",
      "Processed image 7217\n",
      "Processed image 7218\n",
      "Processed image 7219\n",
      "Processed image 7220\n",
      "Processed image 7221\n",
      "Processed image 7222\n",
      "Processed image 7223\n",
      "Processed image 7224\n",
      "Processed image 7225\n",
      "Processed image 7226\n",
      "Processed image 7227\n",
      "Processed image 7228\n",
      "Processed image 7229\n",
      "Processed image 7230\n",
      "Processed image 7231\n",
      "Processed image 7232\n",
      "Processed image 7233\n",
      "Processed image 7234\n",
      "Processed image 7235\n",
      "Processed image 7236\n",
      "Processed image 7237\n",
      "Processed image 7238\n",
      "Processed image 7239\n",
      "Processed image 7240\n",
      "Processed image 7241\n",
      "Processed image 7242\n",
      "Processed image 7243\n",
      "Processed image 7244\n",
      "Processed image 7245\n",
      "Processed image 7246\n",
      "Processed image 7247\n",
      "Processed image 7248\n",
      "Processed image 7249\n",
      "Processed image 7250\n",
      "Processed image 7251\n",
      "Processed image 7252\n",
      "Processed image 7253\n",
      "Processed image 7254\n",
      "Processed image 7255\n",
      "Processed image 7256\n",
      "Processed image 7257\n",
      "Processed image 7258\n",
      "Processed image 7259\n",
      "Processed image 7260\n",
      "Processed image 7261\n",
      "Processed image 7262\n",
      "Processed image 7263\n",
      "Processed image 7264\n",
      "Processed image 7265\n",
      "Processed image 7266\n",
      "Processed image 7267\n",
      "Processed image 7268\n",
      "Processed image 7269\n",
      "Processed image 7270\n",
      "Processed image 7271\n",
      "Processed image 7272\n",
      "Processed image 7273\n",
      "Processed image 7274\n",
      "Processed image 7275\n",
      "Processed image 7276\n",
      "Processed image 7277\n",
      "Processed image 7278\n",
      "Processed image 7279\n",
      "Processed image 7280\n",
      "Processed image 7281\n",
      "Processed image 7282\n",
      "Processed image 7283\n",
      "Processed image 7284\n",
      "Processed image 7285\n",
      "Processed image 7286\n",
      "Processed image 7287\n",
      "Processed image 7288\n",
      "Processed image 7289\n",
      "Processed image 7290\n",
      "Processed image 7291\n",
      "Processed image 7292\n",
      "Processed image 7293\n",
      "Processed image 7294\n",
      "Processed image 7295\n",
      "Processed image 7296\n",
      "Processed image 7297\n",
      "Processed image 7298\n",
      "Processed image 7299\n",
      "Processed image 7300\n",
      "Processed image 7301\n",
      "Processed image 7302\n",
      "Processed image 7303\n",
      "Processed image 7304\n",
      "Processed image 7305\n",
      "Processed image 7306\n",
      "Processed image 7307\n",
      "Processed image 7308\n",
      "Processed image 7309\n",
      "Processed image 7310\n",
      "Processed image 7311\n",
      "Processed image 7312\n",
      "Processed image 7313\n",
      "Processed image 7314\n",
      "Processed image 7315\n",
      "Processed image 7316\n",
      "Processed image 7317\n",
      "Processed image 7318\n",
      "Processed image 7319\n",
      "Processed image 7320\n",
      "Processed image 7321\n",
      "Processed image 7322\n",
      "Processed image 7323\n",
      "Processed image 7324\n",
      "Processed image 7325\n",
      "Processed image 7326\n",
      "Processed image 7327\n",
      "Processed image 7328\n",
      "Processed image 7329\n",
      "Processed image 7330\n",
      "Processed image 7331\n",
      "Processed image 7332\n",
      "Processed image 7333\n",
      "Processed image 7334\n",
      "Processed image 7335\n",
      "Processed image 7336\n",
      "Processed image 7337\n",
      "Processed image 7338\n",
      "Processed image 7339\n",
      "Processed image 7340\n",
      "Processed image 7341\n",
      "Processed image 7342\n",
      "Processed image 7343\n",
      "Processed image 7344\n",
      "Processed image 7345\n",
      "Processed image 7346\n",
      "Processed image 7347\n",
      "Processed image 7348\n",
      "Processed image 7349\n",
      "Processed image 7350\n",
      "Processed image 7351\n",
      "Processed image 7352\n",
      "Processed image 7353\n",
      "Processed image 7354\n",
      "Processed image 7355\n",
      "Processed image 7356\n",
      "Processed image 7357\n",
      "Processed image 7358\n",
      "Processed image 7359\n",
      "Processed image 7360\n",
      "Processed image 7361\n",
      "Processed image 7362\n",
      "Processed image 7363\n",
      "Processed image 7364\n",
      "Processed image 7365\n",
      "Processed image 7366\n",
      "Processed image 7367\n",
      "Processed image 7368\n",
      "Processed image 7369\n",
      "Processed image 7370\n",
      "Processed image 7371\n",
      "Processed image 7372\n",
      "Processed image 7373\n",
      "Processed image 7374\n",
      "Processed image 7375\n",
      "Processed image 7376\n",
      "Processed image 7377\n",
      "Processed image 7378\n",
      "Processed image 7379\n",
      "Processed image 7380\n",
      "Processed image 7381\n",
      "Processed image 7382\n",
      "Processed image 7383\n",
      "Processed image 7384\n",
      "Processed image 7385\n",
      "Processed image 7386\n",
      "Processed image 7387\n",
      "Processed image 7388\n",
      "Processed image 7389\n",
      "Processed image 7390\n",
      "Processed image 7391\n",
      "Processed image 7392\n",
      "Processed image 7393\n",
      "Processed image 7394\n",
      "Processed image 7395\n",
      "Processed image 7396\n",
      "Processed image 7397\n",
      "Processed image 7398\n",
      "Processed image 7399\n",
      "Processed image 7400\n",
      "Processed image 7401\n",
      "Processed image 7402\n",
      "Processed image 7403\n",
      "Processed image 7404\n",
      "Processed image 7405\n",
      "Processed image 7406\n",
      "Processed image 7407\n",
      "Processed image 7408\n",
      "Processed image 7409\n",
      "Processed image 7410\n",
      "Processed image 7411\n",
      "Processed image 7412\n",
      "Processed image 7413\n",
      "Processed image 7414\n",
      "Processed image 7415\n",
      "Processed image 7416\n",
      "Processed image 7417\n",
      "Processed image 7418\n",
      "Processed image 7419\n",
      "Processed image 7420\n",
      "Processed image 7421\n",
      "Processed image 7422\n",
      "Processed image 7423\n",
      "Processed image 7424\n",
      "Processed image 7425\n",
      "Processed image 7426\n",
      "Processed image 7427\n",
      "Processed image 7428\n",
      "Processed image 7429\n",
      "Processed image 7430\n",
      "Processed image 7431\n",
      "Processed image 7432\n",
      "Processed image 7433\n",
      "Processed image 7434\n",
      "Processed image 7435\n",
      "Processed image 7436\n",
      "Processed image 7437\n",
      "Processed image 7438\n",
      "Processed image 7439\n",
      "Processed image 7440\n",
      "Processed image 7441\n",
      "Processed image 7442\n",
      "Processed image 7443\n",
      "Processed image 7444\n",
      "Processed image 7445\n",
      "Processed image 7446\n",
      "Processed image 7447\n",
      "Processed image 7448\n",
      "Processed image 7449\n",
      "Processed image 7450\n",
      "Processed image 7451\n",
      "Processed image 7452\n",
      "Processed image 7453\n",
      "Processed image 7454\n",
      "Processed image 7455\n",
      "Processed image 7456\n",
      "Processed image 7457\n",
      "Processed image 7458\n",
      "Processed image 7459\n",
      "Processed image 7460\n",
      "Processed image 7461\n",
      "Processed image 7462\n",
      "Processed image 7463\n",
      "Processed image 7464\n",
      "Processed image 7465\n",
      "Processed image 7466\n",
      "Processed image 7467\n",
      "Processed image 7468\n",
      "Processed image 7469\n",
      "Processed image 7470\n",
      "Processed image 7471\n",
      "Processed image 7472\n",
      "Processed image 7473\n",
      "Processed image 7474\n",
      "Processed image 7475\n",
      "Processed image 7476\n",
      "Processed image 7477\n",
      "Processed image 7478\n",
      "Processed image 7479\n",
      "Processed image 7480\n",
      "Processed image 7481\n",
      "Processed image 7482\n",
      "Processed image 7483\n",
      "Processed image 7484\n",
      "Processed image 7485\n",
      "Processed image 7486\n",
      "Processed image 7487\n",
      "Processed image 7488\n",
      "Processed image 7489\n",
      "Processed image 7490\n",
      "Processed image 7491\n",
      "Processed image 7492\n",
      "Processed image 7493\n",
      "Processed image 7494\n",
      "Processed image 7495\n",
      "Processed image 7496\n",
      "Processed image 7497\n",
      "Processed image 7498\n",
      "Processed image 7499\n",
      "Processed image 7500\n",
      "Processed image 7501\n",
      "Processed image 7502\n",
      "Processed image 7503\n",
      "Processed image 7504\n",
      "Processed image 7505\n",
      "Processed image 7506\n",
      "Processed image 7507\n",
      "Processed image 7508\n",
      "Processed image 7509\n",
      "Processed image 7510\n",
      "Processed image 7511\n",
      "Processed image 7512\n",
      "Processed image 7513\n",
      "Processed image 7514\n",
      "Processed image 7515\n",
      "Processed image 7516\n",
      "Processed image 7517\n",
      "Processed image 7518\n",
      "Processed image 7519\n",
      "Processed image 7520\n",
      "Processed image 7521\n",
      "Processed image 7522\n",
      "Processed image 7523\n",
      "Processed image 7524\n",
      "Processed image 7525\n",
      "Processed image 7526\n",
      "Processed image 7527\n",
      "Processed image 7528\n",
      "Processed image 7529\n",
      "Processed image 7530\n",
      "Processed image 7531\n",
      "Processed image 7532\n",
      "Processed image 7533\n",
      "Processed image 7534\n",
      "Processed image 7535\n",
      "Processed image 7536\n",
      "Processed image 7537\n",
      "Processed image 7538\n",
      "Processed image 7539\n",
      "Processed image 7540\n",
      "Processed image 7541\n",
      "Processed image 7542\n",
      "Processed image 7543\n",
      "Processed image 7544\n",
      "Processed image 7545\n",
      "Processed image 7546\n",
      "Processed image 7547\n",
      "Processed image 7548\n",
      "Processed image 7549\n",
      "Processed image 7550\n",
      "Processed image 7551\n",
      "Processed image 7552\n",
      "Processed image 7553\n",
      "Processed image 7554\n",
      "Processed image 7555\n",
      "Processed image 7556\n",
      "Processed image 7557\n",
      "Processed image 7558\n",
      "Processed image 7559\n",
      "Processed image 7560\n",
      "Processed image 7561\n",
      "Processed image 7562\n",
      "Processed image 7563\n",
      "Processed image 7564\n",
      "Processed image 7565\n",
      "Processed image 7566\n",
      "Processed image 7567\n",
      "Processed image 7568\n",
      "Processed image 7569\n",
      "Processed image 7570\n",
      "Processed image 7571\n",
      "Processed image 7572\n",
      "Processed image 7573\n",
      "Processed image 7574\n",
      "Processed image 7575\n",
      "Processed image 7576\n",
      "Processed image 7577\n",
      "Processed image 7578\n",
      "Processed image 7579\n",
      "Processed image 7580\n",
      "Processed image 7581\n",
      "Processed image 7582\n",
      "Processed image 7583\n",
      "Processed image 7584\n",
      "Processed image 7585\n",
      "Processed image 7586\n",
      "Processed image 7587\n",
      "Processed image 7588\n",
      "Processed image 7589\n",
      "Processed image 7590\n",
      "Processed image 7591\n",
      "Processed image 7592\n",
      "Processed image 7593\n",
      "Processed image 7594\n",
      "Processed image 7595\n",
      "Processed image 7596\n",
      "Processed image 7597\n",
      "Processed image 7598\n",
      "Processed image 7599\n",
      "Processed image 7600\n",
      "Processed image 7601\n",
      "Processed image 7602\n",
      "Processed image 7603\n",
      "Processed image 7604\n",
      "Processed image 7605\n",
      "Processed image 7606\n",
      "Processed image 7607\n",
      "Processed image 7608\n",
      "Processed image 7609\n",
      "Processed image 7610\n",
      "Processed image 7611\n",
      "Processed image 7612\n",
      "Processed image 7613\n",
      "Processed image 7614\n",
      "Processed image 7615\n",
      "Processed image 7616\n",
      "Processed image 7617\n",
      "Processed image 7618\n",
      "Processed image 7619\n",
      "Processed image 7620\n",
      "Processed image 7621\n",
      "Processed image 7622\n",
      "Processed image 7623\n",
      "Processed image 7624\n",
      "Processed image 7625\n",
      "Processed image 7626\n",
      "Processed image 7627\n",
      "Processed image 7628\n",
      "Processed image 7629\n",
      "Processed image 7630\n",
      "Processed image 7631\n",
      "Processed image 7632\n",
      "Processed image 7633\n",
      "Processed image 7634\n",
      "Processed image 7635\n",
      "Processed image 7636\n",
      "Processed image 7637\n",
      "Processed image 7638\n",
      "Processed image 7639\n",
      "Processed image 7640\n",
      "Processed image 7641\n",
      "Processed image 7642\n",
      "Processed image 7643\n",
      "Processed image 7644\n",
      "Processed image 7645\n",
      "Processed image 7646\n",
      "Processed image 7647\n",
      "Processed image 7648\n",
      "Processed image 7649\n",
      "Processed image 7650\n",
      "Processed image 7651\n",
      "Processed image 7652\n",
      "Processed image 7653\n",
      "Processed image 7654\n",
      "Processed image 7655\n",
      "Processed image 7656\n",
      "Processed image 7657\n",
      "Processed image 7658\n",
      "Processed image 7659\n",
      "Processed image 7660\n",
      "Processed image 7661\n",
      "Processed image 7662\n",
      "Processed image 7663\n",
      "Processed image 7664\n",
      "Processed image 7665\n",
      "Processed image 7666\n",
      "Processed image 7667\n",
      "Processed image 7668\n",
      "Processed image 7669\n",
      "Processed image 7670\n",
      "Processed image 7671\n",
      "Processed image 7672\n",
      "Processed image 7673\n",
      "Processed image 7674\n",
      "Processed image 7675\n",
      "Processed image 7676\n",
      "Processed image 7677\n",
      "Processed image 7678\n",
      "Processed image 7679\n",
      "Processed image 7680\n",
      "Processed image 7681\n",
      "Processed image 7682\n",
      "Processed image 7683\n",
      "Processed image 7684\n",
      "Processed image 7685\n",
      "Processed image 7686\n",
      "Processed image 7687\n",
      "Processed image 7688\n",
      "Processed image 7689\n",
      "Processed image 7690\n",
      "Processed image 7691\n",
      "Processed image 7692\n",
      "Processed image 7693\n",
      "Processed image 7694\n",
      "Processed image 7695\n",
      "Processed image 7696\n",
      "Processed image 7697\n",
      "Processed image 7698\n",
      "Processed image 7699\n",
      "Processed image 7700\n",
      "Processed image 7701\n",
      "Processed image 7702\n",
      "Processed image 7703\n",
      "Processed image 7704\n",
      "Processed image 7705\n",
      "Processed image 7706\n",
      "Processed image 7707\n",
      "Processed image 7708\n",
      "Processed image 7709\n",
      "Processed image 7710\n",
      "Processed image 7711\n",
      "Processed image 7712\n",
      "Processed image 7713\n",
      "Processed image 7714\n",
      "Processed image 7715\n",
      "Processed image 7716\n",
      "Processed image 7717\n",
      "Processed image 7718\n",
      "Processed image 7719\n",
      "Processed image 7720\n",
      "Processed image 7721\n",
      "Processed image 7722\n",
      "Processed image 7723\n",
      "Processed image 7724\n",
      "Processed image 7725\n",
      "Processed image 7726\n",
      "Processed image 7727\n",
      "Processed image 7728\n",
      "Processed image 7729\n",
      "Processed image 7730\n",
      "Processed image 7731\n",
      "Processed image 7732\n",
      "Processed image 7733\n",
      "Processed image 7734\n",
      "Processed image 7735\n",
      "Processed image 7736\n",
      "Processed image 7737\n",
      "Processed image 7738\n",
      "Processed image 7739\n",
      "Processed image 7740\n",
      "Processed image 7741\n",
      "Processed image 7742\n",
      "Processed image 7743\n",
      "Processed image 7744\n",
      "Processed image 7745\n",
      "Processed image 7746\n",
      "Processed image 7747\n",
      "Processed image 7748\n",
      "Processed image 7749\n",
      "Processed image 7750\n",
      "Processed image 7751\n",
      "Processed image 7752\n",
      "Processed image 7753\n",
      "Processed image 7754\n",
      "Processed image 7755\n",
      "Processed image 7756\n",
      "Processed image 7757\n",
      "Processed image 7758\n",
      "Processed image 7759\n",
      "Processed image 7760\n",
      "Processed image 7761\n",
      "Processed image 7762\n",
      "Processed image 7763\n",
      "Processed image 7764\n",
      "Processed image 7765\n",
      "Processed image 7766\n",
      "Processed image 7767\n",
      "Processed image 7768\n",
      "Processed image 7769\n",
      "Processed image 7770\n",
      "Processed image 7771\n",
      "Processed image 7772\n",
      "Processed image 7773\n",
      "Processed image 7774\n",
      "Processed image 7775\n",
      "Processed image 7776\n",
      "Processed image 7777\n",
      "Processed image 7778\n",
      "Processed image 7779\n",
      "Processed image 7780\n",
      "Processed image 7781\n",
      "Processed image 7782\n",
      "Processed image 7783\n",
      "Processed image 7784\n",
      "Processed image 7785\n",
      "Processed image 7786\n",
      "Processed image 7787\n",
      "Processed image 7788\n",
      "Processed image 7789\n",
      "Processed image 7790\n",
      "Processed image 7791\n",
      "Processed image 7792\n",
      "Processed image 7793\n",
      "Processed image 7794\n",
      "Processed image 7795\n",
      "Processed image 7796\n",
      "Processed image 7797\n",
      "Processed image 7798\n",
      "Processed image 7799\n",
      "Processed image 7800\n",
      "Processed image 7801\n",
      "Processed image 7802\n",
      "Processed image 7803\n",
      "Processed image 7804\n",
      "Processed image 7805\n",
      "Processed image 7806\n",
      "Processed image 7807\n",
      "Processed image 7808\n",
      "Processed image 7809\n",
      "Processed image 7810\n",
      "Processed image 7811\n",
      "Processed image 7812\n",
      "Processed image 7813\n",
      "Processed image 7814\n",
      "Processed image 7815\n",
      "Processed image 7816\n",
      "Processed image 7817\n",
      "Processed image 7818\n",
      "Processed image 7819\n",
      "Processed image 7820\n",
      "Processed image 7821\n",
      "Processed image 7822\n",
      "Processed image 7823\n",
      "Processed image 7824\n",
      "Processed image 7825\n",
      "Processed image 7826\n",
      "Processed image 7827\n",
      "Processed image 7828\n",
      "Processed image 7829\n",
      "Processed image 7830\n",
      "Processed image 7831\n",
      "Processed image 7832\n",
      "Processed image 7833\n",
      "Processed image 7834\n",
      "Processed image 7835\n",
      "Processed image 7836\n",
      "Processed image 7837\n",
      "Processed image 7838\n",
      "Processed image 7839\n",
      "Processed image 7840\n",
      "Processed image 7841\n",
      "Processed image 7842\n",
      "Processed image 7843\n",
      "Processed image 7844\n",
      "Processed image 7845\n",
      "Processed image 7846\n",
      "Processed image 7847\n",
      "Processed image 7848\n",
      "Processed image 7849\n",
      "Processed image 7850\n",
      "Processed image 7851\n",
      "Processed image 7852\n",
      "Processed image 7853\n",
      "Processed image 7854\n",
      "Processed image 7855\n",
      "Processed image 7856\n",
      "Processed image 7857\n",
      "Processed image 7858\n",
      "Processed image 7859\n",
      "Processed image 7860\n",
      "Processed image 7861\n",
      "Processed image 7862\n",
      "Processed image 7863\n",
      "Processed image 7864\n",
      "Processed image 7865\n",
      "Processed image 7866\n",
      "Processed image 7867\n",
      "Processed image 7868\n",
      "Processed image 7869\n",
      "Processed image 7870\n",
      "Processed image 7871\n",
      "Processed image 7872\n",
      "Processed image 7873\n",
      "Processed image 7874\n",
      "Processed image 7875\n",
      "Processed image 7876\n",
      "Processed image 7877\n",
      "Processed image 7878\n",
      "Processed image 7879\n",
      "Processed image 7880\n",
      "Processed image 7881\n",
      "Processed image 7882\n",
      "Processed image 7883\n",
      "Processed image 7884\n",
      "Processed image 7885\n",
      "Processed image 7886\n",
      "Processed image 7887\n",
      "Processed image 7888\n",
      "Processed image 7889\n",
      "Processed image 7890\n",
      "Processed image 7891\n",
      "Processed image 7892\n",
      "Processed image 7893\n",
      "Processed image 7894\n",
      "Processed image 7895\n",
      "Processed image 7896\n",
      "Processed image 7897\n",
      "Processed image 7898\n",
      "Processed image 7899\n",
      "Processed image 7900\n",
      "Processed image 7901\n",
      "Processed image 7902\n",
      "Processed image 7903\n",
      "Processed image 7904\n",
      "Processed image 7905\n",
      "Processed image 7906\n",
      "Processed image 7907\n",
      "Processed image 7908\n",
      "Processed image 7909\n",
      "Processed image 7910\n",
      "Processed image 7911\n",
      "Processed image 7912\n",
      "Processed image 7913\n",
      "Processed image 7914\n",
      "Processed image 7915\n",
      "Processed image 7916\n",
      "Processed image 7917\n",
      "Processed image 7918\n",
      "Processed image 7919\n",
      "Processed image 7920\n",
      "Processed image 7921\n",
      "Processed image 7922\n",
      "Processed image 7923\n",
      "Processed image 7924\n",
      "Processed image 7925\n",
      "Processed image 7926\n",
      "Processed image 7927\n",
      "Processed image 7928\n",
      "Processed image 7929\n",
      "Processed image 7930\n",
      "Processed image 7931\n",
      "Processed image 7932\n",
      "Processed image 7933\n",
      "Processed image 7934\n",
      "Processed image 7935\n",
      "Processed image 7936\n",
      "Processed image 7937\n",
      "Processed image 7938\n",
      "Processed image 7939\n",
      "Processed image 7940\n",
      "Processed image 7941\n",
      "Processed image 7942\n",
      "Processed image 7943\n",
      "Processed image 7944\n",
      "Processed image 7945\n",
      "Processed image 7946\n",
      "Processed image 7947\n",
      "Processed image 7948\n",
      "Processed image 7949\n",
      "Processed image 7950\n",
      "Processed image 7951\n",
      "Processed image 7952\n",
      "Processed image 7953\n",
      "Processed image 7954\n",
      "Processed image 7955\n",
      "Processed image 7956\n",
      "Processed image 7957\n",
      "Processed image 7958\n",
      "Processed image 7959\n",
      "Processed image 7960\n",
      "Processed image 7961\n",
      "Processed image 7962\n",
      "Processed image 7963\n",
      "Processed image 7964\n",
      "Processed image 7965\n",
      "Processed image 7966\n",
      "Processed image 7967\n",
      "Processed image 7968\n",
      "Processed image 7969\n",
      "Processed image 7970\n",
      "Processed image 7971\n",
      "Processed image 7972\n",
      "Processed image 7973\n",
      "Processed image 7974\n",
      "Processed image 7975\n",
      "Processed image 7976\n",
      "Processed image 7977\n",
      "Processed image 7978\n",
      "Processed image 7979\n",
      "Processed image 7980\n",
      "Processed image 7981\n",
      "Processed image 7982\n",
      "Processed image 7983\n",
      "Processed image 7984\n",
      "Processed image 7985\n",
      "Processed image 7986\n",
      "Processed image 7987\n",
      "Processed image 7988\n",
      "Processed image 7989\n",
      "Processed image 7990\n",
      "Processed image 7991\n",
      "Processed image 7992\n",
      "Processed image 7993\n",
      "Processed image 7994\n",
      "Processed image 7995\n",
      "Processed image 7996\n",
      "Processed image 7997\n",
      "Processed image 7998\n",
      "Processed image 7999\n",
      "Processed image 8000\n",
      "Processed image 8001\n",
      "Processed image 8002\n",
      "Processed image 8003\n",
      "Processed image 8004\n",
      "Processed image 8005\n",
      "Processed image 8006\n",
      "Processed image 8007\n",
      "Processed image 8008\n",
      "Processed image 8009\n",
      "Processed image 8010\n",
      "Processed image 8011\n",
      "Processed image 8012\n",
      "Processed image 8013\n",
      "Processed image 8014\n",
      "Processed image 8015\n",
      "Processed image 8016\n",
      "Processed image 8017\n",
      "Processed image 8018\n",
      "Processed image 8019\n",
      "Processed image 8020\n",
      "Processed image 8021\n",
      "Processed image 8022\n",
      "Processed image 8023\n",
      "Processed image 8024\n",
      "Processed image 8025\n",
      "Processed image 8026\n",
      "Processed image 8027\n",
      "Processed image 8028\n",
      "Processed image 8029\n",
      "Processed image 8030\n",
      "Processed image 8031\n",
      "Processed image 8032\n",
      "Processed image 8033\n",
      "Processed image 8034\n",
      "Processed image 8035\n",
      "Processed image 8036\n",
      "Processed image 8037\n",
      "Processed image 8038\n",
      "Processed image 8039\n",
      "Processed image 8040\n",
      "Processed image 8041\n",
      "Processed image 8042\n",
      "Processed image 8043\n",
      "Processed image 8044\n",
      "Processed image 8045\n",
      "Processed image 8046\n",
      "Processed image 8047\n",
      "Processed image 8048\n",
      "Processed image 8049\n",
      "Processed image 8050\n",
      "Processed image 8051\n",
      "Processed image 8052\n",
      "Processed image 8053\n",
      "Processed image 8054\n",
      "Processed image 8055\n",
      "Processed image 8056\n",
      "Processed image 8057\n",
      "Processed image 8058\n",
      "Processed image 8059\n",
      "Processed image 8060\n",
      "Processed image 8061\n",
      "Processed image 8062\n",
      "Processed image 8063\n",
      "Processed image 8064\n",
      "Processed image 8065\n",
      "Processed image 8066\n",
      "Processed image 8067\n",
      "Processed image 8068\n",
      "Processed image 8069\n",
      "Processed image 8070\n",
      "Processed image 8071\n",
      "Processed image 8072\n",
      "Processed image 8073\n",
      "Processed image 8074\n",
      "Processed image 8075\n",
      "Processed image 8076\n",
      "Processed image 8077\n",
      "Processed image 8078\n",
      "Processed image 8079\n",
      "Processed image 8080\n",
      "Processed image 8081\n",
      "Processed image 8082\n",
      "Processed image 8083\n",
      "Processed image 8084\n",
      "Processed image 8085\n",
      "Processed image 8086\n",
      "Processed image 8087\n",
      "Processed image 8088\n",
      "Processed image 8089\n",
      "Processed image 8090\n",
      "Processed image 8091\n",
      "Processed image 8092\n",
      "Processed image 8093\n",
      "Processed image 8094\n",
      "Processed image 8095\n",
      "Processed image 8096\n",
      "Processed image 8097\n",
      "Processed image 8098\n",
      "Processed image 8099\n",
      "Processed image 8100\n",
      "Processed image 8101\n",
      "Processed image 8102\n",
      "Processed image 8103\n",
      "Processed image 8104\n",
      "Processed image 8105\n",
      "Processed image 8106\n",
      "Processed image 8107\n",
      "Processed image 8108\n",
      "Processed image 8109\n",
      "Processed image 8110\n",
      "Processed image 8111\n",
      "Processed image 8112\n",
      "Processed image 8113\n",
      "Processed image 8114\n",
      "Processed image 8115\n",
      "Processed image 8116\n",
      "Processed image 8117\n",
      "Processed image 8118\n",
      "Processed image 8119\n",
      "Processed image 8120\n",
      "Processed image 8121\n",
      "Processed image 8122\n",
      "Processed image 8123\n",
      "Processed image 8124\n",
      "Processed image 8125\n",
      "Processed image 8126\n",
      "Processed image 8127\n",
      "Processed image 8128\n",
      "Processed image 8129\n",
      "Processed image 8130\n",
      "Processed image 8131\n",
      "Processed image 8132\n",
      "Processed image 8133\n",
      "Processed image 8134\n",
      "Processed image 8135\n",
      "Processed image 8136\n",
      "Processed image 8137\n",
      "Processed image 8138\n",
      "Processed image 8139\n",
      "Processed image 8140\n",
      "Processed image 8141\n",
      "Processed image 8142\n",
      "Processed image 8143\n",
      "Processed image 8144\n",
      "Processed image 8145\n",
      "Processed image 8146\n",
      "Processed image 8147\n",
      "Processed image 8148\n",
      "Processed image 8149\n",
      "Processed image 8150\n",
      "Processed image 8151\n",
      "Processed image 8152\n",
      "Processed image 8153\n",
      "Processed image 8154\n",
      "Processed image 8155\n",
      "Processed image 8156\n",
      "Processed image 8157\n",
      "Processed image 8158\n",
      "Processed image 8159\n",
      "Processed image 8160\n",
      "Processed image 8161\n",
      "Processed image 8162\n",
      "Processed image 8163\n",
      "Processed image 8164\n",
      "Processed image 8165\n",
      "Processed image 8166\n",
      "Processed image 8167\n",
      "Processed image 8168\n",
      "Processed image 8169\n",
      "Processed image 8170\n",
      "Processed image 8171\n",
      "Processed image 8172\n",
      "Processed image 8173\n",
      "Processed image 8174\n",
      "Processed image 8175\n",
      "Processed image 8176\n",
      "Processed image 8177\n",
      "Processed image 8178\n",
      "Processed image 8179\n",
      "Processed image 8180\n",
      "Processed image 8181\n",
      "Processed image 8182\n",
      "Processed image 8183\n",
      "Processed image 8184\n",
      "Processed image 8185\n",
      "Processed image 8186\n",
      "Processed image 8187\n",
      "Processed image 8188\n",
      "Processed image 8189\n",
      "Processed image 8190\n",
      "Processed image 8191\n",
      "Processed image 8192\n",
      "Processed image 8193\n",
      "Processed image 8194\n",
      "Processed image 8195\n",
      "Processed image 8196\n",
      "Processed image 8197\n",
      "Processed image 8198\n",
      "Processed image 8199\n",
      "Processed image 8200\n",
      "Processed image 8201\n",
      "Processed image 8202\n",
      "Processed image 8203\n",
      "Processed image 8204\n",
      "Processed image 8205\n",
      "Processed image 8206\n",
      "Processed image 8207\n",
      "Processed image 8208\n",
      "Processed image 8209\n",
      "Processed image 8210\n",
      "Processed image 8211\n",
      "Processed image 8212\n",
      "Processed image 8213\n",
      "Processed image 8214\n",
      "Processed image 8215\n",
      "Processed image 8216\n",
      "Processed image 8217\n",
      "Processed image 8218\n",
      "Processed image 8219\n",
      "Processed image 8220\n",
      "Processed image 8221\n",
      "Processed image 8222\n",
      "Processed image 8223\n",
      "Processed image 8224\n",
      "Processed image 8225\n",
      "Processed image 8226\n",
      "Processed image 8227\n",
      "Processed image 8228\n",
      "Processed image 8229\n",
      "Processed image 8230\n",
      "Processed image 8231\n",
      "Processed image 8232\n",
      "Processed image 8233\n",
      "Processed image 8234\n",
      "Processed image 8235\n",
      "Processed image 8236\n",
      "Processed image 8237\n",
      "Processed image 8238\n",
      "Processed image 8239\n",
      "Processed image 8240\n",
      "Processed image 8241\n",
      "Processed image 8242\n",
      "Processed image 8243\n",
      "Processed image 8244\n",
      "Processed image 8245\n",
      "Processed image 8246\n",
      "Processed image 8247\n",
      "Processed image 8248\n",
      "Processed image 8249\n",
      "Processed image 8250\n",
      "Processed image 8251\n",
      "Processed image 8252\n",
      "Processed image 8253\n",
      "Processed image 8254\n",
      "Processed image 8255\n",
      "Processed image 8256\n",
      "Processed image 8257\n",
      "Processed image 8258\n",
      "Processed image 8259\n",
      "Processed image 8260\n",
      "Processed image 8261\n",
      "Processed image 8262\n",
      "Processed image 8263\n",
      "Processed image 8264\n",
      "Processed image 8265\n",
      "Processed image 8266\n",
      "Processed image 8267\n",
      "Processed image 8268\n",
      "Processed image 8269\n",
      "Processed image 8270\n",
      "Processed image 8271\n",
      "Processed image 8272\n",
      "Processed image 8273\n",
      "Processed image 8274\n",
      "Processed image 8275\n",
      "Processed image 8276\n",
      "Processed image 8277\n",
      "Processed image 8278\n",
      "Processed image 8279\n",
      "Processed image 8280\n",
      "Processed image 8281\n",
      "Processed image 8282\n",
      "Processed image 8283\n",
      "Processed image 8284\n",
      "Processed image 8285\n",
      "Processed image 8286\n",
      "Processed image 8287\n",
      "Processed image 8288\n",
      "Processed image 8289\n",
      "Processed image 8290\n",
      "Processed image 8291\n",
      "Processed image 8292\n",
      "Processed image 8293\n",
      "Processed image 8294\n",
      "Processed image 8295\n",
      "Processed image 8296\n",
      "Processed image 8297\n",
      "Processed image 8298\n",
      "Processed image 8299\n",
      "Processed image 8300\n",
      "Processed image 8301\n",
      "Processed image 8302\n",
      "Processed image 8303\n",
      "Processed image 8304\n",
      "Processed image 8305\n",
      "Processed image 8306\n",
      "Processed image 8307\n",
      "Processed image 8308\n",
      "Processed image 8309\n",
      "Processed image 8310\n",
      "Processed image 8311\n",
      "Processed image 8312\n",
      "Processed image 8313\n",
      "Processed image 8314\n",
      "Processed image 8315\n",
      "Processed image 8316\n",
      "Processed image 8317\n",
      "Processed image 8318\n",
      "Processed image 8319\n",
      "Processed image 8320\n",
      "Processed image 8321\n",
      "Processed image 8322\n",
      "Processed image 8323\n",
      "Processed image 8324\n",
      "Processed image 8325\n",
      "Processed image 8326\n",
      "Processed image 8327\n",
      "Processed image 8328\n",
      "Processed image 8329\n",
      "Processed image 8330\n",
      "Processed image 8331\n",
      "Processed image 8332\n",
      "Processed image 8333\n",
      "Processed image 8334\n",
      "Processed image 8335\n",
      "Processed image 8336\n",
      "Processed image 8337\n",
      "Processed image 8338\n",
      "Processed image 8339\n",
      "Processed image 8340\n",
      "Processed image 8341\n",
      "Processed image 8342\n",
      "Processed image 8343\n",
      "Processed image 8344\n",
      "Processed image 8345\n",
      "Processed image 8346\n",
      "Processed image 8347\n",
      "Processed image 8348\n",
      "Processed image 8349\n",
      "Processed image 8350\n",
      "Processed image 8351\n",
      "Processed image 8352\n",
      "Processed image 8353\n",
      "Processed image 8354\n",
      "Processed image 8355\n",
      "Processed image 8356\n",
      "Processed image 8357\n",
      "Processed image 8358\n",
      "Processed image 8359\n",
      "Processed image 8360\n",
      "Processed image 8361\n",
      "Processed image 8362\n",
      "Processed image 8363\n",
      "Processed image 8364\n",
      "Processed image 8365\n",
      "Processed image 8366\n",
      "Processed image 8367\n",
      "Processed image 8368\n",
      "Processed image 8369\n",
      "Processed image 8370\n",
      "Processed image 8371\n",
      "Processed image 8372\n",
      "Processed image 8373\n",
      "Processed image 8374\n",
      "Processed image 8375\n",
      "Processed image 8376\n",
      "Processed image 8377\n",
      "Processed image 8378\n",
      "Processed image 8379\n",
      "Processed image 8380\n",
      "Processed image 8381\n",
      "Processed image 8382\n",
      "Processed image 8383\n",
      "Processed image 8384\n",
      "Processed image 8385\n",
      "Processed image 8386\n",
      "Processed image 8387\n",
      "Processed image 8388\n",
      "Processed image 8389\n",
      "Processed image 8390\n",
      "Processed image 8391\n",
      "Processed image 8392\n",
      "Processed image 8393\n",
      "Processed image 8394\n",
      "Processed image 8395\n",
      "Processed image 8396\n",
      "Processed image 8397\n",
      "Processed image 8398\n",
      "Processed image 8399\n",
      "Processed image 8400\n",
      "Processed image 8401\n",
      "Processed image 8402\n",
      "Processed image 8403\n",
      "Processed image 8404\n",
      "Processed image 8405\n",
      "Processed image 8406\n",
      "Processed image 8407\n",
      "Processed image 8408\n",
      "Processed image 8409\n",
      "Processed image 8410\n",
      "Processed image 8411\n",
      "Processed image 8412\n",
      "Processed image 8413\n",
      "Processed image 8414\n",
      "Processed image 8415\n",
      "Processed image 8416\n",
      "Processed image 8417\n",
      "Processed image 8418\n",
      "Processed image 8419\n",
      "Processed image 8420\n",
      "Processed image 8421\n",
      "Processed image 8422\n",
      "Processed image 8423\n",
      "Processed image 8424\n",
      "Processed image 8425\n",
      "Processed image 8426\n",
      "Processed image 8427\n",
      "Processed image 8428\n",
      "Processed image 8429\n",
      "Processed image 8430\n",
      "Processed image 8431\n",
      "Processed image 8432\n",
      "Processed image 8433\n",
      "Processed image 8434\n",
      "Processed image 8435\n",
      "Processed image 8436\n",
      "Processed image 8437\n",
      "Processed image 8438\n",
      "Processed image 8439\n",
      "Processed image 8440\n",
      "Processed image 8441\n",
      "Processed image 8442\n",
      "Processed image 8443\n",
      "Processed image 8444\n",
      "Processed image 8445\n",
      "Processed image 8446\n",
      "Processed image 8447\n",
      "Processed image 8448\n",
      "Processed image 8449\n",
      "Processed image 8450\n",
      "Processed image 8451\n",
      "Processed image 8452\n",
      "Processed image 8453\n",
      "Processed image 8454\n",
      "Processed image 8455\n",
      "Processed image 8456\n",
      "Processed image 8457\n",
      "Processed image 8458\n",
      "Processed image 8459\n",
      "Processed image 8460\n",
      "Processed image 8461\n",
      "Processed image 8462\n",
      "Processed image 8463\n",
      "Processed image 8464\n",
      "Processed image 8465\n",
      "Processed image 8466\n",
      "Processed image 8467\n",
      "Processed image 8468\n",
      "Processed image 8469\n",
      "Processed image 8470\n",
      "Processed image 8471\n",
      "Processed image 8472\n",
      "Processed image 8473\n",
      "Processed image 8474\n",
      "Processed image 8475\n",
      "Processed image 8476\n",
      "Processed image 8477\n",
      "Processed image 8478\n",
      "Processed image 8479\n",
      "Processed image 8480\n",
      "Processed image 8481\n",
      "Processed image 8482\n",
      "Processed image 8483\n",
      "Processed image 8484\n",
      "Processed image 8485\n",
      "Processed image 8486\n",
      "Processed image 8487\n",
      "Processed image 8488\n",
      "Processed image 8489\n",
      "Processed image 8490\n",
      "Processed image 8491\n",
      "Processed image 8492\n",
      "Processed image 8493\n",
      "Processed image 8494\n",
      "Processed image 8495\n",
      "Processed image 8496\n",
      "Processed image 8497\n",
      "Processed image 8498\n",
      "Processed image 8499\n",
      "Processed image 8500\n",
      "Processed image 8501\n",
      "Processed image 8502\n",
      "Processed image 8503\n",
      "Processed image 8504\n",
      "Processed image 8505\n",
      "Processed image 8506\n",
      "Processed image 8507\n",
      "Processed image 8508\n",
      "Processed image 8509\n",
      "Processed image 8510\n",
      "Processed image 8511\n",
      "Processed image 8512\n",
      "Processed image 8513\n",
      "Processed image 8514\n",
      "Processed image 8515\n",
      "Processed image 8516\n",
      "Processed image 8517\n",
      "Processed image 8518\n",
      "Processed image 8519\n",
      "Processed image 8520\n",
      "Processed image 8521\n",
      "Processed image 8522\n",
      "Processed image 8523\n",
      "Processed image 8524\n",
      "Processed image 8525\n",
      "Processed image 8526\n",
      "Processed image 8527\n",
      "Processed image 8528\n",
      "Processed image 8529\n",
      "Processed image 8530\n",
      "Processed image 8531\n",
      "Processed image 8532\n",
      "Processed image 8533\n",
      "Processed image 8534\n",
      "Processed image 8535\n",
      "Processed image 8536\n",
      "Processed image 8537\n",
      "Processed image 8538\n",
      "Processed image 8539\n",
      "Processed image 8540\n",
      "Processed image 8541\n",
      "Processed image 8542\n",
      "Processed image 8543\n",
      "Processed image 8544\n",
      "Processed image 8545\n",
      "Processed image 8546\n",
      "Processed image 8547\n",
      "Processed image 8548\n",
      "Processed image 8549\n",
      "Processed image 8550\n",
      "Processed image 8551\n",
      "Processed image 8552\n",
      "Processed image 8553\n",
      "Processed image 8554\n",
      "Processed image 8555\n",
      "Processed image 8556\n",
      "Processed image 8557\n",
      "Processed image 8558\n",
      "Processed image 8559\n",
      "Processed image 8560\n",
      "Processed image 8561\n",
      "Processed image 8562\n",
      "Processed image 8563\n",
      "Processed image 8564\n",
      "Processed image 8565\n",
      "Processed image 8566\n",
      "Processed image 8567\n",
      "Processed image 8568\n",
      "Processed image 8569\n",
      "Processed image 8570\n",
      "Processed image 8571\n",
      "Processed image 8572\n",
      "Processed image 8573\n",
      "Processed image 8574\n",
      "Processed image 8575\n",
      "Processed image 8576\n",
      "Processed image 8577\n",
      "Processed image 8578\n",
      "Processed image 8579\n",
      "Processed image 8580\n",
      "Processed image 8581\n",
      "Processed image 8582\n",
      "Processed image 8583\n",
      "Processed image 8584\n",
      "Processed image 8585\n",
      "Processed image 8586\n",
      "Processed image 8587\n",
      "Processed image 8588\n",
      "Processed image 8589\n",
      "Processed image 8590\n",
      "Processed image 8591\n",
      "Processed image 8592\n",
      "Processed image 8593\n",
      "Processed image 8594\n",
      "Processed image 8595\n",
      "Processed image 8596\n",
      "Processed image 8597\n",
      "Processed image 8598\n",
      "Processed image 8599\n",
      "Processed image 8600\n",
      "Processed image 8601\n",
      "Processed image 8602\n",
      "Processed image 8603\n",
      "Processed image 8604\n",
      "Processed image 8605\n",
      "Processed image 8606\n",
      "Processed image 8607\n",
      "Processed image 8608\n",
      "Processed image 8609\n",
      "Processed image 8610\n",
      "Processed image 8611\n",
      "Processed image 8612\n",
      "Processed image 8613\n",
      "Processed image 8614\n",
      "Processed image 8615\n",
      "Processed image 8616\n",
      "Processed image 8617\n",
      "Processed image 8618\n",
      "Processed image 8619\n",
      "Processed image 8620\n",
      "Processed image 8621\n",
      "Processed image 8622\n",
      "Processed image 8623\n",
      "Processed image 8624\n",
      "Processed image 8625\n",
      "Processed image 8626\n",
      "Processed image 8627\n",
      "Processed image 8628\n",
      "Processed image 8629\n",
      "Processed image 8630\n",
      "Processed image 8631\n",
      "Processed image 8632\n",
      "Processed image 8633\n",
      "Processed image 8634\n",
      "Processed image 8635\n",
      "Processed image 8636\n",
      "Processed image 8637\n",
      "Processed image 8638\n",
      "Processed image 8639\n",
      "Processed image 8640\n",
      "Processed image 8641\n",
      "Processed image 8642\n",
      "Processed image 8643\n",
      "Processed image 8644\n",
      "Processed image 8645\n",
      "Processed image 8646\n",
      "Processed image 8647\n",
      "Processed image 8648\n",
      "Processed image 8649\n",
      "Processed image 8650\n",
      "Processed image 8651\n",
      "Processed image 8652\n",
      "Processed image 8653\n",
      "Processed image 8654\n",
      "Processed image 8655\n",
      "Processed image 8656\n",
      "Processed image 8657\n",
      "Processed image 8658\n",
      "Processed image 8659\n",
      "Processed image 8660\n",
      "Processed image 8661\n",
      "Processed image 8662\n",
      "Processed image 8663\n",
      "Processed image 8664\n",
      "Processed image 8665\n",
      "Processed image 8666\n",
      "Processed image 8667\n",
      "Processed image 8668\n",
      "Processed image 8669\n",
      "Processed image 8670\n",
      "Processed image 8671\n",
      "Processed image 8672\n",
      "Processed image 8673\n",
      "Processed image 8674\n",
      "Processed image 8675\n",
      "Processed image 8676\n",
      "Processed image 8677\n",
      "Processed image 8678\n",
      "Processed image 8679\n",
      "Processed image 8680\n",
      "Processed image 8681\n",
      "Processed image 8682\n",
      "Processed image 8683\n",
      "Processed image 8684\n",
      "Processed image 8685\n",
      "Processed image 8686\n",
      "Processed image 8687\n",
      "Processed image 8688\n",
      "Processed image 8689\n",
      "Processed image 8690\n",
      "Processed image 8691\n",
      "Processed image 8692\n",
      "Processed image 8693\n",
      "Processed image 8694\n",
      "Processed image 8695\n",
      "Processed image 8696\n",
      "Processed image 8697\n",
      "Processed image 8698\n",
      "Processed image 8699\n",
      "Processed image 8700\n",
      "Processed image 8701\n",
      "Processed image 8702\n",
      "Processed image 8703\n",
      "Processed image 8704\n",
      "Processed image 8705\n",
      "Processed image 8706\n",
      "Processed image 8707\n",
      "Processed image 8708\n",
      "Processed image 8709\n",
      "Processed image 8710\n",
      "Processed image 8711\n",
      "Processed image 8712\n",
      "Processed image 8713\n",
      "Processed image 8714\n",
      "Processed image 8715\n",
      "Processed image 8716\n",
      "Processed image 8717\n",
      "Processed image 8718\n",
      "Processed image 8719\n",
      "Processed image 8720\n",
      "Processed image 8721\n",
      "Processed image 8722\n",
      "Processed image 8723\n",
      "Processed image 8724\n",
      "Processed image 8725\n",
      "Processed image 8726\n",
      "Processed image 8727\n",
      "Processed image 8728\n",
      "Processed image 8729\n",
      "Processed image 8730\n",
      "Processed image 8731\n",
      "Processed image 8732\n",
      "Processed image 8733\n",
      "Processed image 8734\n",
      "Processed image 8735\n",
      "Processed image 8736\n",
      "Processed image 8737\n",
      "Processed image 8738\n",
      "Processed image 8739\n",
      "Processed image 8740\n",
      "Processed image 8741\n",
      "Processed image 8742\n",
      "Processed image 8743\n",
      "Processed image 8744\n",
      "Processed image 8745\n",
      "Processed image 8746\n",
      "Processed image 8747\n",
      "Processed image 8748\n",
      "Processed image 8749\n",
      "Processed image 8750\n",
      "Processed image 8751\n",
      "Processed image 8752\n",
      "Processed image 8753\n",
      "Processed image 8754\n",
      "Processed image 8755\n",
      "Processed image 8756\n",
      "Processed image 8757\n",
      "Processed image 8758\n",
      "Processed image 8759\n",
      "Processed image 8760\n",
      "Processed image 8761\n",
      "Processed image 8762\n",
      "Processed image 8763\n",
      "Processed image 8764\n",
      "Processed image 8765\n",
      "Processed image 8766\n",
      "Processed image 8767\n",
      "Processed image 8768\n",
      "Processed image 8769\n",
      "Processed image 8770\n",
      "Processed image 8771\n",
      "Processed image 8772\n",
      "Processed image 8773\n",
      "Processed image 8774\n",
      "Processed image 8775\n",
      "Processed image 8776\n",
      "Processed image 8777\n",
      "Processed image 8778\n",
      "Processed image 8779\n",
      "Processed image 8780\n",
      "Processed image 8781\n",
      "Processed image 8782\n",
      "Processed image 8783\n",
      "Processed image 8784\n",
      "Processed image 8785\n",
      "Processed image 8786\n",
      "Processed image 8787\n",
      "Processed image 8788\n",
      "Processed image 8789\n",
      "Processed image 8790\n",
      "Processed image 8791\n",
      "Processed image 8792\n",
      "Processed image 8793\n",
      "Processed image 8794\n",
      "Processed image 8795\n",
      "Processed image 8796\n",
      "Processed image 8797\n",
      "Processed image 8798\n",
      "Processed image 8799\n",
      "Processed image 8800\n",
      "Processed image 8801\n",
      "Processed image 8802\n",
      "Processed image 8803\n",
      "Processed image 8804\n",
      "Processed image 8805\n",
      "Processed image 8806\n",
      "Processed image 8807\n",
      "Processed image 8808\n",
      "Processed image 8809\n",
      "Processed image 8810\n",
      "Processed image 8811\n",
      "Processed image 8812\n",
      "Processed image 8813\n",
      "Processed image 8814\n",
      "Processed image 8815\n",
      "Processed image 8816\n",
      "Processed image 8817\n",
      "Processed image 8818\n",
      "Processed image 8819\n",
      "Processed image 8820\n",
      "Processed image 8821\n",
      "Processed image 8822\n",
      "Processed image 8823\n",
      "Processed image 8824\n",
      "Processed image 8825\n",
      "Processed image 8826\n",
      "Processed image 8827\n",
      "Processed image 8828\n",
      "Processed image 8829\n",
      "Processed image 8830\n",
      "Processed image 8831\n",
      "Processed image 8832\n",
      "Processed image 8833\n",
      "Processed image 8834\n",
      "Processed image 8835\n",
      "Processed image 8836\n",
      "Processed image 8837\n",
      "Processed image 8838\n",
      "Processed image 8839\n",
      "Processed image 8840\n",
      "Processed image 8841\n",
      "Processed image 8842\n",
      "Processed image 8843\n",
      "Processed image 8844\n",
      "Processed image 8845\n",
      "Processed image 8846\n",
      "Processed image 8847\n",
      "Processed image 8848\n",
      "Processed image 8849\n",
      "Processed image 8850\n",
      "Processed image 8851\n",
      "Processed image 8852\n",
      "Processed image 8853\n",
      "Processed image 8854\n",
      "Processed image 8855\n",
      "Processed image 8856\n",
      "Processed image 8857\n",
      "Processed image 8858\n",
      "Processed image 8859\n",
      "Processed image 8860\n",
      "Processed image 8861\n",
      "Processed image 8862\n",
      "Processed image 8863\n",
      "Processed image 8864\n",
      "Processed image 8865\n",
      "Processed image 8866\n",
      "Processed image 8867\n",
      "Processed image 8868\n",
      "Processed image 8869\n",
      "Processed image 8870\n",
      "Processed image 8871\n",
      "Processed image 8872\n",
      "Processed image 8873\n",
      "Processed image 8874\n",
      "Processed image 8875\n",
      "Processed image 8876\n",
      "Processed image 8877\n",
      "Processed image 8878\n",
      "Processed image 8879\n",
      "Processed image 8880\n",
      "Processed image 8881\n",
      "Processed image 8882\n",
      "Processed image 8883\n",
      "Processed image 8884\n",
      "Processed image 8885\n",
      "Processed image 8886\n",
      "Processed image 8887\n",
      "Processed image 8888\n",
      "Processed image 8889\n",
      "Processed image 8890\n",
      "Processed image 8891\n",
      "Processed image 8892\n",
      "Processed image 8893\n",
      "Processed image 8894\n",
      "Processed image 8895\n",
      "Processed image 8896\n",
      "Processed image 8897\n",
      "Processed image 8898\n",
      "Processed image 8899\n",
      "Processed image 8900\n",
      "Processed image 8901\n",
      "Processed image 8902\n",
      "Processed image 8903\n",
      "Processed image 8904\n",
      "Processed image 8905\n",
      "Processed image 8906\n",
      "Processed image 8907\n",
      "Processed image 8908\n",
      "Processed image 8909\n",
      "Processed image 8910\n",
      "Processed image 8911\n",
      "Processed image 8912\n",
      "Processed image 8913\n",
      "Processed image 8914\n",
      "Processed image 8915\n",
      "Processed image 8916\n",
      "Processed image 8917\n",
      "Processed image 8918\n",
      "Processed image 8919\n",
      "Processed image 8920\n",
      "Processed image 8921\n",
      "Processed image 8922\n",
      "Processed image 8923\n",
      "Processed image 8924\n",
      "Processed image 8925\n",
      "Processed image 8926\n",
      "Processed image 8927\n",
      "Processed image 8928\n",
      "Processed image 8929\n",
      "Processed image 8930\n",
      "Processed image 8931\n",
      "Processed image 8932\n",
      "Processed image 8933\n",
      "Processed image 8934\n",
      "Processed image 8935\n",
      "Processed image 8936\n",
      "Processed image 8937\n",
      "Processed image 8938\n",
      "Processed image 8939\n",
      "Processed image 8940\n",
      "Processed image 8941\n",
      "Processed image 8942\n",
      "Processed image 8943\n",
      "Processed image 8944\n",
      "Processed image 8945\n",
      "Processed image 8946\n",
      "Processed image 8947\n",
      "Processed image 8948\n",
      "Processed image 8949\n",
      "Processed image 8950\n",
      "Processed image 8951\n",
      "Processed image 8952\n",
      "Processed image 8953\n",
      "Processed image 8954\n",
      "Processed image 8955\n",
      "Processed image 8956\n",
      "Processed image 8957\n",
      "Processed image 8958\n",
      "Processed image 8959\n",
      "Processed image 8960\n",
      "Processed image 8961\n",
      "Processed image 8962\n",
      "Processed image 8963\n",
      "Processed image 8964\n",
      "Processed image 8965\n",
      "Processed image 8966\n",
      "Processed image 8967\n",
      "Processed image 8968\n",
      "Processed image 8969\n",
      "Processed image 8970\n",
      "Processed image 8971\n",
      "Processed image 8972\n",
      "Processed image 8973\n",
      "Processed image 8974\n",
      "Processed image 8975\n",
      "Processed image 8976\n",
      "Processed image 8977\n",
      "Processed image 8978\n",
      "Processed image 8979\n",
      "Processed image 8980\n",
      "Processed image 8981\n",
      "Processed image 8982\n",
      "Processed image 8983\n",
      "Processed image 8984\n",
      "Processed image 8985\n",
      "Processed image 8986\n",
      "Processed image 8987\n",
      "Processed image 8988\n",
      "Processed image 8989\n",
      "Processed image 8990\n",
      "Processed image 8991\n",
      "Processed image 8992\n",
      "Processed image 8993\n",
      "Processed image 8994\n",
      "Processed image 8995\n",
      "Processed image 8996\n",
      "Processed image 8997\n",
      "Processed image 8998\n",
      "Processed image 8999\n",
      "Processed image 9000\n",
      "Processed image 9001\n",
      "Processed image 9002\n",
      "Processed image 9003\n",
      "Processed image 9004\n",
      "Processed image 9005\n",
      "Processed image 9006\n",
      "Processed image 9007\n",
      "Processed image 9008\n",
      "Processed image 9009\n",
      "Processed image 9010\n",
      "Processed image 9011\n",
      "Processed image 9012\n",
      "Processed image 9013\n",
      "Processed image 9014\n",
      "Processed image 9015\n",
      "Processed image 9016\n",
      "Processed image 9017\n",
      "Processed image 9018\n",
      "Processed image 9019\n",
      "Processed image 9020\n",
      "Processed image 9021\n",
      "Processed image 9022\n",
      "Processed image 9023\n",
      "Processed image 9024\n",
      "Processed image 9025\n",
      "Processed image 9026\n",
      "Processed image 9027\n",
      "Processed image 9028\n",
      "Processed image 9029\n",
      "Processed image 9030\n",
      "Processed image 9031\n",
      "Processed image 9032\n",
      "Processed image 9033\n",
      "Processed image 9034\n",
      "Processed image 9035\n",
      "Processed image 9036\n",
      "Processed image 9037\n",
      "Processed image 9038\n",
      "Processed image 9039\n",
      "Processed image 9040\n",
      "Processed image 9041\n",
      "Processed image 9042\n",
      "Processed image 9043\n",
      "Processed image 9044\n",
      "Processed image 9045\n",
      "Processed image 9046\n",
      "Processed image 9047\n",
      "Processed image 9048\n",
      "Processed image 9049\n",
      "Processed image 9050\n",
      "Processed image 9051\n",
      "Processed image 9052\n",
      "Processed image 9053\n",
      "Processed image 9054\n",
      "Processed image 9055\n",
      "Processed image 9056\n",
      "Processed image 9057\n",
      "Processed image 9058\n",
      "Processed image 9059\n",
      "Processed image 9060\n",
      "Processed image 9061\n",
      "Processed image 9062\n",
      "Processed image 9063\n",
      "Processed image 9064\n",
      "Processed image 9065\n",
      "Processed image 9066\n",
      "Processed image 9067\n",
      "Processed image 9068\n",
      "Processed image 9069\n",
      "Processed image 9070\n",
      "Processed image 9071\n",
      "Processed image 9072\n",
      "Processed image 9073\n",
      "Processed image 9074\n",
      "Processed image 9075\n",
      "Processed image 9076\n",
      "Processed image 9077\n",
      "Processed image 9078\n",
      "Processed image 9079\n",
      "Processed image 9080\n",
      "Processed image 9081\n",
      "Processed image 9082\n",
      "Processed image 9083\n",
      "Processed image 9084\n",
      "Processed image 9085\n",
      "Processed image 9086\n",
      "Processed image 9087\n",
      "Processed image 9088\n",
      "Processed image 9089\n",
      "Processed image 9090\n",
      "Processed image 9091\n",
      "Processed image 9092\n",
      "Processed image 9093\n",
      "Processed image 9094\n",
      "Processed image 9095\n",
      "Processed image 9096\n",
      "Processed image 9097\n",
      "Processed image 9098\n",
      "Processed image 9099\n",
      "Processed image 9100\n",
      "Processed image 9101\n",
      "Processed image 9102\n",
      "Processed image 9103\n",
      "Processed image 9104\n",
      "Processed image 9105\n",
      "Processed image 9106\n",
      "Processed image 9107\n",
      "Processed image 9108\n",
      "Processed image 9109\n",
      "Processed image 9110\n",
      "Processed image 9111\n",
      "Processed image 9112\n",
      "Processed image 9113\n",
      "Processed image 9114\n",
      "Processed image 9115\n",
      "Processed image 9116\n",
      "Processed image 9117\n",
      "Processed image 9118\n",
      "Processed image 9119\n",
      "Processed image 9120\n",
      "Processed image 9121\n",
      "Processed image 9122\n",
      "Processed image 9123\n",
      "Processed image 9124\n",
      "Processed image 9125\n",
      "Processed image 9126\n",
      "Processed image 9127\n",
      "Processed image 9128\n",
      "Processed image 9129\n",
      "Processed image 9130\n",
      "Processed image 9131\n",
      "Processed image 9132\n",
      "Processed image 9133\n",
      "Processed image 9134\n",
      "Processed image 9135\n",
      "Processed image 9136\n",
      "Processed image 9137\n",
      "Processed image 9138\n",
      "Processed image 9139\n",
      "Processed image 9140\n",
      "Processed image 9141\n",
      "Processed image 9142\n",
      "Processed image 9143\n",
      "Processed image 9144\n",
      "Processed image 9145\n",
      "Processed image 9146\n",
      "Processed image 9147\n",
      "Processed image 9148\n",
      "Processed image 9149\n",
      "Processed image 9150\n",
      "Processed image 9151\n",
      "Processed image 9152\n",
      "Processed image 9153\n",
      "Processed image 9154\n",
      "Processed image 9155\n",
      "Processed image 9156\n",
      "Processed image 9157\n",
      "Processed image 9158\n",
      "Processed image 9159\n",
      "Processed image 9160\n",
      "Processed image 9161\n",
      "Processed image 9162\n",
      "Processed image 9163\n",
      "Processed image 9164\n",
      "Processed image 9165\n",
      "Processed image 9166\n",
      "Processed image 9167\n",
      "Processed image 9168\n",
      "Processed image 9169\n",
      "Processed image 9170\n",
      "Processed image 9171\n",
      "Processed image 9172\n",
      "Processed image 9173\n",
      "Processed image 9174\n",
      "Processed image 9175\n",
      "Processed image 9176\n",
      "Processed image 9177\n",
      "Processed image 9178\n",
      "Processed image 9179\n",
      "Processed image 9180\n",
      "Processed image 9181\n",
      "Processed image 9182\n",
      "Processed image 9183\n",
      "Processed image 9184\n",
      "Processed image 9185\n",
      "Processed image 9186\n",
      "Processed image 9187\n",
      "Processed image 9188\n",
      "Processed image 9189\n",
      "Processed image 9190\n",
      "Processed image 9191\n",
      "Processed image 9192\n",
      "Processed image 9193\n",
      "Processed image 9194\n",
      "Processed image 9195\n",
      "Processed image 9196\n",
      "Processed image 9197\n",
      "Processed image 9198\n",
      "Processed image 9199\n",
      "Processed image 9200\n",
      "Processed image 9201\n",
      "Processed image 9202\n",
      "Processed image 9203\n",
      "Processed image 9204\n",
      "Processed image 9205\n",
      "Processed image 9206\n",
      "Processed image 9207\n",
      "Processed image 9208\n",
      "Processed image 9209\n",
      "Processed image 9210\n",
      "Processed image 9211\n",
      "Processed image 9212\n",
      "Processed image 9213\n",
      "Processed image 9214\n",
      "Processed image 9215\n",
      "Processed image 9216\n",
      "Processed image 9217\n",
      "Processed image 9218\n",
      "Processed image 9219\n",
      "Processed image 9220\n",
      "Processed image 9221\n",
      "Processed image 9222\n",
      "Processed image 9223\n",
      "Processed image 9224\n",
      "Processed image 9225\n",
      "Processed image 9226\n",
      "Processed image 9227\n",
      "Processed image 9228\n",
      "Processed image 9229\n",
      "Processed image 9230\n",
      "Processed image 9231\n",
      "Processed image 9232\n",
      "Processed image 9233\n",
      "Processed image 9234\n",
      "Processed image 9235\n",
      "Processed image 9236\n",
      "Processed image 9237\n",
      "Processed image 9238\n",
      "Processed image 9239\n",
      "Processed image 9240\n",
      "Processed image 9241\n",
      "Processed image 9242\n",
      "Processed image 9243\n",
      "Processed image 9244\n",
      "Processed image 9245\n",
      "Processed image 9246\n",
      "Processed image 9247\n",
      "Processed image 9248\n",
      "Processed image 9249\n",
      "Processed image 9250\n",
      "Processed image 9251\n",
      "Processed image 9252\n",
      "Processed image 9253\n",
      "Processed image 9254\n",
      "Processed image 9255\n",
      "Processed image 9256\n",
      "Processed image 9257\n",
      "Processed image 9258\n",
      "Processed image 9259\n",
      "Processed image 9260\n",
      "Processed image 9261\n",
      "Processed image 9262\n",
      "Processed image 9263\n",
      "Processed image 9264\n",
      "Processed image 9265\n",
      "Processed image 9266\n",
      "Processed image 9267\n",
      "Processed image 9268\n",
      "Processed image 9269\n",
      "Processed image 9270\n",
      "Processed image 9271\n",
      "Processed image 9272\n",
      "Processed image 9273\n",
      "Processed image 9274\n",
      "Processed image 9275\n",
      "Processed image 9276\n",
      "Processed image 9277\n",
      "Processed image 9278\n",
      "Processed image 9279\n",
      "Processed image 9280\n",
      "Processed image 9281\n",
      "Processed image 9282\n",
      "Processed image 9283\n",
      "Processed image 9284\n",
      "Processed image 9285\n",
      "Processed image 9286\n",
      "Processed image 9287\n",
      "Processed image 9288\n",
      "Processed image 9289\n",
      "Processed image 9290\n",
      "Processed image 9291\n",
      "Processed image 9292\n",
      "Processed image 9293\n",
      "Processed image 9294\n",
      "Processed image 9295\n",
      "Processed image 9296\n",
      "Processed image 9297\n",
      "Processed image 9298\n",
      "Processed image 9299\n",
      "Processed image 9300\n",
      "Processed image 9301\n",
      "Processed image 9302\n",
      "Processed image 9303\n",
      "Processed image 9304\n",
      "Processed image 9305\n",
      "Processed image 9306\n",
      "Processed image 9307\n",
      "Processed image 9308\n",
      "Processed image 9309\n",
      "Processed image 9310\n",
      "Processed image 9311\n",
      "Processed image 9312\n",
      "Processed image 9313\n",
      "Processed image 9314\n",
      "Processed image 9315\n",
      "Processed image 9316\n",
      "Processed image 9317\n",
      "Processed image 9318\n",
      "Processed image 9319\n",
      "Processed image 9320\n",
      "Processed image 9321\n",
      "Processed image 9322\n",
      "Processed image 9323\n",
      "Processed image 9324\n",
      "Processed image 9325\n",
      "Processed image 9326\n",
      "Processed image 9327\n",
      "Processed image 9328\n",
      "Processed image 9329\n",
      "Processed image 9330\n",
      "Processed image 9331\n",
      "Processed image 9332\n",
      "Processed image 9333\n",
      "Processed image 9334\n",
      "Processed image 9335\n",
      "Processed image 9336\n",
      "Processed image 9337\n",
      "Processed image 9338\n",
      "Processed image 9339\n",
      "Processed image 9340\n",
      "Processed image 9341\n",
      "Processed image 9342\n",
      "Processed image 9343\n",
      "Processed image 9344\n",
      "Processed image 9345\n",
      "Processed image 9346\n",
      "Processed image 9347\n",
      "Processed image 9348\n",
      "Processed image 9349\n",
      "Processed image 9350\n",
      "Processed image 9351\n",
      "Processed image 9352\n",
      "Processed image 9353\n",
      "Processed image 9354\n",
      "Processed image 9355\n",
      "Processed image 9356\n",
      "Processed image 9357\n",
      "Processed image 9358\n",
      "Processed image 9359\n",
      "Processed image 9360\n",
      "Processed image 9361\n",
      "Processed image 9362\n",
      "Processed image 9363\n",
      "Processed image 9364\n",
      "Processed image 9365\n",
      "Processed image 9366\n",
      "Processed image 9367\n",
      "Processed image 9368\n",
      "Processed image 9369\n",
      "Processed image 9370\n",
      "Processed image 9371\n",
      "Processed image 9372\n",
      "Processed image 9373\n",
      "Processed image 9374\n",
      "Processed image 9375\n",
      "Processed image 9376\n",
      "Processed image 9377\n",
      "Processed image 9378\n",
      "Processed image 9379\n",
      "Processed image 9380\n",
      "Processed image 9381\n",
      "Processed image 9382\n",
      "Processed image 9383\n",
      "Processed image 9384\n",
      "Processed image 9385\n",
      "Processed image 9386\n",
      "Processed image 9387\n",
      "Processed image 9388\n",
      "Processed image 9389\n",
      "Processed image 9390\n",
      "Processed image 9391\n",
      "Processed image 9392\n",
      "Processed image 9393\n",
      "Processed image 9394\n",
      "Processed image 9395\n",
      "Processed image 9396\n",
      "Processed image 9397\n",
      "Processed image 9398\n",
      "Processed image 9399\n",
      "Processed image 9400\n",
      "Processed image 9401\n",
      "Processed image 9402\n",
      "Processed image 9403\n",
      "Processed image 9404\n",
      "Processed image 9405\n",
      "Processed image 9406\n",
      "Processed image 9407\n",
      "Processed image 9408\n",
      "Processed image 9409\n",
      "Processed image 9410\n",
      "Processed image 9411\n",
      "Processed image 9412\n",
      "Processed image 9413\n",
      "Processed image 9414\n",
      "Processed image 9415\n",
      "Processed image 9416\n",
      "Processed image 9417\n",
      "Processed image 9418\n",
      "Processed image 9419\n",
      "Processed image 9420\n",
      "Processed image 9421\n",
      "Processed image 9422\n",
      "Processed image 9423\n",
      "Processed image 9424\n",
      "Processed image 9425\n",
      "Processed image 9426\n",
      "Processed image 9427\n",
      "Processed image 9428\n",
      "Processed image 9429\n",
      "Processed image 9430\n",
      "Processed image 9431\n",
      "Processed image 9432\n",
      "Processed image 9433\n",
      "Processed image 9434\n",
      "Processed image 9435\n",
      "Processed image 9436\n",
      "Processed image 9437\n",
      "Processed image 9438\n",
      "Processed image 9439\n",
      "Processed image 9440\n",
      "Processed image 9441\n",
      "Processed image 9442\n",
      "Processed image 9443\n",
      "Processed image 9444\n",
      "Processed image 9445\n",
      "Processed image 9446\n",
      "Processed image 9447\n",
      "Processed image 9448\n",
      "Processed image 9449\n",
      "Processed image 9450\n",
      "Processed image 9451\n",
      "Processed image 9452\n",
      "Processed image 9453\n",
      "Processed image 9454\n",
      "Processed image 9455\n",
      "Processed image 9456\n",
      "Processed image 9457\n",
      "Processed image 9458\n",
      "Processed image 9459\n",
      "Processed image 9460\n",
      "Processed image 9461\n",
      "Processed image 9462\n",
      "Processed image 9463\n",
      "Processed image 9464\n",
      "Processed image 9465\n",
      "Processed image 9466\n",
      "Processed image 9467\n",
      "Processed image 9468\n",
      "Processed image 9469\n",
      "Processed image 9470\n",
      "Processed image 9471\n",
      "Processed image 9472\n",
      "Processed image 9473\n",
      "Processed image 9474\n",
      "Processed image 9475\n",
      "Processed image 9476\n",
      "Processed image 9477\n",
      "Processed image 9478\n",
      "Processed image 9479\n",
      "Processed image 9480\n",
      "Processed image 9481\n",
      "Processed image 9482\n",
      "Processed image 9483\n",
      "Processed image 9484\n",
      "Processed image 9485\n",
      "Processed image 9486\n",
      "Processed image 9487\n",
      "Processed image 9488\n",
      "Processed image 9489\n",
      "Processed image 9490\n",
      "Processed image 9491\n",
      "Processed image 9492\n",
      "Processed image 9493\n",
      "Processed image 9494\n",
      "Processed image 9495\n",
      "Processed image 9496\n",
      "Processed image 9497\n",
      "Processed image 9498\n",
      "Processed image 9499\n",
      "Processed image 9500\n",
      "Processed image 9501\n",
      "Processed image 9502\n",
      "Processed image 9503\n",
      "Processed image 9504\n",
      "Processed image 9505\n",
      "Processed image 9506\n",
      "Processed image 9507\n",
      "Processed image 9508\n",
      "Processed image 9509\n",
      "Processed image 9510\n",
      "Processed image 9511\n",
      "Processed image 9512\n",
      "Processed image 9513\n",
      "Processed image 9514\n",
      "Processed image 9515\n",
      "Processed image 9516\n",
      "Processed image 9517\n",
      "Processed image 9518\n",
      "Processed image 9519\n",
      "Processed image 9520\n",
      "Processed image 9521\n",
      "Processed image 9522\n",
      "Processed image 9523\n",
      "Processed image 9524\n",
      "Processed image 9525\n",
      "Processed image 9526\n",
      "Processed image 9527\n",
      "Processed image 9528\n",
      "Processed image 9529\n",
      "Processed image 9530\n",
      "Processed image 9531\n",
      "Processed image 9532\n",
      "Processed image 9533\n",
      "Processed image 9534\n",
      "Processed image 9535\n",
      "Processed image 9536\n",
      "Processed image 9537\n",
      "Processed image 9538\n",
      "Processed image 9539\n",
      "Processed image 9540\n",
      "Processed image 9541\n",
      "Processed image 9542\n",
      "Processed image 9543\n",
      "Processed image 9544\n",
      "Processed image 9545\n",
      "Processed image 9546\n",
      "Processed image 9547\n",
      "Processed image 9548\n",
      "Processed image 9549\n",
      "Processed image 9550\n",
      "Processed image 9551\n",
      "Processed image 9552\n",
      "Processed image 9553\n",
      "Processed image 9554\n",
      "Processed image 9555\n",
      "Processed image 9556\n",
      "Processed image 9557\n",
      "Processed image 9558\n",
      "Processed image 9559\n",
      "Processed image 9560\n",
      "Processed image 9561\n",
      "Processed image 9562\n",
      "Processed image 9563\n",
      "Processed image 9564\n",
      "Processed image 9565\n",
      "Processed image 9566\n",
      "Processed image 9567\n",
      "Processed image 9568\n",
      "Processed image 9569\n",
      "Processed image 9570\n",
      "Processed image 9571\n",
      "Processed image 9572\n",
      "Processed image 9573\n",
      "Processed image 9574\n",
      "Processed image 9575\n",
      "Processed image 9576\n",
      "Processed image 9577\n",
      "Processed image 9578\n",
      "Processed image 9579\n",
      "Processed image 9580\n",
      "Processed image 9581\n",
      "Processed image 9582\n",
      "Processed image 9583\n",
      "Processed image 9584\n",
      "Processed image 9585\n",
      "Processed image 9586\n",
      "Processed image 9587\n",
      "Processed image 9588\n",
      "Processed image 9589\n",
      "Processed image 9590\n",
      "Processed image 9591\n",
      "Processed image 9592\n",
      "Processed image 9593\n",
      "Processed image 9594\n",
      "Processed image 9595\n",
      "Processed image 9596\n",
      "Processed image 9597\n",
      "Processed image 9598\n",
      "Processed image 9599\n",
      "Processed image 9600\n",
      "Processed image 9601\n",
      "Processed image 9602\n",
      "Processed image 9603\n",
      "Processed image 9604\n",
      "Processed image 9605\n",
      "Processed image 9606\n",
      "Processed image 9607\n",
      "Processed image 9608\n",
      "Processed image 9609\n",
      "Processed image 9610\n",
      "Processed image 9611\n",
      "Processed image 9612\n",
      "Processed image 9613\n",
      "Processed image 9614\n",
      "Processed image 9615\n",
      "Processed image 9616\n",
      "Processed image 9617\n",
      "Processed image 9618\n",
      "Processed image 9619\n",
      "Processed image 9620\n",
      "Processed image 9621\n",
      "Processed image 9622\n",
      "Processed image 9623\n",
      "Processed image 9624\n",
      "Processed image 9625\n",
      "Processed image 9626\n",
      "Processed image 9627\n",
      "Processed image 9628\n",
      "Processed image 9629\n",
      "Processed image 9630\n",
      "Processed image 9631\n",
      "Processed image 9632\n",
      "Processed image 9633\n",
      "Processed image 9634\n",
      "Processed image 9635\n",
      "Processed image 9636\n",
      "Processed image 9637\n",
      "Processed image 9638\n",
      "Processed image 9639\n",
      "Processed image 9640\n",
      "Processed image 9641\n",
      "Processed image 9642\n",
      "Processed image 9643\n",
      "Processed image 9644\n",
      "Processed image 9645\n",
      "Processed image 9646\n",
      "Processed image 9647\n",
      "Processed image 9648\n",
      "Processed image 9649\n",
      "Processed image 9650\n",
      "Processed image 9651\n",
      "Processed image 9652\n",
      "Processed image 9653\n",
      "Processed image 9654\n",
      "Processed image 9655\n",
      "Processed image 9656\n",
      "Processed image 9657\n",
      "Processed image 9658\n",
      "Processed image 9659\n",
      "Processed image 9660\n",
      "Processed image 9661\n",
      "Processed image 9662\n",
      "Processed image 9663\n",
      "Processed image 9664\n",
      "Processed image 9665\n",
      "Processed image 9666\n",
      "Processed image 9667\n",
      "Processed image 9668\n",
      "Processed image 9669\n",
      "Processed image 9670\n",
      "Processed image 9671\n",
      "Processed image 9672\n",
      "Processed image 9673\n",
      "Processed image 9674\n",
      "Processed image 9675\n",
      "Processed image 9676\n",
      "Processed image 9677\n",
      "Processed image 9678\n",
      "Processed image 9679\n",
      "Processed image 9680\n",
      "Processed image 9681\n",
      "Processed image 9682\n",
      "Processed image 9683\n",
      "Processed image 9684\n",
      "Processed image 9685\n",
      "Processed image 9686\n",
      "Processed image 9687\n",
      "Processed image 9688\n",
      "Processed image 9689\n",
      "Processed image 9690\n",
      "Processed image 9691\n",
      "Processed image 9692\n",
      "Processed image 9693\n",
      "Processed image 9694\n",
      "Processed image 9695\n",
      "Processed image 9696\n",
      "Processed image 9697\n",
      "Processed image 9698\n",
      "Processed image 9699\n",
      "Processed image 9700\n",
      "Processed image 9701\n",
      "Processed image 9702\n",
      "Processed image 9703\n",
      "Processed image 9704\n",
      "Processed image 9705\n",
      "Processed image 9706\n",
      "Processed image 9707\n",
      "Processed image 9708\n",
      "Processed image 9709\n",
      "Processed image 9710\n",
      "Processed image 9711\n",
      "Processed image 9712\n",
      "Processed image 9713\n",
      "Processed image 9714\n",
      "Processed image 9715\n",
      "Processed image 9716\n",
      "Processed image 9717\n",
      "Processed image 9718\n",
      "Processed image 9719\n",
      "Processed image 9720\n",
      "Processed image 9721\n",
      "Processed image 9722\n",
      "Processed image 9723\n",
      "Processed image 9724\n",
      "Processed image 9725\n",
      "Processed image 9726\n",
      "Processed image 9727\n",
      "Processed image 9728\n",
      "Processed image 9729\n",
      "Processed image 9730\n",
      "Processed image 9731\n",
      "Processed image 9732\n",
      "Processed image 9733\n",
      "Processed image 9734\n",
      "Processed image 9735\n",
      "Processed image 9736\n",
      "Processed image 9737\n",
      "Processed image 9738\n",
      "Processed image 9739\n",
      "Processed image 9740\n",
      "Processed image 9741\n",
      "Processed image 9742\n",
      "Processed image 9743\n",
      "Processed image 9744\n",
      "Processed image 9745\n",
      "Processed image 9746\n",
      "Processed image 9747\n",
      "Processed image 9748\n",
      "Processed image 9749\n",
      "Processed image 9750\n",
      "Processed image 9751\n",
      "Processed image 9752\n",
      "Processed image 9753\n",
      "Processed image 9754\n",
      "Processed image 9755\n",
      "Processed image 9756\n",
      "Processed image 9757\n",
      "Processed image 9758\n",
      "Processed image 9759\n",
      "Processed image 9760\n",
      "Processed image 9761\n",
      "Processed image 9762\n",
      "Processed image 9763\n",
      "Processed image 9764\n",
      "Processed image 9765\n",
      "Processed image 9766\n",
      "Processed image 9767\n",
      "Processed image 9768\n",
      "Processed image 9769\n",
      "Processed image 9770\n",
      "Processed image 9771\n",
      "Processed image 9772\n",
      "Processed image 9773\n",
      "Processed image 9774\n",
      "Processed image 9775\n",
      "Processed image 9776\n",
      "Processed image 9777\n",
      "Processed image 9778\n",
      "Processed image 9779\n",
      "Processed image 9780\n",
      "Processed image 9781\n",
      "Processed image 9782\n",
      "Processed image 9783\n",
      "Processed image 9784\n",
      "Processed image 9785\n",
      "Processed image 9786\n",
      "Processed image 9787\n",
      "Processed image 9788\n",
      "Processed image 9789\n",
      "Processed image 9790\n",
      "Processed image 9791\n",
      "Processed image 9792\n",
      "Processed image 9793\n",
      "Processed image 9794\n",
      "Processed image 9795\n",
      "Processed image 9796\n",
      "Processed image 9797\n",
      "Processed image 9798\n",
      "Processed image 9799\n",
      "Processed image 9800\n",
      "Processed image 9801\n",
      "Processed image 9802\n",
      "Processed image 9803\n",
      "Processed image 9804\n",
      "Processed image 9805\n",
      "Processed image 9806\n",
      "Processed image 9807\n",
      "Processed image 9808\n",
      "Processed image 9809\n",
      "Processed image 9810\n",
      "Processed image 9811\n",
      "Processed image 9812\n",
      "Processed image 9813\n",
      "Processed image 9814\n",
      "Processed image 9815\n",
      "Processed image 9816\n",
      "Processed image 9817\n",
      "Processed image 9818\n",
      "Processed image 9819\n",
      "Processed image 9820\n",
      "Processed image 9821\n",
      "Processed image 9822\n",
      "Processed image 9823\n",
      "Processed image 9824\n",
      "Processed image 9825\n",
      "Processed image 9826\n",
      "Processed image 9827\n",
      "Processed image 9828\n",
      "Processed image 9829\n",
      "Processed image 9830\n",
      "Processed image 9831\n",
      "Processed image 9832\n",
      "Processed image 9833\n",
      "Processed image 9834\n",
      "Processed image 9835\n",
      "Processed image 9836\n",
      "Processed image 9837\n",
      "Processed image 9838\n",
      "Processed image 9839\n",
      "Processed image 9840\n",
      "Processed image 9841\n",
      "Processed image 9842\n",
      "Processed image 9843\n",
      "Processed image 9844\n",
      "Processed image 9845\n",
      "Processed image 9846\n",
      "Processed image 9847\n",
      "Processed image 9848\n",
      "Processed image 9849\n",
      "Processed image 9850\n",
      "Processed image 9851\n",
      "Processed image 9852\n",
      "Processed image 9853\n",
      "Processed image 9854\n",
      "Processed image 9855\n",
      "Processed image 9856\n",
      "Processed image 9857\n",
      "Processed image 9858\n",
      "Processed image 9859\n",
      "Processed image 9860\n",
      "Processed image 9861\n",
      "Processed image 9862\n",
      "Processed image 9863\n",
      "Processed image 9864\n",
      "Processed image 9865\n",
      "Processed image 9866\n",
      "Processed image 9867\n",
      "Processed image 9868\n",
      "Processed image 9869\n",
      "Processed image 9870\n",
      "Processed image 9871\n",
      "Processed image 9872\n",
      "Processed image 9873\n",
      "Processed image 9874\n",
      "Processed image 9875\n",
      "Processed image 9876\n",
      "Processed image 9877\n",
      "Processed image 9878\n",
      "Processed image 9879\n",
      "Processed image 9880\n",
      "Processed image 9881\n",
      "Processed image 9882\n",
      "Processed image 9883\n",
      "Processed image 9884\n",
      "Processed image 9885\n",
      "Processed image 9886\n",
      "Processed image 9887\n",
      "Processed image 9888\n",
      "Processed image 9889\n",
      "Processed image 9890\n",
      "Processed image 9891\n",
      "Processed image 9892\n",
      "Processed image 9893\n",
      "Processed image 9894\n",
      "Processed image 9895\n",
      "Processed image 9896\n",
      "Processed image 9897\n",
      "Processed image 9898\n",
      "Processed image 9899\n",
      "Processed image 9900\n",
      "Processed image 9901\n",
      "Processed image 9902\n",
      "Processed image 9903\n",
      "Processed image 9904\n",
      "Processed image 9905\n",
      "Processed image 9906\n",
      "Processed image 9907\n",
      "Processed image 9908\n",
      "Processed image 9909\n",
      "Processed image 9910\n",
      "Processed image 9911\n",
      "Processed image 9912\n",
      "Processed image 9913\n",
      "Processed image 9914\n",
      "Processed image 9915\n",
      "Processed image 9916\n",
      "Processed image 9917\n",
      "Processed image 9918\n",
      "Processed image 9919\n",
      "Processed image 9920\n",
      "Processed image 9921\n",
      "Processed image 9922\n",
      "Processed image 9923\n",
      "Processed image 9924\n",
      "Processed image 9925\n",
      "Processed image 9926\n",
      "Processed image 9927\n",
      "Processed image 9928\n",
      "Processed image 9929\n",
      "Processed image 9930\n",
      "Processed image 9931\n",
      "Processed image 9932\n",
      "Processed image 9933\n",
      "Processed image 9934\n",
      "Processed image 9935\n",
      "Processed image 9936\n",
      "Processed image 9937\n",
      "Processed image 9938\n",
      "Processed image 9939\n",
      "Processed image 9940\n",
      "Processed image 9941\n",
      "Processed image 9942\n",
      "Processed image 9943\n",
      "Processed image 9944\n",
      "Processed image 9945\n",
      "Processed image 9946\n",
      "Processed image 9947\n",
      "Processed image 9948\n",
      "Processed image 9949\n",
      "Processed image 9950\n",
      "Processed image 9951\n",
      "Processed image 9952\n",
      "Processed image 9953\n",
      "Processed image 9954\n",
      "Processed image 9955\n",
      "Processed image 9956\n",
      "Processed image 9957\n",
      "Processed image 9958\n",
      "Processed image 9959\n",
      "Processed image 9960\n",
      "Processed image 9961\n",
      "Processed image 9962\n",
      "Processed image 9963\n",
      "Processed image 9964\n",
      "Processed image 9965\n",
      "Processed image 9966\n",
      "Processed image 9967\n",
      "Processed image 9968\n",
      "Processed image 9969\n",
      "Processed image 9970\n",
      "Processed image 9971\n",
      "Processed image 9972\n",
      "Processed image 9973\n",
      "Processed image 9974\n",
      "Processed image 9975\n",
      "Processed image 9976\n",
      "Processed image 9977\n",
      "Processed image 9978\n",
      "Processed image 9979\n",
      "Processed image 9980\n",
      "Processed image 9981\n",
      "Processed image 9982\n",
      "Processed image 9983\n",
      "Processed image 9984\n",
      "Processed image 9985\n",
      "Processed image 9986\n",
      "Processed image 9987\n",
      "Processed image 9988\n",
      "Processed image 9989\n",
      "Processed image 9990\n",
      "Processed image 9991\n",
      "Processed image 9992\n",
      "Processed image 9993\n",
      "Processed image 9994\n",
      "Processed image 9995\n",
      "Processed image 9996\n",
      "Processed image 9997\n",
      "Processed image 9998\n",
      "Processed image 9999\n",
      "Processed image 10000\n",
      "Processed image 10001\n",
      "Processed image 10002\n",
      "Processed image 10003\n",
      "Processed image 10004\n",
      "Processed image 10005\n",
      "Processed image 10006\n",
      "Processed image 10007\n",
      "Processed image 10008\n",
      "Processed image 10009\n",
      "Processed image 10010\n",
      "Processed image 10011\n",
      "Processed image 10012\n",
      "Processed image 10013\n",
      "Processed image 10014\n",
      "Processed image 10015\n",
      "Processed image 10016\n",
      "Processed image 10017\n",
      "Processed image 10018\n",
      "Processed image 10019\n",
      "Processed image 10020\n",
      "Processed image 10021\n",
      "Processed image 10022\n",
      "Processed image 10023\n",
      "Processed image 10024\n",
      "Processed image 10025\n",
      "Processed image 10026\n",
      "Processed image 10027\n",
      "Processed image 10028\n",
      "Processed image 10029\n",
      "Processed image 10030\n",
      "Processed image 10031\n",
      "Processed image 10032\n",
      "Processed image 10033\n",
      "Processed image 10034\n",
      "Processed image 10035\n",
      "Processed image 10036\n",
      "Processed image 10037\n",
      "Processed image 10038\n",
      "Processed image 10039\n",
      "Processed image 10040\n",
      "Processed image 10041\n",
      "Processed image 10042\n",
      "Processed image 10043\n",
      "Processed image 10044\n",
      "Processed image 10045\n",
      "Processed image 10046\n",
      "Processed image 10047\n",
      "Processed image 10048\n",
      "Processed image 10049\n",
      "Processed image 10050\n",
      "Processed image 10051\n",
      "Processed image 10052\n",
      "Processed image 10053\n",
      "Processed image 10054\n",
      "Processed image 10055\n",
      "Processed image 10056\n",
      "Processed image 10057\n",
      "Processed image 10058\n",
      "Processed image 10059\n",
      "Processed image 10060\n",
      "Processed image 10061\n",
      "Processed image 10062\n",
      "Processed image 10063\n",
      "Processed image 10064\n",
      "Processed image 10065\n",
      "Processed image 10066\n",
      "Processed image 10067\n",
      "Processed image 10068\n",
      "Processed image 10069\n",
      "Processed image 10070\n",
      "Processed image 10071\n",
      "Processed image 10072\n",
      "Processed image 10073\n",
      "Processed image 10074\n",
      "Processed image 10075\n",
      "Processed image 10076\n",
      "Processed image 10077\n",
      "Processed image 10078\n",
      "Processed image 10079\n",
      "Processed image 10080\n",
      "Processed image 10081\n",
      "Processed image 10082\n",
      "Processed image 10083\n",
      "Processed image 10084\n",
      "Processed image 10085\n",
      "Processed image 10086\n",
      "Processed image 10087\n",
      "Processed image 10088\n",
      "Processed image 10089\n",
      "Processed image 10090\n",
      "Processed image 10091\n",
      "Processed image 10092\n",
      "Processed image 10093\n",
      "Processed image 10094\n",
      "Processed image 10095\n",
      "Processed image 10096\n",
      "Processed image 10097\n",
      "Processed image 10098\n",
      "Processed image 10099\n",
      "Processed image 10100\n",
      "Processed image 10101\n",
      "Processed image 10102\n",
      "Processed image 10103\n",
      "Processed image 10104\n",
      "Processed image 10105\n",
      "Processed image 10106\n",
      "Processed image 10107\n",
      "Processed image 10108\n",
      "Processed image 10109\n",
      "Processed image 10110\n",
      "Processed image 10111\n",
      "Processed image 10112\n",
      "Processed image 10113\n",
      "Processed image 10114\n",
      "Processed image 10115\n",
      "Processed image 10116\n",
      "Processed image 10117\n",
      "Processed image 10118\n",
      "Processed image 10119\n",
      "Processed image 10120\n",
      "Processed image 10121\n",
      "Processed image 10122\n",
      "Processed image 10123\n",
      "Processed image 10124\n",
      "Processed image 10125\n",
      "Processed image 10126\n",
      "Processed image 10127\n",
      "Processed image 10128\n",
      "Processed image 10129\n",
      "Processed image 10130\n",
      "Processed image 10131\n",
      "Processed image 10132\n",
      "Processed image 10133\n",
      "Processed image 10134\n",
      "Processed image 10135\n",
      "Processed image 10136\n",
      "Processed image 10137\n",
      "Processed image 10138\n",
      "Processed image 10139\n",
      "Processed image 10140\n",
      "Processed image 10141\n",
      "Processed image 10142\n",
      "Processed image 10143\n",
      "Processed image 10144\n",
      "Processed image 10145\n",
      "Processed image 10146\n",
      "Processed image 10147\n",
      "Processed image 10148\n",
      "Processed image 10149\n",
      "Processed image 10150\n",
      "Processed image 10151\n",
      "Processed image 10152\n",
      "Processed image 10153\n",
      "Processed image 10154\n",
      "Processed image 10155\n",
      "Processed image 10156\n",
      "Processed image 10157\n",
      "Processed image 10158\n",
      "Processed image 10159\n",
      "Processed image 10160\n",
      "Processed image 10161\n",
      "Processed image 10162\n",
      "Processed image 10163\n",
      "Processed image 10164\n",
      "Processed image 10165\n",
      "Processed image 10166\n",
      "Processed image 10167\n",
      "Processed image 10168\n",
      "Processed image 10169\n",
      "Processed image 10170\n",
      "Processed image 10171\n",
      "Processed image 10172\n",
      "Processed image 10173\n",
      "Processed image 10174\n",
      "Processed image 10175\n",
      "Processed image 10176\n",
      "Processed image 10177\n",
      "Processed image 10178\n",
      "Processed image 10179\n",
      "Processed image 10180\n",
      "Processed image 10181\n",
      "Processed image 10182\n",
      "Processed image 10183\n",
      "Processed image 10184\n",
      "Processed image 10185\n",
      "Processed image 10186\n",
      "Processed image 10187\n",
      "Processed image 10188\n",
      "Processed image 10189\n",
      "Processed image 10190\n",
      "Processed image 10191\n",
      "Processed image 10192\n",
      "Processed image 10193\n",
      "Processed image 10194\n",
      "Processed image 10195\n",
      "Processed image 10196\n",
      "Processed image 10197\n",
      "Processed image 10198\n",
      "Processed image 10199\n",
      "Processed image 10200\n",
      "Processed image 10201\n",
      "Processed image 10202\n",
      "Processed image 10203\n",
      "Processed image 10204\n",
      "Processed image 10205\n",
      "Processed image 10206\n",
      "Processed image 10207\n",
      "Processed image 10208\n",
      "Processed image 10209\n",
      "Processed image 10210\n",
      "Processed image 10211\n",
      "Processed image 10212\n",
      "Processed image 10213\n",
      "Processed image 10214\n",
      "Processed image 10215\n",
      "Processed image 10216\n",
      "Processed image 10217\n",
      "Processed image 10218\n",
      "Processed image 10219\n",
      "Processed image 10220\n",
      "Processed image 10221\n",
      "Processed image 10222\n",
      "Processed image 10223\n",
      "Processed image 10224\n",
      "Processed image 10225\n",
      "Processed image 10226\n",
      "Processed image 10227\n",
      "Processed image 10228\n",
      "Processed image 10229\n",
      "Processed image 10230\n",
      "Processed image 10231\n",
      "Processed image 10232\n",
      "Processed image 10233\n",
      "Processed image 10234\n",
      "Processed image 10235\n",
      "Processed image 10236\n",
      "Processed image 10237\n",
      "Processed image 10238\n",
      "Processed image 10239\n",
      "Processed image 10240\n",
      "Processed image 10241\n",
      "Processed image 10242\n",
      "Processed image 10243\n",
      "Processed image 10244\n",
      "Processed image 10245\n",
      "Processed image 10246\n",
      "Processed image 10247\n",
      "Processed image 10248\n",
      "Processed image 10249\n",
      "Processed image 10250\n",
      "Processed image 10251\n",
      "Processed image 10252\n",
      "Processed image 10253\n",
      "Processed image 10254\n",
      "Processed image 10255\n",
      "Processed image 10256\n",
      "Processed image 10257\n",
      "Processed image 10258\n",
      "Processed image 10259\n",
      "Processed image 10260\n",
      "Processed image 10261\n",
      "Processed image 10262\n",
      "Processed image 10263\n",
      "Processed image 10264\n",
      "Processed image 10265\n",
      "Processed image 10266\n",
      "Processed image 10267\n",
      "Processed image 10268\n",
      "Processed image 10269\n",
      "Processed image 10270\n",
      "Processed image 10271\n",
      "Processed image 10272\n",
      "Processed image 10273\n",
      "Processed image 10274\n",
      "Processed image 10275\n",
      "Processed image 10276\n",
      "Processed image 10277\n",
      "Processed image 10278\n",
      "Processed image 10279\n",
      "Processed image 10280\n",
      "Processed image 10281\n",
      "Processed image 10282\n",
      "Processed image 10283\n",
      "Processed image 10284\n",
      "Processed image 10285\n",
      "Processed image 10286\n",
      "Processed image 10287\n",
      "Processed image 10288\n",
      "Processed image 10289\n",
      "Processed image 10290\n",
      "Processed image 10291\n",
      "Processed image 10292\n",
      "Processed image 10293\n",
      "Processed image 10294\n",
      "Processed image 10295\n",
      "Processed image 10296\n",
      "Processed image 10297\n",
      "Processed image 10298\n",
      "Processed image 10299\n",
      "Processed image 10300\n",
      "Processed image 10301\n",
      "Processed image 10302\n",
      "Processed image 10303\n",
      "Processed image 10304\n",
      "Processed image 10305\n",
      "Processed image 10306\n",
      "Processed image 10307\n",
      "Processed image 10308\n",
      "Processed image 10309\n",
      "Processed image 10310\n",
      "Processed image 10311\n",
      "Processed image 10312\n",
      "Processed image 10313\n",
      "Processed image 10314\n",
      "Processed image 10315\n",
      "Processed image 10316\n",
      "Processed image 10317\n",
      "Processed image 10318\n",
      "Processed image 10319\n",
      "Processed image 10320\n",
      "Processed image 10321\n",
      "Processed image 10322\n",
      "Processed image 10323\n",
      "Processed image 10324\n",
      "Processed image 10325\n",
      "Processed image 10326\n",
      "Processed image 10327\n",
      "Processed image 10328\n",
      "Processed image 10329\n",
      "Processed image 10330\n",
      "Processed image 10331\n",
      "Processed image 10332\n",
      "Processed image 10333\n",
      "Processed image 10334\n",
      "Processed image 10335\n",
      "Processed image 10336\n",
      "Processed image 10337\n",
      "Processed image 10338\n",
      "Processed image 10339\n",
      "Processed image 10340\n",
      "Processed image 10341\n",
      "Processed image 10342\n",
      "Processed image 10343\n",
      "Processed image 10344\n",
      "Processed image 10345\n",
      "Processed image 10346\n",
      "Processed image 10347\n",
      "Processed image 10348\n",
      "Processed image 10349\n",
      "Processed image 10350\n",
      "Processed image 10351\n",
      "Processed image 10352\n",
      "Processed image 10353\n",
      "Processed image 10354\n",
      "Processed image 10355\n",
      "Processed image 10356\n",
      "Processed image 10357\n",
      "Processed image 10358\n",
      "Processed image 10359\n",
      "Processed image 10360\n",
      "Processed image 10361\n",
      "Processed image 10362\n",
      "Processed image 10363\n",
      "Processed image 10364\n",
      "Processed image 10365\n",
      "Processed image 10366\n",
      "Processed image 10367\n",
      "Processed image 10368\n",
      "Processed image 10369\n",
      "Processed image 10370\n",
      "Processed image 10371\n",
      "Processed image 10372\n",
      "Processed image 10373\n",
      "Processed image 10374\n",
      "Processed image 10375\n",
      "Processed image 10376\n",
      "Processed image 10377\n",
      "Processed image 10378\n",
      "Processed image 10379\n",
      "Processed image 10380\n",
      "Processed image 10381\n",
      "Processed image 10382\n",
      "Processed image 10383\n",
      "Processed image 10384\n",
      "Processed image 10385\n",
      "Processed image 10386\n",
      "Processed image 10387\n",
      "Processed image 10388\n",
      "Processed image 10389\n",
      "Processed image 10390\n",
      "Processed image 10391\n",
      "Processed image 10392\n",
      "Processed image 10393\n",
      "Processed image 10394\n",
      "Processed image 10395\n",
      "Processed image 10396\n",
      "Processed image 10397\n",
      "Processed image 10398\n",
      "Processed image 10399\n",
      "Processed image 10400\n",
      "Processed image 10401\n",
      "Processed image 10402\n",
      "Processed image 10403\n",
      "Processed image 10404\n",
      "Processed image 10405\n",
      "Processed image 10406\n",
      "Processed image 10407\n",
      "Processed image 10408\n",
      "Processed image 10409\n",
      "Processed image 10410\n",
      "Processed image 10411\n",
      "Processed image 10412\n",
      "Processed image 10413\n",
      "Processed image 10414\n",
      "Processed image 10415\n",
      "Processed image 10416\n",
      "Processed image 10417\n",
      "Processed image 10418\n",
      "Processed image 10419\n",
      "Processed image 10420\n",
      "Processed image 10421\n",
      "Processed image 10422\n",
      "Processed image 10423\n",
      "Processed image 10424\n",
      "Processed image 10425\n",
      "Processed image 10426\n",
      "Processed image 10427\n",
      "Processed image 10428\n",
      "Processed image 10429\n",
      "Processed image 10430\n",
      "Processed image 10431\n",
      "Processed image 10432\n",
      "Processed image 10433\n",
      "Processed image 10434\n",
      "Processed image 10435\n",
      "Processed image 10436\n",
      "Processed image 10437\n",
      "Processed image 10438\n",
      "Processed image 10439\n",
      "Processed image 10440\n",
      "Processed image 10441\n",
      "Processed image 10442\n",
      "Processed image 10443\n",
      "Processed image 10444\n",
      "Processed image 10445\n",
      "Processed image 10446\n",
      "Processed image 10447\n",
      "Processed image 10448\n",
      "Processed image 10449\n",
      "Processed image 10450\n",
      "Processed image 10451\n",
      "Processed image 10452\n",
      "Processed image 10453\n",
      "Processed image 10454\n",
      "Processed image 10455\n",
      "Processed image 10456\n",
      "Processed image 10457\n",
      "Processed image 10458\n",
      "Processed image 10459\n",
      "Processed image 10460\n",
      "Processed image 10461\n",
      "Processed image 10462\n",
      "Processed image 10463\n",
      "Processed image 10464\n",
      "Processed image 10465\n",
      "Processed image 10466\n",
      "Processed image 10467\n",
      "Processed image 10468\n",
      "Processed image 10469\n",
      "Processed image 10470\n",
      "Processed image 10471\n",
      "Processed image 10472\n",
      "Processed image 10473\n",
      "Processed image 10474\n",
      "Processed image 10475\n",
      "Processed image 10476\n",
      "Processed image 10477\n",
      "Processed image 10478\n",
      "Processed image 10479\n",
      "Processed image 10480\n",
      "Processed image 10481\n",
      "Processed image 10482\n",
      "Processed image 10483\n",
      "Processed image 10484\n",
      "Processed image 10485\n",
      "Processed image 10486\n",
      "Processed image 10487\n",
      "Processed image 10488\n",
      "Processed image 10489\n",
      "Processed image 10490\n",
      "Processed image 10491\n",
      "Processed image 10492\n",
      "Processed image 10493\n",
      "Processed image 10494\n",
      "Processed image 10495\n",
      "Processed image 10496\n",
      "Processed image 10497\n",
      "Processed image 10498\n",
      "Processed image 10499\n",
      "Processed image 10500\n",
      "Processed image 10501\n",
      "Processed image 10502\n",
      "Processed image 10503\n",
      "Processed image 10504\n",
      "Processed image 10505\n",
      "Processed image 10506\n",
      "Processed image 10507\n",
      "Processed image 10508\n",
      "Processed image 10509\n",
      "Processed image 10510\n",
      "Processed image 10511\n",
      "Processed image 10512\n",
      "Processed image 10513\n",
      "Processed image 10514\n",
      "Processed image 10515\n",
      "Processed image 10516\n",
      "Processed image 10517\n",
      "Processed image 10518\n",
      "Processed image 10519\n",
      "Processed image 10520\n",
      "Processed image 10521\n",
      "Processed image 10522\n",
      "Processed image 10523\n",
      "Processed image 10524\n",
      "Processed image 10525\n",
      "Processed image 10526\n",
      "Processed image 10527\n",
      "Processed image 10528\n",
      "Processed image 10529\n",
      "Processed image 10530\n",
      "Processed image 10531\n",
      "Processed image 10532\n",
      "Processed image 10533\n",
      "Processed image 10534\n",
      "Processed image 10535\n",
      "Processed image 10536\n",
      "Processed image 10537\n",
      "Processed image 10538\n",
      "Processed image 10539\n",
      "Processed image 10540\n",
      "Processed image 10541\n",
      "Processed image 10542\n",
      "Processed image 10543\n",
      "Processed image 10544\n",
      "Processed image 10545\n",
      "Processed image 10546\n",
      "Processed image 10547\n",
      "Processed image 10548\n",
      "Processed image 10549\n",
      "Processed image 10550\n",
      "Processed image 10551\n",
      "Processed image 10552\n",
      "Processed image 10553\n",
      "Processed image 10554\n",
      "Processed image 10555\n",
      "Processed image 10556\n",
      "Processed image 10557\n",
      "Processed image 10558\n",
      "Processed image 10559\n",
      "Processed image 10560\n",
      "Processed image 10561\n",
      "Processed image 10562\n",
      "Processed image 10563\n",
      "Processed image 10564\n",
      "Processed image 10565\n",
      "Processed image 10566\n",
      "Processed image 10567\n",
      "Processed image 10568\n",
      "Processed image 10569\n",
      "Processed image 10570\n",
      "Processed image 10571\n",
      "Processed image 10572\n",
      "Processed image 10573\n",
      "Processed image 10574\n",
      "Processed image 10575\n",
      "Processed image 10576\n",
      "Processed image 10577\n",
      "Processed image 10578\n",
      "Processed image 10579\n",
      "Processed image 10580\n",
      "Processed image 10581\n",
      "Processed image 10582\n",
      "Processed image 10583\n",
      "Processed image 10584\n",
      "Processed image 10585\n",
      "Processed image 10586\n",
      "Processed image 10587\n",
      "Processed image 10588\n",
      "Processed image 10589\n",
      "Processed image 10590\n",
      "Processed image 10591\n",
      "Processed image 10592\n",
      "Processed image 10593\n",
      "Processed image 10594\n",
      "Processed image 10595\n",
      "Processed image 10596\n",
      "Processed image 10597\n",
      "Processed image 10598\n",
      "Processed image 10599\n",
      "Processed image 10600\n",
      "Processed image 10601\n",
      "Processed image 10602\n",
      "Processed image 10603\n",
      "Processed image 10604\n",
      "Processed image 10605\n",
      "Processed image 10606\n",
      "Processed image 10607\n",
      "Processed image 10608\n",
      "Processed image 10609\n",
      "Processed image 10610\n",
      "Processed image 10611\n",
      "Processed image 10612\n",
      "Processed image 10613\n",
      "Processed image 10614\n",
      "Processed image 10615\n",
      "Processed image 10616\n",
      "Processed image 10617\n",
      "Processed image 10618\n",
      "Processed image 10619\n",
      "Processed image 10620\n",
      "Processed image 10621\n",
      "Processed image 10622\n",
      "Processed image 10623\n",
      "Processed image 10624\n",
      "Processed image 10625\n",
      "Processed image 10626\n",
      "Processed image 10627\n",
      "Processed image 10628\n",
      "Processed image 10629\n",
      "Processed image 10630\n",
      "Processed image 10631\n",
      "Processed image 10632\n",
      "Processed image 10633\n",
      "Processed image 10634\n",
      "Processed image 10635\n",
      "Processed image 10636\n",
      "Processed image 10637\n",
      "Processed image 10638\n",
      "Processed image 10639\n",
      "Processed image 10640\n",
      "Processed image 10641\n",
      "Processed image 10642\n",
      "Processed image 10643\n",
      "Processed image 10644\n",
      "Processed image 10645\n",
      "Processed image 10646\n",
      "Processed image 10647\n",
      "Processed image 10648\n",
      "Processed image 10649\n",
      "Processed image 10650\n",
      "Processed image 10651\n",
      "Processed image 10652\n",
      "Processed image 10653\n",
      "Processed image 10654\n",
      "Processed image 10655\n",
      "Processed image 10656\n",
      "Processed image 10657\n",
      "Processed image 10658\n",
      "Processed image 10659\n",
      "Processed image 10660\n",
      "Processed image 10661\n",
      "Processed image 10662\n",
      "Processed image 10663\n",
      "Processed image 10664\n",
      "Processed image 10665\n",
      "Processed image 10666\n",
      "Processed image 10667\n",
      "Processed image 10668\n",
      "Processed image 10669\n",
      "Processed image 10670\n",
      "Processed image 10671\n",
      "Processed image 10672\n",
      "Processed image 10673\n",
      "Processed image 10674\n",
      "Processed image 10675\n",
      "Processed image 10676\n",
      "Processed image 10677\n",
      "Processed image 10678\n",
      "Processed image 10679\n",
      "Processed image 10680\n",
      "Processed image 10681\n",
      "Processed image 10682\n",
      "Processed image 10683\n",
      "Processed image 10684\n",
      "Processed image 10685\n",
      "Processed image 10686\n",
      "Processed image 10687\n",
      "Processed image 10688\n",
      "Processed image 10689\n",
      "Processed image 10690\n",
      "Processed image 10691\n",
      "Processed image 10692\n",
      "Processed image 10693\n",
      "Processed image 10694\n",
      "Processed image 10695\n",
      "Processed image 10696\n",
      "Processed image 10697\n",
      "Processed image 10698\n",
      "Processed image 10699\n",
      "Processed image 10700\n",
      "Processed image 10701\n",
      "Processed image 10702\n",
      "Processed image 10703\n",
      "Processed image 10704\n",
      "Processed image 10705\n",
      "Processed image 10706\n",
      "Processed image 10707\n",
      "Processed image 10708\n",
      "Processed image 10709\n",
      "Processed image 10710\n",
      "Processed image 10711\n",
      "Processed image 10712\n",
      "Processed image 10713\n",
      "Processed image 10714\n",
      "Processed image 10715\n",
      "Processed image 10716\n",
      "Processed image 10717\n",
      "Processed image 10718\n",
      "Processed image 10719\n",
      "Processed image 10720\n",
      "Processed image 10721\n",
      "Processed image 10722\n",
      "Processed image 10723\n",
      "Processed image 10724\n",
      "Processed image 10725\n",
      "Processed image 10726\n",
      "Processed image 10727\n",
      "Processed image 10728\n",
      "Processed image 10729\n",
      "Processed image 10730\n",
      "Processed image 10731\n",
      "Processed image 10732\n",
      "Processed image 10733\n",
      "Processed image 10734\n",
      "Processed image 10735\n",
      "Processed image 10736\n",
      "Processed image 10737\n",
      "Processed image 10738\n",
      "Processed image 10739\n",
      "Processed image 10740\n",
      "Processed image 10741\n",
      "Processed image 10742\n",
      "Processed image 10743\n",
      "Processed image 10744\n",
      "Processed image 10745\n",
      "Processed image 10746\n",
      "Processed image 10747\n",
      "Processed image 10748\n",
      "Processed image 10749\n",
      "Processed image 10750\n",
      "Processed image 10751\n",
      "Processed image 10752\n",
      "Processed image 10753\n",
      "Processed image 10754\n",
      "Processed image 10755\n",
      "Processed image 10756\n",
      "Processed image 10757\n",
      "Processed image 10758\n",
      "Processed image 10759\n",
      "Processed image 10760\n",
      "Processed image 10761\n",
      "Processed image 10762\n",
      "Processed image 10763\n",
      "Processed image 10764\n",
      "Processed image 10765\n",
      "Processed image 10766\n",
      "Processed image 10767\n",
      "Processed image 10768\n",
      "Processed image 10769\n",
      "Processed image 10770\n",
      "Processed image 10771\n",
      "Processed image 10772\n",
      "Processed image 10773\n",
      "Processed image 10774\n",
      "Processed image 10775\n",
      "Processed image 10776\n",
      "Processed image 10777\n",
      "Processed image 10778\n",
      "Processed image 10779\n",
      "Processed image 10780\n",
      "Processed image 10781\n",
      "Processed image 10782\n",
      "Processed image 10783\n",
      "Processed image 10784\n",
      "Processed image 10785\n",
      "Processed image 10786\n",
      "Processed image 10787\n",
      "Processed image 10788\n",
      "Processed image 10789\n",
      "Processed image 10790\n",
      "Processed image 10791\n",
      "Processed image 10792\n",
      "Processed image 10793\n",
      "Processed image 10794\n",
      "Processed image 10795\n",
      "Processed image 10796\n",
      "Processed image 10797\n",
      "Processed image 10798\n",
      "Processed image 10799\n",
      "Processed image 10800\n",
      "Processed image 10801\n",
      "Processed image 10802\n",
      "Processed image 10803\n",
      "Processed image 10804\n",
      "Processed image 10805\n",
      "Processed image 10806\n",
      "Processed image 10807\n",
      "Processed image 10808\n",
      "Processed image 10809\n",
      "Processed image 10810\n",
      "Processed image 10811\n",
      "Processed image 10812\n",
      "Processed image 10813\n",
      "Processed image 10814\n",
      "Processed image 10815\n",
      "Processed image 10816\n",
      "Processed image 10817\n",
      "Processed image 10818\n",
      "Processed image 10819\n",
      "Processed image 10820\n",
      "Processed image 10821\n",
      "Processed image 10822\n",
      "Processed image 10823\n",
      "Processed image 10824\n",
      "Processed image 10825\n",
      "Processed image 10826\n",
      "Processed image 10827\n",
      "Processed image 10828\n",
      "Processed image 10829\n",
      "Processed image 10830\n",
      "Processed image 10831\n",
      "Processed image 10832\n",
      "Processed image 10833\n",
      "Processed image 10834\n",
      "Processed image 10835\n",
      "Processed image 10836\n",
      "Processed image 10837\n",
      "Processed image 10838\n",
      "Processed image 10839\n",
      "Processed image 10840\n",
      "Processed image 10841\n",
      "Processed image 10842\n",
      "Processed image 10843\n",
      "Processed image 10844\n",
      "Processed image 10845\n",
      "Processed image 10846\n",
      "Processed image 10847\n",
      "Processed image 10848\n",
      "Processed image 10849\n",
      "Processed image 10850\n",
      "Processed image 10851\n",
      "Processed image 10852\n",
      "Processed image 10853\n",
      "Processed image 10854\n",
      "Processed image 10855\n",
      "Processed image 10856\n",
      "Processed image 10857\n",
      "Processed image 10858\n",
      "Processed image 10859\n",
      "Processed image 10860\n",
      "Processed image 10861\n",
      "Processed image 10862\n",
      "Processed image 10863\n",
      "Processed image 10864\n",
      "Processed image 10865\n",
      "Processed image 10866\n",
      "Processed image 10867\n",
      "Processed image 10868\n",
      "Processed image 10869\n",
      "Processed image 10870\n",
      "Processed image 10871\n",
      "Processed image 10872\n",
      "Processed image 10873\n",
      "Processed image 10874\n",
      "Processed image 10875\n",
      "Processed image 10876\n",
      "Processed image 10877\n",
      "Processed image 10878\n",
      "Processed image 10879\n",
      "Processed image 10880\n",
      "Processed image 10881\n",
      "Processed image 10882\n",
      "Processed image 10883\n",
      "Processed image 10884\n",
      "Processed image 10885\n",
      "Processed image 10886\n",
      "Processed image 10887\n",
      "Processed image 10888\n",
      "Processed image 10889\n",
      "Processed image 10890\n",
      "Processed image 10891\n",
      "Processed image 10892\n",
      "Processed image 10893\n",
      "Processed image 10894\n",
      "Processed image 10895\n",
      "Processed image 10896\n",
      "Processed image 10897\n",
      "Processed image 10898\n",
      "Processed image 10899\n",
      "Processed image 10900\n",
      "Processed image 10901\n",
      "Processed image 10902\n",
      "Processed image 10903\n",
      "Processed image 10904\n",
      "Processed image 10905\n",
      "Processed image 10906\n",
      "Processed image 10907\n",
      "Processed image 10908\n",
      "Processed image 10909\n",
      "Processed image 10910\n",
      "Processed image 10911\n",
      "Processed image 10912\n",
      "Processed image 10913\n",
      "Processed image 10914\n",
      "Processed image 10915\n",
      "Processed image 10916\n",
      "Processed image 10917\n",
      "Processed image 10918\n",
      "Processed image 10919\n",
      "Processed image 10920\n",
      "Processed image 10921\n",
      "Processed image 10922\n",
      "Processed image 10923\n",
      "Processed image 10924\n",
      "Processed image 10925\n",
      "Processed image 10926\n",
      "Processed image 10927\n",
      "Processed image 10928\n",
      "Processed image 10929\n",
      "Processed image 10930\n",
      "Processed image 10931\n",
      "Processed image 10932\n",
      "Processed image 10933\n",
      "Processed image 10934\n",
      "Processed image 10935\n",
      "Processed image 10936\n",
      "Processed image 10937\n",
      "Processed image 10938\n",
      "Processed image 10939\n",
      "Processed image 10940\n",
      "Processed image 10941\n",
      "Processed image 10942\n",
      "Processed image 10943\n",
      "Processed image 10944\n",
      "Processed image 10945\n",
      "Processed image 10946\n",
      "Processed image 10947\n",
      "Processed image 10948\n",
      "Processed image 10949\n",
      "Processed image 10950\n",
      "Processed image 10951\n",
      "Processed image 10952\n",
      "Processed image 10953\n",
      "Processed image 10954\n",
      "Processed image 10955\n",
      "Processed image 10956\n",
      "Processed image 10957\n",
      "Processed image 10958\n",
      "Processed image 10959\n",
      "Processed image 10960\n",
      "Processed image 10961\n",
      "Processed image 10962\n",
      "Processed image 10963\n",
      "Processed image 10964\n",
      "Processed image 10965\n",
      "Processed image 10966\n",
      "Processed image 10967\n",
      "Processed image 10968\n",
      "Processed image 10969\n",
      "Processed image 10970\n",
      "Processed image 10971\n",
      "Processed image 10972\n",
      "Processed image 10973\n",
      "Processed image 10974\n",
      "Processed image 10975\n",
      "Processed image 10976\n",
      "Processed image 10977\n",
      "Processed image 10978\n",
      "Processed image 10979\n",
      "Processed image 10980\n",
      "Processed image 10981\n",
      "Processed image 10982\n",
      "Processed image 10983\n",
      "Processed image 10984\n",
      "Processed image 10985\n",
      "Processed image 10986\n",
      "Processed image 10987\n",
      "Processed image 10988\n",
      "Processed image 10989\n",
      "Processed image 10990\n",
      "Processed image 10991\n",
      "Processed image 10992\n",
      "Processed image 10993\n",
      "Processed image 10994\n",
      "Processed image 10995\n",
      "Processed image 10996\n",
      "Processed image 10997\n",
      "Processed image 10998\n",
      "Processed image 10999\n",
      "Processed image 11000\n",
      "Processed image 11001\n",
      "Processed image 11002\n",
      "Processed image 11003\n",
      "Processed image 11004\n",
      "Processed image 11005\n",
      "Processed image 11006\n",
      "Processed image 11007\n",
      "Processed image 11008\n",
      "Processed image 11009\n",
      "Processed image 11010\n",
      "Processed image 11011\n",
      "Processed image 11012\n",
      "Processed image 11013\n",
      "Processed image 11014\n",
      "Processed image 11015\n",
      "Processed image 11016\n",
      "Processed image 11017\n",
      "Processed image 11018\n",
      "Processed image 11019\n",
      "Processed image 11020\n",
      "Processed image 11021\n",
      "Processed image 11022\n",
      "Processed image 11023\n",
      "Processed image 11024\n",
      "Processed image 11025\n",
      "Processed image 11026\n",
      "Processed image 11027\n",
      "Processed image 11028\n",
      "Processed image 11029\n",
      "Processed image 11030\n",
      "Processed image 11031\n",
      "Processed image 11032\n",
      "Processed image 11033\n",
      "Processed image 11034\n",
      "Processed image 11035\n",
      "Processed image 11036\n",
      "Processed image 11037\n",
      "Processed image 11038\n",
      "Processed image 11039\n",
      "Processed image 11040\n",
      "Processed image 11041\n",
      "Processed image 11042\n",
      "Processed image 11043\n",
      "Processed image 11044\n",
      "Processed image 11045\n",
      "Processed image 11046\n",
      "Processed image 11047\n",
      "Processed image 11048\n",
      "Processed image 11049\n",
      "Processed image 11050\n",
      "Processed image 11051\n",
      "Processed image 11052\n",
      "Processed image 11053\n",
      "Processed image 11054\n",
      "Processed image 11055\n",
      "Processed image 11056\n",
      "Processed image 11057\n",
      "Processed image 11058\n",
      "Processed image 11059\n",
      "Processed image 11060\n",
      "Processed image 11061\n",
      "Processed image 11062\n",
      "Processed image 11063\n",
      "Processed image 11064\n",
      "Processed image 11065\n",
      "Processed image 11066\n",
      "Processed image 11067\n",
      "Processed image 11068\n",
      "Processed image 11069\n",
      "Processed image 11070\n",
      "Processed image 11071\n",
      "Processed image 11072\n",
      "Processed image 11073\n",
      "Processed image 11074\n",
      "Processed image 11075\n",
      "Processed image 11076\n",
      "Processed image 11077\n",
      "Processed image 11078\n",
      "Processed image 11079\n",
      "Processed image 11080\n",
      "Processed image 11081\n",
      "Processed image 11082\n",
      "Processed image 11083\n",
      "Processed image 11084\n",
      "Processed image 11085\n",
      "Processed image 11086\n",
      "Processed image 11087\n",
      "Processed image 11088\n",
      "Processed image 11089\n",
      "Processed image 11090\n",
      "Processed image 11091\n",
      "Processed image 11092\n",
      "Processed image 11093\n",
      "Processed image 11094\n",
      "Processed image 11095\n",
      "Processed image 11096\n",
      "Processed image 11097\n",
      "Processed image 11098\n",
      "Processed image 11099\n",
      "Processed image 11100\n",
      "Processed image 11101\n",
      "Processed image 11102\n",
      "Processed image 11103\n",
      "Processed image 11104\n",
      "Processed image 11105\n",
      "Processed image 11106\n",
      "Processed image 11107\n",
      "Processed image 11108\n",
      "Processed image 11109\n",
      "Processed image 11110\n",
      "Processed image 11111\n",
      "Processed image 11112\n",
      "Processed image 11113\n",
      "Processed image 11114\n",
      "Processed image 11115\n",
      "Processed image 11116\n",
      "Processed image 11117\n",
      "Processed image 11118\n",
      "Processed image 11119\n",
      "Processed image 11120\n",
      "Processed image 11121\n",
      "Processed image 11122\n",
      "Processed image 11123\n",
      "Processed image 11124\n",
      "Processed image 11125\n",
      "Processed image 11126\n",
      "Processed image 11127\n",
      "Processed image 11128\n",
      "Processed image 11129\n",
      "Processed image 11130\n",
      "Processed image 11131\n",
      "Processed image 11132\n",
      "Processed image 11133\n",
      "Processed image 11134\n",
      "Processed image 11135\n",
      "Processed image 11136\n",
      "Processed image 11137\n",
      "Processed image 11138\n",
      "Processed image 11139\n",
      "Processed image 11140\n",
      "Processed image 11141\n",
      "Processed image 11142\n",
      "Processed image 11143\n",
      "Processed image 11144\n",
      "Processed image 11145\n",
      "Processed image 11146\n",
      "Processed image 11147\n",
      "Processed image 11148\n",
      "Processed image 11149\n",
      "Processed image 11150\n",
      "Processed image 11151\n",
      "Processed image 11152\n",
      "Processed image 11153\n",
      "Processed image 11154\n",
      "Processed image 11155\n",
      "Processed image 11156\n",
      "Processed image 11157\n",
      "Processed image 11158\n",
      "Processed image 11159\n",
      "Processed image 11160\n",
      "Processed image 11161\n",
      "Processed image 11162\n",
      "Processed image 11163\n",
      "Processed image 11164\n",
      "Processed image 11165\n",
      "Processed image 11166\n",
      "Processed image 11167\n",
      "Processed image 11168\n",
      "Processed image 11169\n",
      "Processed image 11170\n",
      "Processed image 11171\n",
      "Processed image 11172\n",
      "Processed image 11173\n",
      "Processed image 11174\n",
      "Processed image 11175\n",
      "Processed image 11176\n",
      "Processed image 11177\n",
      "Processed image 11178\n",
      "Processed image 11179\n",
      "Processed image 11180\n",
      "Processed image 11181\n",
      "Processed image 11182\n",
      "Processed image 11183\n",
      "Processed image 11184\n",
      "Processed image 11185\n",
      "Processed image 11186\n",
      "Processed image 11187\n",
      "Processed image 11188\n",
      "Processed image 11189\n",
      "Processed image 11190\n",
      "Processed image 11191\n",
      "Processed image 11192\n",
      "Processed image 11193\n",
      "Processed image 11194\n",
      "Processed image 11195\n",
      "Processed image 11196\n",
      "Processed image 11197\n",
      "Processed image 11198\n",
      "Processed image 11199\n",
      "Processed image 11200\n",
      "Processed image 11201\n",
      "Processed image 11202\n",
      "Processed image 11203\n",
      "Processed image 11204\n",
      "Processed image 11205\n",
      "Processed image 11206\n",
      "Processed image 11207\n",
      "Processed image 11208\n",
      "Processed image 11209\n",
      "Processed image 11210\n",
      "Processed image 11211\n",
      "Processed image 11212\n",
      "Processed image 11213\n",
      "Processed image 11214\n",
      "Processed image 11215\n",
      "Processed image 11216\n",
      "Processed image 11217\n",
      "Processed image 11218\n",
      "Processed image 11219\n",
      "Processed image 11220\n",
      "Processed image 11221\n",
      "Processed image 11222\n",
      "Processed image 11223\n",
      "Processed image 11224\n",
      "Processed image 11225\n",
      "Processed image 11226\n",
      "Processed image 11227\n",
      "Processed image 11228\n",
      "Processed image 11229\n",
      "Processed image 11230\n",
      "Processed image 11231\n",
      "Processed image 11232\n",
      "Processed image 11233\n",
      "Processed image 11234\n",
      "Processed image 11235\n",
      "Processed image 11236\n",
      "Processed image 11237\n",
      "Processed image 11238\n",
      "Processed image 11239\n",
      "Processed image 11240\n",
      "Processed image 11241\n",
      "Processed image 11242\n",
      "Processed image 11243\n",
      "Processed image 11244\n",
      "Processed image 11245\n",
      "Processed image 11246\n",
      "Processed image 11247\n",
      "Processed image 11248\n",
      "Processed image 11249\n",
      "Processed image 11250\n",
      "Processed image 11251\n",
      "Processed image 11252\n",
      "Processed image 11253\n",
      "Processed image 11254\n",
      "Processed image 11255\n",
      "Processed image 11256\n",
      "Processed image 11257\n",
      "Processed image 11258\n",
      "Processed image 11259\n",
      "Processed image 11260\n",
      "Processed image 11261\n",
      "Processed image 11262\n",
      "Processed image 11263\n",
      "Processed image 11264\n",
      "Processed image 11265\n",
      "Processed image 11266\n",
      "Processed image 11267\n",
      "Processed image 11268\n",
      "Processed image 11269\n",
      "Processed image 11270\n",
      "Processed image 11271\n",
      "Processed image 11272\n",
      "Processed image 11273\n",
      "Processed image 11274\n",
      "Processed image 11275\n",
      "Processed image 11276\n",
      "Processed image 11277\n",
      "Processed image 11278\n",
      "Processed image 11279\n",
      "Processed image 11280\n",
      "Processed image 11281\n",
      "Processed image 11282\n",
      "Processed image 11283\n",
      "Processed image 11284\n",
      "Processed image 11285\n",
      "Processed image 11286\n",
      "Processed image 11287\n",
      "Processed image 11288\n",
      "Processed image 11289\n",
      "Processed image 11290\n",
      "Processed image 11291\n",
      "Processed image 11292\n",
      "Processed image 11293\n",
      "Processed image 11294\n",
      "Processed image 11295\n",
      "Processed image 11296\n",
      "Processed image 11297\n",
      "Processed image 11298\n",
      "Processed image 11299\n",
      "Processed image 11300\n",
      "Processed image 11301\n",
      "Processed image 11302\n",
      "Processed image 11303\n",
      "Processed image 11304\n",
      "Processed image 11305\n",
      "Processed image 11306\n",
      "Processed image 11307\n",
      "Processed image 11308\n",
      "Processed image 11309\n",
      "Processed image 11310\n",
      "Processed image 11311\n",
      "Processed image 11312\n",
      "Processed image 11313\n",
      "Processed image 11314\n",
      "Processed image 11315\n",
      "Processed image 11316\n",
      "Processed image 11317\n",
      "Processed image 11318\n",
      "Processed image 11319\n",
      "Processed image 11320\n",
      "Processed image 11321\n",
      "Processed image 11322\n",
      "Processed image 11323\n",
      "Processed image 11324\n",
      "Processed image 11325\n",
      "Processed image 11326\n",
      "Processed image 11327\n",
      "Processed image 11328\n",
      "Processed image 11329\n",
      "Processed image 11330\n",
      "Processed image 11331\n",
      "Processed image 11332\n",
      "Processed image 11333\n",
      "Processed image 11334\n",
      "Processed image 11335\n",
      "Processed image 11336\n",
      "Processed image 11337\n",
      "Processed image 11338\n",
      "Processed image 11339\n",
      "Processed image 11340\n",
      "Processed image 11341\n",
      "Processed image 11342\n",
      "Processed image 11343\n",
      "Processed image 11344\n",
      "Processed image 11345\n",
      "Processed image 11346\n",
      "Processed image 11347\n",
      "Processed image 11348\n",
      "Processed image 11349\n",
      "Processed image 11350\n",
      "Processed image 11351\n",
      "Processed image 11352\n",
      "Processed image 11353\n",
      "Processed image 11354\n",
      "Processed image 11355\n",
      "Processed image 11356\n",
      "Processed image 11357\n",
      "Processed image 11358\n",
      "Processed image 11359\n",
      "Processed image 11360\n",
      "Processed image 11361\n",
      "Processed image 11362\n",
      "Processed image 11363\n",
      "Processed image 11364\n",
      "Processed image 11365\n",
      "Processed image 11366\n",
      "Processed image 11367\n",
      "Processed image 11368\n",
      "Processed image 11369\n",
      "Processed image 11370\n",
      "Processed image 11371\n",
      "Processed image 11372\n",
      "Processed image 11373\n",
      "Processed image 11374\n",
      "Processed image 11375\n",
      "Processed image 11376\n",
      "Processed image 11377\n",
      "Processed image 11378\n",
      "Processed image 11379\n",
      "Processed image 11380\n",
      "Processed image 11381\n",
      "Processed image 11382\n",
      "Processed image 11383\n",
      "Processed image 11384\n",
      "Processed image 11385\n",
      "Processed image 11386\n",
      "Processed image 11387\n",
      "Processed image 11388\n",
      "Processed image 11389\n",
      "Processed image 11390\n",
      "Processed image 11391\n",
      "Processed image 11392\n",
      "Processed image 11393\n",
      "Processed image 11394\n",
      "Processed image 11395\n",
      "Processed image 11396\n",
      "Processed image 11397\n",
      "Processed image 11398\n",
      "Processed image 11399\n",
      "Processed image 11400\n",
      "Processed image 11401\n",
      "Processed image 11402\n",
      "Processed image 11403\n",
      "Processed image 11404\n",
      "Processed image 11405\n",
      "Processed image 11406\n",
      "Processed image 11407\n",
      "Processed image 11408\n",
      "Processed image 11409\n",
      "Processed image 11410\n",
      "Processed image 11411\n",
      "Processed image 11412\n",
      "Processed image 11413\n",
      "Processed image 11414\n",
      "Processed image 11415\n",
      "Processed image 11416\n",
      "Processed image 11417\n",
      "Processed image 11418\n",
      "Processed image 11419\n",
      "Processed image 11420\n",
      "Processed image 11421\n",
      "Processed image 11422\n",
      "Processed image 11423\n",
      "Processed image 11424\n",
      "Processed image 11425\n",
      "Processed image 11426\n",
      "Processed image 11427\n",
      "Processed image 11428\n",
      "Processed image 11429\n",
      "Processed image 11430\n",
      "Processed image 11431\n",
      "Processed image 11432\n",
      "Processed image 11433\n",
      "Processed image 11434\n",
      "Processed image 11435\n",
      "Processed image 11436\n",
      "Processed image 11437\n",
      "Processed image 11438\n",
      "Processed image 11439\n",
      "Processed image 11440\n",
      "Processed image 11441\n",
      "Processed image 11442\n",
      "Processed image 11443\n",
      "Processed image 11444\n",
      "Processed image 11445\n",
      "Processed image 11446\n",
      "Processed image 11447\n",
      "Processed image 11448\n",
      "Processed image 11449\n",
      "Processed image 11450\n",
      "Processed image 11451\n",
      "Processed image 11452\n",
      "Processed image 11453\n",
      "Processed image 11454\n",
      "Processed image 11455\n",
      "Processed image 11456\n",
      "Processed image 11457\n",
      "Processed image 11458\n",
      "Processed image 11459\n",
      "Processed image 11460\n",
      "Processed image 11461\n",
      "Processed image 11462\n",
      "Processed image 11463\n",
      "Processed image 11464\n",
      "Processed image 11465\n",
      "Processed image 11466\n",
      "Processed image 11467\n",
      "Processed image 11468\n",
      "Processed image 11469\n",
      "Processed image 11470\n",
      "Processed image 11471\n",
      "Processed image 11472\n",
      "Processed image 11473\n",
      "Processed image 11474\n",
      "Processed image 11475\n",
      "Processed image 11476\n",
      "Processed image 11477\n",
      "Processed image 11478\n",
      "Processed image 11479\n",
      "Processed image 11480\n",
      "Processed image 11481\n",
      "Processed image 11482\n",
      "Processed image 11483\n",
      "Processed image 11484\n",
      "Processed image 11485\n",
      "Processed image 11486\n",
      "Processed image 11487\n",
      "Processed image 11488\n",
      "Processed image 11489\n",
      "Processed image 11490\n",
      "Processed image 11491\n",
      "Processed image 11492\n",
      "Processed image 11493\n",
      "Processed image 11494\n",
      "Processed image 11495\n",
      "Processed image 11496\n",
      "Processed image 11497\n",
      "Processed image 11498\n",
      "Processed image 11499\n",
      "Processed image 11500\n",
      "Processed image 11501\n",
      "Processed image 11502\n",
      "Processed image 11503\n",
      "Processed image 11504\n",
      "Processed image 11505\n",
      "Processed image 11506\n",
      "Processed image 11507\n",
      "Processed image 11508\n",
      "Processed image 11509\n",
      "Processed image 11510\n",
      "Processed image 11511\n",
      "Processed image 11512\n",
      "Processed image 11513\n",
      "Processed image 11514\n",
      "Processed image 11515\n",
      "Processed image 11516\n",
      "Processed image 11517\n",
      "Processed image 11518\n",
      "Processed image 11519\n",
      "Processed image 11520\n",
      "Processed image 11521\n",
      "Processed image 11522\n",
      "Processed image 11523\n",
      "Processed image 11524\n",
      "Processed image 11525\n",
      "Processed image 11526\n",
      "Processed image 11527\n",
      "Processed image 11528\n",
      "Processed image 11529\n",
      "Processed image 11530\n",
      "Processed image 11531\n",
      "Processed image 11532\n",
      "Processed image 11533\n",
      "Processed image 11534\n",
      "Processed image 11535\n",
      "Processed image 11536\n",
      "Processed image 11537\n",
      "Processed image 11538\n",
      "Processed image 11539\n",
      "Processed image 11540\n",
      "Processed image 11541\n",
      "Processed image 11542\n",
      "Processed image 11543\n",
      "Processed image 11544\n",
      "Processed image 11545\n",
      "Processed image 11546\n",
      "Processed image 11547\n",
      "Processed image 11548\n",
      "Processed image 11549\n",
      "Processed image 11550\n",
      "Processed image 11551\n",
      "Processed image 11552\n",
      "Processed image 11553\n",
      "Processed image 11554\n",
      "Processed image 11555\n",
      "Processed image 11556\n",
      "Processed image 11557\n",
      "Processed image 11558\n",
      "Processed image 11559\n",
      "Processed image 11560\n",
      "Processed image 11561\n",
      "Processed image 11562\n",
      "Processed image 11563\n",
      "Processed image 11564\n",
      "Processed image 11565\n",
      "Processed image 11566\n",
      "Processed image 11567\n",
      "Processed image 11568\n",
      "Processed image 11569\n",
      "Processed image 11570\n",
      "Processed image 11571\n",
      "Processed image 11572\n",
      "Processed image 11573\n",
      "Processed image 11574\n",
      "Processed image 11575\n",
      "Processed image 11576\n",
      "Processed image 11577\n",
      "Processed image 11578\n",
      "Processed image 11579\n",
      "Processed image 11580\n",
      "Processed image 11581\n",
      "Processed image 11582\n",
      "Processed image 11583\n",
      "Processed image 11584\n",
      "Processed image 11585\n",
      "Processed image 11586\n",
      "Processed image 11587\n",
      "Processed image 11588\n",
      "Processed image 11589\n",
      "Processed image 11590\n",
      "Processed image 11591\n",
      "Processed image 11592\n",
      "Processed image 11593\n",
      "Processed image 11594\n",
      "Processed image 11595\n",
      "Processed image 11596\n",
      "Processed image 11597\n",
      "Processed image 11598\n",
      "Processed image 11599\n",
      "Processed image 11600\n",
      "Processed image 11601\n",
      "Processed image 11602\n",
      "Processed image 11603\n",
      "Processed image 11604\n",
      "Processed image 11605\n",
      "Processed image 11606\n",
      "Processed image 11607\n",
      "Processed image 11608\n",
      "Processed image 11609\n",
      "Processed image 11610\n",
      "Processed image 11611\n",
      "Processed image 11612\n",
      "Processed image 11613\n",
      "Processed image 11614\n",
      "Processed image 11615\n",
      "Processed image 11616\n",
      "Processed image 11617\n",
      "Processed image 11618\n",
      "Processed image 11619\n",
      "Processed image 11620\n",
      "Processed image 11621\n",
      "Processed image 11622\n",
      "Processed image 11623\n",
      "Processed image 11624\n",
      "Processed image 11625\n",
      "Processed image 11626\n",
      "Processed image 11627\n",
      "Processed image 11628\n",
      "Processed image 11629\n",
      "Processed image 11630\n",
      "Processed image 11631\n",
      "Processed image 11632\n",
      "Processed image 11633\n",
      "Processed image 11634\n",
      "Processed image 11635\n",
      "Processed image 11636\n",
      "Processed image 11637\n",
      "Processed image 11638\n",
      "Processed image 11639\n",
      "Processed image 11640\n",
      "Processed image 11641\n",
      "Processed image 11642\n",
      "Processed image 11643\n",
      "Processed image 11644\n",
      "Processed image 11645\n",
      "Processed image 11646\n",
      "Processed image 11647\n",
      "Processed image 11648\n",
      "Processed image 11649\n",
      "Processed image 11650\n",
      "Processed image 11651\n",
      "Processed image 11652\n",
      "Processed image 11653\n",
      "Processed image 11654\n",
      "Processed image 11655\n",
      "Processed image 11656\n",
      "Processed image 11657\n",
      "Processed image 11658\n",
      "Processed image 11659\n",
      "Processed image 11660\n",
      "Processed image 11661\n",
      "Processed image 11662\n",
      "Processed image 11663\n",
      "Processed image 11664\n",
      "Processed image 11665\n",
      "Processed image 11666\n",
      "Processed image 11667\n",
      "Processed image 11668\n",
      "Processed image 11669\n",
      "Processed image 11670\n",
      "Processed image 11671\n",
      "Processed image 11672\n",
      "Processed image 11673\n",
      "Processed image 11674\n",
      "Processed image 11675\n",
      "Processed image 11676\n",
      "Processed image 11677\n",
      "Processed image 11678\n",
      "Processed image 11679\n",
      "Processed image 11680\n",
      "Processed image 11681\n",
      "Processed image 11682\n",
      "Processed image 11683\n",
      "Processed image 11684\n",
      "Processed image 11685\n",
      "Processed image 11686\n",
      "Processed image 11687\n",
      "Processed image 11688\n",
      "Processed image 11689\n",
      "Processed image 11690\n",
      "Processed image 11691\n",
      "Processed image 11692\n",
      "Processed image 11693\n",
      "Processed image 11694\n",
      "Processed image 11695\n",
      "Processed image 11696\n",
      "Processed image 11697\n",
      "Processed image 11698\n",
      "Processed image 11699\n",
      "Processed image 11700\n",
      "Processed image 11701\n",
      "Processed image 11702\n",
      "Processed image 11703\n",
      "Processed image 11704\n",
      "Processed image 11705\n",
      "Processed image 11706\n",
      "Processed image 11707\n",
      "Processed image 11708\n",
      "Processed image 11709\n",
      "Processed image 11710\n",
      "Processed image 11711\n",
      "Processed image 11712\n",
      "Processed image 11713\n",
      "Processed image 11714\n",
      "Processed image 11715\n",
      "Processed image 11716\n",
      "Processed image 11717\n",
      "Processed image 11718\n",
      "Processed image 11719\n",
      "Processed image 11720\n",
      "Processed image 11721\n",
      "Processed image 11722\n",
      "Processed image 11723\n",
      "Processed image 11724\n",
      "Processed image 11725\n",
      "Processed image 11726\n",
      "Processed image 11727\n",
      "Processed image 11728\n",
      "Processed image 11729\n",
      "Processed image 11730\n",
      "Processed image 11731\n",
      "Processed image 11732\n",
      "Processed image 11733\n",
      "Processed image 11734\n",
      "Processed image 11735\n",
      "Processed image 11736\n",
      "Processed image 11737\n",
      "Processed image 11738\n",
      "Processed image 11739\n",
      "Processed image 11740\n",
      "Processed image 11741\n",
      "Processed image 11742\n",
      "Processed image 11743\n",
      "Processed image 11744\n",
      "Processed image 11745\n",
      "Processed image 11746\n",
      "Processed image 11747\n",
      "Processed image 11748\n",
      "Processed image 11749\n",
      "Processed image 11750\n",
      "Processed image 11751\n",
      "Processed image 11752\n",
      "Processed image 11753\n",
      "Processed image 11754\n",
      "Processed image 11755\n",
      "Processed image 11756\n",
      "Processed image 11757\n",
      "Processed image 11758\n",
      "Processed image 11759\n",
      "Processed image 11760\n",
      "Processed image 11761\n",
      "Processed image 11762\n",
      "Processed image 11763\n",
      "Processed image 11764\n",
      "Processed image 11765\n",
      "Processed image 11766\n",
      "Processed image 11767\n",
      "Processed image 11768\n",
      "Processed image 11769\n",
      "Processed image 11770\n",
      "Processed image 11771\n",
      "Processed image 11772\n",
      "Processed image 11773\n",
      "Processed image 11774\n",
      "Processed image 11775\n",
      "Processed image 11776\n",
      "Processed image 11777\n",
      "Processed image 11778\n",
      "Processed image 11779\n",
      "Processed image 11780\n",
      "Processed image 11781\n",
      "Processed image 11782\n",
      "Processed image 11783\n",
      "Processed image 11784\n",
      "Processed image 11785\n",
      "Processed image 11786\n",
      "Processed image 11787\n",
      "Processed image 11788\n",
      "Processed image 11789\n",
      "Processed image 11790\n",
      "Processed image 11791\n",
      "Processed image 11792\n",
      "Processed image 11793\n",
      "Processed image 11794\n",
      "Processed image 11795\n",
      "Processed image 11796\n",
      "Processed image 11797\n",
      "Processed image 11798\n",
      "Processed image 11799\n",
      "Processed image 11800\n",
      "Processed image 11801\n",
      "Processed image 11802\n",
      "Processed image 11803\n",
      "Processed image 11804\n",
      "Processed image 11805\n",
      "Processed image 11806\n",
      "Processed image 11807\n",
      "Processed image 11808\n",
      "Processed image 11809\n",
      "Processed image 11810\n",
      "Processed image 11811\n",
      "Processed image 11812\n",
      "Processed image 11813\n",
      "Processed image 11814\n",
      "Processed image 11815\n",
      "Processed image 11816\n",
      "Processed image 11817\n",
      "Processed image 11818\n",
      "Processed image 11819\n",
      "Processed image 11820\n",
      "Processed image 11821\n",
      "Processed image 11822\n",
      "Processed image 11823\n",
      "Processed image 11824\n",
      "Processed image 11825\n",
      "Processed image 11826\n",
      "Processed image 11827\n",
      "Processed image 11828\n",
      "Processed image 11829\n",
      "Processed image 11830\n",
      "Processed image 11831\n",
      "Processed image 11832\n",
      "Processed image 11833\n",
      "Processed image 11834\n",
      "Processed image 11835\n",
      "Processed image 11836\n",
      "Processed image 11837\n",
      "Processed image 11838\n",
      "Processed image 11839\n",
      "Processed image 11840\n",
      "Processed image 11841\n",
      "Processed image 11842\n",
      "Processed image 11843\n",
      "Processed image 11844\n",
      "Processed image 11845\n",
      "Processed image 11846\n",
      "Processed image 11847\n",
      "Processed image 11848\n",
      "Processed image 11849\n",
      "Processed image 11850\n",
      "Processed image 11851\n",
      "Processed image 11852\n",
      "Processed image 11853\n",
      "Processed image 11854\n",
      "Processed image 11855\n",
      "Processed image 11856\n",
      "Processed image 11857\n",
      "Processed image 11858\n",
      "Processed image 11859\n",
      "Processed image 11860\n",
      "Processed image 11861\n",
      "Processed image 11862\n",
      "Processed image 11863\n",
      "Processed image 11864\n",
      "Processed image 11865\n",
      "Processed image 11866\n",
      "Processed image 11867\n",
      "Processed image 11868\n",
      "Processed image 11869\n",
      "Processed image 11870\n",
      "Processed image 11871\n",
      "Processed image 11872\n",
      "Processed image 11873\n",
      "Processed image 11874\n",
      "Processed image 11875\n",
      "Processed image 11876\n",
      "Processed image 11877\n",
      "Processed image 11878\n",
      "Processed image 11879\n",
      "Processed image 11880\n",
      "Processed image 11881\n",
      "Processed image 11882\n",
      "Processed image 11883\n",
      "Processed image 11884\n",
      "Processed image 11885\n",
      "Processed image 11886\n",
      "Processed image 11887\n",
      "Processed image 11888\n",
      "Processed image 11889\n",
      "Processed image 11890\n",
      "Processed image 11891\n",
      "Processed image 11892\n",
      "Processed image 11893\n",
      "Processed image 11894\n",
      "Processed image 11895\n",
      "Processed image 11896\n",
      "Processed image 11897\n",
      "Processed image 11898\n",
      "Processed image 11899\n",
      "Processed image 11900\n",
      "Processed image 11901\n",
      "Processed image 11902\n",
      "Processed image 11903\n",
      "Processed image 11904\n",
      "Processed image 11905\n",
      "Processed image 11906\n",
      "Processed image 11907\n",
      "Processed image 11908\n",
      "Processed image 11909\n",
      "Processed image 11910\n",
      "Processed image 11911\n",
      "Processed image 11912\n",
      "Processed image 11913\n",
      "Processed image 11914\n",
      "Processed image 11915\n",
      "Processed image 11916\n",
      "Processed image 11917\n",
      "Processed image 11918\n",
      "Processed image 11919\n",
      "Processed image 11920\n",
      "Processed image 11921\n",
      "Processed image 11922\n",
      "Processed image 11923\n",
      "Processed image 11924\n",
      "Processed image 11925\n",
      "Processed image 11926\n",
      "Processed image 11927\n",
      "Processed image 11928\n",
      "Processed image 11929\n",
      "Processed image 11930\n",
      "Processed image 11931\n",
      "Processed image 11932\n",
      "Processed image 11933\n",
      "Processed image 11934\n",
      "Processed image 11935\n",
      "Processed image 11936\n",
      "Processed image 11937\n",
      "Processed image 11938\n",
      "Processed image 11939\n",
      "Processed image 11940\n",
      "Processed image 11941\n",
      "Processed image 11942\n",
      "Processed image 11943\n",
      "Processed image 11944\n",
      "Processed image 11945\n",
      "Processed image 11946\n",
      "Processed image 11947\n",
      "Processed image 11948\n",
      "Processed image 11949\n",
      "Processed image 11950\n",
      "Processed image 11951\n",
      "Processed image 11952\n",
      "Processed image 11953\n",
      "Processed image 11954\n",
      "Processed image 11955\n",
      "Processed image 11956\n",
      "Processed image 11957\n",
      "Processed image 11958\n",
      "Processed image 11959\n",
      "Processed image 11960\n",
      "Processed image 11961\n",
      "Processed image 11962\n",
      "Processed image 11963\n",
      "Processed image 11964\n",
      "Processed image 11965\n",
      "Processed image 11966\n",
      "Processed image 11967\n",
      "Processed image 11968\n",
      "Processed image 11969\n",
      "Processed image 11970\n",
      "Processed image 11971\n",
      "Processed image 11972\n",
      "Processed image 11973\n",
      "Processed image 11974\n",
      "Processed image 11975\n",
      "Processed image 11976\n",
      "Processed image 11977\n",
      "Processed image 11978\n",
      "Processed image 11979\n",
      "Processed image 11980\n",
      "Processed image 11981\n",
      "Processed image 11982\n",
      "Processed image 11983\n",
      "Processed image 11984\n",
      "Processed image 11985\n",
      "Processed image 11986\n",
      "Processed image 11987\n",
      "Processed image 11988\n",
      "Processed image 11989\n",
      "Processed image 11990\n",
      "Processed image 11991\n",
      "Processed image 11992\n",
      "Processed image 11993\n",
      "Processed image 11994\n",
      "Processed image 11995\n",
      "Processed image 11996\n",
      "Processed image 11997\n",
      "Processed image 11998\n",
      "Processed image 11999\n",
      "Processed image 12000\n",
      "Processed image 12001\n",
      "Processed image 12002\n",
      "Processed image 12003\n",
      "Processed image 12004\n",
      "Processed image 12005\n",
      "Processed image 12006\n",
      "Processed image 12007\n",
      "Processed image 12008\n",
      "Processed image 12009\n",
      "Processed image 12010\n",
      "Processed image 12011\n",
      "Processed image 12012\n",
      "Processed image 12013\n",
      "Processed image 12014\n",
      "Processed image 12015\n",
      "Processed image 12016\n",
      "Processed image 12017\n",
      "Processed image 12018\n",
      "Processed image 12019\n",
      "Processed image 12020\n",
      "Processed image 12021\n",
      "Processed image 12022\n",
      "Processed image 12023\n",
      "Processed image 12024\n",
      "Processed image 12025\n",
      "Processed image 12026\n",
      "Processed image 12027\n",
      "Processed image 12028\n",
      "Processed image 12029\n",
      "Processed image 12030\n",
      "Processed image 12031\n",
      "Processed image 12032\n",
      "Processed image 12033\n",
      "Processed image 12034\n",
      "Processed image 12035\n",
      "Processed image 12036\n",
      "Processed image 12037\n",
      "Processed image 12038\n",
      "Processed image 12039\n",
      "Processed image 12040\n",
      "Processed image 12041\n",
      "Processed image 12042\n",
      "Processed image 12043\n",
      "Processed image 12044\n",
      "Processed image 12045\n",
      "Processed image 12046\n",
      "Processed image 12047\n",
      "Processed image 12048\n",
      "Processed image 12049\n",
      "Processed image 12050\n",
      "Processed image 12051\n",
      "Processed image 12052\n",
      "Processed image 12053\n",
      "Processed image 12054\n",
      "Processed image 12055\n",
      "Processed image 12056\n",
      "Processed image 12057\n",
      "Processed image 12058\n",
      "Processed image 12059\n",
      "Processed image 12060\n",
      "Processed image 12061\n",
      "Processed image 12062\n",
      "Processed image 12063\n",
      "Processed image 12064\n",
      "Processed image 12065\n",
      "Processed image 12066\n",
      "Processed image 12067\n",
      "Processed image 12068\n",
      "Processed image 12069\n",
      "Processed image 12070\n",
      "Processed image 12071\n",
      "Processed image 12072\n",
      "Processed image 12073\n",
      "Processed image 12074\n",
      "Processed image 12075\n",
      "Processed image 12076\n",
      "Processed image 12077\n",
      "Processed image 12078\n",
      "Processed image 12079\n",
      "Processed image 12080\n",
      "Processed image 12081\n",
      "Processed image 12082\n",
      "Processed image 12083\n",
      "Processed image 12084\n",
      "Processed image 12085\n",
      "Processed image 12086\n",
      "Processed image 12087\n",
      "Processed image 12088\n",
      "Processed image 12089\n",
      "Processed image 12090\n",
      "Processed image 12091\n",
      "Processed image 12092\n",
      "Processed image 12093\n",
      "Processed image 12094\n",
      "Processed image 12095\n",
      "Processed image 12096\n",
      "Processed image 12097\n",
      "Processed image 12098\n",
      "Processed image 12099\n",
      "Processed image 12100\n",
      "Processed image 12101\n",
      "Processed image 12102\n",
      "Processed image 12103\n",
      "Processed image 12104\n",
      "Processed image 12105\n",
      "Processed image 12106\n",
      "Processed image 12107\n",
      "Processed image 12108\n",
      "Processed image 12109\n",
      "Processed image 12110\n",
      "Processed image 12111\n",
      "Processed image 12112\n",
      "Processed image 12113\n",
      "Processed image 12114\n",
      "Processed image 12115\n",
      "Processed image 12116\n",
      "Processed image 12117\n",
      "Processed image 12118\n",
      "Processed image 12119\n",
      "Processed image 12120\n",
      "Processed image 12121\n",
      "Processed image 12122\n",
      "Processed image 12123\n",
      "Processed image 12124\n",
      "Processed image 12125\n",
      "Processed image 12126\n",
      "Processed image 12127\n",
      "Processed image 12128\n",
      "Processed image 12129\n",
      "Processed image 12130\n",
      "Processed image 12131\n",
      "Processed image 12132\n",
      "Processed image 12133\n",
      "Processed image 12134\n",
      "Processed image 12135\n",
      "Processed image 12136\n",
      "Processed image 12137\n",
      "Processed image 12138\n",
      "Processed image 12139\n",
      "Processed image 12140\n",
      "Processed image 12141\n",
      "Processed image 12142\n",
      "Processed image 12143\n",
      "Processed image 12144\n",
      "Processed image 12145\n",
      "Processed image 12146\n",
      "Processed image 12147\n",
      "Processed image 12148\n",
      "Processed image 12149\n",
      "Processed image 12150\n",
      "Processed image 12151\n",
      "Processed image 12152\n",
      "Processed image 12153\n",
      "Processed image 12154\n",
      "Processed image 12155\n",
      "Processed image 12156\n",
      "Processed image 12157\n",
      "Processed image 12158\n",
      "Processed image 12159\n",
      "Processed image 12160\n",
      "Processed image 12161\n",
      "Processed image 12162\n",
      "Processed image 12163\n",
      "Processed image 12164\n",
      "Processed image 12165\n",
      "Processed image 12166\n",
      "Processed image 12167\n",
      "Processed image 12168\n",
      "Processed image 12169\n",
      "Processed image 12170\n",
      "Processed image 12171\n",
      "Processed image 12172\n",
      "Processed image 12173\n",
      "Processed image 12174\n",
      "Processed image 12175\n",
      "Processed image 12176\n",
      "Processed image 12177\n",
      "Processed image 12178\n",
      "Processed image 12179\n",
      "Processed image 12180\n",
      "Processed image 12181\n",
      "Processed image 12182\n",
      "Processed image 12183\n",
      "Processed image 12184\n",
      "Processed image 12185\n",
      "Processed image 12186\n",
      "Processed image 12187\n",
      "Processed image 12188\n",
      "Processed image 12189\n",
      "Processed image 12190\n",
      "Processed image 12191\n",
      "Processed image 12192\n",
      "Processed image 12193\n",
      "Processed image 12194\n",
      "Processed image 12195\n",
      "Processed image 12196\n",
      "Processed image 12197\n",
      "Processed image 12198\n",
      "Processed image 12199\n",
      "Processed image 12200\n",
      "Processed image 12201\n",
      "Processed image 12202\n",
      "Processed image 12203\n",
      "Processed image 12204\n",
      "Processed image 12205\n",
      "Processed image 12206\n",
      "Processed image 12207\n",
      "Processed image 12208\n",
      "Processed image 12209\n",
      "Processed image 12210\n",
      "Processed image 12211\n",
      "Processed image 12212\n",
      "Processed image 12213\n",
      "Processed image 12214\n",
      "Processed image 12215\n",
      "Processed image 12216\n",
      "Processed image 12217\n",
      "Processed image 12218\n",
      "Processed image 12219\n",
      "Processed image 12220\n",
      "Processed image 12221\n",
      "Processed image 12222\n",
      "Processed image 12223\n",
      "Processed image 12224\n",
      "Processed image 12225\n",
      "Processed image 12226\n",
      "Processed image 12227\n",
      "Processed image 12228\n",
      "Processed image 12229\n",
      "Processed image 12230\n",
      "Processed image 12231\n",
      "Processed image 12232\n",
      "Processed image 12233\n",
      "Processed image 12234\n",
      "Processed image 12235\n",
      "Processed image 12236\n",
      "Processed image 12237\n",
      "Processed image 12238\n",
      "Processed image 12239\n",
      "Processed image 12240\n",
      "Processed image 12241\n",
      "Processed image 12242\n",
      "Processed image 12243\n",
      "Processed image 12244\n",
      "Processed image 12245\n",
      "Processed image 12246\n",
      "Processed image 12247\n",
      "Processed image 12248\n",
      "Processed image 12249\n",
      "Processed image 12250\n",
      "Processed image 12251\n",
      "Processed image 12252\n",
      "Processed image 12253\n",
      "Processed image 12254\n",
      "Processed image 12255\n",
      "Processed image 12256\n",
      "Processed image 12257\n",
      "Processed image 12258\n",
      "Processed image 12259\n",
      "Processed image 12260\n",
      "Processed image 12261\n",
      "Processed image 12262\n",
      "Processed image 12263\n",
      "Processed image 12264\n",
      "Processed image 12265\n",
      "Processed image 12266\n",
      "Processed image 12267\n",
      "Processed image 12268\n",
      "Processed image 12269\n",
      "Processed image 12270\n",
      "Processed image 12271\n",
      "Processed image 12272\n",
      "Processed image 12273\n",
      "Processed image 12274\n",
      "Processed image 12275\n",
      "Processed image 12276\n",
      "Processed image 12277\n",
      "Processed image 12278\n",
      "Processed image 12279\n",
      "Processed image 12280\n",
      "Processed image 12281\n",
      "Processed image 12282\n",
      "Processed image 12283\n",
      "Processed image 12284\n",
      "Processed image 12285\n",
      "Processed image 12286\n",
      "Processed image 12287\n",
      "Processed image 12288\n",
      "Processed image 12289\n",
      "Processed image 12290\n",
      "Processed image 12291\n",
      "Processed image 12292\n",
      "Processed image 12293\n",
      "Processed image 12294\n",
      "Processed image 12295\n",
      "Processed image 12296\n",
      "Processed image 12297\n",
      "Processed image 12298\n",
      "Processed image 12299\n",
      "Processed image 12300\n",
      "Processed image 12301\n",
      "Processed image 12302\n",
      "Processed image 12303\n",
      "Processed image 12304\n",
      "Processed image 12305\n",
      "Processed image 12306\n",
      "Processed image 12307\n",
      "Processed image 12308\n",
      "Processed image 12309\n",
      "Processed image 12310\n",
      "Processed image 12311\n",
      "Processed image 12312\n",
      "Processed image 12313\n",
      "Processed image 12314\n",
      "Processed image 12315\n",
      "Processed image 12316\n",
      "Processed image 12317\n",
      "Processed image 12318\n",
      "Processed image 12319\n",
      "Processed image 12320\n",
      "Processed image 12321\n",
      "Processed image 12322\n",
      "Processed image 12323\n",
      "Processed image 12324\n",
      "Processed image 12325\n",
      "Processed image 12326\n",
      "Processed image 12327\n",
      "Processed image 12328\n",
      "Processed image 12329\n",
      "Processed image 12330\n",
      "Processed image 12331\n",
      "Processed image 12332\n",
      "Processed image 12333\n",
      "Processed image 12334\n",
      "Processed image 12335\n",
      "Processed image 12336\n",
      "Processed image 12337\n",
      "Processed image 12338\n",
      "Processed image 12339\n",
      "Processed image 12340\n",
      "Processed image 12341\n",
      "Processed image 12342\n",
      "Processed image 12343\n",
      "Processed image 12344\n",
      "Processed image 12345\n",
      "Processed image 12346\n",
      "Processed image 12347\n",
      "Processed image 12348\n",
      "Processed image 12349\n",
      "Processed image 12350\n",
      "Processed image 12351\n",
      "Processed image 12352\n",
      "Processed image 12353\n",
      "Processed image 12354\n",
      "Processed image 12355\n",
      "Processed image 12356\n",
      "Processed image 12357\n",
      "Processed image 12358\n",
      "Processed image 12359\n",
      "Processed image 12360\n",
      "Processed image 12361\n",
      "Processed image 12362\n",
      "Processed image 12363\n",
      "Processed image 12364\n",
      "Processed image 12365\n",
      "Processed image 12366\n",
      "Processed image 12367\n",
      "Processed image 12368\n",
      "Processed image 12369\n",
      "Processed image 12370\n",
      "Processed image 12371\n",
      "Processed image 12372\n",
      "Processed image 12373\n",
      "Processed image 12374\n",
      "Processed image 12375\n",
      "Processed image 12376\n",
      "Processed image 12377\n",
      "Processed image 12378\n",
      "Processed image 12379\n",
      "Processed image 12380\n",
      "Processed image 12381\n",
      "Processed image 12382\n",
      "Processed image 12383\n",
      "Processed image 12384\n",
      "Processed image 12385\n",
      "Processed image 12386\n",
      "Processed image 12387\n",
      "Processed image 12388\n",
      "Processed image 12389\n",
      "Processed image 12390\n",
      "Processed image 12391\n",
      "Processed image 12392\n",
      "Processed image 12393\n",
      "Processed image 12394\n",
      "Processed image 12395\n",
      "Processed image 12396\n",
      "Processed image 12397\n",
      "Processed image 12398\n",
      "Processed image 12399\n",
      "Processed image 12400\n",
      "Processed image 12401\n",
      "Processed image 12402\n",
      "Processed image 12403\n",
      "Processed image 12404\n",
      "Processed image 12405\n",
      "Processed image 12406\n",
      "Processed image 12407\n",
      "Processed image 12408\n",
      "Processed image 12409\n",
      "Processed image 12410\n",
      "Processed image 12411\n",
      "Processed image 12412\n",
      "Processed image 12413\n",
      "Processed image 12414\n",
      "Processed image 12415\n",
      "Processed image 12416\n",
      "Processed image 12417\n",
      "Processed image 12418\n",
      "Processed image 12419\n",
      "Processed image 12420\n",
      "Processed image 12421\n",
      "Processed image 12422\n",
      "Processed image 12423\n",
      "Processed image 12424\n",
      "Processed image 12425\n",
      "Processed image 12426\n",
      "Processed image 12427\n",
      "Processed image 12428\n",
      "Processed image 12429\n",
      "Processed image 12430\n",
      "Processed image 12431\n",
      "Processed image 12432\n",
      "Processed image 12433\n",
      "Processed image 12434\n",
      "Processed image 12435\n",
      "Processed image 12436\n",
      "Processed image 12437\n",
      "Processed image 12438\n",
      "Processed image 12439\n",
      "Processed image 12440\n",
      "Processed image 12441\n",
      "Processed image 12442\n",
      "Processed image 12443\n",
      "Processed image 12444\n",
      "Processed image 12445\n",
      "Processed image 12446\n",
      "Processed image 12447\n",
      "Processed image 12448\n",
      "Processed image 12449\n",
      "Processed image 12450\n",
      "Processed image 12451\n",
      "Processed image 12452\n",
      "Processed image 12453\n",
      "Processed image 12454\n",
      "Processed image 12455\n",
      "Processed image 12456\n",
      "Processed image 12457\n",
      "Processed image 12458\n",
      "Processed image 12459\n",
      "Processed image 12460\n",
      "Processed image 12461\n",
      "Processed image 12462\n",
      "Processed image 12463\n",
      "Processed image 12464\n",
      "Processed image 12465\n",
      "Processed image 12466\n",
      "Processed image 12467\n",
      "Processed image 12468\n",
      "Processed image 12469\n",
      "Processed image 12470\n",
      "Processed image 12471\n",
      "Processed image 12472\n",
      "Processed image 12473\n",
      "Processed image 12474\n",
      "Processed image 12475\n",
      "Processed image 12476\n",
      "Processed image 12477\n",
      "Processed image 12478\n",
      "Processed image 12479\n",
      "Processed image 12480\n",
      "Processed image 12481\n",
      "Processed image 12482\n",
      "Processed image 12483\n",
      "Processed image 12484\n",
      "Processed image 12485\n",
      "Processed image 12486\n",
      "Processed image 12487\n",
      "Processed image 12488\n",
      "Processed image 12489\n",
      "Processed image 12490\n",
      "Processed image 12491\n",
      "Processed image 12492\n",
      "Processed image 12493\n",
      "Processed image 12494\n",
      "Processed image 12495\n",
      "Processed image 12496\n",
      "Processed image 12497\n",
      "Processed image 12498\n",
      "Processed image 12499\n",
      "Processed image 12500\n",
      "Processed image 12501\n",
      "Processed image 12502\n",
      "Processed image 12503\n",
      "Processed image 12504\n",
      "Processed image 12505\n",
      "Processed image 12506\n",
      "Processed image 12507\n",
      "Processed image 12508\n",
      "Processed image 12509\n",
      "Processed image 12510\n",
      "Processed image 12511\n",
      "Processed image 12512\n",
      "Processed image 12513\n",
      "Processed image 12514\n",
      "Processed image 12515\n",
      "Processed image 12516\n",
      "Processed image 12517\n",
      "Processed image 12518\n",
      "Processed image 12519\n",
      "Processed image 12520\n",
      "Processed image 12521\n",
      "Processed image 12522\n",
      "Processed image 12523\n",
      "Processed image 12524\n",
      "Processed image 12525\n",
      "Processed image 12526\n",
      "Processed image 12527\n",
      "Processed image 12528\n",
      "Processed image 12529\n",
      "Processed image 12530\n",
      "Processed image 12531\n",
      "Processed image 12532\n",
      "Processed image 12533\n",
      "Processed image 12534\n",
      "Processed image 12535\n",
      "Processed image 12536\n",
      "Processed image 12537\n",
      "Processed image 12538\n",
      "Processed image 12539\n",
      "Processed image 12540\n",
      "Processed image 12541\n",
      "Processed image 12542\n",
      "Processed image 12543\n",
      "Processed image 12544\n",
      "Processed image 12545\n",
      "Processed image 12546\n",
      "Processed image 12547\n",
      "Processed image 12548\n",
      "Processed image 12549\n",
      "Processed image 12550\n",
      "Processed image 12551\n",
      "Processed image 12552\n",
      "Processed image 12553\n",
      "Processed image 12554\n",
      "Processed image 12555\n",
      "Processed image 12556\n",
      "Processed image 12557\n",
      "Processed image 12558\n",
      "Processed image 12559\n",
      "Processed image 12560\n",
      "Processed image 12561\n",
      "Processed image 12562\n",
      "Processed image 12563\n",
      "Processed image 12564\n",
      "Processed image 12565\n",
      "Processed image 12566\n",
      "Processed image 12567\n",
      "Processed image 12568\n",
      "Processed image 12569\n",
      "Processed image 12570\n",
      "Processed image 12571\n",
      "Processed image 12572\n",
      "Processed image 12573\n",
      "Processed image 12574\n",
      "Processed image 12575\n",
      "Processed image 12576\n",
      "Processed image 12577\n",
      "Processed image 12578\n",
      "Processed image 12579\n",
      "Processed image 12580\n",
      "Processed image 12581\n",
      "Processed image 12582\n",
      "Processed image 12583\n",
      "Processed image 12584\n",
      "Processed image 12585\n",
      "Processed image 12586\n",
      "Processed image 12587\n",
      "Processed image 12588\n",
      "Processed image 12589\n",
      "Processed image 12590\n",
      "Processed image 12591\n",
      "Processed image 12592\n",
      "Processed image 12593\n",
      "Processed image 12594\n",
      "Processed image 12595\n",
      "Processed image 12596\n",
      "Processed image 12597\n",
      "Processed image 12598\n",
      "Processed image 12599\n",
      "Processed image 12600\n",
      "Processed image 12601\n",
      "Processed image 12602\n",
      "Processed image 12603\n",
      "Processed image 12604\n",
      "Processed image 12605\n",
      "Processed image 12606\n",
      "Processed image 12607\n",
      "Processed image 12608\n",
      "Processed image 12609\n",
      "Processed image 12610\n",
      "Processed image 12611\n",
      "Processed image 12612\n",
      "Processed image 12613\n",
      "Processed image 12614\n",
      "Processed image 12615\n",
      "Processed image 12616\n",
      "Processed image 12617\n",
      "Processed image 12618\n",
      "Processed image 12619\n",
      "Processed image 12620\n",
      "Processed image 12621\n",
      "Processed image 12622\n",
      "Processed image 12623\n",
      "Processed image 12624\n",
      "Processed image 12625\n",
      "Processed image 12626\n",
      "Processed image 12627\n",
      "Processed image 12628\n",
      "Processed image 12629\n",
      "Processed image 12630\n",
      "Processed image 12631\n",
      "Processed image 12632\n",
      "Processed image 12633\n",
      "Processed image 12634\n",
      "Processed image 12635\n",
      "Processed image 12636\n",
      "Processed image 12637\n",
      "Processed image 12638\n",
      "Processed image 12639\n",
      "Processed image 12640\n",
      "Processed image 12641\n",
      "Processed image 12642\n",
      "Processed image 12643\n",
      "Processed image 12644\n",
      "Processed image 12645\n",
      "Processed image 12646\n",
      "Processed image 12647\n",
      "Processed image 12648\n",
      "Processed image 12649\n",
      "Processed image 12650\n",
      "Processed image 12651\n",
      "Processed image 12652\n",
      "Processed image 12653\n",
      "Processed image 12654\n",
      "Processed image 12655\n",
      "Processed image 12656\n",
      "Processed image 12657\n",
      "Processed image 12658\n",
      "Processed image 12659\n",
      "Processed image 12660\n",
      "Processed image 12661\n",
      "Processed image 12662\n",
      "Processed image 12663\n",
      "Processed image 12664\n",
      "Processed image 12665\n",
      "Processed image 12666\n",
      "Processed image 12667\n",
      "Processed image 12668\n",
      "Processed image 12669\n",
      "Processed image 12670\n",
      "Processed image 12671\n",
      "Processed image 12672\n",
      "Processed image 12673\n",
      "Processed image 12674\n",
      "Processed image 12675\n",
      "Processed image 12676\n",
      "Processed image 12677\n",
      "Processed image 12678\n",
      "Processed image 12679\n",
      "Processed image 12680\n",
      "Processed image 12681\n",
      "Processed image 12682\n",
      "Processed image 12683\n",
      "Processed image 12684\n",
      "Processed image 12685\n",
      "Processed image 12686\n",
      "Processed image 12687\n",
      "Processed image 12688\n",
      "Processed image 12689\n",
      "Processed image 12690\n",
      "Processed image 12691\n",
      "Processed image 12692\n",
      "Processed image 12693\n",
      "Processed image 12694\n",
      "Processed image 12695\n",
      "Processed image 12696\n",
      "Processed image 12697\n",
      "Processed image 12698\n",
      "Processed image 12699\n",
      "Processed image 12700\n",
      "Processed image 12701\n",
      "Processed image 12702\n",
      "Processed image 12703\n",
      "Processed image 12704\n",
      "Processed image 12705\n",
      "Processed image 12706\n",
      "Processed image 12707\n",
      "Processed image 12708\n",
      "Processed image 12709\n",
      "Processed image 12710\n",
      "Processed image 12711\n",
      "Processed image 12712\n",
      "Processed image 12713\n",
      "Processed image 12714\n",
      "Processed image 12715\n",
      "Processed image 12716\n",
      "Processed image 12717\n",
      "Processed image 12718\n",
      "Processed image 12719\n",
      "Processed image 12720\n",
      "Processed image 12721\n",
      "Processed image 12722\n",
      "Processed image 12723\n",
      "Processed image 12724\n",
      "Processed image 12725\n",
      "Processed image 12726\n",
      "Processed image 12727\n",
      "Processed image 12728\n",
      "Processed image 12729\n",
      "Processed image 12730\n",
      "Processed image 12731\n",
      "Processed image 12732\n",
      "Processed image 12733\n",
      "Processed image 12734\n",
      "Processed image 12735\n",
      "Processed image 12736\n",
      "Processed image 12737\n",
      "Processed image 12738\n",
      "Processed image 12739\n",
      "Processed image 12740\n",
      "Processed image 12741\n",
      "Processed image 12742\n",
      "Processed image 12743\n",
      "Processed image 12744\n",
      "Processed image 12745\n",
      "Processed image 12746\n",
      "Processed image 12747\n",
      "Processed image 12748\n",
      "Processed image 12749\n",
      "Processed image 12750\n",
      "Processed image 12751\n",
      "Processed image 12752\n",
      "Processed image 12753\n",
      "Processed image 12754\n",
      "Processed image 12755\n",
      "Processed image 12756\n",
      "Processed image 12757\n",
      "Processed image 12758\n",
      "Processed image 12759\n",
      "Processed image 12760\n",
      "Processed image 12761\n",
      "Processed image 12762\n",
      "Processed image 12763\n",
      "Processed image 12764\n",
      "Processed image 12765\n",
      "Processed image 12766\n",
      "Processed image 12767\n",
      "Processed image 12768\n",
      "Processed image 12769\n",
      "Processed image 12770\n",
      "Processed image 12771\n",
      "Processed image 12772\n",
      "Processed image 12773\n",
      "Processed image 12774\n",
      "Processed image 12775\n",
      "Processed image 12776\n",
      "Processed image 12777\n",
      "Processed image 12778\n",
      "Processed image 12779\n",
      "Processed image 12780\n",
      "Processed image 12781\n",
      "Processed image 12782\n",
      "Processed image 12783\n",
      "Processed image 12784\n",
      "Processed image 12785\n",
      "Processed image 12786\n",
      "Processed image 12787\n",
      "Processed image 12788\n",
      "Processed image 12789\n",
      "Processed image 12790\n",
      "Processed image 12791\n",
      "Processed image 12792\n",
      "Processed image 12793\n",
      "Processed image 12794\n",
      "Processed image 12795\n",
      "Processed image 12796\n",
      "Processed image 12797\n",
      "Processed image 12798\n",
      "Processed image 12799\n",
      "Processed image 12800\n",
      "Processed image 12801\n",
      "Processed image 12802\n",
      "Processed image 12803\n",
      "Processed image 12804\n",
      "Processed image 12805\n",
      "Processed image 12806\n",
      "Processed image 12807\n",
      "Processed image 12808\n",
      "Processed image 12809\n",
      "Processed image 12810\n",
      "Processed image 12811\n",
      "Processed image 12812\n",
      "Processed image 12813\n",
      "Processed image 12814\n",
      "Processed image 12815\n",
      "Processed image 12816\n",
      "Processed image 12817\n",
      "Processed image 12818\n",
      "Processed image 12819\n",
      "Processed image 12820\n",
      "Processed image 12821\n",
      "Processed image 12822\n",
      "Processed image 12823\n",
      "Processed image 12824\n",
      "Processed image 12825\n",
      "Processed image 12826\n",
      "Processed image 12827\n",
      "Processed image 12828\n",
      "Processed image 12829\n",
      "Processed image 12830\n",
      "Processed image 12831\n",
      "Processed image 12832\n",
      "Processed image 12833\n",
      "Processed image 12834\n",
      "Processed image 12835\n",
      "Processed image 12836\n",
      "Processed image 12837\n",
      "Processed image 12838\n",
      "Processed image 12839\n",
      "Processed image 12840\n",
      "Processed image 12841\n",
      "Processed image 12842\n",
      "Processed image 12843\n",
      "Processed image 12844\n",
      "Processed image 12845\n",
      "Processed image 12846\n",
      "Processed image 12847\n",
      "Processed image 12848\n",
      "Processed image 12849\n",
      "Processed image 12850\n",
      "Processed image 12851\n",
      "Processed image 12852\n",
      "Processed image 12853\n",
      "Processed image 12854\n",
      "Processed image 12855\n",
      "Processed image 12856\n",
      "Processed image 12857\n",
      "Processed image 12858\n",
      "Processed image 12859\n",
      "Processed image 12860\n",
      "Processed image 12861\n",
      "Processed image 12862\n",
      "Processed image 12863\n",
      "Processed image 12864\n",
      "Processed image 12865\n",
      "Processed image 12866\n",
      "Processed image 12867\n",
      "Processed image 12868\n",
      "Processed image 12869\n",
      "Processed image 12870\n",
      "Processed image 12871\n",
      "Processed image 12872\n",
      "Processed image 12873\n",
      "Processed image 12874\n",
      "Processed image 12875\n",
      "Processed image 12876\n",
      "Processed image 12877\n",
      "Processed image 12878\n",
      "Processed image 12879\n",
      "Processed image 12880\n",
      "Processed image 12881\n",
      "Processed image 12882\n",
      "Processed image 12883\n",
      "Processed image 12884\n",
      "Processed image 12885\n",
      "Processed image 12886\n",
      "Processed image 12887\n",
      "Processed image 12888\n",
      "Processed image 12889\n",
      "Processed image 12890\n",
      "Processed image 12891\n",
      "Processed image 12892\n",
      "Processed image 12893\n",
      "Processed image 12894\n",
      "Processed image 12895\n",
      "Processed image 12896\n",
      "Processed image 12897\n",
      "Processed image 12898\n",
      "Processed image 12899\n",
      "Processed image 12900\n",
      "Processed image 12901\n",
      "Processed image 12902\n",
      "Processed image 12903\n",
      "Processed image 12904\n",
      "Processed image 12905\n",
      "Processed image 12906\n",
      "Processed image 12907\n",
      "Processed image 12908\n",
      "Processed image 12909\n",
      "Processed image 12910\n",
      "Processed image 12911\n",
      "Processed image 12912\n",
      "Processed image 12913\n",
      "Processed image 12914\n",
      "Processed image 12915\n",
      "Processed image 12916\n",
      "Processed image 12917\n",
      "Processed image 12918\n",
      "Processed image 12919\n",
      "Processed image 12920\n",
      "Processed image 12921\n",
      "Processed image 12922\n",
      "Processed image 12923\n",
      "Processed image 12924\n",
      "Processed image 12925\n",
      "Processed image 12926\n",
      "Processed image 12927\n",
      "Processed image 12928\n",
      "Processed image 12929\n",
      "Processed image 12930\n",
      "Processed image 12931\n",
      "Processed image 12932\n",
      "Processed image 12933\n",
      "Processed image 12934\n",
      "Processed image 12935\n",
      "Processed image 12936\n",
      "Processed image 12937\n",
      "Processed image 12938\n",
      "Processed image 12939\n",
      "Processed image 12940\n",
      "Processed image 12941\n",
      "Processed image 12942\n",
      "Processed image 12943\n",
      "Processed image 12944\n",
      "Processed image 12945\n",
      "Processed image 12946\n",
      "Processed image 12947\n",
      "Processed image 12948\n",
      "Processed image 12949\n",
      "Processed image 12950\n",
      "Processed image 12951\n",
      "Processed image 12952\n",
      "Processed image 12953\n",
      "Processed image 12954\n",
      "Processed image 12955\n",
      "Processed image 12956\n",
      "Processed image 12957\n",
      "Processed image 12958\n",
      "Processed image 12959\n",
      "Processed image 12960\n",
      "Processed image 12961\n",
      "Processed image 12962\n",
      "Processed image 12963\n",
      "Processed image 12964\n",
      "Processed image 12965\n",
      "Processed image 12966\n",
      "Processed image 12967\n",
      "Processed image 12968\n",
      "Processed image 12969\n",
      "Processed image 12970\n",
      "Processed image 12971\n",
      "Processed image 12972\n",
      "Processed image 12973\n",
      "Processed image 12974\n",
      "Processed image 12975\n",
      "Processed image 12976\n",
      "Processed image 12977\n",
      "Processed image 12978\n",
      "Processed image 12979\n",
      "Processed image 12980\n",
      "Processed image 12981\n",
      "Processed image 12982\n",
      "Processed image 12983\n",
      "Processed image 12984\n",
      "Processed image 12985\n",
      "Processed image 12986\n",
      "Processed image 12987\n",
      "Processed image 12988\n",
      "Processed image 12989\n",
      "Processed image 12990\n",
      "Processed image 12991\n",
      "Processed image 12992\n",
      "Processed image 12993\n",
      "Processed image 12994\n",
      "Processed image 12995\n",
      "Processed image 12996\n",
      "Processed image 12997\n",
      "Processed image 12998\n",
      "Processed image 12999\n",
      "Processed image 13000\n",
      "Processed image 13001\n",
      "Processed image 13002\n",
      "Processed image 13003\n",
      "Processed image 13004\n",
      "Processed image 13005\n",
      "Processed image 13006\n",
      "Processed image 13007\n",
      "Processed image 13008\n",
      "Processed image 13009\n",
      "Processed image 13010\n",
      "Processed image 13011\n",
      "Processed image 13012\n",
      "Processed image 13013\n",
      "Processed image 13014\n",
      "Processed image 13015\n",
      "Processed image 13016\n",
      "Processed image 13017\n",
      "Processed image 13018\n",
      "Processed image 13019\n",
      "Processed image 13020\n",
      "Processed image 13021\n",
      "Processed image 13022\n",
      "Processed image 13023\n",
      "Processed image 13024\n",
      "Processed image 13025\n",
      "Processed image 13026\n",
      "Processed image 13027\n",
      "Processed image 13028\n",
      "Processed image 13029\n",
      "Processed image 13030\n",
      "Processed image 13031\n",
      "Processed image 13032\n",
      "Processed image 13033\n",
      "Processed image 13034\n",
      "Processed image 13035\n",
      "Processed image 13036\n",
      "Processed image 13037\n",
      "Processed image 13038\n",
      "Processed image 13039\n",
      "Processed image 13040\n",
      "Processed image 13041\n",
      "Processed image 13042\n",
      "Processed image 13043\n",
      "Processed image 13044\n",
      "Processed image 13045\n",
      "Processed image 13046\n",
      "Processed image 13047\n",
      "Processed image 13048\n",
      "Processed image 13049\n",
      "Processed image 13050\n",
      "Processed image 13051\n",
      "Processed image 13052\n",
      "Processed image 13053\n",
      "Processed image 13054\n",
      "Processed image 13055\n",
      "Processed image 13056\n",
      "Processed image 13057\n",
      "Processed image 13058\n",
      "Processed image 13059\n",
      "Processed image 13060\n",
      "Processed image 13061\n",
      "Processed image 13062\n",
      "Processed image 13063\n",
      "Processed image 13064\n",
      "Processed image 13065\n",
      "Processed image 13066\n",
      "Processed image 13067\n",
      "Processed image 13068\n",
      "Processed image 13069\n",
      "Processed image 13070\n",
      "Processed image 13071\n",
      "Processed image 13072\n",
      "Processed image 13073\n",
      "Processed image 13074\n",
      "Processed image 13075\n",
      "Processed image 13076\n",
      "Processed image 13077\n",
      "Processed image 13078\n",
      "Processed image 13079\n",
      "Processed image 13080\n",
      "Processed image 13081\n",
      "Processed image 13082\n",
      "Processed image 13083\n",
      "Processed image 13084\n",
      "Processed image 13085\n",
      "Processed image 13086\n",
      "Processed image 13087\n",
      "Processed image 13088\n",
      "Processed image 13089\n",
      "Processed image 13090\n",
      "Processed image 13091\n",
      "Processed image 13092\n",
      "Processed image 13093\n",
      "Processed image 13094\n",
      "Processed image 13095\n",
      "Processed image 13096\n",
      "Processed image 13097\n",
      "Processed image 13098\n",
      "Processed image 13099\n",
      "Processed image 13100\n",
      "Processed image 13101\n",
      "Processed image 13102\n",
      "Processed image 13103\n",
      "Processed image 13104\n",
      "Processed image 13105\n",
      "Processed image 13106\n",
      "Processed image 13107\n",
      "Processed image 13108\n",
      "Processed image 13109\n",
      "Processed image 13110\n",
      "Processed image 13111\n",
      "Processed image 13112\n",
      "Processed image 13113\n",
      "Processed image 13114\n",
      "Processed image 13115\n",
      "Processed image 13116\n",
      "Processed image 13117\n",
      "Processed image 13118\n",
      "Processed image 13119\n",
      "Processed image 13120\n",
      "Processed image 13121\n",
      "Processed image 13122\n",
      "Processed image 13123\n",
      "Processed image 13124\n",
      "Processed image 13125\n",
      "Processed image 13126\n",
      "Processed image 13127\n",
      "Processed image 13128\n",
      "Processed image 13129\n",
      "Processed image 13130\n",
      "Processed image 13131\n",
      "Processed image 13132\n",
      "Processed image 13133\n",
      "Processed image 13134\n",
      "Processed image 13135\n",
      "Processed image 13136\n",
      "Processed image 13137\n",
      "Processed image 13138\n",
      "Processed image 13139\n",
      "Processed image 13140\n",
      "Processed image 13141\n",
      "Processed image 13142\n",
      "Processed image 13143\n",
      "Processed image 13144\n",
      "Processed image 13145\n",
      "Processed image 13146\n",
      "Processed image 13147\n",
      "Processed image 13148\n",
      "Processed image 13149\n",
      "Processed image 13150\n",
      "Processed image 13151\n",
      "Processed image 13152\n",
      "Processed image 13153\n",
      "Processed image 13154\n",
      "Processed image 13155\n",
      "Processed image 13156\n",
      "Processed image 13157\n",
      "Processed image 13158\n",
      "Processed image 13159\n",
      "Processed image 13160\n",
      "Processed image 13161\n",
      "Processed image 13162\n",
      "Processed image 13163\n",
      "Processed image 13164\n",
      "Processed image 13165\n",
      "Processed image 13166\n",
      "Processed image 13167\n",
      "Processed image 13168\n",
      "Processed image 13169\n",
      "Processed image 13170\n",
      "Processed image 13171\n",
      "Processed image 13172\n",
      "Processed image 13173\n",
      "Processed image 13174\n",
      "Processed image 13175\n",
      "Processed image 13176\n",
      "Processed image 13177\n",
      "Processed image 13178\n",
      "Processed image 13179\n",
      "Processed image 13180\n",
      "Processed image 13181\n",
      "Processed image 13182\n",
      "Processed image 13183\n",
      "Processed image 13184\n",
      "Processed image 13185\n",
      "Processed image 13186\n",
      "Processed image 13187\n",
      "Processed image 13188\n",
      "Processed image 13189\n",
      "Processed image 13190\n",
      "Processed image 13191\n",
      "Processed image 13192\n",
      "Processed image 13193\n",
      "Processed image 13194\n",
      "Processed image 13195\n",
      "Processed image 13196\n",
      "Processed image 13197\n",
      "Processed image 13198\n",
      "Processed image 13199\n",
      "Processed image 13200\n",
      "Processed image 13201\n",
      "Processed image 13202\n",
      "Processed image 13203\n",
      "Processed image 13204\n",
      "Processed image 13205\n",
      "Processed image 13206\n",
      "Processed image 13207\n",
      "Processed image 13208\n",
      "Processed image 13209\n",
      "Processed image 13210\n",
      "Processed image 13211\n",
      "Processed image 13212\n",
      "Processed image 13213\n",
      "Processed image 13214\n",
      "Processed image 13215\n",
      "Processed image 13216\n",
      "Processed image 13217\n",
      "Processed image 13218\n",
      "Processed image 13219\n",
      "Processed image 13220\n",
      "Processed image 13221\n",
      "Processed image 13222\n",
      "Processed image 13223\n",
      "Processed image 13224\n",
      "Processed image 13225\n",
      "Processed image 13226\n",
      "Processed image 13227\n",
      "Processed image 13228\n",
      "Processed image 13229\n",
      "Processed image 13230\n",
      "Processed image 13231\n",
      "Processed image 13232\n",
      "Processed image 13233\n",
      "Processed image 13234\n",
      "Processed image 13235\n",
      "Processed image 13236\n",
      "Processed image 13237\n",
      "Processed image 13238\n",
      "Processed image 13239\n",
      "Processed image 13240\n",
      "Processed image 13241\n",
      "Processed image 13242\n",
      "Processed image 13243\n",
      "Processed image 13244\n",
      "Processed image 13245\n",
      "Processed image 13246\n",
      "Processed image 13247\n",
      "Processed image 13248\n",
      "Processed image 13249\n",
      "Processed image 13250\n",
      "Processed image 13251\n",
      "Processed image 13252\n",
      "Processed image 13253\n",
      "Processed image 13254\n",
      "Processed image 13255\n",
      "Processed image 13256\n",
      "Processed image 13257\n",
      "Processed image 13258\n",
      "Processed image 13259\n",
      "Processed image 13260\n",
      "Processed image 13261\n",
      "Processed image 13262\n",
      "Processed image 13263\n",
      "Processed image 13264\n",
      "Processed image 13265\n",
      "Processed image 13266\n",
      "Processed image 13267\n",
      "Processed image 13268\n",
      "Processed image 13269\n",
      "Processed image 13270\n",
      "Processed image 13271\n",
      "Processed image 13272\n",
      "Processed image 13273\n",
      "Processed image 13274\n",
      "Processed image 13275\n",
      "Processed image 13276\n",
      "Processed image 13277\n",
      "Processed image 13278\n",
      "Processed image 13279\n",
      "Processed image 13280\n",
      "Processed image 13281\n",
      "Processed image 13282\n",
      "Processed image 13283\n",
      "Processed image 13284\n",
      "Processed image 13285\n",
      "Processed image 13286\n",
      "Processed image 13287\n",
      "Processed image 13288\n",
      "Processed image 13289\n",
      "Processed image 13290\n",
      "Processed image 13291\n",
      "Processed image 13292\n",
      "Processed image 13293\n",
      "Processed image 13294\n",
      "Processed image 13295\n",
      "Processed image 13296\n",
      "Processed image 13297\n",
      "Processed image 13298\n",
      "Processed image 13299\n",
      "Processed image 13300\n",
      "Processed image 13301\n",
      "Processed image 13302\n",
      "Processed image 13303\n",
      "Processed image 13304\n",
      "Processed image 13305\n",
      "Processed image 13306\n",
      "Processed image 13307\n",
      "Processed image 13308\n",
      "Processed image 13309\n",
      "Processed image 13310\n",
      "Processed image 13311\n",
      "Processed image 13312\n",
      "Processed image 13313\n",
      "Processed image 13314\n",
      "Processed image 13315\n",
      "Processed image 13316\n",
      "Processed image 13317\n",
      "Processed image 13318\n",
      "Processed image 13319\n",
      "Processed image 13320\n",
      "Processed image 13321\n",
      "Processed image 13322\n",
      "Processed image 13323\n",
      "Processed image 13324\n",
      "Processed image 13325\n",
      "Processed image 13326\n",
      "Processed image 13327\n",
      "Processed image 13328\n",
      "Processed image 13329\n",
      "Processed image 13330\n",
      "Processed image 13331\n",
      "Processed image 13332\n",
      "Processed image 13333\n",
      "Processed image 13334\n",
      "Processed image 13335\n",
      "Processed image 13336\n",
      "Processed image 13337\n",
      "Processed image 13338\n",
      "Processed image 13339\n",
      "Processed image 13340\n",
      "Processed image 13341\n",
      "Processed image 13342\n",
      "Processed image 13343\n",
      "Processed image 13344\n",
      "Processed image 13345\n",
      "Processed image 13346\n",
      "Processed image 13347\n",
      "Processed image 13348\n",
      "Processed image 13349\n",
      "Processed image 13350\n",
      "Processed image 13351\n",
      "Processed image 13352\n",
      "Processed image 13353\n",
      "Processed image 13354\n",
      "Processed image 13355\n",
      "Processed image 13356\n",
      "Processed image 13357\n",
      "Processed image 13358\n",
      "Processed image 13359\n",
      "Processed image 13360\n",
      "Processed image 13361\n",
      "Processed image 13362\n",
      "Processed image 13363\n",
      "Processed image 13364\n",
      "Processed image 13365\n",
      "Processed image 13366\n",
      "Processed image 13367\n",
      "Processed image 13368\n",
      "Processed image 13369\n",
      "Processed image 13370\n",
      "Processed image 13371\n",
      "Processed image 13372\n",
      "Processed image 13373\n",
      "Processed image 13374\n",
      "Processed image 13375\n",
      "Processed image 13376\n",
      "Processed image 13377\n",
      "Processed image 13378\n",
      "Processed image 13379\n",
      "Processed image 13380\n",
      "Processed image 13381\n",
      "Processed image 13382\n",
      "Processed image 13383\n",
      "Processed image 13384\n",
      "Processed image 13385\n",
      "Processed image 13386\n",
      "Processed image 13387\n",
      "Processed image 13388\n",
      "Processed image 13389\n",
      "Processed image 13390\n",
      "Processed image 13391\n",
      "Processed image 13392\n",
      "Processed image 13393\n",
      "Processed image 13394\n",
      "Processed image 13395\n",
      "Processed image 13396\n",
      "Processed image 13397\n",
      "Processed image 13398\n",
      "Processed image 13399\n",
      "Processed image 13400\n",
      "Processed image 13401\n",
      "Processed image 13402\n",
      "Processed image 13403\n",
      "Processed image 13404\n",
      "Processed image 13405\n",
      "Processed image 13406\n",
      "Processed image 13407\n",
      "Processed image 13408\n",
      "Processed image 13409\n",
      "Processed image 13410\n",
      "Processed image 13411\n",
      "Processed image 13412\n",
      "Processed image 13413\n",
      "Processed image 13414\n",
      "Processed image 13415\n",
      "Processed image 13416\n",
      "Processed image 13417\n",
      "Processed image 13418\n",
      "Processed image 13419\n",
      "Processed image 13420\n",
      "Processed image 13421\n",
      "Processed image 13422\n",
      "Processed image 13423\n",
      "Processed image 13424\n",
      "Processed image 13425\n",
      "Processed image 13426\n",
      "Processed image 13427\n",
      "Processed image 13428\n",
      "Processed image 13429\n",
      "Processed image 13430\n",
      "Processed image 13431\n",
      "Processed image 13432\n",
      "Processed image 13433\n",
      "Processed image 13434\n",
      "Processed image 13435\n",
      "Processed image 13436\n",
      "Processed image 13437\n",
      "Processed image 13438\n",
      "Processed image 13439\n",
      "Processed image 13440\n",
      "Processed image 13441\n",
      "Processed image 13442\n",
      "Processed image 13443\n",
      "Processed image 13444\n",
      "Processed image 13445\n",
      "Processed image 13446\n",
      "Processed image 13447\n",
      "Processed image 13448\n",
      "Processed image 13449\n",
      "Processed image 13450\n",
      "Processed image 13451\n",
      "Processed image 13452\n",
      "Processed image 13453\n",
      "Processed image 13454\n",
      "Processed image 13455\n",
      "Processed image 13456\n",
      "Processed image 13457\n",
      "Processed image 13458\n",
      "Processed image 13459\n",
      "Processed image 13460\n",
      "Processed image 13461\n",
      "Processed image 13462\n",
      "Processed image 13463\n",
      "Processed image 13464\n",
      "Processed image 13465\n",
      "Processed image 13466\n",
      "Processed image 13467\n",
      "Processed image 13468\n",
      "Processed image 13469\n",
      "Processed image 13470\n",
      "Processed image 13471\n",
      "Processed image 13472\n",
      "Processed image 13473\n",
      "Processed image 13474\n",
      "Processed image 13475\n",
      "Processed image 13476\n",
      "Processed image 13477\n",
      "Processed image 13478\n",
      "Processed image 13479\n",
      "Processed image 13480\n",
      "Processed image 13481\n",
      "Processed image 13482\n",
      "Processed image 13483\n",
      "Processed image 13484\n",
      "Processed image 13485\n",
      "Processed image 13486\n",
      "Processed image 13487\n",
      "Processed image 13488\n",
      "Processed image 13489\n",
      "Processed image 13490\n",
      "Processed image 13491\n",
      "Processed image 13492\n",
      "Processed image 13493\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))])\n",
    "image_list=[]\n",
    "for i in range(len(df_train)):\n",
    "    s = 'sports/'+df_train['filepaths'][i]\n",
    "    if('.jpg' not in s)and('.jpeg' not in s):\n",
    "        continue\n",
    "    image = Image.open(s).convert('RGB')\n",
    "    image_tensor=transform(image)\n",
    "    image_list.append(image_tensor)\n",
    "    print(f'Processed image {i+1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_list)==len(df_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'canoe slamon': 0, 'surfing': 1, 'uneven bars': 2, 'pole dancing': 3, 'roller derby': 4, 'harness racing': 5, 'luge': 6, 'steer wrestling': 7, 'bike polo': 8, 'bungee jumping': 9, 'shuffleboard': 10, 'wheelchair basketball': 11, 'field hockey': 12, 'pole vault': 13, 'skydiving': 14, 'jousting': 15, 'bowling': 16, 'curling': 17, 'hydroplane racing': 18, 'bmx': 19, 'ampute football': 20, 'tug of war': 21, 'croquet': 22, 'rugby': 23, 'bobsled': 24, 'log rolling': 25, 'football': 26, 'archery': 27, 'figure skating men': 28, 'sky surfing': 29, 'judo': 30, 'pommel horse': 31, 'cricket': 32, 'hockey': 33, 'wheelchair racing': 34, 'figure skating women': 35, 'sailboat racing': 36, 'snowmobile racing': 37, 'bull riding': 38, 'rock climbing': 39, 'water polo': 40, 'arm wrestling': 41, 'formula 1 racing': 42, 'fly fishing': 43, 'balance beam': 44, 'hammer throw': 45, 'volleyball': 46, 'sidecar racing': 47, 'baseball': 48, 'speed skating': 49, 'track bicycle': 50, 'pole climbing': 51, 'horseshoe pitching': 52, 'fencing': 53, 'ice climbing': 54, 'motorcycle racing': 55, 'cheerleading': 56, 'snow boarding': 57, 'mushing': 58, 'javelin': 59, 'horse racing': 60, 'jai alai': 61, 'barell racing': 62, 'ski jumping': 63, 'parallel bar': 64, 'swimming': 65, 'shot put': 66, 'giant slalom': 67, 'disc golf': 68, 'rollerblade racing': 69, 'axe throwing': 70, 'weightlifting': 71, 'wingsuit flying': 72, 'nascar racing': 73, 'baton twirling': 74, 'hang gliding': 75, 'rowing': 76, 'horse jumping': 77, 'frisbee': 78, 'hurdles': 79, 'water cycling': 80, 'gaga': 81, 'boxing': 82, 'trapeze': 83, 'billiards': 84, 'lacrosse': 85, 'ultimate': 86, 'sumo wrestling': 87, 'olympic wrestling': 88, 'figure skating pairs': 89, 'basketball': 90, 'high jump': 91, 'rings': 92, 'golf': 93, 'chuckwagon racing': 94, 'tennis': 95, 'table tennis': 96, 'ice yachting': 97, 'polo': 98, 'air hockey': 99}\n"
     ]
    }
   ],
   "source": [
    "sports_dict={}\n",
    "k=0\n",
    "for i in set(df_train['labels']):\n",
    "    if i in sports_dict.keys():\n",
    "        continue\n",
    "    else:\n",
    "        sports_dict[i] = k\n",
    "        k+=1\n",
    "\n",
    "print(sports_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sports_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_31344\\1332283892.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_op['enc_label'][i] = sports_dict[df_train_op['labels'][i]]\n"
     ]
    }
   ],
   "source": [
    "df_train_op['enc_label'] = df_train_op['labels']\n",
    "for i in range(len(df_train_op)):\n",
    "    if(i==5621):\n",
    "        continue\n",
    "    df_train_op['enc_label'][i] = sports_dict[df_train_op['labels'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class id', 'filepaths', 'labels', 'data set', 'enc_label'], dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_op.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SportsDataset(image_list, df_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_31344\\3733320619.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['enc_label'] = df_test['labels']\n",
      "C:\\Users\\KIIT\\AppData\\Local\\Temp\\ipykernel_31344\\3733320619.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['enc_label'][i] = sports_dict[df_test['labels'][i]]\n"
     ]
    }
   ],
   "source": [
    "df_test['enc_label'] = df_test['labels']\n",
    "for i in range(13493,13493+len(df_test)):\n",
    "    # if(i==5621):\n",
    "        # continue\n",
    "    df_test['enc_label'][i] = sports_dict[df_test['labels'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class id</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "      <th>enc_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13493</th>\n",
       "      <td>0</td>\n",
       "      <td>test/air hockey/1.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13494</th>\n",
       "      <td>0</td>\n",
       "      <td>test/air hockey/2.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13495</th>\n",
       "      <td>0</td>\n",
       "      <td>test/air hockey/3.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13496</th>\n",
       "      <td>0</td>\n",
       "      <td>test/air hockey/4.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13497</th>\n",
       "      <td>0</td>\n",
       "      <td>test/air hockey/5.jpg</td>\n",
       "      <td>air hockey</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13988</th>\n",
       "      <td>99</td>\n",
       "      <td>test/wingsuit flying/1.jpg</td>\n",
       "      <td>wingsuit flying</td>\n",
       "      <td>test</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>99</td>\n",
       "      <td>test/wingsuit flying/2.jpg</td>\n",
       "      <td>wingsuit flying</td>\n",
       "      <td>test</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13990</th>\n",
       "      <td>99</td>\n",
       "      <td>test/wingsuit flying/3.jpg</td>\n",
       "      <td>wingsuit flying</td>\n",
       "      <td>test</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>99</td>\n",
       "      <td>test/wingsuit flying/4.jpg</td>\n",
       "      <td>wingsuit flying</td>\n",
       "      <td>test</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13992</th>\n",
       "      <td>99</td>\n",
       "      <td>test/wingsuit flying/5.jpg</td>\n",
       "      <td>wingsuit flying</td>\n",
       "      <td>test</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class id                   filepaths           labels data set  \\\n",
       "13493         0       test/air hockey/1.jpg       air hockey     test   \n",
       "13494         0       test/air hockey/2.jpg       air hockey     test   \n",
       "13495         0       test/air hockey/3.jpg       air hockey     test   \n",
       "13496         0       test/air hockey/4.jpg       air hockey     test   \n",
       "13497         0       test/air hockey/5.jpg       air hockey     test   \n",
       "...         ...                         ...              ...      ...   \n",
       "13988        99  test/wingsuit flying/1.jpg  wingsuit flying     test   \n",
       "13989        99  test/wingsuit flying/2.jpg  wingsuit flying     test   \n",
       "13990        99  test/wingsuit flying/3.jpg  wingsuit flying     test   \n",
       "13991        99  test/wingsuit flying/4.jpg  wingsuit flying     test   \n",
       "13992        99  test/wingsuit flying/5.jpg  wingsuit flying     test   \n",
       "\n",
       "      enc_label  \n",
       "13493        99  \n",
       "13494        99  \n",
       "13495        99  \n",
       "13496        99  \n",
       "13497        99  \n",
       "...         ...  \n",
       "13988        72  \n",
       "13989        72  \n",
       "13990        72  \n",
       "13991        72  \n",
       "13992        72  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 13494\n",
      "Processed image 13495\n",
      "Processed image 13496\n",
      "Processed image 13497\n",
      "Processed image 13498\n",
      "Processed image 13499\n",
      "Processed image 13500\n",
      "Processed image 13501\n",
      "Processed image 13502\n",
      "Processed image 13503\n",
      "Processed image 13504\n",
      "Processed image 13505\n",
      "Processed image 13506\n",
      "Processed image 13507\n",
      "Processed image 13508\n",
      "Processed image 13509\n",
      "Processed image 13510\n",
      "Processed image 13511\n",
      "Processed image 13512\n",
      "Processed image 13513\n",
      "Processed image 13514\n",
      "Processed image 13515\n",
      "Processed image 13516\n",
      "Processed image 13517\n",
      "Processed image 13518\n",
      "Processed image 13519\n",
      "Processed image 13520\n",
      "Processed image 13521\n",
      "Processed image 13522\n",
      "Processed image 13523\n",
      "Processed image 13524\n",
      "Processed image 13525\n",
      "Processed image 13526\n",
      "Processed image 13527\n",
      "Processed image 13528\n",
      "Processed image 13529\n",
      "Processed image 13530\n",
      "Processed image 13531\n",
      "Processed image 13532\n",
      "Processed image 13533\n",
      "Processed image 13534\n",
      "Processed image 13535\n",
      "Processed image 13536\n",
      "Processed image 13537\n",
      "Processed image 13538\n",
      "Processed image 13539\n",
      "Processed image 13540\n",
      "Processed image 13541\n",
      "Processed image 13542\n",
      "Processed image 13543\n",
      "Processed image 13544\n",
      "Processed image 13545\n",
      "Processed image 13546\n",
      "Processed image 13547\n",
      "Processed image 13548\n",
      "Processed image 13549\n",
      "Processed image 13550\n",
      "Processed image 13551\n",
      "Processed image 13552\n",
      "Processed image 13553\n",
      "Processed image 13554\n",
      "Processed image 13555\n",
      "Processed image 13556\n",
      "Processed image 13557\n",
      "Processed image 13558\n",
      "Processed image 13559\n",
      "Processed image 13560\n",
      "Processed image 13561\n",
      "Processed image 13562\n",
      "Processed image 13563\n",
      "Processed image 13564\n",
      "Processed image 13565\n",
      "Processed image 13566\n",
      "Processed image 13567\n",
      "Processed image 13568\n",
      "Processed image 13569\n",
      "Processed image 13570\n",
      "Processed image 13571\n",
      "Processed image 13572\n",
      "Processed image 13573\n",
      "Processed image 13574\n",
      "Processed image 13575\n",
      "Processed image 13576\n",
      "Processed image 13577\n",
      "Processed image 13578\n",
      "Processed image 13579\n",
      "Processed image 13580\n",
      "Processed image 13581\n",
      "Processed image 13582\n",
      "Processed image 13583\n",
      "Processed image 13584\n",
      "Processed image 13585\n",
      "Processed image 13586\n",
      "Processed image 13587\n",
      "Processed image 13588\n",
      "Processed image 13589\n",
      "Processed image 13590\n",
      "Processed image 13591\n",
      "Processed image 13592\n",
      "Processed image 13593\n",
      "Processed image 13594\n",
      "Processed image 13595\n",
      "Processed image 13596\n",
      "Processed image 13597\n",
      "Processed image 13598\n",
      "Processed image 13599\n",
      "Processed image 13600\n",
      "Processed image 13601\n",
      "Processed image 13602\n",
      "Processed image 13603\n",
      "Processed image 13604\n",
      "Processed image 13605\n",
      "Processed image 13606\n",
      "Processed image 13607\n",
      "Processed image 13608\n",
      "Processed image 13609\n",
      "Processed image 13610\n",
      "Processed image 13611\n",
      "Processed image 13612\n",
      "Processed image 13613\n",
      "Processed image 13614\n",
      "Processed image 13615\n",
      "Processed image 13616\n",
      "Processed image 13617\n",
      "Processed image 13618\n",
      "Processed image 13619\n",
      "Processed image 13620\n",
      "Processed image 13621\n",
      "Processed image 13622\n",
      "Processed image 13623\n",
      "Processed image 13624\n",
      "Processed image 13625\n",
      "Processed image 13626\n",
      "Processed image 13627\n",
      "Processed image 13628\n",
      "Processed image 13629\n",
      "Processed image 13630\n",
      "Processed image 13631\n",
      "Processed image 13632\n",
      "Processed image 13633\n",
      "Processed image 13634\n",
      "Processed image 13635\n",
      "Processed image 13636\n",
      "Processed image 13637\n",
      "Processed image 13638\n",
      "Processed image 13639\n",
      "Processed image 13640\n",
      "Processed image 13641\n",
      "Processed image 13642\n",
      "Processed image 13643\n",
      "Processed image 13644\n",
      "Processed image 13645\n",
      "Processed image 13646\n",
      "Processed image 13647\n",
      "Processed image 13648\n",
      "Processed image 13649\n",
      "Processed image 13650\n",
      "Processed image 13651\n",
      "Processed image 13652\n",
      "Processed image 13653\n",
      "Processed image 13654\n",
      "Processed image 13655\n",
      "Processed image 13656\n",
      "Processed image 13657\n",
      "Processed image 13658\n",
      "Processed image 13659\n",
      "Processed image 13660\n",
      "Processed image 13661\n",
      "Processed image 13662\n",
      "Processed image 13663\n",
      "Processed image 13664\n",
      "Processed image 13665\n",
      "Processed image 13666\n",
      "Processed image 13667\n",
      "Processed image 13668\n",
      "Processed image 13669\n",
      "Processed image 13670\n",
      "Processed image 13671\n",
      "Processed image 13672\n",
      "Processed image 13673\n",
      "Processed image 13674\n",
      "Processed image 13675\n",
      "Processed image 13676\n",
      "Processed image 13677\n",
      "Processed image 13678\n",
      "Processed image 13679\n",
      "Processed image 13680\n",
      "Processed image 13681\n",
      "Processed image 13682\n",
      "Processed image 13683\n",
      "Processed image 13684\n",
      "Processed image 13685\n",
      "Processed image 13686\n",
      "Processed image 13687\n",
      "Processed image 13688\n",
      "Processed image 13689\n",
      "Processed image 13690\n",
      "Processed image 13691\n",
      "Processed image 13692\n",
      "Processed image 13693\n",
      "Processed image 13694\n",
      "Processed image 13695\n",
      "Processed image 13696\n",
      "Processed image 13697\n",
      "Processed image 13698\n",
      "Processed image 13699\n",
      "Processed image 13700\n",
      "Processed image 13701\n",
      "Processed image 13702\n",
      "Processed image 13703\n",
      "Processed image 13704\n",
      "Processed image 13705\n",
      "Processed image 13706\n",
      "Processed image 13707\n",
      "Processed image 13708\n",
      "Processed image 13709\n",
      "Processed image 13710\n",
      "Processed image 13711\n",
      "Processed image 13712\n",
      "Processed image 13713\n",
      "Processed image 13714\n",
      "Processed image 13715\n",
      "Processed image 13716\n",
      "Processed image 13717\n",
      "Processed image 13718\n",
      "Processed image 13719\n",
      "Processed image 13720\n",
      "Processed image 13721\n",
      "Processed image 13722\n",
      "Processed image 13723\n",
      "Processed image 13724\n",
      "Processed image 13725\n",
      "Processed image 13726\n",
      "Processed image 13727\n",
      "Processed image 13728\n",
      "Processed image 13729\n",
      "Processed image 13730\n",
      "Processed image 13731\n",
      "Processed image 13732\n",
      "Processed image 13733\n",
      "Processed image 13734\n",
      "Processed image 13735\n",
      "Processed image 13736\n",
      "Processed image 13737\n",
      "Processed image 13738\n",
      "Processed image 13739\n",
      "Processed image 13740\n",
      "Processed image 13741\n",
      "Processed image 13742\n",
      "Processed image 13743\n",
      "Processed image 13744\n",
      "Processed image 13745\n",
      "Processed image 13746\n",
      "Processed image 13747\n",
      "Processed image 13748\n",
      "Processed image 13749\n",
      "Processed image 13750\n",
      "Processed image 13751\n",
      "Processed image 13752\n",
      "Processed image 13753\n",
      "Processed image 13754\n",
      "Processed image 13755\n",
      "Processed image 13756\n",
      "Processed image 13757\n",
      "Processed image 13758\n",
      "Processed image 13759\n",
      "Processed image 13760\n",
      "Processed image 13761\n",
      "Processed image 13762\n",
      "Processed image 13763\n",
      "Processed image 13764\n",
      "Processed image 13765\n",
      "Processed image 13766\n",
      "Processed image 13767\n",
      "Processed image 13768\n",
      "Processed image 13769\n",
      "Processed image 13770\n",
      "Processed image 13771\n",
      "Processed image 13772\n",
      "Processed image 13773\n",
      "Processed image 13774\n",
      "Processed image 13775\n",
      "Processed image 13776\n",
      "Processed image 13777\n",
      "Processed image 13778\n",
      "Processed image 13779\n",
      "Processed image 13780\n",
      "Processed image 13781\n",
      "Processed image 13782\n",
      "Processed image 13783\n",
      "Processed image 13784\n",
      "Processed image 13785\n",
      "Processed image 13786\n",
      "Processed image 13787\n",
      "Processed image 13788\n",
      "Processed image 13789\n",
      "Processed image 13790\n",
      "Processed image 13791\n",
      "Processed image 13792\n",
      "Processed image 13793\n",
      "Processed image 13794\n",
      "Processed image 13795\n",
      "Processed image 13796\n",
      "Processed image 13797\n",
      "Processed image 13798\n",
      "Processed image 13799\n",
      "Processed image 13800\n",
      "Processed image 13801\n",
      "Processed image 13802\n",
      "Processed image 13803\n",
      "Processed image 13804\n",
      "Processed image 13805\n",
      "Processed image 13806\n",
      "Processed image 13807\n",
      "Processed image 13808\n",
      "Processed image 13809\n",
      "Processed image 13810\n",
      "Processed image 13811\n",
      "Processed image 13812\n",
      "Processed image 13813\n",
      "Processed image 13814\n",
      "Processed image 13815\n",
      "Processed image 13816\n",
      "Processed image 13817\n",
      "Processed image 13818\n",
      "Processed image 13819\n",
      "Processed image 13820\n",
      "Processed image 13821\n",
      "Processed image 13822\n",
      "Processed image 13823\n",
      "Processed image 13824\n",
      "Processed image 13825\n",
      "Processed image 13826\n",
      "Processed image 13827\n",
      "Processed image 13828\n",
      "Processed image 13829\n",
      "Processed image 13830\n",
      "Processed image 13831\n",
      "Processed image 13832\n",
      "Processed image 13833\n",
      "Processed image 13834\n",
      "Processed image 13835\n",
      "Processed image 13836\n",
      "Processed image 13837\n",
      "Processed image 13838\n",
      "Processed image 13839\n",
      "Processed image 13840\n",
      "Processed image 13841\n",
      "Processed image 13842\n",
      "Processed image 13843\n",
      "Processed image 13844\n",
      "Processed image 13845\n",
      "Processed image 13846\n",
      "Processed image 13847\n",
      "Processed image 13848\n",
      "Processed image 13849\n",
      "Processed image 13850\n",
      "Processed image 13851\n",
      "Processed image 13852\n",
      "Processed image 13853\n",
      "Processed image 13854\n",
      "Processed image 13855\n",
      "Processed image 13856\n",
      "Processed image 13857\n",
      "Processed image 13858\n",
      "Processed image 13859\n",
      "Processed image 13860\n",
      "Processed image 13861\n",
      "Processed image 13862\n",
      "Processed image 13863\n",
      "Processed image 13864\n",
      "Processed image 13865\n",
      "Processed image 13866\n",
      "Processed image 13867\n",
      "Processed image 13868\n",
      "Processed image 13869\n",
      "Processed image 13870\n",
      "Processed image 13871\n",
      "Processed image 13872\n",
      "Processed image 13873\n",
      "Processed image 13874\n",
      "Processed image 13875\n",
      "Processed image 13876\n",
      "Processed image 13877\n",
      "Processed image 13878\n",
      "Processed image 13879\n",
      "Processed image 13880\n",
      "Processed image 13881\n",
      "Processed image 13882\n",
      "Processed image 13883\n",
      "Processed image 13884\n",
      "Processed image 13885\n",
      "Processed image 13886\n",
      "Processed image 13887\n",
      "Processed image 13888\n",
      "Processed image 13889\n",
      "Processed image 13890\n",
      "Processed image 13891\n",
      "Processed image 13892\n",
      "Processed image 13893\n",
      "Processed image 13894\n",
      "Processed image 13895\n",
      "Processed image 13896\n",
      "Processed image 13897\n",
      "Processed image 13898\n",
      "Processed image 13899\n",
      "Processed image 13900\n",
      "Processed image 13901\n",
      "Processed image 13902\n",
      "Processed image 13903\n",
      "Processed image 13904\n",
      "Processed image 13905\n",
      "Processed image 13906\n",
      "Processed image 13907\n",
      "Processed image 13908\n",
      "Processed image 13909\n",
      "Processed image 13910\n",
      "Processed image 13911\n",
      "Processed image 13912\n",
      "Processed image 13913\n",
      "Processed image 13914\n",
      "Processed image 13915\n",
      "Processed image 13916\n",
      "Processed image 13917\n",
      "Processed image 13918\n",
      "Processed image 13919\n",
      "Processed image 13920\n",
      "Processed image 13921\n",
      "Processed image 13922\n",
      "Processed image 13923\n",
      "Processed image 13924\n",
      "Processed image 13925\n",
      "Processed image 13926\n",
      "Processed image 13927\n",
      "Processed image 13928\n",
      "Processed image 13929\n",
      "Processed image 13930\n",
      "Processed image 13931\n",
      "Processed image 13932\n",
      "Processed image 13933\n",
      "Processed image 13934\n",
      "Processed image 13935\n",
      "Processed image 13936\n",
      "Processed image 13937\n",
      "Processed image 13938\n",
      "Processed image 13939\n",
      "Processed image 13940\n",
      "Processed image 13941\n",
      "Processed image 13942\n",
      "Processed image 13943\n",
      "Processed image 13944\n",
      "Processed image 13945\n",
      "Processed image 13946\n",
      "Processed image 13947\n",
      "Processed image 13948\n",
      "Processed image 13949\n",
      "Processed image 13950\n",
      "Processed image 13951\n",
      "Processed image 13952\n",
      "Processed image 13953\n",
      "Processed image 13954\n",
      "Processed image 13955\n",
      "Processed image 13956\n",
      "Processed image 13957\n",
      "Processed image 13958\n",
      "Processed image 13959\n",
      "Processed image 13960\n",
      "Processed image 13961\n",
      "Processed image 13962\n",
      "Processed image 13963\n",
      "Processed image 13964\n",
      "Processed image 13965\n",
      "Processed image 13966\n",
      "Processed image 13967\n",
      "Processed image 13968\n",
      "Processed image 13969\n",
      "Processed image 13970\n",
      "Processed image 13971\n",
      "Processed image 13972\n",
      "Processed image 13973\n",
      "Processed image 13974\n",
      "Processed image 13975\n",
      "Processed image 13976\n",
      "Processed image 13977\n",
      "Processed image 13978\n",
      "Processed image 13979\n",
      "Processed image 13980\n",
      "Processed image 13981\n",
      "Processed image 13982\n",
      "Processed image 13983\n",
      "Processed image 13984\n",
      "Processed image 13985\n",
      "Processed image 13986\n",
      "Processed image 13987\n",
      "Processed image 13988\n",
      "Processed image 13989\n",
      "Processed image 13990\n",
      "Processed image 13991\n",
      "Processed image 13992\n",
      "Processed image 13993\n"
     ]
    }
   ],
   "source": [
    "test_image_list=[]\n",
    "banned=[]\n",
    "for i in range(13493,13493+len(df_test)):\n",
    "    s = 'sports/'+df_test['filepaths'][i]\n",
    "    if('.jpg' not in s)and('.jpeg' not in s):\n",
    "        banned.append(i)\n",
    "        continue\n",
    "    image = Image.open(s).convert('RGB')\n",
    "    image_tensor=transform(image)\n",
    "    test_image_list.append(image_tensor)\n",
    "    print(f'Processed image {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SportsDataset(test_image_list, df_test)\n",
    "# print(len(test_image_list)==len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('air hockey', 'ampute football', 'archery', 'arm wrestling', 'axe throwing', 'balance beam', 'barell racing', 'baseball', 'basketball', 'baton twirling', 'bike polo', 'billiards', 'bmx', 'bobsled', 'bowling', 'boxing', 'bull riding', 'bungee jumping', 'canoe slamon', 'cheerleading', 'chuckwagon racing', 'cricket', 'croquet', 'curling', 'disc golf', 'fencing', 'field hockey', 'figure skating men', 'figure skating pairs', 'figure skating women', 'fly fishing', 'football', 'formula 1 racing', 'frisbee', 'gaga', 'giant slalom', 'golf', 'hammer throw', 'hang gliding', 'harness racing', 'high jump', 'hockey', 'horse jumping', 'horse racing', 'horseshoe pitching', 'hurdles', 'hydroplane racing', 'ice climbing', 'ice yachting', 'jai alai', 'javelin', 'jousting', 'judo', 'lacrosse', 'log rolling', 'luge', 'motorcycle racing', 'mushing', 'nascar racing', 'olympic wrestling', 'parallel bar', 'pole climbing', 'pole dancing', 'pole vault', 'polo', 'pommel horse', 'rings', 'rock climbing', 'roller derby', 'rollerblade racing', 'rowing', 'rugby', 'sailboat racing', 'shot put', 'shuffleboard', 'sidecar racing', 'ski jumping', 'sky surfing', 'skydiving', 'snow boarding', 'snowmobile racing', 'speed skating', 'steer wrestling', 'sumo wrestling', 'surfing', 'swimming', 'table tennis', 'tennis', 'track bicycle', 'trapeze', 'tug of war', 'ultimate', 'uneven bars', 'volleyball', 'water cycling', 'water polo', 'weightlifting', 'wheelchair basketball', 'wheelchair racing', 'wingsuit flying')\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "classes = tuple(df['labels'].unique())\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-0.3255, -0.2784, -0.1608,  ..., -0.3333, -0.2627, -0.2471],\n",
      "          [-0.0353,  0.0196,  0.0745,  ..., -0.3255, -0.2627, -0.2471],\n",
      "          [ 0.1686,  0.2000,  0.2078,  ..., -0.3569, -0.3020, -0.3020],\n",
      "          ...,\n",
      "          [ 0.0196,  0.0588,  0.0431,  ..., -0.1059, -0.1059,  0.0510],\n",
      "          [ 0.0431,  0.0588,  0.0431,  ..., -0.0039, -0.0745, -0.0824],\n",
      "          [ 0.0745,  0.0902,  0.0980,  ...,  0.0431, -0.0275, -0.0667]],\n",
      "\n",
      "         [[-0.4039, -0.3490, -0.2314,  ..., -0.3647, -0.3098, -0.2941],\n",
      "          [-0.1137, -0.0588,  0.0039,  ..., -0.3569, -0.3020, -0.2863],\n",
      "          [ 0.0667,  0.1216,  0.1373,  ..., -0.3647, -0.3255, -0.3255],\n",
      "          ...,\n",
      "          [ 0.2941,  0.3333,  0.3098,  ...,  0.1765,  0.1765,  0.3333],\n",
      "          [ 0.3098,  0.3255,  0.3098,  ...,  0.2627,  0.1922,  0.1843],\n",
      "          [ 0.3412,  0.3569,  0.3647,  ...,  0.3098,  0.2392,  0.2000]],\n",
      "\n",
      "         [[-0.4745, -0.4196, -0.3020,  ..., -0.3882, -0.3098, -0.2941],\n",
      "          [-0.1843, -0.1294, -0.0667,  ..., -0.3804, -0.3255, -0.3098],\n",
      "          [ 0.0039,  0.0510,  0.0667,  ..., -0.4039, -0.3647, -0.3647],\n",
      "          ...,\n",
      "          [-0.3333, -0.2941, -0.2941,  ..., -0.5294, -0.5294, -0.3725],\n",
      "          [-0.2941, -0.2784, -0.2941,  ..., -0.4196, -0.4902, -0.4980],\n",
      "          [-0.2627, -0.2471, -0.2314,  ..., -0.3647, -0.4431, -0.4824]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0745,  0.1216,  0.2471,  ...,  0.5373,  0.4980,  0.4980],\n",
      "          [ 0.1686,  0.2784,  0.3725,  ...,  0.5451,  0.5373,  0.5373],\n",
      "          [ 0.3255,  0.3569,  0.3804,  ...,  0.5686,  0.5608,  0.5529],\n",
      "          ...,\n",
      "          [ 0.6706,  0.6157,  0.6549,  ...,  0.7098,  0.7333,  0.7333],\n",
      "          [ 0.6706,  0.6706,  0.6784,  ...,  0.6941,  0.7255,  0.7725],\n",
      "          [ 0.5765,  0.6863,  0.5608,  ...,  0.7333,  0.7098,  0.7569]],\n",
      "\n",
      "         [[ 0.2863,  0.2941,  0.3490,  ...,  0.5765,  0.5608,  0.5608],\n",
      "          [ 0.3412,  0.4196,  0.4510,  ...,  0.5843,  0.5765,  0.5765],\n",
      "          [ 0.4588,  0.4745,  0.4353,  ...,  0.5765,  0.5686,  0.5765],\n",
      "          ...,\n",
      "          [ 0.6627,  0.6078,  0.6471,  ...,  0.7020,  0.7255,  0.7255],\n",
      "          [ 0.6392,  0.6392,  0.6706,  ...,  0.6863,  0.7176,  0.7725],\n",
      "          [ 0.5451,  0.6549,  0.5529,  ...,  0.7255,  0.7098,  0.7569]],\n",
      "\n",
      "         [[ 0.3725,  0.3804,  0.4118,  ...,  0.6078,  0.5843,  0.5843],\n",
      "          [ 0.4275,  0.4980,  0.5216,  ...,  0.6157,  0.6078,  0.6078],\n",
      "          [ 0.5216,  0.5294,  0.4824,  ...,  0.6078,  0.6078,  0.6157],\n",
      "          ...,\n",
      "          [ 0.6314,  0.5765,  0.6157,  ...,  0.6627,  0.6863,  0.6863],\n",
      "          [ 0.6314,  0.6314,  0.6549,  ...,  0.6471,  0.6784,  0.7098],\n",
      "          [ 0.5373,  0.6471,  0.5373,  ...,  0.6863,  0.6471,  0.6941]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0431,  0.0431,  0.0431,  ..., -0.6000, -0.6471, -0.6784],\n",
      "          [ 0.0667,  0.0588,  0.0588,  ..., -0.6392, -0.6627, -0.6706],\n",
      "          [ 0.0902,  0.0902,  0.0902,  ..., -0.6549, -0.6706, -0.7020],\n",
      "          ...,\n",
      "          [-0.7961, -0.7961, -0.7804,  ..., -0.7412, -0.7020, -0.7020],\n",
      "          [-0.7882, -0.7882, -0.7725,  ..., -0.7412, -0.7020, -0.7020],\n",
      "          [-0.7882, -0.7882, -0.7569,  ..., -0.7412, -0.7020, -0.7020]],\n",
      "\n",
      "         [[-0.2314, -0.2314, -0.2314,  ..., -0.9843, -1.0000, -1.0000],\n",
      "          [-0.2078, -0.2157, -0.2157,  ..., -1.0000, -0.9922, -0.9843],\n",
      "          [-0.1843, -0.1843, -0.1843,  ..., -1.0000, -0.9922, -1.0000],\n",
      "          ...,\n",
      "          [-0.9843, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.3804, -0.3804, -0.3804,  ..., -0.9608, -0.9765, -0.9922],\n",
      "          [-0.3569, -0.3647, -0.3647,  ..., -0.9843, -0.9765, -0.9765],\n",
      "          [-0.3333, -0.3333, -0.3333,  ..., -0.9843, -0.9608, -0.9843],\n",
      "          ...,\n",
      "          [-0.9843, -0.9922, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9843,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9765,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0824, -0.1137, -0.1137,  ...,  0.6314,  0.6627,  0.6314],\n",
      "          [-0.1137, -0.1765, -0.1529,  ...,  0.6471,  0.6157,  0.6235],\n",
      "          [-0.0039, -0.1216, -0.0902,  ...,  0.6314,  0.5765,  0.6314],\n",
      "          ...,\n",
      "          [ 0.0353,  0.0118, -0.0196,  ...,  0.1137, -0.0275,  0.4353],\n",
      "          [-0.0196,  0.0196,  0.0902,  ...,  0.4510,  0.4902,  0.5373],\n",
      "          [-0.0510,  0.0118, -0.0039,  ...,  0.4275,  0.3961,  0.5059]],\n",
      "\n",
      "         [[ 0.1373,  0.1059,  0.0667,  ...,  0.6941,  0.7412,  0.7098],\n",
      "          [ 0.1059,  0.0275,  0.0275,  ...,  0.7098,  0.6941,  0.7020],\n",
      "          [ 0.2000,  0.0824,  0.0902,  ...,  0.7020,  0.6471,  0.7020],\n",
      "          ...,\n",
      "          [ 0.2157,  0.1922,  0.1608,  ...,  0.2078,  0.0824,  0.5451],\n",
      "          [ 0.1608,  0.2000,  0.2706,  ...,  0.5608,  0.6000,  0.6471],\n",
      "          [ 0.1294,  0.1922,  0.1765,  ...,  0.5373,  0.5059,  0.6157]],\n",
      "\n",
      "         [[ 0.2471,  0.2157,  0.1765,  ...,  0.7961,  0.8353,  0.8039],\n",
      "          [ 0.2157,  0.1451,  0.1373,  ...,  0.8118,  0.7882,  0.7961],\n",
      "          [ 0.3176,  0.2000,  0.2000,  ...,  0.8196,  0.7647,  0.8196],\n",
      "          ...,\n",
      "          [ 0.3255,  0.3020,  0.2863,  ...,  0.3176,  0.1843,  0.6471],\n",
      "          [ 0.2706,  0.3098,  0.3804,  ...,  0.6627,  0.7020,  0.7490],\n",
      "          [ 0.2392,  0.3020,  0.2863,  ...,  0.6392,  0.6078,  0.7176]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8431,  0.8275,  0.8275,  ...,  0.2000,  0.3020,  0.3569],\n",
      "          [ 0.8431,  0.8275,  0.8275,  ...,  0.2863,  0.3176,  0.4353],\n",
      "          [ 0.8196,  0.8118,  0.8118,  ...,  0.2078,  0.2471,  0.3647],\n",
      "          ...,\n",
      "          [-0.5843, -0.5765, -0.5843,  ..., -0.5451, -0.5686, -0.5765],\n",
      "          [-0.6000, -0.6000, -0.6000,  ..., -0.5608, -0.5529, -0.5451],\n",
      "          [-0.6078, -0.6000, -0.6000,  ..., -0.5922, -0.5843, -0.5686]],\n",
      "\n",
      "         [[ 0.9059,  0.8902,  0.8980,  ...,  0.2392,  0.3333,  0.3882],\n",
      "          [ 0.9059,  0.8902,  0.8980,  ...,  0.3255,  0.3490,  0.4667],\n",
      "          [ 0.8824,  0.8745,  0.8745,  ...,  0.2392,  0.2784,  0.4039],\n",
      "          ...,\n",
      "          [ 0.2392,  0.2471,  0.2471,  ...,  0.1765,  0.1686,  0.1765],\n",
      "          [ 0.2235,  0.2235,  0.2235,  ...,  0.1765,  0.1843,  0.1922],\n",
      "          [ 0.2157,  0.2235,  0.2235,  ...,  0.1608,  0.1529,  0.1686]],\n",
      "\n",
      "         [[ 0.9922,  0.9765,  0.9686,  ...,  0.2627,  0.3412,  0.3804],\n",
      "          [ 0.9922,  0.9765,  0.9686,  ...,  0.3490,  0.3569,  0.4588],\n",
      "          [ 0.9686,  0.9608,  0.9608,  ...,  0.2627,  0.2706,  0.3725],\n",
      "          ...,\n",
      "          [-0.0745, -0.0667, -0.0667,  ..., -0.1137, -0.1216, -0.1216],\n",
      "          [-0.0902, -0.0902, -0.0902,  ..., -0.1137, -0.1059, -0.0980],\n",
      "          [-0.0980, -0.0902, -0.0902,  ..., -0.1373, -0.1373, -0.1216]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3647,  0.3412,  0.1294,  ...,  0.0588,  0.0745,  0.1451],\n",
      "          [ 0.4196,  0.1765,  0.0196,  ...,  0.0588,  0.0039,  0.0196],\n",
      "          [ 0.4745,  0.2549, -0.0118,  ...,  0.0588,  0.0039,  0.0275],\n",
      "          ...,\n",
      "          [ 1.0000,  0.9922,  0.9922,  ...,  1.0000,  0.9922,  0.9843],\n",
      "          [ 0.9922,  0.9608,  0.9922,  ...,  0.9922,  0.9686,  1.0000],\n",
      "          [ 0.9686,  0.9765,  0.9686,  ...,  1.0000,  0.9922,  0.9922]],\n",
      "\n",
      "         [[ 0.2157,  0.2000,  0.0196,  ..., -0.1608, -0.1765, -0.1294],\n",
      "          [ 0.2471,  0.0353, -0.0902,  ..., -0.1451, -0.2549, -0.2314],\n",
      "          [ 0.2941,  0.1059, -0.1216,  ..., -0.1373, -0.2392, -0.2157],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9922,  0.9843],\n",
      "          [ 1.0000,  0.9765,  1.0000,  ...,  0.9922,  0.9686,  1.0000],\n",
      "          [ 1.0000,  1.0000,  0.9765,  ...,  1.0000,  0.9922,  0.9922]],\n",
      "\n",
      "         [[ 0.1059,  0.1059, -0.0510,  ..., -0.3333, -0.3412, -0.2863],\n",
      "          [ 0.1451, -0.0588, -0.1608,  ..., -0.3255, -0.4039, -0.3961],\n",
      "          [ 0.1843, -0.0039, -0.2078,  ..., -0.3098, -0.3804, -0.3725],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  0.9843,  ...,  1.0000,  0.9922,  0.9843],\n",
      "          [ 0.9686,  0.9373,  0.9608,  ...,  0.9922,  0.9686,  1.0000],\n",
      "          [ 0.9529,  0.9608,  0.9137,  ...,  1.0000,  0.9922,  0.9922]]]]), tensor([60, 35, 84, 88, 84, 98, 88,  5, 89,  5, 16, 69, 58, 66,  0, 40, 93, 68,\n",
      "        43, 83,  6, 58,  2,  8, 21, 43, 69, 38, 75, 54, 12, 12])]\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "data = dataiter.__next__()\n",
    "images, labels = data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.fc1= nn.Linear(128 * (224//8)*(224//8), 512)\n",
    "        self.fc2 = nn.Linear(512, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128*(224//8)*(224//8))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 32, 224, 224])\n",
      "torch.Size([32, 32, 112, 112])\n",
      "torch.Size([32, 64, 112, 112])\n",
      "torch.Size([32, 64, 56, 56])\n",
      "torch.Size([32, 128, 56, 56])\n",
      "torch.Size([32, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(images.size())\n",
    "op1 = model.conv1(images)\n",
    "print(op1.size())\n",
    "op2 = model.pool(op1)\n",
    "print(op2.size())\n",
    "op3 = model.conv2(op2)\n",
    "print(op3.size())\n",
    "op4 = model.pool(op3)\n",
    "print(op4.size())\n",
    "op5 = model.conv3(op4)\n",
    "print(op5.size())\n",
    "op6 = model.pool(op5)\n",
    "print(op6.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_dataloader)\n",
    "print(n_total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | step: 1/422 | loss: 4.606741905212402\n",
      "Epoch: 1/100 | step: 2/422 | loss: 4.59805965423584\n",
      "Epoch: 1/100 | step: 3/422 | loss: 4.591215133666992\n",
      "Epoch: 1/100 | step: 4/422 | loss: 4.599302291870117\n",
      "Epoch: 1/100 | step: 5/422 | loss: 4.600444316864014\n",
      "Epoch: 1/100 | step: 6/422 | loss: 4.600710868835449\n",
      "Epoch: 1/100 | step: 7/422 | loss: 4.590400695800781\n",
      "Epoch: 1/100 | step: 8/422 | loss: 4.5993733406066895\n",
      "Epoch: 1/100 | step: 9/422 | loss: 4.606795310974121\n",
      "Epoch: 1/100 | step: 10/422 | loss: 4.592353820800781\n",
      "Epoch: 1/100 | step: 11/422 | loss: 4.599428176879883\n",
      "Epoch: 1/100 | step: 12/422 | loss: 4.606945991516113\n",
      "Epoch: 1/100 | step: 13/422 | loss: 4.598843097686768\n",
      "Epoch: 1/100 | step: 14/422 | loss: 4.599743366241455\n",
      "Epoch: 1/100 | step: 15/422 | loss: 4.597687721252441\n",
      "Epoch: 1/100 | step: 16/422 | loss: 4.594789505004883\n",
      "Epoch: 1/100 | step: 17/422 | loss: 4.603553771972656\n",
      "Epoch: 1/100 | step: 18/422 | loss: 4.610654354095459\n",
      "Epoch: 1/100 | step: 19/422 | loss: 4.6078314781188965\n",
      "Epoch: 1/100 | step: 20/422 | loss: 4.603280067443848\n",
      "Epoch: 1/100 | step: 21/422 | loss: 4.599483966827393\n",
      "Epoch: 1/100 | step: 22/422 | loss: 4.60881233215332\n",
      "Epoch: 1/100 | step: 23/422 | loss: 4.5957136154174805\n",
      "Epoch: 1/100 | step: 24/422 | loss: 4.597253322601318\n",
      "Epoch: 1/100 | step: 25/422 | loss: 4.605485439300537\n",
      "Epoch: 1/100 | step: 26/422 | loss: 4.593962669372559\n",
      "Epoch: 1/100 | step: 27/422 | loss: 4.6074628829956055\n",
      "Epoch: 1/100 | step: 28/422 | loss: 4.602239608764648\n",
      "Epoch: 1/100 | step: 29/422 | loss: 4.599496841430664\n",
      "Epoch: 1/100 | step: 30/422 | loss: 4.587596416473389\n",
      "Epoch: 1/100 | step: 31/422 | loss: 4.599037170410156\n",
      "Epoch: 1/100 | step: 32/422 | loss: 4.60469388961792\n",
      "Epoch: 1/100 | step: 33/422 | loss: 4.596573829650879\n",
      "Epoch: 1/100 | step: 34/422 | loss: 4.6048502922058105\n",
      "Epoch: 1/100 | step: 35/422 | loss: 4.595824718475342\n",
      "Epoch: 1/100 | step: 36/422 | loss: 4.595444202423096\n",
      "Epoch: 1/100 | step: 37/422 | loss: 4.6015214920043945\n",
      "Epoch: 1/100 | step: 38/422 | loss: 4.611470699310303\n",
      "Epoch: 1/100 | step: 39/422 | loss: 4.591317176818848\n",
      "Epoch: 1/100 | step: 40/422 | loss: 4.604635715484619\n",
      "Epoch: 1/100 | step: 41/422 | loss: 4.60511589050293\n",
      "Epoch: 1/100 | step: 42/422 | loss: 4.602973461151123\n",
      "Epoch: 1/100 | step: 43/422 | loss: 4.6053338050842285\n",
      "Epoch: 1/100 | step: 44/422 | loss: 4.601318836212158\n",
      "Epoch: 1/100 | step: 45/422 | loss: 4.5915846824646\n",
      "Epoch: 1/100 | step: 46/422 | loss: 4.602412223815918\n",
      "Epoch: 1/100 | step: 47/422 | loss: 4.59843635559082\n",
      "Epoch: 1/100 | step: 48/422 | loss: 4.590540885925293\n",
      "Epoch: 1/100 | step: 49/422 | loss: 4.596522808074951\n",
      "Epoch: 1/100 | step: 50/422 | loss: 4.592179775238037\n",
      "Epoch: 1/100 | step: 51/422 | loss: 4.603140354156494\n",
      "Epoch: 1/100 | step: 52/422 | loss: 4.597239017486572\n",
      "Epoch: 1/100 | step: 53/422 | loss: 4.600218772888184\n",
      "Epoch: 1/100 | step: 54/422 | loss: 4.594764709472656\n",
      "Epoch: 1/100 | step: 55/422 | loss: 4.593313694000244\n",
      "Epoch: 1/100 | step: 56/422 | loss: 4.5959038734436035\n",
      "Epoch: 1/100 | step: 57/422 | loss: 4.5959038734436035\n",
      "Epoch: 1/100 | step: 58/422 | loss: 4.606564044952393\n",
      "Epoch: 1/100 | step: 59/422 | loss: 4.5990471839904785\n",
      "Epoch: 1/100 | step: 60/422 | loss: 4.599526882171631\n",
      "Epoch: 1/100 | step: 61/422 | loss: 4.5915093421936035\n",
      "Epoch: 1/100 | step: 62/422 | loss: 4.5917439460754395\n",
      "Epoch: 1/100 | step: 63/422 | loss: 4.610547065734863\n",
      "Epoch: 1/100 | step: 64/422 | loss: 4.599693775177002\n",
      "Epoch: 1/100 | step: 65/422 | loss: 4.594955921173096\n",
      "Epoch: 1/100 | step: 66/422 | loss: 4.600793838500977\n",
      "Epoch: 1/100 | step: 67/422 | loss: 4.605430603027344\n",
      "Epoch: 1/100 | step: 68/422 | loss: 4.59368371963501\n",
      "Epoch: 1/100 | step: 69/422 | loss: 4.60957145690918\n",
      "Epoch: 1/100 | step: 70/422 | loss: 4.599157810211182\n",
      "Epoch: 1/100 | step: 71/422 | loss: 4.59939432144165\n",
      "Epoch: 1/100 | step: 72/422 | loss: 4.602531433105469\n",
      "Epoch: 1/100 | step: 73/422 | loss: 4.5975260734558105\n",
      "Epoch: 1/100 | step: 74/422 | loss: 4.598121643066406\n",
      "Epoch: 1/100 | step: 75/422 | loss: 4.597045421600342\n",
      "Epoch: 1/100 | step: 76/422 | loss: 4.60149621963501\n",
      "Epoch: 1/100 | step: 77/422 | loss: 4.598738193511963\n",
      "Epoch: 1/100 | step: 78/422 | loss: 4.599049091339111\n",
      "Epoch: 1/100 | step: 79/422 | loss: 4.591996192932129\n",
      "Epoch: 1/100 | step: 80/422 | loss: 4.600696563720703\n",
      "Epoch: 1/100 | step: 81/422 | loss: 4.5949320793151855\n",
      "Epoch: 1/100 | step: 82/422 | loss: 4.58845853805542\n",
      "Epoch: 1/100 | step: 83/422 | loss: 4.603933334350586\n",
      "Epoch: 1/100 | step: 84/422 | loss: 4.6067681312561035\n",
      "Epoch: 1/100 | step: 85/422 | loss: 4.605945110321045\n",
      "Epoch: 1/100 | step: 86/422 | loss: 4.589022159576416\n",
      "Epoch: 1/100 | step: 87/422 | loss: 4.599172115325928\n",
      "Epoch: 1/100 | step: 88/422 | loss: 4.606555938720703\n",
      "Epoch: 1/100 | step: 89/422 | loss: 4.591665744781494\n",
      "Epoch: 1/100 | step: 90/422 | loss: 4.592398643493652\n",
      "Epoch: 1/100 | step: 91/422 | loss: 4.596076011657715\n",
      "Epoch: 1/100 | step: 92/422 | loss: 4.602765083312988\n",
      "Epoch: 1/100 | step: 93/422 | loss: 4.5975871086120605\n",
      "Epoch: 1/100 | step: 94/422 | loss: 4.6020636558532715\n",
      "Epoch: 1/100 | step: 95/422 | loss: 4.597724437713623\n",
      "Epoch: 1/100 | step: 96/422 | loss: 4.594053268432617\n",
      "Epoch: 1/100 | step: 97/422 | loss: 4.587093830108643\n",
      "Epoch: 1/100 | step: 98/422 | loss: 4.614736080169678\n",
      "Epoch: 1/100 | step: 99/422 | loss: 4.59496545791626\n",
      "Epoch: 1/100 | step: 100/422 | loss: 4.595056533813477\n",
      "Epoch: 1/100 | step: 101/422 | loss: 4.6011152267456055\n",
      "Epoch: 1/100 | step: 102/422 | loss: 4.593209266662598\n",
      "Epoch: 1/100 | step: 103/422 | loss: 4.5941267013549805\n",
      "Epoch: 1/100 | step: 104/422 | loss: 4.591099739074707\n",
      "Epoch: 1/100 | step: 105/422 | loss: 4.593846321105957\n",
      "Epoch: 1/100 | step: 106/422 | loss: 4.5869140625\n",
      "Epoch: 1/100 | step: 107/422 | loss: 4.590975284576416\n",
      "Epoch: 1/100 | step: 108/422 | loss: 4.588225841522217\n",
      "Epoch: 1/100 | step: 109/422 | loss: 4.595516204833984\n",
      "Epoch: 1/100 | step: 110/422 | loss: 4.6059980392456055\n",
      "Epoch: 1/100 | step: 111/422 | loss: 4.588606834411621\n",
      "Epoch: 1/100 | step: 112/422 | loss: 4.593667984008789\n",
      "Epoch: 1/100 | step: 113/422 | loss: 4.597898960113525\n",
      "Epoch: 1/100 | step: 114/422 | loss: 4.607035160064697\n",
      "Epoch: 1/100 | step: 115/422 | loss: 4.598296165466309\n",
      "Epoch: 1/100 | step: 116/422 | loss: 4.592025279998779\n",
      "Epoch: 1/100 | step: 117/422 | loss: 4.600314140319824\n",
      "Epoch: 1/100 | step: 118/422 | loss: 4.591716766357422\n",
      "Epoch: 1/100 | step: 119/422 | loss: 4.590258598327637\n",
      "Epoch: 1/100 | step: 120/422 | loss: 4.587621212005615\n",
      "Epoch: 1/100 | step: 121/422 | loss: 4.598725318908691\n",
      "Epoch: 1/100 | step: 122/422 | loss: 4.596581935882568\n",
      "Epoch: 1/100 | step: 123/422 | loss: 4.595557689666748\n",
      "Epoch: 1/100 | step: 124/422 | loss: 4.605013847351074\n",
      "Epoch: 1/100 | step: 125/422 | loss: 4.594578742980957\n",
      "Epoch: 1/100 | step: 126/422 | loss: 4.599870681762695\n",
      "Epoch: 1/100 | step: 127/422 | loss: 4.581226348876953\n",
      "Epoch: 1/100 | step: 128/422 | loss: 4.606903076171875\n",
      "Epoch: 1/100 | step: 129/422 | loss: 4.603630065917969\n",
      "Epoch: 1/100 | step: 130/422 | loss: 4.603628635406494\n",
      "Epoch: 1/100 | step: 131/422 | loss: 4.587324142456055\n",
      "Epoch: 1/100 | step: 132/422 | loss: 4.584712028503418\n",
      "Epoch: 1/100 | step: 133/422 | loss: 4.599328517913818\n",
      "Epoch: 1/100 | step: 134/422 | loss: 4.590040683746338\n",
      "Epoch: 1/100 | step: 135/422 | loss: 4.596072196960449\n",
      "Epoch: 1/100 | step: 136/422 | loss: 4.606086254119873\n",
      "Epoch: 1/100 | step: 137/422 | loss: 4.589318752288818\n",
      "Epoch: 1/100 | step: 138/422 | loss: 4.594796180725098\n",
      "Epoch: 1/100 | step: 139/422 | loss: 4.597233295440674\n",
      "Epoch: 1/100 | step: 140/422 | loss: 4.604867458343506\n",
      "Epoch: 1/100 | step: 141/422 | loss: 4.589579105377197\n",
      "Epoch: 1/100 | step: 142/422 | loss: 4.606070518493652\n",
      "Epoch: 1/100 | step: 143/422 | loss: 4.591469764709473\n",
      "Epoch: 1/100 | step: 144/422 | loss: 4.593295097351074\n",
      "Epoch: 1/100 | step: 145/422 | loss: 4.5937323570251465\n",
      "Epoch: 1/100 | step: 146/422 | loss: 4.594099521636963\n",
      "Epoch: 1/100 | step: 147/422 | loss: 4.592330455780029\n",
      "Epoch: 1/100 | step: 148/422 | loss: 4.601307392120361\n",
      "Epoch: 1/100 | step: 149/422 | loss: 4.59773063659668\n",
      "Epoch: 1/100 | step: 150/422 | loss: 4.5998029708862305\n",
      "Epoch: 1/100 | step: 151/422 | loss: 4.596383571624756\n",
      "Epoch: 1/100 | step: 152/422 | loss: 4.597514629364014\n",
      "Epoch: 1/100 | step: 153/422 | loss: 4.603355884552002\n",
      "Epoch: 1/100 | step: 154/422 | loss: 4.598432540893555\n",
      "Epoch: 1/100 | step: 155/422 | loss: 4.5983805656433105\n",
      "Epoch: 1/100 | step: 156/422 | loss: 4.604511737823486\n",
      "Epoch: 1/100 | step: 157/422 | loss: 4.594789981842041\n",
      "Epoch: 1/100 | step: 158/422 | loss: 4.594526767730713\n",
      "Epoch: 1/100 | step: 159/422 | loss: 4.601462364196777\n",
      "Epoch: 1/100 | step: 160/422 | loss: 4.602939128875732\n",
      "Epoch: 1/100 | step: 161/422 | loss: 4.586756229400635\n",
      "Epoch: 1/100 | step: 162/422 | loss: 4.601067543029785\n",
      "Epoch: 1/100 | step: 163/422 | loss: 4.584549427032471\n",
      "Epoch: 1/100 | step: 164/422 | loss: 4.594366550445557\n",
      "Epoch: 1/100 | step: 165/422 | loss: 4.596862316131592\n",
      "Epoch: 1/100 | step: 166/422 | loss: 4.59271240234375\n",
      "Epoch: 1/100 | step: 167/422 | loss: 4.596236705780029\n",
      "Epoch: 1/100 | step: 168/422 | loss: 4.597322463989258\n",
      "Epoch: 1/100 | step: 169/422 | loss: 4.594796657562256\n",
      "Epoch: 1/100 | step: 170/422 | loss: 4.608469486236572\n",
      "Epoch: 1/100 | step: 171/422 | loss: 4.6025071144104\n",
      "Epoch: 1/100 | step: 172/422 | loss: 4.602647304534912\n",
      "Epoch: 1/100 | step: 173/422 | loss: 4.587765216827393\n",
      "Epoch: 1/100 | step: 174/422 | loss: 4.605116367340088\n",
      "Epoch: 1/100 | step: 175/422 | loss: 4.5918731689453125\n",
      "Epoch: 1/100 | step: 176/422 | loss: 4.60022497177124\n",
      "Epoch: 1/100 | step: 177/422 | loss: 4.587116241455078\n",
      "Epoch: 1/100 | step: 178/422 | loss: 4.588825702667236\n",
      "Epoch: 1/100 | step: 179/422 | loss: 4.595566272735596\n",
      "Epoch: 1/100 | step: 180/422 | loss: 4.5916972160339355\n",
      "Epoch: 1/100 | step: 181/422 | loss: 4.592789649963379\n",
      "Epoch: 1/100 | step: 182/422 | loss: 4.594300746917725\n",
      "Epoch: 1/100 | step: 183/422 | loss: 4.598847389221191\n",
      "Epoch: 1/100 | step: 184/422 | loss: 4.6048479080200195\n",
      "Epoch: 1/100 | step: 185/422 | loss: 4.59349250793457\n",
      "Epoch: 1/100 | step: 186/422 | loss: 4.578340530395508\n",
      "Epoch: 1/100 | step: 187/422 | loss: 4.596371650695801\n",
      "Epoch: 1/100 | step: 188/422 | loss: 4.596895694732666\n",
      "Epoch: 1/100 | step: 189/422 | loss: 4.596776485443115\n",
      "Epoch: 1/100 | step: 190/422 | loss: 4.601486682891846\n",
      "Epoch: 1/100 | step: 191/422 | loss: 4.594001293182373\n",
      "Epoch: 1/100 | step: 192/422 | loss: 4.60414457321167\n",
      "Epoch: 1/100 | step: 193/422 | loss: 4.585532188415527\n",
      "Epoch: 1/100 | step: 194/422 | loss: 4.615023612976074\n",
      "Epoch: 1/100 | step: 195/422 | loss: 4.589149475097656\n",
      "Epoch: 1/100 | step: 196/422 | loss: 4.597689151763916\n",
      "Epoch: 1/100 | step: 197/422 | loss: 4.591335296630859\n",
      "Epoch: 1/100 | step: 198/422 | loss: 4.590145111083984\n",
      "Epoch: 1/100 | step: 199/422 | loss: 4.595008373260498\n",
      "Epoch: 1/100 | step: 200/422 | loss: 4.593261241912842\n",
      "Epoch: 1/100 | step: 201/422 | loss: 4.592586040496826\n",
      "Epoch: 1/100 | step: 202/422 | loss: 4.579638957977295\n",
      "Epoch: 1/100 | step: 203/422 | loss: 4.60304069519043\n",
      "Epoch: 1/100 | step: 204/422 | loss: 4.59528112411499\n",
      "Epoch: 1/100 | step: 205/422 | loss: 4.596214771270752\n",
      "Epoch: 1/100 | step: 206/422 | loss: 4.6035475730896\n",
      "Epoch: 1/100 | step: 207/422 | loss: 4.6077880859375\n",
      "Epoch: 1/100 | step: 208/422 | loss: 4.59231424331665\n",
      "Epoch: 1/100 | step: 209/422 | loss: 4.604275703430176\n",
      "Epoch: 1/100 | step: 210/422 | loss: 4.591434955596924\n",
      "Epoch: 1/100 | step: 211/422 | loss: 4.59671688079834\n",
      "Epoch: 1/100 | step: 212/422 | loss: 4.587725639343262\n",
      "Epoch: 1/100 | step: 213/422 | loss: 4.588957786560059\n",
      "Epoch: 1/100 | step: 214/422 | loss: 4.599987506866455\n",
      "Epoch: 1/100 | step: 215/422 | loss: 4.596002101898193\n",
      "Epoch: 1/100 | step: 216/422 | loss: 4.586213111877441\n",
      "Epoch: 1/100 | step: 217/422 | loss: 4.5856852531433105\n",
      "Epoch: 1/100 | step: 218/422 | loss: 4.586429595947266\n",
      "Epoch: 1/100 | step: 219/422 | loss: 4.601834297180176\n",
      "Epoch: 1/100 | step: 220/422 | loss: 4.605688571929932\n",
      "Epoch: 1/100 | step: 221/422 | loss: 4.599359512329102\n",
      "Epoch: 1/100 | step: 222/422 | loss: 4.5917253494262695\n",
      "Epoch: 1/100 | step: 223/422 | loss: 4.592433452606201\n",
      "Epoch: 1/100 | step: 224/422 | loss: 4.611849308013916\n",
      "Epoch: 1/100 | step: 225/422 | loss: 4.579346656799316\n",
      "Epoch: 1/100 | step: 226/422 | loss: 4.596297740936279\n",
      "Epoch: 1/100 | step: 227/422 | loss: 4.58164119720459\n",
      "Epoch: 1/100 | step: 228/422 | loss: 4.591396808624268\n",
      "Epoch: 1/100 | step: 229/422 | loss: 4.601741790771484\n",
      "Epoch: 1/100 | step: 230/422 | loss: 4.5893874168396\n",
      "Epoch: 1/100 | step: 231/422 | loss: 4.592299461364746\n",
      "Epoch: 1/100 | step: 232/422 | loss: 4.586681842803955\n",
      "Epoch: 1/100 | step: 233/422 | loss: 4.59503173828125\n",
      "Epoch: 1/100 | step: 234/422 | loss: 4.585257053375244\n",
      "Epoch: 1/100 | step: 235/422 | loss: 4.598419666290283\n",
      "Epoch: 1/100 | step: 236/422 | loss: 4.587550163269043\n",
      "Epoch: 1/100 | step: 237/422 | loss: 4.5932769775390625\n",
      "Epoch: 1/100 | step: 238/422 | loss: 4.586437225341797\n",
      "Epoch: 1/100 | step: 239/422 | loss: 4.582254886627197\n",
      "Epoch: 1/100 | step: 240/422 | loss: 4.605626583099365\n",
      "Epoch: 1/100 | step: 241/422 | loss: 4.601294040679932\n",
      "Epoch: 1/100 | step: 242/422 | loss: 4.595619201660156\n",
      "Epoch: 1/100 | step: 243/422 | loss: 4.5861945152282715\n",
      "Epoch: 1/100 | step: 244/422 | loss: 4.597631454467773\n",
      "Epoch: 1/100 | step: 245/422 | loss: 4.586931228637695\n",
      "Epoch: 1/100 | step: 246/422 | loss: 4.5970587730407715\n",
      "Epoch: 1/100 | step: 247/422 | loss: 4.602989196777344\n",
      "Epoch: 1/100 | step: 248/422 | loss: 4.582766056060791\n",
      "Epoch: 1/100 | step: 249/422 | loss: 4.593261241912842\n",
      "Epoch: 1/100 | step: 250/422 | loss: 4.585299968719482\n",
      "Epoch: 1/100 | step: 251/422 | loss: 4.59471321105957\n",
      "Epoch: 1/100 | step: 252/422 | loss: 4.58709716796875\n",
      "Epoch: 1/100 | step: 253/422 | loss: 4.587127685546875\n",
      "Epoch: 1/100 | step: 254/422 | loss: 4.598559856414795\n",
      "Epoch: 1/100 | step: 255/422 | loss: 4.597555637359619\n",
      "Epoch: 1/100 | step: 256/422 | loss: 4.599386692047119\n",
      "Epoch: 1/100 | step: 257/422 | loss: 4.589916229248047\n",
      "Epoch: 1/100 | step: 258/422 | loss: 4.59290075302124\n",
      "Epoch: 1/100 | step: 259/422 | loss: 4.592942714691162\n",
      "Epoch: 1/100 | step: 260/422 | loss: 4.586678981781006\n",
      "Epoch: 1/100 | step: 261/422 | loss: 4.602734088897705\n",
      "Epoch: 1/100 | step: 262/422 | loss: 4.589496612548828\n",
      "Epoch: 1/100 | step: 263/422 | loss: 4.5945000648498535\n",
      "Epoch: 1/100 | step: 264/422 | loss: 4.593214511871338\n",
      "Epoch: 1/100 | step: 265/422 | loss: 4.59406852722168\n",
      "Epoch: 1/100 | step: 266/422 | loss: 4.596989154815674\n",
      "Epoch: 1/100 | step: 267/422 | loss: 4.581769943237305\n",
      "Epoch: 1/100 | step: 268/422 | loss: 4.594314098358154\n",
      "Epoch: 1/100 | step: 269/422 | loss: 4.602087497711182\n",
      "Epoch: 1/100 | step: 270/422 | loss: 4.599940299987793\n",
      "Epoch: 1/100 | step: 271/422 | loss: 4.610469818115234\n",
      "Epoch: 1/100 | step: 272/422 | loss: 4.59020471572876\n",
      "Epoch: 1/100 | step: 273/422 | loss: 4.603610992431641\n",
      "Epoch: 1/100 | step: 274/422 | loss: 4.585177898406982\n",
      "Epoch: 1/100 | step: 275/422 | loss: 4.585241794586182\n",
      "Epoch: 1/100 | step: 276/422 | loss: 4.596637725830078\n",
      "Epoch: 1/100 | step: 277/422 | loss: 4.604825019836426\n",
      "Epoch: 1/100 | step: 278/422 | loss: 4.593832492828369\n",
      "Epoch: 1/100 | step: 279/422 | loss: 4.6002984046936035\n",
      "Epoch: 1/100 | step: 280/422 | loss: 4.594876766204834\n",
      "Epoch: 1/100 | step: 281/422 | loss: 4.596004962921143\n",
      "Epoch: 1/100 | step: 282/422 | loss: 4.597461700439453\n",
      "Epoch: 1/100 | step: 283/422 | loss: 4.590610980987549\n",
      "Epoch: 1/100 | step: 284/422 | loss: 4.596414089202881\n",
      "Epoch: 1/100 | step: 285/422 | loss: 4.593315601348877\n",
      "Epoch: 1/100 | step: 286/422 | loss: 4.605831623077393\n",
      "Epoch: 1/100 | step: 287/422 | loss: 4.602267742156982\n",
      "Epoch: 1/100 | step: 288/422 | loss: 4.587826728820801\n",
      "Epoch: 1/100 | step: 289/422 | loss: 4.585089206695557\n",
      "Epoch: 1/100 | step: 290/422 | loss: 4.592953681945801\n",
      "Epoch: 1/100 | step: 291/422 | loss: 4.585013389587402\n",
      "Epoch: 1/100 | step: 292/422 | loss: 4.600412845611572\n",
      "Epoch: 1/100 | step: 293/422 | loss: 4.583968639373779\n",
      "Epoch: 1/100 | step: 294/422 | loss: 4.583260536193848\n",
      "Epoch: 1/100 | step: 295/422 | loss: 4.589082717895508\n",
      "Epoch: 1/100 | step: 296/422 | loss: 4.587251663208008\n",
      "Epoch: 1/100 | step: 297/422 | loss: 4.583113670349121\n",
      "Epoch: 1/100 | step: 298/422 | loss: 4.577978610992432\n",
      "Epoch: 1/100 | step: 299/422 | loss: 4.594143390655518\n",
      "Epoch: 1/100 | step: 300/422 | loss: 4.586284160614014\n",
      "Epoch: 1/100 | step: 301/422 | loss: 4.580181121826172\n",
      "Epoch: 1/100 | step: 302/422 | loss: 4.579565525054932\n",
      "Epoch: 1/100 | step: 303/422 | loss: 4.598866939544678\n",
      "Epoch: 1/100 | step: 304/422 | loss: 4.5968756675720215\n",
      "Epoch: 1/100 | step: 305/422 | loss: 4.583896636962891\n",
      "Epoch: 1/100 | step: 306/422 | loss: 4.595143795013428\n",
      "Epoch: 1/100 | step: 307/422 | loss: 4.589108467102051\n",
      "Epoch: 1/100 | step: 308/422 | loss: 4.586852073669434\n",
      "Epoch: 1/100 | step: 309/422 | loss: 4.603256702423096\n",
      "Epoch: 1/100 | step: 310/422 | loss: 4.580793380737305\n",
      "Epoch: 1/100 | step: 311/422 | loss: 4.592288017272949\n",
      "Epoch: 1/100 | step: 312/422 | loss: 4.579301834106445\n",
      "Epoch: 1/100 | step: 313/422 | loss: 4.586278438568115\n",
      "Epoch: 1/100 | step: 314/422 | loss: 4.588853359222412\n",
      "Epoch: 1/100 | step: 315/422 | loss: 4.5873236656188965\n",
      "Epoch: 1/100 | step: 316/422 | loss: 4.58965539932251\n",
      "Epoch: 1/100 | step: 317/422 | loss: 4.598553657531738\n",
      "Epoch: 1/100 | step: 318/422 | loss: 4.589838981628418\n",
      "Epoch: 1/100 | step: 319/422 | loss: 4.5947370529174805\n",
      "Epoch: 1/100 | step: 320/422 | loss: 4.590279579162598\n",
      "Epoch: 1/100 | step: 321/422 | loss: 4.584826469421387\n",
      "Epoch: 1/100 | step: 322/422 | loss: 4.591623306274414\n",
      "Epoch: 1/100 | step: 323/422 | loss: 4.579968452453613\n",
      "Epoch: 1/100 | step: 324/422 | loss: 4.595520973205566\n",
      "Epoch: 1/100 | step: 325/422 | loss: 4.6072564125061035\n",
      "Epoch: 1/100 | step: 326/422 | loss: 4.591270446777344\n",
      "Epoch: 1/100 | step: 327/422 | loss: 4.583019733428955\n",
      "Epoch: 1/100 | step: 328/422 | loss: 4.59603214263916\n",
      "Epoch: 1/100 | step: 329/422 | loss: 4.590203285217285\n",
      "Epoch: 1/100 | step: 330/422 | loss: 4.592828750610352\n",
      "Epoch: 1/100 | step: 331/422 | loss: 4.594964027404785\n",
      "Epoch: 1/100 | step: 332/422 | loss: 4.586843490600586\n",
      "Epoch: 1/100 | step: 333/422 | loss: 4.584953784942627\n",
      "Epoch: 1/100 | step: 334/422 | loss: 4.565609931945801\n",
      "Epoch: 1/100 | step: 335/422 | loss: 4.59243631362915\n",
      "Epoch: 1/100 | step: 336/422 | loss: 4.5820417404174805\n",
      "Epoch: 1/100 | step: 337/422 | loss: 4.590311050415039\n",
      "Epoch: 1/100 | step: 338/422 | loss: 4.601268291473389\n",
      "Epoch: 1/100 | step: 339/422 | loss: 4.582688808441162\n",
      "Epoch: 1/100 | step: 340/422 | loss: 4.58574104309082\n",
      "Epoch: 1/100 | step: 341/422 | loss: 4.57658052444458\n",
      "Epoch: 1/100 | step: 342/422 | loss: 4.585443019866943\n",
      "Epoch: 1/100 | step: 343/422 | loss: 4.581727981567383\n",
      "Epoch: 1/100 | step: 344/422 | loss: 4.593712329864502\n",
      "Epoch: 1/100 | step: 345/422 | loss: 4.588172435760498\n",
      "Epoch: 1/100 | step: 346/422 | loss: 4.590713024139404\n",
      "Epoch: 1/100 | step: 347/422 | loss: 4.579655170440674\n",
      "Epoch: 1/100 | step: 348/422 | loss: 4.591032028198242\n",
      "Epoch: 1/100 | step: 349/422 | loss: 4.585886478424072\n",
      "Epoch: 1/100 | step: 350/422 | loss: 4.587809085845947\n",
      "Epoch: 1/100 | step: 351/422 | loss: 4.570762634277344\n",
      "Epoch: 1/100 | step: 352/422 | loss: 4.582319259643555\n",
      "Epoch: 1/100 | step: 353/422 | loss: 4.598484516143799\n",
      "Epoch: 1/100 | step: 354/422 | loss: 4.59122896194458\n",
      "Epoch: 1/100 | step: 355/422 | loss: 4.593316078186035\n",
      "Epoch: 1/100 | step: 356/422 | loss: 4.60319185256958\n",
      "Epoch: 1/100 | step: 357/422 | loss: 4.576450824737549\n",
      "Epoch: 1/100 | step: 358/422 | loss: 4.590609073638916\n",
      "Epoch: 1/100 | step: 359/422 | loss: 4.608200550079346\n",
      "Epoch: 1/100 | step: 360/422 | loss: 4.594388484954834\n",
      "Epoch: 1/100 | step: 361/422 | loss: 4.590969085693359\n",
      "Epoch: 1/100 | step: 362/422 | loss: 4.590347766876221\n",
      "Epoch: 1/100 | step: 363/422 | loss: 4.58601713180542\n",
      "Epoch: 1/100 | step: 364/422 | loss: 4.592371463775635\n",
      "Epoch: 1/100 | step: 365/422 | loss: 4.573586940765381\n",
      "Epoch: 1/100 | step: 366/422 | loss: 4.592532634735107\n",
      "Epoch: 1/100 | step: 367/422 | loss: 4.583159446716309\n",
      "Epoch: 1/100 | step: 368/422 | loss: 4.592282772064209\n",
      "Epoch: 1/100 | step: 369/422 | loss: 4.579558372497559\n",
      "Epoch: 1/100 | step: 370/422 | loss: 4.594034194946289\n",
      "Epoch: 1/100 | step: 371/422 | loss: 4.590680122375488\n",
      "Epoch: 1/100 | step: 372/422 | loss: 4.580137252807617\n",
      "Epoch: 1/100 | step: 373/422 | loss: 4.590970993041992\n",
      "Epoch: 1/100 | step: 374/422 | loss: 4.588345527648926\n",
      "Epoch: 1/100 | step: 375/422 | loss: 4.594971656799316\n",
      "Epoch: 1/100 | step: 376/422 | loss: 4.605749607086182\n",
      "Epoch: 1/100 | step: 377/422 | loss: 4.5876336097717285\n",
      "Epoch: 1/100 | step: 378/422 | loss: 4.610124111175537\n",
      "Epoch: 1/100 | step: 379/422 | loss: 4.578612804412842\n",
      "Epoch: 1/100 | step: 380/422 | loss: 4.578767776489258\n",
      "Epoch: 1/100 | step: 381/422 | loss: 4.577413558959961\n",
      "Epoch: 1/100 | step: 382/422 | loss: 4.5985188484191895\n",
      "Epoch: 1/100 | step: 383/422 | loss: 4.5911760330200195\n",
      "Epoch: 1/100 | step: 384/422 | loss: 4.587491512298584\n",
      "Epoch: 1/100 | step: 385/422 | loss: 4.601781845092773\n",
      "Epoch: 1/100 | step: 386/422 | loss: 4.57487154006958\n",
      "Epoch: 1/100 | step: 387/422 | loss: 4.583969593048096\n",
      "Epoch: 1/100 | step: 388/422 | loss: 4.573369026184082\n",
      "Epoch: 1/100 | step: 389/422 | loss: 4.592036247253418\n",
      "Epoch: 1/100 | step: 390/422 | loss: 4.594776153564453\n",
      "Epoch: 1/100 | step: 391/422 | loss: 4.587423801422119\n",
      "Epoch: 1/100 | step: 392/422 | loss: 4.593421459197998\n",
      "Epoch: 1/100 | step: 393/422 | loss: 4.601545333862305\n",
      "Epoch: 1/100 | step: 394/422 | loss: 4.597535610198975\n",
      "Epoch: 1/100 | step: 395/422 | loss: 4.6092681884765625\n",
      "Epoch: 1/100 | step: 396/422 | loss: 4.5842719078063965\n",
      "Epoch: 1/100 | step: 397/422 | loss: 4.59581184387207\n",
      "Epoch: 1/100 | step: 398/422 | loss: 4.588746070861816\n",
      "Epoch: 1/100 | step: 399/422 | loss: 4.587474346160889\n",
      "Epoch: 1/100 | step: 400/422 | loss: 4.575639247894287\n",
      "Epoch: 1/100 | step: 401/422 | loss: 4.578096389770508\n",
      "Epoch: 1/100 | step: 402/422 | loss: 4.6038336753845215\n",
      "Epoch: 1/100 | step: 403/422 | loss: 4.60444974899292\n",
      "Epoch: 1/100 | step: 404/422 | loss: 4.603767395019531\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 2/100 | step: 1/422 | loss: 4.577258586883545\n",
      "Epoch: 2/100 | step: 2/422 | loss: 4.584334373474121\n",
      "Epoch: 2/100 | step: 3/422 | loss: 4.569433212280273\n",
      "Epoch: 2/100 | step: 4/422 | loss: 4.579370498657227\n",
      "Epoch: 2/100 | step: 5/422 | loss: 4.584299087524414\n",
      "Epoch: 2/100 | step: 6/422 | loss: 4.589247226715088\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 3/100 | step: 1/422 | loss: 4.587042331695557\n",
      "Epoch: 3/100 | step: 2/422 | loss: 4.585375785827637\n",
      "Epoch: 3/100 | step: 3/422 | loss: 4.58605432510376\n",
      "Epoch: 3/100 | step: 4/422 | loss: 4.595133304595947\n",
      "Epoch: 3/100 | step: 5/422 | loss: 4.56278133392334\n",
      "Epoch: 3/100 | step: 6/422 | loss: 4.589822769165039\n",
      "Epoch: 3/100 | step: 7/422 | loss: 4.604368686676025\n",
      "Epoch: 3/100 | step: 8/422 | loss: 4.596392631530762\n",
      "Epoch: 3/100 | step: 9/422 | loss: 4.577639102935791\n",
      "Epoch: 3/100 | step: 10/422 | loss: 4.585178852081299\n",
      "Epoch: 3/100 | step: 11/422 | loss: 4.588063716888428\n",
      "Epoch: 3/100 | step: 12/422 | loss: 4.577892780303955\n",
      "Epoch: 3/100 | step: 13/422 | loss: 4.580713272094727\n",
      "Epoch: 3/100 | step: 14/422 | loss: 4.603961944580078\n",
      "Epoch: 3/100 | step: 15/422 | loss: 4.583512783050537\n",
      "Epoch: 3/100 | step: 16/422 | loss: 4.590860366821289\n",
      "Epoch: 3/100 | step: 17/422 | loss: 4.5963521003723145\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 4/100 | step: 1/422 | loss: 4.575928688049316\n",
      "Epoch: 4/100 | step: 2/422 | loss: 4.604371070861816\n",
      "Epoch: 4/100 | step: 3/422 | loss: 4.5999345779418945\n",
      "Epoch: 4/100 | step: 4/422 | loss: 4.591943740844727\n",
      "Epoch: 4/100 | step: 5/422 | loss: 4.585718154907227\n",
      "Epoch: 4/100 | step: 6/422 | loss: 4.588008403778076\n",
      "Epoch: 4/100 | step: 7/422 | loss: 4.603094577789307\n",
      "Epoch: 4/100 | step: 8/422 | loss: 4.578837871551514\n",
      "Epoch: 4/100 | step: 9/422 | loss: 4.572059631347656\n",
      "Epoch: 4/100 | step: 10/422 | loss: 4.564811706542969\n",
      "Epoch: 4/100 | step: 11/422 | loss: 4.581544876098633\n",
      "Epoch: 4/100 | step: 12/422 | loss: 4.587549209594727\n",
      "Epoch: 4/100 | step: 13/422 | loss: 4.5974650382995605\n",
      "Epoch: 4/100 | step: 14/422 | loss: 4.574672222137451\n",
      "Epoch: 4/100 | step: 15/422 | loss: 4.600383758544922\n",
      "Epoch: 4/100 | step: 16/422 | loss: 4.581470489501953\n",
      "Epoch: 4/100 | step: 17/422 | loss: 4.582681179046631\n",
      "Epoch: 4/100 | step: 18/422 | loss: 4.576845169067383\n",
      "Epoch: 4/100 | step: 19/422 | loss: 4.583918571472168\n",
      "Epoch: 4/100 | step: 20/422 | loss: 4.59453010559082\n",
      "Epoch: 4/100 | step: 21/422 | loss: 4.582045555114746\n",
      "Epoch: 4/100 | step: 22/422 | loss: 4.5708842277526855\n",
      "Epoch: 4/100 | step: 23/422 | loss: 4.578215599060059\n",
      "Epoch: 4/100 | step: 24/422 | loss: 4.595543384552002\n",
      "Epoch: 4/100 | step: 25/422 | loss: 4.585753917694092\n",
      "Epoch: 4/100 | step: 26/422 | loss: 4.589121341705322\n",
      "Epoch: 4/100 | step: 27/422 | loss: 4.577672481536865\n",
      "Epoch: 4/100 | step: 28/422 | loss: 4.597940444946289\n",
      "Epoch: 4/100 | step: 29/422 | loss: 4.569215774536133\n",
      "Epoch: 4/100 | step: 30/422 | loss: 4.594901084899902\n",
      "Epoch: 4/100 | step: 31/422 | loss: 4.5965800285339355\n",
      "Epoch: 4/100 | step: 32/422 | loss: 4.585197925567627\n",
      "Epoch: 4/100 | step: 33/422 | loss: 4.566527843475342\n",
      "Epoch: 4/100 | step: 34/422 | loss: 4.5904717445373535\n",
      "Epoch: 4/100 | step: 35/422 | loss: 4.586887359619141\n",
      "Epoch: 4/100 | step: 36/422 | loss: 4.58769416809082\n",
      "Epoch: 4/100 | step: 37/422 | loss: 4.585810661315918\n",
      "Epoch: 4/100 | step: 38/422 | loss: 4.573240280151367\n",
      "Epoch: 4/100 | step: 39/422 | loss: 4.586975574493408\n",
      "Epoch: 4/100 | step: 40/422 | loss: 4.576656818389893\n",
      "Epoch: 4/100 | step: 41/422 | loss: 4.590948104858398\n",
      "Epoch: 4/100 | step: 42/422 | loss: 4.581395626068115\n",
      "Epoch: 4/100 | step: 43/422 | loss: 4.593000411987305\n",
      "Epoch: 4/100 | step: 44/422 | loss: 4.568326950073242\n",
      "Epoch: 4/100 | step: 45/422 | loss: 4.5592756271362305\n",
      "Epoch: 4/100 | step: 46/422 | loss: 4.581149578094482\n",
      "Epoch: 4/100 | step: 47/422 | loss: 4.574079513549805\n",
      "Epoch: 4/100 | step: 48/422 | loss: 4.5697197914123535\n",
      "Epoch: 4/100 | step: 49/422 | loss: 4.5866546630859375\n",
      "Epoch: 4/100 | step: 50/422 | loss: 4.580886363983154\n",
      "Epoch: 4/100 | step: 51/422 | loss: 4.579595565795898\n",
      "Epoch: 4/100 | step: 52/422 | loss: 4.580151557922363\n",
      "Epoch: 4/100 | step: 53/422 | loss: 4.570228099822998\n",
      "Epoch: 4/100 | step: 54/422 | loss: 4.584748268127441\n",
      "Epoch: 4/100 | step: 55/422 | loss: 4.593766212463379\n",
      "Epoch: 4/100 | step: 56/422 | loss: 4.570458889007568\n",
      "Epoch: 4/100 | step: 57/422 | loss: 4.573406219482422\n",
      "Epoch: 4/100 | step: 58/422 | loss: 4.5708417892456055\n",
      "Epoch: 4/100 | step: 59/422 | loss: 4.579737663269043\n",
      "Epoch: 4/100 | step: 60/422 | loss: 4.588605880737305\n",
      "Epoch: 4/100 | step: 61/422 | loss: 4.5656280517578125\n",
      "Epoch: 4/100 | step: 62/422 | loss: 4.601020336151123\n",
      "Epoch: 4/100 | step: 63/422 | loss: 4.581843852996826\n",
      "Epoch: 4/100 | step: 64/422 | loss: 4.591507911682129\n",
      "Epoch: 4/100 | step: 65/422 | loss: 4.579838752746582\n",
      "Epoch: 4/100 | step: 66/422 | loss: 4.584141731262207\n",
      "Epoch: 4/100 | step: 67/422 | loss: 4.574029445648193\n",
      "Epoch: 4/100 | step: 68/422 | loss: 4.560245990753174\n",
      "Epoch: 4/100 | step: 69/422 | loss: 4.562093734741211\n",
      "Epoch: 4/100 | step: 70/422 | loss: 4.5746049880981445\n",
      "Epoch: 4/100 | step: 71/422 | loss: 4.569406032562256\n",
      "Epoch: 4/100 | step: 72/422 | loss: 4.5845441818237305\n",
      "Epoch: 4/100 | step: 73/422 | loss: 4.581770896911621\n",
      "Epoch: 4/100 | step: 74/422 | loss: 4.5658183097839355\n",
      "Epoch: 4/100 | step: 75/422 | loss: 4.588690280914307\n",
      "Epoch: 4/100 | step: 76/422 | loss: 4.562117576599121\n",
      "Epoch: 4/100 | step: 77/422 | loss: 4.568822383880615\n",
      "Epoch: 4/100 | step: 78/422 | loss: 4.5693182945251465\n",
      "Epoch: 4/100 | step: 79/422 | loss: 4.563535213470459\n",
      "Epoch: 4/100 | step: 80/422 | loss: 4.58577823638916\n",
      "Epoch: 4/100 | step: 81/422 | loss: 4.577788829803467\n",
      "Epoch: 4/100 | step: 82/422 | loss: 4.586402893066406\n",
      "Epoch: 4/100 | step: 83/422 | loss: 4.58527135848999\n",
      "Epoch: 4/100 | step: 84/422 | loss: 4.607990741729736\n",
      "Epoch: 4/100 | step: 85/422 | loss: 4.611875534057617\n",
      "Epoch: 4/100 | step: 86/422 | loss: 4.590114593505859\n",
      "Epoch: 4/100 | step: 87/422 | loss: 4.578193187713623\n",
      "Epoch: 4/100 | step: 88/422 | loss: 4.564617156982422\n",
      "Epoch: 4/100 | step: 89/422 | loss: 4.5964484214782715\n",
      "Epoch: 4/100 | step: 90/422 | loss: 4.581245422363281\n",
      "Epoch: 4/100 | step: 91/422 | loss: 4.566951751708984\n",
      "Epoch: 4/100 | step: 92/422 | loss: 4.561010837554932\n",
      "Epoch: 4/100 | step: 93/422 | loss: 4.575525283813477\n",
      "Epoch: 4/100 | step: 94/422 | loss: 4.580027103424072\n",
      "Epoch: 4/100 | step: 95/422 | loss: 4.580105781555176\n",
      "Epoch: 4/100 | step: 96/422 | loss: 4.570065975189209\n",
      "Epoch: 4/100 | step: 97/422 | loss: 4.568202972412109\n",
      "Epoch: 4/100 | step: 98/422 | loss: 4.573702812194824\n",
      "Epoch: 4/100 | step: 99/422 | loss: 4.577174663543701\n",
      "Epoch: 4/100 | step: 100/422 | loss: 4.5663347244262695\n",
      "Epoch: 4/100 | step: 101/422 | loss: 4.576888561248779\n",
      "Epoch: 4/100 | step: 102/422 | loss: 4.574892997741699\n",
      "Epoch: 4/100 | step: 103/422 | loss: 4.5828142166137695\n",
      "Epoch: 4/100 | step: 104/422 | loss: 4.579550266265869\n",
      "Epoch: 4/100 | step: 105/422 | loss: 4.5837836265563965\n",
      "Epoch: 4/100 | step: 106/422 | loss: 4.586798191070557\n",
      "Epoch: 4/100 | step: 107/422 | loss: 4.572059631347656\n",
      "Epoch: 4/100 | step: 108/422 | loss: 4.588175296783447\n",
      "Epoch: 4/100 | step: 109/422 | loss: 4.588836193084717\n",
      "Epoch: 4/100 | step: 110/422 | loss: 4.583118915557861\n",
      "Epoch: 4/100 | step: 111/422 | loss: 4.569000244140625\n",
      "Epoch: 4/100 | step: 112/422 | loss: 4.586392402648926\n",
      "Epoch: 4/100 | step: 113/422 | loss: 4.579277992248535\n",
      "Epoch: 4/100 | step: 114/422 | loss: 4.5706787109375\n",
      "Epoch: 4/100 | step: 115/422 | loss: 4.575677394866943\n",
      "Epoch: 4/100 | step: 116/422 | loss: 4.590192794799805\n",
      "Epoch: 4/100 | step: 117/422 | loss: 4.5742573738098145\n",
      "Epoch: 4/100 | step: 118/422 | loss: 4.595801830291748\n",
      "Epoch: 4/100 | step: 119/422 | loss: 4.576407432556152\n",
      "Epoch: 4/100 | step: 120/422 | loss: 4.578029632568359\n",
      "Epoch: 4/100 | step: 121/422 | loss: 4.572591781616211\n",
      "Epoch: 4/100 | step: 122/422 | loss: 4.586562633514404\n",
      "Epoch: 4/100 | step: 123/422 | loss: 4.5708699226379395\n",
      "Epoch: 4/100 | step: 124/422 | loss: 4.574988842010498\n",
      "Epoch: 4/100 | step: 125/422 | loss: 4.5838398933410645\n",
      "Epoch: 4/100 | step: 126/422 | loss: 4.552210807800293\n",
      "Epoch: 4/100 | step: 127/422 | loss: 4.5580878257751465\n",
      "Epoch: 4/100 | step: 128/422 | loss: 4.573945999145508\n",
      "Epoch: 4/100 | step: 129/422 | loss: 4.568995952606201\n",
      "Epoch: 4/100 | step: 130/422 | loss: 4.581784725189209\n",
      "Epoch: 4/100 | step: 131/422 | loss: 4.570274829864502\n",
      "Epoch: 4/100 | step: 132/422 | loss: 4.580924987792969\n",
      "Epoch: 4/100 | step: 133/422 | loss: 4.585469722747803\n",
      "Epoch: 4/100 | step: 134/422 | loss: 4.5932135581970215\n",
      "Epoch: 4/100 | step: 135/422 | loss: 4.577327728271484\n",
      "Epoch: 4/100 | step: 136/422 | loss: 4.578017711639404\n",
      "Epoch: 4/100 | step: 137/422 | loss: 4.575045108795166\n",
      "Epoch: 4/100 | step: 138/422 | loss: 4.5777177810668945\n",
      "Epoch: 4/100 | step: 139/422 | loss: 4.579078197479248\n",
      "Epoch: 4/100 | step: 140/422 | loss: 4.596019744873047\n",
      "Epoch: 4/100 | step: 141/422 | loss: 4.610012531280518\n",
      "Epoch: 4/100 | step: 142/422 | loss: 4.580467700958252\n",
      "Epoch: 4/100 | step: 143/422 | loss: 4.572782516479492\n",
      "Epoch: 4/100 | step: 144/422 | loss: 4.546543121337891\n",
      "Epoch: 4/100 | step: 145/422 | loss: 4.585962772369385\n",
      "Epoch: 4/100 | step: 146/422 | loss: 4.583354473114014\n",
      "Epoch: 4/100 | step: 147/422 | loss: 4.568720817565918\n",
      "Epoch: 4/100 | step: 148/422 | loss: 4.574148178100586\n",
      "Epoch: 4/100 | step: 149/422 | loss: 4.57899808883667\n",
      "Epoch: 4/100 | step: 150/422 | loss: 4.5691237449646\n",
      "Epoch: 4/100 | step: 151/422 | loss: 4.587867259979248\n",
      "Epoch: 4/100 | step: 152/422 | loss: 4.5671563148498535\n",
      "Epoch: 4/100 | step: 153/422 | loss: 4.6009297370910645\n",
      "Epoch: 4/100 | step: 154/422 | loss: 4.561112880706787\n",
      "Epoch: 4/100 | step: 155/422 | loss: 4.581497669219971\n",
      "Epoch: 4/100 | step: 156/422 | loss: 4.555074214935303\n",
      "Epoch: 4/100 | step: 157/422 | loss: 4.573236465454102\n",
      "Epoch: 4/100 | step: 158/422 | loss: 4.5677642822265625\n",
      "Epoch: 4/100 | step: 159/422 | loss: 4.602988243103027\n",
      "Epoch: 4/100 | step: 160/422 | loss: 4.577966213226318\n",
      "Epoch: 4/100 | step: 161/422 | loss: 4.5734052658081055\n",
      "Epoch: 4/100 | step: 162/422 | loss: 4.588348865509033\n",
      "Epoch: 4/100 | step: 163/422 | loss: 4.562554359436035\n",
      "Epoch: 4/100 | step: 164/422 | loss: 4.552460670471191\n",
      "Epoch: 4/100 | step: 165/422 | loss: 4.583676815032959\n",
      "Epoch: 4/100 | step: 166/422 | loss: 4.564254283905029\n",
      "Epoch: 4/100 | step: 167/422 | loss: 4.567255973815918\n",
      "Epoch: 4/100 | step: 168/422 | loss: 4.573643207550049\n",
      "Epoch: 4/100 | step: 169/422 | loss: 4.595129013061523\n",
      "Epoch: 4/100 | step: 170/422 | loss: 4.569448471069336\n",
      "Epoch: 4/100 | step: 171/422 | loss: 4.565106391906738\n",
      "Epoch: 4/100 | step: 172/422 | loss: 4.577160835266113\n",
      "Epoch: 4/100 | step: 173/422 | loss: 4.578427791595459\n",
      "Epoch: 4/100 | step: 174/422 | loss: 4.597562313079834\n",
      "Epoch: 4/100 | step: 175/422 | loss: 4.57849645614624\n",
      "Epoch: 4/100 | step: 176/422 | loss: 4.571272850036621\n",
      "Epoch: 4/100 | step: 177/422 | loss: 4.559433460235596\n",
      "Epoch: 4/100 | step: 178/422 | loss: 4.574467658996582\n",
      "Epoch: 4/100 | step: 179/422 | loss: 4.576416969299316\n",
      "Epoch: 4/100 | step: 180/422 | loss: 4.562715530395508\n",
      "Epoch: 4/100 | step: 181/422 | loss: 4.550339698791504\n",
      "Epoch: 4/100 | step: 182/422 | loss: 4.591203689575195\n",
      "Epoch: 4/100 | step: 183/422 | loss: 4.563443183898926\n",
      "Epoch: 4/100 | step: 184/422 | loss: 4.582919120788574\n",
      "Epoch: 4/100 | step: 185/422 | loss: 4.569340705871582\n",
      "Epoch: 4/100 | step: 186/422 | loss: 4.580140113830566\n",
      "Epoch: 4/100 | step: 187/422 | loss: 4.571209907531738\n",
      "Epoch: 4/100 | step: 188/422 | loss: 4.569039821624756\n",
      "Epoch: 4/100 | step: 189/422 | loss: 4.582073211669922\n",
      "Epoch: 4/100 | step: 190/422 | loss: 4.585324764251709\n",
      "Epoch: 4/100 | step: 191/422 | loss: 4.561671257019043\n",
      "Epoch: 4/100 | step: 192/422 | loss: 4.586198329925537\n",
      "Epoch: 4/100 | step: 193/422 | loss: 4.577782154083252\n",
      "Epoch: 4/100 | step: 194/422 | loss: 4.586508274078369\n",
      "Epoch: 4/100 | step: 195/422 | loss: 4.593652248382568\n",
      "Epoch: 4/100 | step: 196/422 | loss: 4.566115379333496\n",
      "Epoch: 4/100 | step: 197/422 | loss: 4.5650715827941895\n",
      "Epoch: 4/100 | step: 198/422 | loss: 4.579895973205566\n",
      "Epoch: 4/100 | step: 199/422 | loss: 4.5615715980529785\n",
      "Epoch: 4/100 | step: 200/422 | loss: 4.586425304412842\n",
      "Epoch: 4/100 | step: 201/422 | loss: 4.56743049621582\n",
      "Epoch: 4/100 | step: 202/422 | loss: 4.593929290771484\n",
      "Epoch: 4/100 | step: 203/422 | loss: 4.556262493133545\n",
      "Epoch: 4/100 | step: 204/422 | loss: 4.589118957519531\n",
      "Epoch: 4/100 | step: 205/422 | loss: 4.5896897315979\n",
      "Epoch: 4/100 | step: 206/422 | loss: 4.583978176116943\n",
      "Epoch: 4/100 | step: 207/422 | loss: 4.594722747802734\n",
      "Epoch: 4/100 | step: 208/422 | loss: 4.588489532470703\n",
      "Epoch: 4/100 | step: 209/422 | loss: 4.575061321258545\n",
      "Epoch: 4/100 | step: 210/422 | loss: 4.5843987464904785\n",
      "Epoch: 4/100 | step: 211/422 | loss: 4.585118293762207\n",
      "Epoch: 4/100 | step: 212/422 | loss: 4.555654525756836\n",
      "Epoch: 4/100 | step: 213/422 | loss: 4.574915885925293\n",
      "Epoch: 4/100 | step: 214/422 | loss: 4.577385425567627\n",
      "Epoch: 4/100 | step: 215/422 | loss: 4.56569766998291\n",
      "Epoch: 4/100 | step: 216/422 | loss: 4.568357944488525\n",
      "Epoch: 4/100 | step: 217/422 | loss: 4.575601577758789\n",
      "Epoch: 4/100 | step: 218/422 | loss: 4.591452598571777\n",
      "Epoch: 4/100 | step: 219/422 | loss: 4.562206745147705\n",
      "Epoch: 4/100 | step: 220/422 | loss: 4.564022541046143\n",
      "Epoch: 4/100 | step: 221/422 | loss: 4.548523426055908\n",
      "Epoch: 4/100 | step: 222/422 | loss: 4.589116096496582\n",
      "Epoch: 4/100 | step: 223/422 | loss: 4.57678747177124\n",
      "Epoch: 4/100 | step: 224/422 | loss: 4.581708908081055\n",
      "Epoch: 4/100 | step: 225/422 | loss: 4.562758922576904\n",
      "Epoch: 4/100 | step: 226/422 | loss: 4.584918022155762\n",
      "Epoch: 4/100 | step: 227/422 | loss: 4.575941562652588\n",
      "Epoch: 4/100 | step: 228/422 | loss: 4.599331378936768\n",
      "Epoch: 4/100 | step: 229/422 | loss: 4.560335159301758\n",
      "Epoch: 4/100 | step: 230/422 | loss: 4.596576690673828\n",
      "Epoch: 4/100 | step: 231/422 | loss: 4.568854808807373\n",
      "Epoch: 4/100 | step: 232/422 | loss: 4.56381368637085\n",
      "Epoch: 4/100 | step: 233/422 | loss: 4.580099105834961\n",
      "Epoch: 4/100 | step: 234/422 | loss: 4.586475372314453\n",
      "Epoch: 4/100 | step: 235/422 | loss: 4.582434177398682\n",
      "Epoch: 4/100 | step: 236/422 | loss: 4.545181751251221\n",
      "Epoch: 4/100 | step: 237/422 | loss: 4.566495418548584\n",
      "Epoch: 4/100 | step: 238/422 | loss: 4.5735764503479\n",
      "Epoch: 4/100 | step: 239/422 | loss: 4.594666004180908\n",
      "Epoch: 4/100 | step: 240/422 | loss: 4.572628498077393\n",
      "Epoch: 4/100 | step: 241/422 | loss: 4.570339202880859\n",
      "Epoch: 4/100 | step: 242/422 | loss: 4.564891815185547\n",
      "Epoch: 4/100 | step: 243/422 | loss: 4.5671916007995605\n",
      "Epoch: 4/100 | step: 244/422 | loss: 4.563146591186523\n",
      "Epoch: 4/100 | step: 245/422 | loss: 4.585031509399414\n",
      "Epoch: 4/100 | step: 246/422 | loss: 4.558262348175049\n",
      "Epoch: 4/100 | step: 247/422 | loss: 4.57960844039917\n",
      "Epoch: 4/100 | step: 248/422 | loss: 4.603846549987793\n",
      "Epoch: 4/100 | step: 249/422 | loss: 4.596517562866211\n",
      "Epoch: 4/100 | step: 250/422 | loss: 4.5749688148498535\n",
      "Epoch: 4/100 | step: 251/422 | loss: 4.581241130828857\n",
      "Epoch: 4/100 | step: 252/422 | loss: 4.569208145141602\n",
      "Epoch: 4/100 | step: 253/422 | loss: 4.575656414031982\n",
      "Epoch: 4/100 | step: 254/422 | loss: 4.6109395027160645\n",
      "Epoch: 4/100 | step: 255/422 | loss: 4.591872215270996\n",
      "Epoch: 4/100 | step: 256/422 | loss: 4.573882102966309\n",
      "Epoch: 4/100 | step: 257/422 | loss: 4.578716278076172\n",
      "Epoch: 4/100 | step: 258/422 | loss: 4.5814104080200195\n",
      "Epoch: 4/100 | step: 259/422 | loss: 4.571997165679932\n",
      "Epoch: 4/100 | step: 260/422 | loss: 4.569021701812744\n",
      "Epoch: 4/100 | step: 261/422 | loss: 4.581740856170654\n",
      "Epoch: 4/100 | step: 262/422 | loss: 4.552873611450195\n",
      "Epoch: 4/100 | step: 263/422 | loss: 4.573592185974121\n",
      "Epoch: 4/100 | step: 264/422 | loss: 4.572390079498291\n",
      "Epoch: 4/100 | step: 265/422 | loss: 4.5830078125\n",
      "Epoch: 4/100 | step: 266/422 | loss: 4.577242851257324\n",
      "Epoch: 4/100 | step: 267/422 | loss: 4.5586371421813965\n",
      "Epoch: 4/100 | step: 268/422 | loss: 4.572536468505859\n",
      "Epoch: 4/100 | step: 269/422 | loss: 4.585666656494141\n",
      "Epoch: 4/100 | step: 270/422 | loss: 4.569613933563232\n",
      "Epoch: 4/100 | step: 271/422 | loss: 4.587856292724609\n",
      "Epoch: 4/100 | step: 272/422 | loss: 4.5666022300720215\n",
      "Epoch: 4/100 | step: 273/422 | loss: 4.551458835601807\n",
      "Epoch: 4/100 | step: 274/422 | loss: 4.5493316650390625\n",
      "Epoch: 4/100 | step: 275/422 | loss: 4.567911148071289\n",
      "Epoch: 4/100 | step: 276/422 | loss: 4.611057281494141\n",
      "Epoch: 4/100 | step: 277/422 | loss: 4.576407432556152\n",
      "Epoch: 4/100 | step: 278/422 | loss: 4.586068153381348\n",
      "Epoch: 4/100 | step: 279/422 | loss: 4.579771995544434\n",
      "Epoch: 4/100 | step: 280/422 | loss: 4.549444198608398\n",
      "Epoch: 4/100 | step: 281/422 | loss: 4.561771392822266\n",
      "Epoch: 4/100 | step: 282/422 | loss: 4.569252967834473\n",
      "Epoch: 4/100 | step: 283/422 | loss: 4.5884785652160645\n",
      "Epoch: 4/100 | step: 284/422 | loss: 4.57348108291626\n",
      "Epoch: 4/100 | step: 285/422 | loss: 4.541205883026123\n",
      "Epoch: 4/100 | step: 286/422 | loss: 4.559069633483887\n",
      "Epoch: 4/100 | step: 287/422 | loss: 4.554826736450195\n",
      "Epoch: 4/100 | step: 288/422 | loss: 4.55948543548584\n",
      "Epoch: 4/100 | step: 289/422 | loss: 4.570292949676514\n",
      "Epoch: 4/100 | step: 290/422 | loss: 4.581766605377197\n",
      "Epoch: 4/100 | step: 291/422 | loss: 4.5836591720581055\n",
      "Epoch: 4/100 | step: 292/422 | loss: 4.571681976318359\n",
      "Epoch: 4/100 | step: 293/422 | loss: 4.556385040283203\n",
      "Epoch: 4/100 | step: 294/422 | loss: 4.571760654449463\n",
      "Epoch: 4/100 | step: 295/422 | loss: 4.565508842468262\n",
      "Epoch: 4/100 | step: 296/422 | loss: 4.566798210144043\n",
      "Epoch: 4/100 | step: 297/422 | loss: 4.5784196853637695\n",
      "Epoch: 4/100 | step: 298/422 | loss: 4.583614826202393\n",
      "Epoch: 4/100 | step: 299/422 | loss: 4.5496954917907715\n",
      "Epoch: 4/100 | step: 300/422 | loss: 4.5650787353515625\n",
      "Epoch: 4/100 | step: 301/422 | loss: 4.584330081939697\n",
      "Epoch: 4/100 | step: 302/422 | loss: 4.5930962562561035\n",
      "Epoch: 4/100 | step: 303/422 | loss: 4.574051380157471\n",
      "Epoch: 4/100 | step: 304/422 | loss: 4.551030158996582\n",
      "Epoch: 4/100 | step: 305/422 | loss: 4.568544387817383\n",
      "Epoch: 4/100 | step: 306/422 | loss: 4.57446813583374\n",
      "Epoch: 4/100 | step: 307/422 | loss: 4.5823493003845215\n",
      "Epoch: 4/100 | step: 308/422 | loss: 4.565236568450928\n",
      "Epoch: 4/100 | step: 309/422 | loss: 4.572440147399902\n",
      "Epoch: 4/100 | step: 310/422 | loss: 4.586336135864258\n",
      "Epoch: 4/100 | step: 311/422 | loss: 4.570591449737549\n",
      "Epoch: 4/100 | step: 312/422 | loss: 4.556448459625244\n",
      "Epoch: 4/100 | step: 313/422 | loss: 4.6065473556518555\n",
      "Epoch: 4/100 | step: 314/422 | loss: 4.547910690307617\n",
      "Epoch: 4/100 | step: 315/422 | loss: 4.546204566955566\n",
      "Epoch: 4/100 | step: 316/422 | loss: 4.552137851715088\n",
      "Epoch: 4/100 | step: 317/422 | loss: 4.573002338409424\n",
      "Epoch: 4/100 | step: 318/422 | loss: 4.531038761138916\n",
      "Epoch: 4/100 | step: 319/422 | loss: 4.586628437042236\n",
      "Epoch: 4/100 | step: 320/422 | loss: 4.552652835845947\n",
      "Epoch: 4/100 | step: 321/422 | loss: 4.5508012771606445\n",
      "Epoch: 4/100 | step: 322/422 | loss: 4.573800086975098\n",
      "Epoch: 4/100 | step: 323/422 | loss: 4.564050674438477\n",
      "Epoch: 4/100 | step: 324/422 | loss: 4.5708160400390625\n",
      "Epoch: 4/100 | step: 325/422 | loss: 4.582369327545166\n",
      "Epoch: 4/100 | step: 326/422 | loss: 4.592840671539307\n",
      "Epoch: 4/100 | step: 327/422 | loss: 4.5830183029174805\n",
      "Epoch: 4/100 | step: 328/422 | loss: 4.5895915031433105\n",
      "Epoch: 4/100 | step: 329/422 | loss: 4.567389011383057\n",
      "Epoch: 4/100 | step: 330/422 | loss: 4.5710954666137695\n",
      "Epoch: 4/100 | step: 331/422 | loss: 4.557442665100098\n",
      "Epoch: 4/100 | step: 332/422 | loss: 4.554119110107422\n",
      "Epoch: 4/100 | step: 333/422 | loss: 4.582146167755127\n",
      "Epoch: 4/100 | step: 334/422 | loss: 4.585389137268066\n",
      "Epoch: 4/100 | step: 335/422 | loss: 4.558011054992676\n",
      "Epoch: 4/100 | step: 336/422 | loss: 4.552921772003174\n",
      "Epoch: 4/100 | step: 337/422 | loss: 4.586120128631592\n",
      "Epoch: 4/100 | step: 338/422 | loss: 4.564198017120361\n",
      "Epoch: 4/100 | step: 339/422 | loss: 4.5834574699401855\n",
      "Epoch: 4/100 | step: 340/422 | loss: 4.5647172927856445\n",
      "Epoch: 4/100 | step: 341/422 | loss: 4.552827835083008\n",
      "Epoch: 4/100 | step: 342/422 | loss: 4.576953887939453\n",
      "Epoch: 4/100 | step: 343/422 | loss: 4.576725959777832\n",
      "Epoch: 4/100 | step: 344/422 | loss: 4.5584588050842285\n",
      "Epoch: 4/100 | step: 345/422 | loss: 4.5752058029174805\n",
      "Epoch: 4/100 | step: 346/422 | loss: 4.554903030395508\n",
      "Epoch: 4/100 | step: 347/422 | loss: 4.537337779998779\n",
      "Epoch: 4/100 | step: 348/422 | loss: 4.570178508758545\n",
      "Epoch: 4/100 | step: 349/422 | loss: 4.554113388061523\n",
      "Epoch: 4/100 | step: 350/422 | loss: 4.574827194213867\n",
      "Epoch: 4/100 | step: 351/422 | loss: 4.562142848968506\n",
      "Epoch: 4/100 | step: 352/422 | loss: 4.572166442871094\n",
      "Epoch: 4/100 | step: 353/422 | loss: 4.5797247886657715\n",
      "Epoch: 4/100 | step: 354/422 | loss: 4.587856292724609\n",
      "Epoch: 4/100 | step: 355/422 | loss: 4.569858074188232\n",
      "Epoch: 4/100 | step: 356/422 | loss: 4.572855472564697\n",
      "Epoch: 4/100 | step: 357/422 | loss: 4.546690464019775\n",
      "Epoch: 4/100 | step: 358/422 | loss: 4.588197708129883\n",
      "Epoch: 4/100 | step: 359/422 | loss: 4.571588516235352\n",
      "Epoch: 4/100 | step: 360/422 | loss: 4.5826215744018555\n",
      "Epoch: 4/100 | step: 361/422 | loss: 4.563415050506592\n",
      "Epoch: 4/100 | step: 362/422 | loss: 4.579078674316406\n",
      "Epoch: 4/100 | step: 363/422 | loss: 4.565563201904297\n",
      "Epoch: 4/100 | step: 364/422 | loss: 4.564220428466797\n",
      "Epoch: 4/100 | step: 365/422 | loss: 4.575666427612305\n",
      "Epoch: 4/100 | step: 366/422 | loss: 4.583200454711914\n",
      "Epoch: 4/100 | step: 367/422 | loss: 4.5597615242004395\n",
      "Epoch: 4/100 | step: 368/422 | loss: 4.565531253814697\n",
      "Epoch: 4/100 | step: 369/422 | loss: 4.564466953277588\n",
      "Epoch: 4/100 | step: 370/422 | loss: 4.532626152038574\n",
      "Epoch: 4/100 | step: 371/422 | loss: 4.554451942443848\n",
      "Epoch: 4/100 | step: 372/422 | loss: 4.544155120849609\n",
      "Epoch: 4/100 | step: 373/422 | loss: 4.546755790710449\n",
      "Epoch: 4/100 | step: 374/422 | loss: 4.563251495361328\n",
      "Epoch: 4/100 | step: 375/422 | loss: 4.5518999099731445\n",
      "Epoch: 4/100 | step: 376/422 | loss: 4.55547571182251\n",
      "Epoch: 4/100 | step: 377/422 | loss: 4.5674591064453125\n",
      "Epoch: 4/100 | step: 378/422 | loss: 4.522961616516113\n",
      "Epoch: 4/100 | step: 379/422 | loss: 4.5589399337768555\n",
      "Epoch: 4/100 | step: 380/422 | loss: 4.552540302276611\n",
      "Epoch: 4/100 | step: 381/422 | loss: 4.596697807312012\n",
      "Epoch: 4/100 | step: 382/422 | loss: 4.552635669708252\n",
      "Epoch: 4/100 | step: 383/422 | loss: 4.559242248535156\n",
      "Epoch: 4/100 | step: 384/422 | loss: 4.549843788146973\n",
      "Epoch: 4/100 | step: 385/422 | loss: 4.572666168212891\n",
      "Epoch: 4/100 | step: 386/422 | loss: 4.551393985748291\n",
      "Epoch: 4/100 | step: 387/422 | loss: 4.548165321350098\n",
      "Epoch: 4/100 | step: 388/422 | loss: 4.545629501342773\n",
      "Epoch: 4/100 | step: 389/422 | loss: 4.563575744628906\n",
      "Epoch: 4/100 | step: 390/422 | loss: 4.527931213378906\n",
      "Epoch: 4/100 | step: 391/422 | loss: 4.550624847412109\n",
      "Epoch: 4/100 | step: 392/422 | loss: 4.5723676681518555\n",
      "Epoch: 4/100 | step: 393/422 | loss: 4.588640213012695\n",
      "Epoch: 4/100 | step: 394/422 | loss: 4.563504695892334\n",
      "Epoch: 4/100 | step: 395/422 | loss: 4.574006080627441\n",
      "Epoch: 4/100 | step: 396/422 | loss: 4.576924800872803\n",
      "Epoch: 4/100 | step: 397/422 | loss: 4.526991367340088\n",
      "Epoch: 4/100 | step: 398/422 | loss: 4.589311599731445\n",
      "Epoch: 4/100 | step: 399/422 | loss: 4.55978536605835\n",
      "Epoch: 4/100 | step: 400/422 | loss: 4.533612251281738\n",
      "Epoch: 4/100 | step: 401/422 | loss: 4.564445972442627\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 5/100 | step: 1/422 | loss: 4.5902018547058105\n",
      "Epoch: 5/100 | step: 2/422 | loss: 4.5517964363098145\n",
      "Epoch: 5/100 | step: 3/422 | loss: 4.546538829803467\n",
      "Epoch: 5/100 | step: 4/422 | loss: 4.566007137298584\n",
      "Epoch: 5/100 | step: 5/422 | loss: 4.565128803253174\n",
      "Epoch: 5/100 | step: 6/422 | loss: 4.532140254974365\n",
      "Epoch: 5/100 | step: 7/422 | loss: 4.55351448059082\n",
      "Epoch: 5/100 | step: 8/422 | loss: 4.5377655029296875\n",
      "Epoch: 5/100 | step: 9/422 | loss: 4.555946350097656\n",
      "Epoch: 5/100 | step: 10/422 | loss: 4.5772480964660645\n",
      "Epoch: 5/100 | step: 11/422 | loss: 4.574017524719238\n",
      "Epoch: 5/100 | step: 12/422 | loss: 4.531708717346191\n",
      "Epoch: 5/100 | step: 13/422 | loss: 4.542139053344727\n",
      "Epoch: 5/100 | step: 14/422 | loss: 4.545205593109131\n",
      "Epoch: 5/100 | step: 15/422 | loss: 4.5620527267456055\n",
      "Epoch: 5/100 | step: 16/422 | loss: 4.567782878875732\n",
      "Epoch: 5/100 | step: 17/422 | loss: 4.5032243728637695\n",
      "Epoch: 5/100 | step: 18/422 | loss: 4.5361480712890625\n",
      "Epoch: 5/100 | step: 19/422 | loss: 4.537005424499512\n",
      "Epoch: 5/100 | step: 20/422 | loss: 4.561864852905273\n",
      "Epoch: 5/100 | step: 21/422 | loss: 4.554659366607666\n",
      "Epoch: 5/100 | step: 22/422 | loss: 4.579342365264893\n",
      "Epoch: 5/100 | step: 23/422 | loss: 4.530271053314209\n",
      "Epoch: 5/100 | step: 24/422 | loss: 4.558053493499756\n",
      "Epoch: 5/100 | step: 25/422 | loss: 4.581033229827881\n",
      "Epoch: 5/100 | step: 26/422 | loss: 4.532290458679199\n",
      "Epoch: 5/100 | step: 27/422 | loss: 4.575211048126221\n",
      "Epoch: 5/100 | step: 28/422 | loss: 4.53825569152832\n",
      "Epoch: 5/100 | step: 29/422 | loss: 4.576465606689453\n",
      "Epoch: 5/100 | step: 30/422 | loss: 4.562155246734619\n",
      "Epoch: 5/100 | step: 31/422 | loss: 4.527360439300537\n",
      "Epoch: 5/100 | step: 32/422 | loss: 4.563722133636475\n",
      "Epoch: 5/100 | step: 33/422 | loss: 4.539220333099365\n",
      "Epoch: 5/100 | step: 34/422 | loss: 4.581256866455078\n",
      "Epoch: 5/100 | step: 35/422 | loss: 4.545457363128662\n",
      "Epoch: 5/100 | step: 36/422 | loss: 4.543074607849121\n",
      "Epoch: 5/100 | step: 37/422 | loss: 4.520505428314209\n",
      "Epoch: 5/100 | step: 38/422 | loss: 4.560204982757568\n",
      "Epoch: 5/100 | step: 39/422 | loss: 4.540966033935547\n",
      "Epoch: 5/100 | step: 40/422 | loss: 4.564927577972412\n",
      "Epoch: 5/100 | step: 41/422 | loss: 4.574819087982178\n",
      "Epoch: 5/100 | step: 42/422 | loss: 4.507590293884277\n",
      "Epoch: 5/100 | step: 43/422 | loss: 4.573765754699707\n",
      "Epoch: 5/100 | step: 44/422 | loss: 4.547142505645752\n",
      "Epoch: 5/100 | step: 45/422 | loss: 4.557526588439941\n",
      "Epoch: 5/100 | step: 46/422 | loss: 4.566235542297363\n",
      "Epoch: 5/100 | step: 47/422 | loss: 4.5408782958984375\n",
      "Epoch: 5/100 | step: 48/422 | loss: 4.560741424560547\n",
      "Epoch: 5/100 | step: 49/422 | loss: 4.524657726287842\n",
      "Epoch: 5/100 | step: 50/422 | loss: 4.556934356689453\n",
      "Epoch: 5/100 | step: 51/422 | loss: 4.533202171325684\n",
      "Epoch: 5/100 | step: 52/422 | loss: 4.576505184173584\n",
      "Epoch: 5/100 | step: 53/422 | loss: 4.566732883453369\n",
      "Epoch: 5/100 | step: 54/422 | loss: 4.561985492706299\n",
      "Epoch: 5/100 | step: 55/422 | loss: 4.555378437042236\n",
      "Epoch: 5/100 | step: 56/422 | loss: 4.535935401916504\n",
      "Epoch: 5/100 | step: 57/422 | loss: 4.56563663482666\n",
      "Epoch: 5/100 | step: 58/422 | loss: 4.578317165374756\n",
      "Epoch: 5/100 | step: 59/422 | loss: 4.562196254730225\n",
      "Epoch: 5/100 | step: 60/422 | loss: 4.523768424987793\n",
      "Epoch: 5/100 | step: 61/422 | loss: 4.53004789352417\n",
      "Epoch: 5/100 | step: 62/422 | loss: 4.5318193435668945\n",
      "Epoch: 5/100 | step: 63/422 | loss: 4.543947219848633\n",
      "Epoch: 5/100 | step: 64/422 | loss: 4.549317359924316\n",
      "Epoch: 5/100 | step: 65/422 | loss: 4.534162998199463\n",
      "Epoch: 5/100 | step: 66/422 | loss: 4.552333831787109\n",
      "Epoch: 5/100 | step: 67/422 | loss: 4.564519882202148\n",
      "Epoch: 5/100 | step: 68/422 | loss: 4.477057933807373\n",
      "Epoch: 5/100 | step: 69/422 | loss: 4.537595272064209\n",
      "Epoch: 5/100 | step: 70/422 | loss: 4.569777488708496\n",
      "Epoch: 5/100 | step: 71/422 | loss: 4.540261745452881\n",
      "Epoch: 5/100 | step: 72/422 | loss: 4.553140163421631\n",
      "Epoch: 5/100 | step: 73/422 | loss: 4.553093433380127\n",
      "Epoch: 5/100 | step: 74/422 | loss: 4.598190784454346\n",
      "Epoch: 5/100 | step: 75/422 | loss: 4.527257442474365\n",
      "Epoch: 5/100 | step: 76/422 | loss: 4.541529178619385\n",
      "Epoch: 5/100 | step: 77/422 | loss: 4.551609992980957\n",
      "Epoch: 5/100 | step: 78/422 | loss: 4.554633140563965\n",
      "Epoch: 5/100 | step: 79/422 | loss: 4.586961269378662\n",
      "Epoch: 5/100 | step: 80/422 | loss: 4.560369491577148\n",
      "Epoch: 5/100 | step: 81/422 | loss: 4.52534818649292\n",
      "Epoch: 5/100 | step: 82/422 | loss: 4.584349155426025\n",
      "Epoch: 5/100 | step: 83/422 | loss: 4.523043155670166\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 6/100 | step: 1/422 | loss: 4.574466228485107\n",
      "Epoch: 6/100 | step: 2/422 | loss: 4.573428153991699\n",
      "Epoch: 6/100 | step: 3/422 | loss: 4.570510387420654\n",
      "Epoch: 6/100 | step: 4/422 | loss: 4.557536602020264\n",
      "Epoch: 6/100 | step: 5/422 | loss: 4.557581901550293\n",
      "Epoch: 6/100 | step: 6/422 | loss: 4.557718753814697\n",
      "Epoch: 6/100 | step: 7/422 | loss: 4.563156604766846\n",
      "Epoch: 6/100 | step: 8/422 | loss: 4.552367687225342\n",
      "Epoch: 6/100 | step: 9/422 | loss: 4.542942047119141\n",
      "Epoch: 6/100 | step: 10/422 | loss: 4.5407538414001465\n",
      "Epoch: 6/100 | step: 11/422 | loss: 4.510064601898193\n",
      "Epoch: 6/100 | step: 12/422 | loss: 4.576292037963867\n",
      "Epoch: 6/100 | step: 13/422 | loss: 4.514636039733887\n",
      "Epoch: 6/100 | step: 14/422 | loss: 4.563525676727295\n",
      "Epoch: 6/100 | step: 15/422 | loss: 4.52910041809082\n",
      "Epoch: 6/100 | step: 16/422 | loss: 4.5595703125\n",
      "Epoch: 6/100 | step: 17/422 | loss: 4.573569297790527\n",
      "Epoch: 6/100 | step: 18/422 | loss: 4.5061421394348145\n",
      "Epoch: 6/100 | step: 19/422 | loss: 4.532540798187256\n",
      "Epoch: 6/100 | step: 20/422 | loss: 4.538795471191406\n",
      "Epoch: 6/100 | step: 21/422 | loss: 4.537481784820557\n",
      "Epoch: 6/100 | step: 22/422 | loss: 4.563656806945801\n",
      "Epoch: 6/100 | step: 23/422 | loss: 4.538144111633301\n",
      "Epoch: 6/100 | step: 24/422 | loss: 4.5510735511779785\n",
      "Epoch: 6/100 | step: 25/422 | loss: 4.557076930999756\n",
      "Epoch: 6/100 | step: 26/422 | loss: 4.507846355438232\n",
      "Epoch: 6/100 | step: 27/422 | loss: 4.551374435424805\n",
      "Epoch: 6/100 | step: 28/422 | loss: 4.585822582244873\n",
      "Epoch: 6/100 | step: 29/422 | loss: 4.534780502319336\n",
      "Epoch: 6/100 | step: 30/422 | loss: 4.513437271118164\n",
      "Epoch: 6/100 | step: 31/422 | loss: 4.603806972503662\n",
      "Epoch: 6/100 | step: 32/422 | loss: 4.53326940536499\n",
      "Epoch: 6/100 | step: 33/422 | loss: 4.538169860839844\n",
      "Epoch: 6/100 | step: 34/422 | loss: 4.5995588302612305\n",
      "Epoch: 6/100 | step: 35/422 | loss: 4.566197395324707\n",
      "Epoch: 6/100 | step: 36/422 | loss: 4.521109580993652\n",
      "Epoch: 6/100 | step: 37/422 | loss: 4.5125861167907715\n",
      "Epoch: 6/100 | step: 38/422 | loss: 4.5447845458984375\n",
      "Epoch: 6/100 | step: 39/422 | loss: 4.4807586669921875\n",
      "Epoch: 6/100 | step: 40/422 | loss: 4.520364761352539\n",
      "Epoch: 6/100 | step: 41/422 | loss: 4.560756683349609\n",
      "Epoch: 6/100 | step: 42/422 | loss: 4.4815993309021\n",
      "Epoch: 6/100 | step: 43/422 | loss: 4.535768985748291\n",
      "Epoch: 6/100 | step: 44/422 | loss: 4.579361915588379\n",
      "Epoch: 6/100 | step: 45/422 | loss: 4.57717227935791\n",
      "Epoch: 6/100 | step: 46/422 | loss: 4.553156852722168\n",
      "Epoch: 6/100 | step: 47/422 | loss: 4.565882682800293\n",
      "Epoch: 6/100 | step: 48/422 | loss: 4.568374156951904\n",
      "Epoch: 6/100 | step: 49/422 | loss: 4.544699192047119\n",
      "Epoch: 6/100 | step: 50/422 | loss: 4.538830757141113\n",
      "Epoch: 6/100 | step: 51/422 | loss: 4.578653335571289\n",
      "Epoch: 6/100 | step: 52/422 | loss: 4.550385475158691\n",
      "Epoch: 6/100 | step: 53/422 | loss: 4.515048027038574\n",
      "Epoch: 6/100 | step: 54/422 | loss: 4.562153339385986\n",
      "Epoch: 6/100 | step: 55/422 | loss: 4.544511795043945\n",
      "Epoch: 6/100 | step: 56/422 | loss: 4.531124591827393\n",
      "Epoch: 6/100 | step: 57/422 | loss: 4.585850715637207\n",
      "Epoch: 6/100 | step: 58/422 | loss: 4.550205230712891\n",
      "Epoch: 6/100 | step: 59/422 | loss: 4.550601482391357\n",
      "Epoch: 6/100 | step: 60/422 | loss: 4.542873859405518\n",
      "Epoch: 6/100 | step: 61/422 | loss: 4.607942581176758\n",
      "Epoch: 6/100 | step: 62/422 | loss: 4.567681312561035\n",
      "Epoch: 6/100 | step: 63/422 | loss: 4.539646625518799\n",
      "Epoch: 6/100 | step: 64/422 | loss: 4.523361682891846\n",
      "Epoch: 6/100 | step: 65/422 | loss: 4.52449893951416\n",
      "Epoch: 6/100 | step: 66/422 | loss: 4.5594482421875\n",
      "Epoch: 6/100 | step: 67/422 | loss: 4.5771660804748535\n",
      "Epoch: 6/100 | step: 68/422 | loss: 4.52234411239624\n",
      "Epoch: 6/100 | step: 69/422 | loss: 4.572173118591309\n",
      "Epoch: 6/100 | step: 70/422 | loss: 4.5280046463012695\n",
      "Epoch: 6/100 | step: 71/422 | loss: 4.5405192375183105\n",
      "Epoch: 6/100 | step: 72/422 | loss: 4.519770622253418\n",
      "Epoch: 6/100 | step: 73/422 | loss: 4.545083045959473\n",
      "Epoch: 6/100 | step: 74/422 | loss: 4.503723621368408\n",
      "Epoch: 6/100 | step: 75/422 | loss: 4.57231330871582\n",
      "Epoch: 6/100 | step: 76/422 | loss: 4.557086944580078\n",
      "Epoch: 6/100 | step: 77/422 | loss: 4.538703918457031\n",
      "Epoch: 6/100 | step: 78/422 | loss: 4.497133255004883\n",
      "Epoch: 6/100 | step: 79/422 | loss: 4.523252010345459\n",
      "Epoch: 6/100 | step: 80/422 | loss: 4.529401779174805\n",
      "Epoch: 6/100 | step: 81/422 | loss: 4.513130187988281\n",
      "Epoch: 6/100 | step: 82/422 | loss: 4.506924152374268\n",
      "Epoch: 6/100 | step: 83/422 | loss: 4.569876194000244\n",
      "Epoch: 6/100 | step: 84/422 | loss: 4.528273105621338\n",
      "Epoch: 6/100 | step: 85/422 | loss: 4.530023574829102\n",
      "Epoch: 6/100 | step: 86/422 | loss: 4.589076519012451\n",
      "Epoch: 6/100 | step: 87/422 | loss: 4.564491271972656\n",
      "Epoch: 6/100 | step: 88/422 | loss: 4.514188766479492\n",
      "Epoch: 6/100 | step: 89/422 | loss: 4.522594451904297\n",
      "Epoch: 6/100 | step: 90/422 | loss: 4.486809730529785\n",
      "Epoch: 6/100 | step: 91/422 | loss: 4.546565055847168\n",
      "Epoch: 6/100 | step: 92/422 | loss: 4.533067226409912\n",
      "Epoch: 6/100 | step: 93/422 | loss: 4.531106948852539\n",
      "Epoch: 6/100 | step: 94/422 | loss: 4.534594535827637\n",
      "Epoch: 6/100 | step: 95/422 | loss: 4.513143062591553\n",
      "Epoch: 6/100 | step: 96/422 | loss: 4.504875183105469\n",
      "Epoch: 6/100 | step: 97/422 | loss: 4.5597357749938965\n",
      "Epoch: 6/100 | step: 98/422 | loss: 4.555910587310791\n",
      "Epoch: 6/100 | step: 99/422 | loss: 4.530025005340576\n",
      "Epoch: 6/100 | step: 100/422 | loss: 4.525378227233887\n",
      "Epoch: 6/100 | step: 101/422 | loss: 4.582964897155762\n",
      "Epoch: 6/100 | step: 102/422 | loss: 4.492640495300293\n",
      "Epoch: 6/100 | step: 103/422 | loss: 4.568033695220947\n",
      "Epoch: 6/100 | step: 104/422 | loss: 4.540217876434326\n",
      "Epoch: 6/100 | step: 105/422 | loss: 4.510647773742676\n",
      "Epoch: 6/100 | step: 106/422 | loss: 4.56178092956543\n",
      "Epoch: 6/100 | step: 107/422 | loss: 4.509730339050293\n",
      "Epoch: 6/100 | step: 108/422 | loss: 4.496641159057617\n",
      "Epoch: 6/100 | step: 109/422 | loss: 4.555206298828125\n",
      "Epoch: 6/100 | step: 110/422 | loss: 4.551423072814941\n",
      "Epoch: 6/100 | step: 111/422 | loss: 4.522528171539307\n",
      "Epoch: 6/100 | step: 112/422 | loss: 4.524256229400635\n",
      "Epoch: 6/100 | step: 113/422 | loss: 4.572994709014893\n",
      "Epoch: 6/100 | step: 114/422 | loss: 4.5170722007751465\n",
      "Epoch: 6/100 | step: 115/422 | loss: 4.557773113250732\n",
      "Epoch: 6/100 | step: 116/422 | loss: 4.5827507972717285\n",
      "Epoch: 6/100 | step: 117/422 | loss: 4.502947807312012\n",
      "Epoch: 6/100 | step: 118/422 | loss: 4.4984307289123535\n",
      "Epoch: 6/100 | step: 119/422 | loss: 4.491652011871338\n",
      "Epoch: 6/100 | step: 120/422 | loss: 4.517299652099609\n",
      "Epoch: 6/100 | step: 121/422 | loss: 4.477646350860596\n",
      "Epoch: 6/100 | step: 122/422 | loss: 4.583317756652832\n",
      "Epoch: 6/100 | step: 123/422 | loss: 4.545533180236816\n",
      "Epoch: 6/100 | step: 124/422 | loss: 4.504232883453369\n",
      "Epoch: 6/100 | step: 125/422 | loss: 4.527707576751709\n",
      "Epoch: 6/100 | step: 126/422 | loss: 4.5227813720703125\n",
      "Epoch: 6/100 | step: 127/422 | loss: 4.5661773681640625\n",
      "Epoch: 6/100 | step: 128/422 | loss: 4.455409526824951\n",
      "Epoch: 6/100 | step: 129/422 | loss: 4.51023006439209\n",
      "Epoch: 6/100 | step: 130/422 | loss: 4.5276007652282715\n",
      "Epoch: 6/100 | step: 131/422 | loss: 4.56087589263916\n",
      "Epoch: 6/100 | step: 132/422 | loss: 4.520071506500244\n",
      "Epoch: 6/100 | step: 133/422 | loss: 4.505343437194824\n",
      "Epoch: 6/100 | step: 134/422 | loss: 4.576025009155273\n",
      "Epoch: 6/100 | step: 135/422 | loss: 4.548421859741211\n",
      "Epoch: 6/100 | step: 136/422 | loss: 4.561686038970947\n",
      "Epoch: 6/100 | step: 137/422 | loss: 4.564502239227295\n",
      "Epoch: 6/100 | step: 138/422 | loss: 4.557344436645508\n",
      "Epoch: 6/100 | step: 139/422 | loss: 4.520946502685547\n",
      "Epoch: 6/100 | step: 140/422 | loss: 4.563648700714111\n",
      "Epoch: 6/100 | step: 141/422 | loss: 4.514102458953857\n",
      "Epoch: 6/100 | step: 142/422 | loss: 4.551848411560059\n",
      "Epoch: 6/100 | step: 143/422 | loss: 4.524051189422607\n",
      "Epoch: 6/100 | step: 144/422 | loss: 4.5143280029296875\n",
      "Epoch: 6/100 | step: 145/422 | loss: 4.5638628005981445\n",
      "Epoch: 6/100 | step: 146/422 | loss: 4.545577526092529\n",
      "Epoch: 6/100 | step: 147/422 | loss: 4.5566182136535645\n",
      "Epoch: 6/100 | step: 148/422 | loss: 4.553119659423828\n",
      "Epoch: 6/100 | step: 149/422 | loss: 4.472624778747559\n",
      "Epoch: 6/100 | step: 150/422 | loss: 4.572744369506836\n",
      "Epoch: 6/100 | step: 151/422 | loss: 4.4884419441223145\n",
      "Epoch: 6/100 | step: 152/422 | loss: 4.545197010040283\n",
      "Epoch: 6/100 | step: 153/422 | loss: 4.566168308258057\n",
      "Epoch: 6/100 | step: 154/422 | loss: 4.486065864562988\n",
      "Epoch: 6/100 | step: 155/422 | loss: 4.461277008056641\n",
      "Epoch: 6/100 | step: 156/422 | loss: 4.546908378601074\n",
      "Epoch: 6/100 | step: 157/422 | loss: 4.516817092895508\n",
      "Epoch: 6/100 | step: 158/422 | loss: 4.51986026763916\n",
      "Epoch: 6/100 | step: 159/422 | loss: 4.526193618774414\n",
      "Epoch: 6/100 | step: 160/422 | loss: 4.474103927612305\n",
      "Epoch: 6/100 | step: 161/422 | loss: 4.458213806152344\n",
      "Epoch: 6/100 | step: 162/422 | loss: 4.47923469543457\n",
      "Epoch: 6/100 | step: 163/422 | loss: 4.531559944152832\n",
      "Epoch: 6/100 | step: 164/422 | loss: 4.538312911987305\n",
      "Epoch: 6/100 | step: 165/422 | loss: 4.509578704833984\n",
      "Epoch: 6/100 | step: 166/422 | loss: 4.569849967956543\n",
      "Epoch: 6/100 | step: 167/422 | loss: 4.498764991760254\n",
      "Epoch: 6/100 | step: 168/422 | loss: 4.539670467376709\n",
      "Epoch: 6/100 | step: 169/422 | loss: 4.492529392242432\n",
      "Epoch: 6/100 | step: 170/422 | loss: 4.547123908996582\n",
      "Epoch: 6/100 | step: 171/422 | loss: 4.569774627685547\n",
      "Epoch: 6/100 | step: 172/422 | loss: 4.485788345336914\n",
      "Epoch: 6/100 | step: 173/422 | loss: 4.546691417694092\n",
      "Epoch: 6/100 | step: 174/422 | loss: 4.498528003692627\n",
      "Epoch: 6/100 | step: 175/422 | loss: 4.585057258605957\n",
      "Epoch: 6/100 | step: 176/422 | loss: 4.513518333435059\n",
      "Epoch: 6/100 | step: 177/422 | loss: 4.523676872253418\n",
      "Epoch: 6/100 | step: 178/422 | loss: 4.529908180236816\n",
      "Epoch: 6/100 | step: 179/422 | loss: 4.569407939910889\n",
      "Epoch: 6/100 | step: 180/422 | loss: 4.518966197967529\n",
      "Epoch: 6/100 | step: 181/422 | loss: 4.531433582305908\n",
      "Epoch: 6/100 | step: 182/422 | loss: 4.523126125335693\n",
      "Epoch: 6/100 | step: 183/422 | loss: 4.525086879730225\n",
      "Epoch: 6/100 | step: 184/422 | loss: 4.543377876281738\n",
      "Epoch: 6/100 | step: 185/422 | loss: 4.550161838531494\n",
      "Epoch: 6/100 | step: 186/422 | loss: 4.475827693939209\n",
      "Epoch: 6/100 | step: 187/422 | loss: 4.522979736328125\n",
      "Epoch: 6/100 | step: 188/422 | loss: 4.551188945770264\n",
      "Epoch: 6/100 | step: 189/422 | loss: 4.5066609382629395\n",
      "Epoch: 6/100 | step: 190/422 | loss: 4.500457763671875\n",
      "Epoch: 6/100 | step: 191/422 | loss: 4.500874996185303\n",
      "Epoch: 6/100 | step: 192/422 | loss: 4.514217376708984\n",
      "Epoch: 6/100 | step: 193/422 | loss: 4.49228048324585\n",
      "Epoch: 6/100 | step: 194/422 | loss: 4.512561798095703\n",
      "Epoch: 6/100 | step: 195/422 | loss: 4.539484024047852\n",
      "Epoch: 6/100 | step: 196/422 | loss: 4.545366287231445\n",
      "Epoch: 6/100 | step: 197/422 | loss: 4.564265251159668\n",
      "Epoch: 6/100 | step: 198/422 | loss: 4.475142478942871\n",
      "Epoch: 6/100 | step: 199/422 | loss: 4.578375339508057\n",
      "Epoch: 6/100 | step: 200/422 | loss: 4.529682159423828\n",
      "Epoch: 6/100 | step: 201/422 | loss: 4.568016052246094\n",
      "Epoch: 6/100 | step: 202/422 | loss: 4.5261759757995605\n",
      "Epoch: 6/100 | step: 203/422 | loss: 4.537734031677246\n",
      "Epoch: 6/100 | step: 204/422 | loss: 4.60964298248291\n",
      "Epoch: 6/100 | step: 205/422 | loss: 4.56189489364624\n",
      "Epoch: 6/100 | step: 206/422 | loss: 4.4999260902404785\n",
      "Epoch: 6/100 | step: 207/422 | loss: 4.4922308921813965\n",
      "Epoch: 6/100 | step: 208/422 | loss: 4.535184383392334\n",
      "Epoch: 6/100 | step: 209/422 | loss: 4.458202838897705\n",
      "Epoch: 6/100 | step: 210/422 | loss: 4.557614326477051\n",
      "Epoch: 6/100 | step: 211/422 | loss: 4.552289009094238\n",
      "Epoch: 6/100 | step: 212/422 | loss: 4.561675548553467\n",
      "Epoch: 6/100 | step: 213/422 | loss: 4.502950191497803\n",
      "Epoch: 6/100 | step: 214/422 | loss: 4.515040397644043\n",
      "Epoch: 6/100 | step: 215/422 | loss: 4.4545087814331055\n",
      "Epoch: 6/100 | step: 216/422 | loss: 4.535877227783203\n",
      "Epoch: 6/100 | step: 217/422 | loss: 4.484469890594482\n",
      "Epoch: 6/100 | step: 218/422 | loss: 4.549986362457275\n",
      "Epoch: 6/100 | step: 219/422 | loss: 4.518886566162109\n",
      "Epoch: 6/100 | step: 220/422 | loss: 4.4769392013549805\n",
      "Epoch: 6/100 | step: 221/422 | loss: 4.573286056518555\n",
      "Epoch: 6/100 | step: 222/422 | loss: 4.463394641876221\n",
      "Epoch: 6/100 | step: 223/422 | loss: 4.461542129516602\n",
      "Epoch: 6/100 | step: 224/422 | loss: 4.518226146697998\n",
      "Epoch: 6/100 | step: 225/422 | loss: 4.489802360534668\n",
      "Epoch: 6/100 | step: 226/422 | loss: 4.501962661743164\n",
      "Epoch: 6/100 | step: 227/422 | loss: 4.548626899719238\n",
      "Epoch: 6/100 | step: 228/422 | loss: 4.515742778778076\n",
      "Epoch: 6/100 | step: 229/422 | loss: 4.465152740478516\n",
      "Epoch: 6/100 | step: 230/422 | loss: 4.4699883460998535\n",
      "Epoch: 6/100 | step: 231/422 | loss: 4.582404136657715\n",
      "Epoch: 6/100 | step: 232/422 | loss: 4.458192348480225\n",
      "Epoch: 6/100 | step: 233/422 | loss: 4.513096809387207\n",
      "Epoch: 6/100 | step: 234/422 | loss: 4.480825424194336\n",
      "Epoch: 6/100 | step: 235/422 | loss: 4.495074272155762\n",
      "Epoch: 6/100 | step: 236/422 | loss: 4.484140396118164\n",
      "Epoch: 6/100 | step: 237/422 | loss: 4.560260772705078\n",
      "Epoch: 6/100 | step: 238/422 | loss: 4.504437446594238\n",
      "Epoch: 6/100 | step: 239/422 | loss: 4.5308685302734375\n",
      "Epoch: 6/100 | step: 240/422 | loss: 4.53754186630249\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 7/100 | step: 1/422 | loss: 4.484606742858887\n",
      "Epoch: 7/100 | step: 2/422 | loss: 4.487856864929199\n",
      "Epoch: 7/100 | step: 3/422 | loss: 4.542175769805908\n",
      "Epoch: 7/100 | step: 4/422 | loss: 4.519456386566162\n",
      "Epoch: 7/100 | step: 5/422 | loss: 4.519360065460205\n",
      "Epoch: 7/100 | step: 6/422 | loss: 4.480898380279541\n",
      "Epoch: 7/100 | step: 7/422 | loss: 4.446852207183838\n",
      "Epoch: 7/100 | step: 8/422 | loss: 4.5234503746032715\n",
      "Epoch: 7/100 | step: 9/422 | loss: 4.562411785125732\n",
      "Epoch: 7/100 | step: 10/422 | loss: 4.497045516967773\n",
      "Epoch: 7/100 | step: 11/422 | loss: 4.491304874420166\n",
      "Epoch: 7/100 | step: 12/422 | loss: 4.491710186004639\n",
      "Epoch: 7/100 | step: 13/422 | loss: 4.53671407699585\n",
      "Epoch: 7/100 | step: 14/422 | loss: 4.526074409484863\n",
      "Epoch: 7/100 | step: 15/422 | loss: 4.54691219329834\n",
      "Epoch: 7/100 | step: 16/422 | loss: 4.518942832946777\n",
      "Epoch: 7/100 | step: 17/422 | loss: 4.496242523193359\n",
      "Epoch: 7/100 | step: 18/422 | loss: 4.478306293487549\n",
      "Epoch: 7/100 | step: 19/422 | loss: 4.503990173339844\n",
      "Epoch: 7/100 | step: 20/422 | loss: 4.505342960357666\n",
      "Epoch: 7/100 | step: 21/422 | loss: 4.474688529968262\n",
      "Epoch: 7/100 | step: 22/422 | loss: 4.516434192657471\n",
      "Epoch: 7/100 | step: 23/422 | loss: 4.515883922576904\n",
      "Epoch: 7/100 | step: 24/422 | loss: 4.527053356170654\n",
      "Epoch: 7/100 | step: 25/422 | loss: 4.463647842407227\n",
      "Epoch: 7/100 | step: 26/422 | loss: 4.509189605712891\n",
      "Epoch: 7/100 | step: 27/422 | loss: 4.502013206481934\n",
      "Epoch: 7/100 | step: 28/422 | loss: 4.537768363952637\n",
      "Epoch: 7/100 | step: 29/422 | loss: 4.506939888000488\n",
      "Epoch: 7/100 | step: 30/422 | loss: 4.503876686096191\n",
      "Epoch: 7/100 | step: 31/422 | loss: 4.491928577423096\n",
      "Epoch: 7/100 | step: 32/422 | loss: 4.5282301902771\n",
      "Epoch: 7/100 | step: 33/422 | loss: 4.561671733856201\n",
      "Epoch: 7/100 | step: 34/422 | loss: 4.492143154144287\n",
      "Epoch: 7/100 | step: 35/422 | loss: 4.490505218505859\n",
      "Epoch: 7/100 | step: 36/422 | loss: 4.505656719207764\n",
      "Epoch: 7/100 | step: 37/422 | loss: 4.529647350311279\n",
      "Epoch: 7/100 | step: 38/422 | loss: 4.503968715667725\n",
      "Epoch: 7/100 | step: 39/422 | loss: 4.503675937652588\n",
      "Epoch: 7/100 | step: 40/422 | loss: 4.507336139678955\n",
      "Epoch: 7/100 | step: 41/422 | loss: 4.492615222930908\n",
      "Epoch: 7/100 | step: 42/422 | loss: 4.5329132080078125\n",
      "Epoch: 7/100 | step: 43/422 | loss: 4.475290775299072\n",
      "Epoch: 7/100 | step: 44/422 | loss: 4.4957499504089355\n",
      "Epoch: 7/100 | step: 45/422 | loss: 4.6298346519470215\n",
      "Epoch: 7/100 | step: 46/422 | loss: 4.466914176940918\n",
      "Epoch: 7/100 | step: 47/422 | loss: 4.53657341003418\n",
      "Epoch: 7/100 | step: 48/422 | loss: 4.509482383728027\n",
      "Epoch: 7/100 | step: 49/422 | loss: 4.447738170623779\n",
      "Epoch: 7/100 | step: 50/422 | loss: 4.453672885894775\n",
      "Epoch: 7/100 | step: 51/422 | loss: 4.562405109405518\n",
      "Epoch: 7/100 | step: 52/422 | loss: 4.525370121002197\n",
      "Epoch: 7/100 | step: 53/422 | loss: 4.46799898147583\n",
      "Epoch: 7/100 | step: 54/422 | loss: 4.432575702667236\n",
      "Epoch: 7/100 | step: 55/422 | loss: 4.518660545349121\n",
      "Epoch: 7/100 | step: 56/422 | loss: 4.525310516357422\n",
      "Epoch: 7/100 | step: 57/422 | loss: 4.52233362197876\n",
      "Epoch: 7/100 | step: 58/422 | loss: 4.509313106536865\n",
      "Epoch: 7/100 | step: 59/422 | loss: 4.5004682540893555\n",
      "Epoch: 7/100 | step: 60/422 | loss: 4.4875359535217285\n",
      "Epoch: 7/100 | step: 61/422 | loss: 4.488563537597656\n",
      "Epoch: 7/100 | step: 62/422 | loss: 4.450369358062744\n",
      "Epoch: 7/100 | step: 63/422 | loss: 4.477708339691162\n",
      "Epoch: 7/100 | step: 64/422 | loss: 4.559684753417969\n",
      "Epoch: 7/100 | step: 65/422 | loss: 4.461657524108887\n",
      "Epoch: 7/100 | step: 66/422 | loss: 4.487540245056152\n",
      "Epoch: 7/100 | step: 67/422 | loss: 4.47351598739624\n",
      "Epoch: 7/100 | step: 68/422 | loss: 4.47958517074585\n",
      "Epoch: 7/100 | step: 69/422 | loss: 4.458612442016602\n",
      "Epoch: 7/100 | step: 70/422 | loss: 4.455916881561279\n",
      "Epoch: 7/100 | step: 71/422 | loss: 4.4477410316467285\n",
      "Epoch: 7/100 | step: 72/422 | loss: 4.414780616760254\n",
      "Epoch: 7/100 | step: 73/422 | loss: 4.517585277557373\n",
      "Epoch: 7/100 | step: 74/422 | loss: 4.495290279388428\n",
      "Epoch: 7/100 | step: 75/422 | loss: 4.525226593017578\n",
      "Epoch: 7/100 | step: 76/422 | loss: 4.487360000610352\n",
      "Epoch: 7/100 | step: 77/422 | loss: 4.432921409606934\n",
      "Epoch: 7/100 | step: 78/422 | loss: 4.521678924560547\n",
      "Epoch: 7/100 | step: 79/422 | loss: 4.5018510818481445\n",
      "Epoch: 7/100 | step: 80/422 | loss: 4.491990089416504\n",
      "Epoch: 7/100 | step: 81/422 | loss: 4.525584697723389\n",
      "Epoch: 7/100 | step: 82/422 | loss: 4.419350624084473\n",
      "Epoch: 7/100 | step: 83/422 | loss: 4.463205337524414\n",
      "Epoch: 7/100 | step: 84/422 | loss: 4.418300628662109\n",
      "Epoch: 7/100 | step: 85/422 | loss: 4.514046669006348\n",
      "Epoch: 7/100 | step: 86/422 | loss: 4.471036434173584\n",
      "Epoch: 7/100 | step: 87/422 | loss: 4.475000381469727\n",
      "Epoch: 7/100 | step: 88/422 | loss: 4.447970390319824\n",
      "Epoch: 7/100 | step: 89/422 | loss: 4.464480876922607\n",
      "Epoch: 7/100 | step: 90/422 | loss: 4.534180641174316\n",
      "Epoch: 7/100 | step: 91/422 | loss: 4.480412483215332\n",
      "Epoch: 7/100 | step: 92/422 | loss: 4.451404094696045\n",
      "Epoch: 7/100 | step: 93/422 | loss: 4.492705821990967\n",
      "Epoch: 7/100 | step: 94/422 | loss: 4.477931976318359\n",
      "Epoch: 7/100 | step: 95/422 | loss: 4.530419826507568\n",
      "Epoch: 7/100 | step: 96/422 | loss: 4.562653064727783\n",
      "Epoch: 7/100 | step: 97/422 | loss: 4.446666240692139\n",
      "Epoch: 7/100 | step: 98/422 | loss: 4.523796081542969\n",
      "Epoch: 7/100 | step: 99/422 | loss: 4.458489894866943\n",
      "Epoch: 7/100 | step: 100/422 | loss: 4.4615068435668945\n",
      "Epoch: 7/100 | step: 101/422 | loss: 4.517807483673096\n",
      "Epoch: 7/100 | step: 102/422 | loss: 4.491815090179443\n",
      "Epoch: 7/100 | step: 103/422 | loss: 4.392744064331055\n",
      "Epoch: 7/100 | step: 104/422 | loss: 4.451633930206299\n",
      "Epoch: 7/100 | step: 105/422 | loss: 4.487746238708496\n",
      "Epoch: 7/100 | step: 106/422 | loss: 4.438776969909668\n",
      "Epoch: 7/100 | step: 107/422 | loss: 4.465573310852051\n",
      "Epoch: 7/100 | step: 108/422 | loss: 4.461548328399658\n",
      "Epoch: 7/100 | step: 109/422 | loss: 4.472273826599121\n",
      "Epoch: 7/100 | step: 110/422 | loss: 4.467016696929932\n",
      "Epoch: 7/100 | step: 111/422 | loss: 4.481929302215576\n",
      "Epoch: 7/100 | step: 112/422 | loss: 4.458921909332275\n",
      "Epoch: 7/100 | step: 113/422 | loss: 4.4642415046691895\n",
      "Epoch: 7/100 | step: 114/422 | loss: 4.50282096862793\n",
      "Epoch: 7/100 | step: 115/422 | loss: 4.502651214599609\n",
      "Epoch: 7/100 | step: 116/422 | loss: 4.490461826324463\n",
      "Epoch: 7/100 | step: 117/422 | loss: 4.470464706420898\n",
      "Epoch: 7/100 | step: 118/422 | loss: 4.52031946182251\n",
      "Epoch: 7/100 | step: 119/422 | loss: 4.452948570251465\n",
      "Epoch: 7/100 | step: 120/422 | loss: 4.43345832824707\n",
      "Epoch: 7/100 | step: 121/422 | loss: 4.521549701690674\n",
      "Epoch: 7/100 | step: 122/422 | loss: 4.509514808654785\n",
      "Epoch: 7/100 | step: 123/422 | loss: 4.403769493103027\n",
      "Epoch: 7/100 | step: 124/422 | loss: 4.451448440551758\n",
      "Epoch: 7/100 | step: 125/422 | loss: 4.5022711753845215\n",
      "Epoch: 7/100 | step: 126/422 | loss: 4.428086280822754\n",
      "Epoch: 7/100 | step: 127/422 | loss: 4.524399757385254\n",
      "Epoch: 7/100 | step: 128/422 | loss: 4.5412421226501465\n",
      "Epoch: 7/100 | step: 129/422 | loss: 4.458500862121582\n",
      "Epoch: 7/100 | step: 130/422 | loss: 4.437966346740723\n",
      "Epoch: 7/100 | step: 131/422 | loss: 4.411704063415527\n",
      "Epoch: 7/100 | step: 132/422 | loss: 4.5213422775268555\n",
      "Epoch: 7/100 | step: 133/422 | loss: 4.558365821838379\n",
      "Epoch: 7/100 | step: 134/422 | loss: 4.470329284667969\n",
      "Epoch: 7/100 | step: 135/422 | loss: 4.451049327850342\n",
      "Epoch: 7/100 | step: 136/422 | loss: 4.5773024559021\n",
      "Epoch: 7/100 | step: 137/422 | loss: 4.5102152824401855\n",
      "Epoch: 7/100 | step: 138/422 | loss: 4.542150497436523\n",
      "Epoch: 7/100 | step: 139/422 | loss: 4.568458080291748\n",
      "Epoch: 7/100 | step: 140/422 | loss: 4.533169746398926\n",
      "Epoch: 7/100 | step: 141/422 | loss: 4.476048469543457\n",
      "Epoch: 7/100 | step: 142/422 | loss: 4.460158824920654\n",
      "Epoch: 7/100 | step: 143/422 | loss: 4.503022193908691\n",
      "Epoch: 7/100 | step: 144/422 | loss: 4.5171332359313965\n",
      "Epoch: 7/100 | step: 145/422 | loss: 4.478028297424316\n",
      "Epoch: 7/100 | step: 146/422 | loss: 4.444762706756592\n",
      "Epoch: 7/100 | step: 147/422 | loss: 4.485262870788574\n",
      "Epoch: 7/100 | step: 148/422 | loss: 4.46905517578125\n",
      "Epoch: 7/100 | step: 149/422 | loss: 4.415825366973877\n",
      "Epoch: 7/100 | step: 150/422 | loss: 4.5685601234436035\n",
      "Epoch: 7/100 | step: 151/422 | loss: 4.463038921356201\n",
      "Epoch: 7/100 | step: 152/422 | loss: 4.510467052459717\n",
      "Epoch: 7/100 | step: 153/422 | loss: 4.3594584465026855\n",
      "Epoch: 7/100 | step: 154/422 | loss: 4.496906757354736\n",
      "Epoch: 7/100 | step: 155/422 | loss: 4.506367206573486\n",
      "Epoch: 7/100 | step: 156/422 | loss: 4.465770721435547\n",
      "Epoch: 7/100 | step: 157/422 | loss: 4.469165325164795\n",
      "Epoch: 7/100 | step: 158/422 | loss: 4.45062255859375\n",
      "Epoch: 7/100 | step: 159/422 | loss: 4.440152168273926\n",
      "Epoch: 7/100 | step: 160/422 | loss: 4.4770026206970215\n",
      "Epoch: 7/100 | step: 161/422 | loss: 4.519632339477539\n",
      "Epoch: 7/100 | step: 162/422 | loss: 4.454353332519531\n",
      "Epoch: 7/100 | step: 163/422 | loss: 4.509169101715088\n",
      "Epoch: 7/100 | step: 164/422 | loss: 4.439612865447998\n",
      "Epoch: 7/100 | step: 165/422 | loss: 4.515053749084473\n",
      "Epoch: 7/100 | step: 166/422 | loss: 4.50191068649292\n",
      "Epoch: 7/100 | step: 167/422 | loss: 4.590402126312256\n",
      "Epoch: 7/100 | step: 168/422 | loss: 4.538015365600586\n",
      "Epoch: 7/100 | step: 169/422 | loss: 4.4284186363220215\n",
      "Epoch: 7/100 | step: 170/422 | loss: 4.45170783996582\n",
      "Epoch: 7/100 | step: 171/422 | loss: 4.398087978363037\n",
      "Epoch: 7/100 | step: 172/422 | loss: 4.478652477264404\n",
      "Epoch: 7/100 | step: 173/422 | loss: 4.41810941696167\n",
      "Epoch: 7/100 | step: 174/422 | loss: 4.464569568634033\n",
      "Epoch: 7/100 | step: 175/422 | loss: 4.5001983642578125\n",
      "Epoch: 7/100 | step: 176/422 | loss: 4.472238540649414\n",
      "Epoch: 7/100 | step: 177/422 | loss: 4.472222328186035\n",
      "Epoch: 7/100 | step: 178/422 | loss: 4.447718143463135\n",
      "Epoch: 7/100 | step: 179/422 | loss: 4.543838024139404\n",
      "Epoch: 7/100 | step: 180/422 | loss: 4.410367965698242\n",
      "Epoch: 7/100 | step: 181/422 | loss: 4.502534866333008\n",
      "Epoch: 7/100 | step: 182/422 | loss: 4.476724624633789\n",
      "Epoch: 7/100 | step: 183/422 | loss: 4.521491050720215\n",
      "Epoch: 7/100 | step: 184/422 | loss: 4.541301250457764\n",
      "Epoch: 7/100 | step: 185/422 | loss: 4.396080493927002\n",
      "Epoch: 7/100 | step: 186/422 | loss: 4.445530891418457\n",
      "Epoch: 7/100 | step: 187/422 | loss: 4.447109699249268\n",
      "Epoch: 7/100 | step: 188/422 | loss: 4.432072639465332\n",
      "Epoch: 7/100 | step: 189/422 | loss: 4.422290802001953\n",
      "Epoch: 7/100 | step: 190/422 | loss: 4.453081130981445\n",
      "Epoch: 7/100 | step: 191/422 | loss: 4.514680862426758\n",
      "Epoch: 7/100 | step: 192/422 | loss: 4.423605442047119\n",
      "Epoch: 7/100 | step: 193/422 | loss: 4.493728160858154\n",
      "Epoch: 7/100 | step: 194/422 | loss: 4.488097667694092\n",
      "Epoch: 7/100 | step: 195/422 | loss: 4.4518818855285645\n",
      "Epoch: 7/100 | step: 196/422 | loss: 4.505836486816406\n",
      "Epoch: 7/100 | step: 197/422 | loss: 4.511493682861328\n",
      "Epoch: 7/100 | step: 198/422 | loss: 4.429548263549805\n",
      "Epoch: 7/100 | step: 199/422 | loss: 4.480668067932129\n",
      "Epoch: 7/100 | step: 200/422 | loss: 4.353291988372803\n",
      "Epoch: 7/100 | step: 201/422 | loss: 4.471678733825684\n",
      "Epoch: 7/100 | step: 202/422 | loss: 4.43195104598999\n",
      "Epoch: 7/100 | step: 203/422 | loss: 4.480470657348633\n",
      "Epoch: 7/100 | step: 204/422 | loss: 4.4406585693359375\n",
      "Epoch: 7/100 | step: 205/422 | loss: 4.3918986320495605\n",
      "Epoch: 7/100 | step: 206/422 | loss: 4.447575092315674\n",
      "Epoch: 7/100 | step: 207/422 | loss: 4.47755765914917\n",
      "Epoch: 7/100 | step: 208/422 | loss: 4.397583961486816\n",
      "Epoch: 7/100 | step: 209/422 | loss: 4.385033130645752\n",
      "Epoch: 7/100 | step: 210/422 | loss: 4.502274990081787\n",
      "Epoch: 7/100 | step: 211/422 | loss: 4.482316970825195\n",
      "Epoch: 7/100 | step: 212/422 | loss: 4.46315860748291\n",
      "Epoch: 7/100 | step: 213/422 | loss: 4.456601619720459\n",
      "Epoch: 7/100 | step: 214/422 | loss: 4.522770881652832\n",
      "Epoch: 7/100 | step: 215/422 | loss: 4.507038116455078\n",
      "Epoch: 7/100 | step: 216/422 | loss: 4.422574996948242\n",
      "Epoch: 7/100 | step: 217/422 | loss: 4.458353042602539\n",
      "Epoch: 7/100 | step: 218/422 | loss: 4.436554908752441\n",
      "Epoch: 7/100 | step: 219/422 | loss: 4.4677228927612305\n",
      "Epoch: 7/100 | step: 220/422 | loss: 4.464313507080078\n",
      "Epoch: 7/100 | step: 221/422 | loss: 4.474020004272461\n",
      "Epoch: 7/100 | step: 222/422 | loss: 4.545221328735352\n",
      "Epoch: 7/100 | step: 223/422 | loss: 4.3905510902404785\n",
      "Epoch: 7/100 | step: 224/422 | loss: 4.394987106323242\n",
      "Epoch: 7/100 | step: 225/422 | loss: 4.36892557144165\n",
      "Epoch: 7/100 | step: 226/422 | loss: 4.450968265533447\n",
      "Epoch: 7/100 | step: 227/422 | loss: 4.511297225952148\n",
      "Epoch: 7/100 | step: 228/422 | loss: 4.37922477722168\n",
      "Epoch: 7/100 | step: 229/422 | loss: 4.4376068115234375\n",
      "Epoch: 7/100 | step: 230/422 | loss: 4.488590717315674\n",
      "Epoch: 7/100 | step: 231/422 | loss: 4.447091579437256\n",
      "Epoch: 7/100 | step: 232/422 | loss: 4.532855987548828\n",
      "Epoch: 7/100 | step: 233/422 | loss: 4.565230369567871\n",
      "Epoch: 7/100 | step: 234/422 | loss: 4.344579696655273\n",
      "Epoch: 7/100 | step: 235/422 | loss: 4.561649322509766\n",
      "Epoch: 7/100 | step: 236/422 | loss: 4.513662338256836\n",
      "Epoch: 7/100 | step: 237/422 | loss: 4.530896186828613\n",
      "Epoch: 7/100 | step: 238/422 | loss: 4.43479061126709\n",
      "Epoch: 7/100 | step: 239/422 | loss: 4.412049770355225\n",
      "Epoch: 7/100 | step: 240/422 | loss: 4.540096282958984\n",
      "Epoch: 7/100 | step: 241/422 | loss: 4.527107238769531\n",
      "Epoch: 7/100 | step: 242/422 | loss: 4.454237461090088\n",
      "Epoch: 7/100 | step: 243/422 | loss: 4.395817279815674\n",
      "Epoch: 7/100 | step: 244/422 | loss: 4.560908317565918\n",
      "Epoch: 7/100 | step: 245/422 | loss: 4.3788886070251465\n",
      "Epoch: 7/100 | step: 246/422 | loss: 4.415137767791748\n",
      "Epoch: 7/100 | step: 247/422 | loss: 4.380962371826172\n",
      "Epoch: 7/100 | step: 248/422 | loss: 4.415766716003418\n",
      "Epoch: 7/100 | step: 249/422 | loss: 4.432626247406006\n",
      "Epoch: 7/100 | step: 250/422 | loss: 4.465588569641113\n",
      "Epoch: 7/100 | step: 251/422 | loss: 4.387883186340332\n",
      "Epoch: 7/100 | step: 252/422 | loss: 4.466823101043701\n",
      "Epoch: 7/100 | step: 253/422 | loss: 4.384577751159668\n",
      "Epoch: 7/100 | step: 254/422 | loss: 4.454217433929443\n",
      "Epoch: 7/100 | step: 255/422 | loss: 4.478509902954102\n",
      "Epoch: 7/100 | step: 256/422 | loss: 4.420220375061035\n",
      "Epoch: 7/100 | step: 257/422 | loss: 4.429625511169434\n",
      "Epoch: 7/100 | step: 258/422 | loss: 4.435368061065674\n",
      "Epoch: 7/100 | step: 259/422 | loss: 4.498272895812988\n",
      "Epoch: 7/100 | step: 260/422 | loss: 4.406065940856934\n",
      "Epoch: 7/100 | step: 261/422 | loss: 4.423441410064697\n",
      "Epoch: 7/100 | step: 262/422 | loss: 4.423314094543457\n",
      "Epoch: 7/100 | step: 263/422 | loss: 4.53123664855957\n",
      "Epoch: 7/100 | step: 264/422 | loss: 4.460235118865967\n",
      "Epoch: 7/100 | step: 265/422 | loss: 4.465487480163574\n",
      "Epoch: 7/100 | step: 266/422 | loss: 4.474653244018555\n",
      "Epoch: 7/100 | step: 267/422 | loss: 4.518939018249512\n",
      "Epoch: 7/100 | step: 268/422 | loss: 4.427758693695068\n",
      "Epoch: 7/100 | step: 269/422 | loss: 4.506830215454102\n",
      "Epoch: 7/100 | step: 270/422 | loss: 4.500549793243408\n",
      "Epoch: 7/100 | step: 271/422 | loss: 4.491213798522949\n",
      "Epoch: 7/100 | step: 272/422 | loss: 4.460286617279053\n",
      "Epoch: 7/100 | step: 273/422 | loss: 4.398083686828613\n",
      "Epoch: 7/100 | step: 274/422 | loss: 4.420661449432373\n",
      "Epoch: 7/100 | step: 275/422 | loss: 4.550950527191162\n",
      "Epoch: 7/100 | step: 276/422 | loss: 4.552964210510254\n",
      "Epoch: 7/100 | step: 277/422 | loss: 4.485278606414795\n",
      "Epoch: 7/100 | step: 278/422 | loss: 4.44163703918457\n",
      "Epoch: 7/100 | step: 279/422 | loss: 4.478199005126953\n",
      "Epoch: 7/100 | step: 280/422 | loss: 4.435091972351074\n",
      "Epoch: 7/100 | step: 281/422 | loss: 4.446309566497803\n",
      "Epoch: 7/100 | step: 282/422 | loss: 4.4923906326293945\n",
      "Epoch: 7/100 | step: 283/422 | loss: 4.390047550201416\n",
      "Epoch: 7/100 | step: 284/422 | loss: 4.483467102050781\n",
      "Epoch: 7/100 | step: 285/422 | loss: 4.419508934020996\n",
      "Epoch: 7/100 | step: 286/422 | loss: 4.285135269165039\n",
      "Epoch: 7/100 | step: 287/422 | loss: 4.405383586883545\n",
      "Epoch: 7/100 | step: 288/422 | loss: 4.490246772766113\n",
      "Epoch: 7/100 | step: 289/422 | loss: 4.401415824890137\n",
      "Epoch: 7/100 | step: 290/422 | loss: 4.457441329956055\n",
      "Epoch: 7/100 | step: 291/422 | loss: 4.41933012008667\n",
      "Epoch: 7/100 | step: 292/422 | loss: 4.39214563369751\n",
      "Epoch: 7/100 | step: 293/422 | loss: 4.3131842613220215\n",
      "Epoch: 7/100 | step: 294/422 | loss: 4.479520320892334\n",
      "Epoch: 7/100 | step: 295/422 | loss: 4.4769463539123535\n",
      "Epoch: 7/100 | step: 296/422 | loss: 4.480056285858154\n",
      "Epoch: 7/100 | step: 297/422 | loss: 4.416464328765869\n",
      "Epoch: 7/100 | step: 298/422 | loss: 4.461308002471924\n",
      "Epoch: 7/100 | step: 299/422 | loss: 4.323953151702881\n",
      "Epoch: 7/100 | step: 300/422 | loss: 4.485867023468018\n",
      "Epoch: 7/100 | step: 301/422 | loss: 4.432994842529297\n",
      "Epoch: 7/100 | step: 302/422 | loss: 4.436413288116455\n",
      "Epoch: 7/100 | step: 303/422 | loss: 4.432828426361084\n",
      "Epoch: 7/100 | step: 304/422 | loss: 4.514368057250977\n",
      "Epoch: 7/100 | step: 305/422 | loss: 4.40912389755249\n",
      "Epoch: 7/100 | step: 306/422 | loss: 4.304362773895264\n",
      "Epoch: 7/100 | step: 307/422 | loss: 4.404222011566162\n",
      "Epoch: 7/100 | step: 308/422 | loss: 4.352225303649902\n",
      "Epoch: 7/100 | step: 309/422 | loss: 4.406673431396484\n",
      "Epoch: 7/100 | step: 310/422 | loss: 4.31331205368042\n",
      "Epoch: 7/100 | step: 311/422 | loss: 4.3727030754089355\n",
      "Epoch: 7/100 | step: 312/422 | loss: 4.385956287384033\n",
      "Epoch: 7/100 | step: 313/422 | loss: 4.4003424644470215\n",
      "Epoch: 7/100 | step: 314/422 | loss: 4.465802192687988\n",
      "Epoch: 7/100 | step: 315/422 | loss: 4.488671779632568\n",
      "Epoch: 7/100 | step: 316/422 | loss: 4.472366809844971\n",
      "Epoch: 7/100 | step: 317/422 | loss: 4.54148006439209\n",
      "Epoch: 7/100 | step: 318/422 | loss: 4.453738689422607\n",
      "Epoch: 7/100 | step: 319/422 | loss: 4.393265247344971\n",
      "Epoch: 7/100 | step: 320/422 | loss: 4.343307971954346\n",
      "Epoch: 7/100 | step: 321/422 | loss: 4.349269390106201\n",
      "Epoch: 7/100 | step: 322/422 | loss: 4.357583045959473\n",
      "Epoch: 7/100 | step: 323/422 | loss: 4.5575995445251465\n",
      "Epoch: 7/100 | step: 324/422 | loss: 4.316332817077637\n",
      "Epoch: 7/100 | step: 325/422 | loss: 4.504103183746338\n",
      "Epoch: 7/100 | step: 326/422 | loss: 4.36728048324585\n",
      "Epoch: 7/100 | step: 327/422 | loss: 4.4330220222473145\n",
      "Epoch: 7/100 | step: 328/422 | loss: 4.378165245056152\n",
      "Epoch: 7/100 | step: 329/422 | loss: 4.328482627868652\n",
      "Epoch: 7/100 | step: 330/422 | loss: 4.3364152908325195\n",
      "Epoch: 7/100 | step: 331/422 | loss: 4.314286231994629\n",
      "Epoch: 7/100 | step: 332/422 | loss: 4.511909484863281\n",
      "Epoch: 7/100 | step: 333/422 | loss: 4.4358673095703125\n",
      "Epoch: 7/100 | step: 334/422 | loss: 4.421079158782959\n",
      "Epoch: 7/100 | step: 335/422 | loss: 4.413926124572754\n",
      "Epoch: 7/100 | step: 336/422 | loss: 4.363744258880615\n",
      "Epoch: 7/100 | step: 337/422 | loss: 4.383822441101074\n",
      "Epoch: 7/100 | step: 338/422 | loss: 4.486170291900635\n",
      "Epoch: 7/100 | step: 339/422 | loss: 4.332100868225098\n",
      "Epoch: 7/100 | step: 340/422 | loss: 4.406475067138672\n",
      "Epoch: 7/100 | step: 341/422 | loss: 4.376204967498779\n",
      "Epoch: 7/100 | step: 342/422 | loss: 4.40985631942749\n",
      "Epoch: 7/100 | step: 343/422 | loss: 4.392488956451416\n",
      "Epoch: 7/100 | step: 344/422 | loss: 4.286832809448242\n",
      "Epoch: 7/100 | step: 345/422 | loss: 4.437685012817383\n",
      "Epoch: 7/100 | step: 346/422 | loss: 4.449570655822754\n",
      "Epoch: 7/100 | step: 347/422 | loss: 4.364123821258545\n",
      "Epoch: 7/100 | step: 348/422 | loss: 4.462869167327881\n",
      "Epoch: 7/100 | step: 349/422 | loss: 4.521149158477783\n",
      "Epoch: 7/100 | step: 350/422 | loss: 4.379236698150635\n",
      "Epoch: 7/100 | step: 351/422 | loss: 4.509101867675781\n",
      "Epoch: 7/100 | step: 352/422 | loss: 4.451951503753662\n",
      "Epoch: 7/100 | step: 353/422 | loss: 4.453083515167236\n",
      "Epoch: 7/100 | step: 354/422 | loss: 4.303762435913086\n",
      "Epoch: 7/100 | step: 355/422 | loss: 4.457735538482666\n",
      "Epoch: 7/100 | step: 356/422 | loss: 4.456743240356445\n",
      "Epoch: 7/100 | step: 357/422 | loss: 4.361645698547363\n",
      "Epoch: 7/100 | step: 358/422 | loss: 4.384296417236328\n",
      "Epoch: 7/100 | step: 359/422 | loss: 4.394159317016602\n",
      "Epoch: 7/100 | step: 360/422 | loss: 4.340352535247803\n",
      "Epoch: 7/100 | step: 361/422 | loss: 4.278881549835205\n",
      "Epoch: 7/100 | step: 362/422 | loss: 4.40022611618042\n",
      "Epoch: 7/100 | step: 363/422 | loss: 4.386301517486572\n",
      "Epoch: 7/100 | step: 364/422 | loss: 4.354403495788574\n",
      "Epoch: 7/100 | step: 365/422 | loss: 4.321747303009033\n",
      "Epoch: 7/100 | step: 366/422 | loss: 4.415513038635254\n",
      "Epoch: 7/100 | step: 367/422 | loss: 4.29448938369751\n",
      "Epoch: 7/100 | step: 368/422 | loss: 4.346236705780029\n",
      "Epoch: 7/100 | step: 369/422 | loss: 4.321230411529541\n",
      "Epoch: 7/100 | step: 370/422 | loss: 4.365941524505615\n",
      "Epoch: 7/100 | step: 371/422 | loss: 4.3255510330200195\n",
      "Epoch: 7/100 | step: 372/422 | loss: 4.39885950088501\n",
      "Epoch: 7/100 | step: 373/422 | loss: 4.411677837371826\n",
      "Epoch: 7/100 | step: 374/422 | loss: 4.325136661529541\n",
      "Epoch: 7/100 | step: 375/422 | loss: 4.322540760040283\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 8/100 | step: 1/422 | loss: 4.4230828285217285\n",
      "Epoch: 8/100 | step: 2/422 | loss: 4.5199079513549805\n",
      "Epoch: 8/100 | step: 3/422 | loss: 4.420529365539551\n",
      "Epoch: 8/100 | step: 4/422 | loss: 4.36846399307251\n",
      "Epoch: 8/100 | step: 5/422 | loss: 4.418264865875244\n",
      "Epoch: 8/100 | step: 6/422 | loss: 4.420581340789795\n",
      "Epoch: 8/100 | step: 7/422 | loss: 4.354718208312988\n",
      "Epoch: 8/100 | step: 8/422 | loss: 4.5514235496521\n",
      "Epoch: 8/100 | step: 9/422 | loss: 4.499826908111572\n",
      "Epoch: 8/100 | step: 10/422 | loss: 4.377847671508789\n",
      "Epoch: 8/100 | step: 11/422 | loss: 4.387817859649658\n",
      "Epoch: 8/100 | step: 12/422 | loss: 4.340500354766846\n",
      "Epoch: 8/100 | step: 13/422 | loss: 4.397394180297852\n",
      "Epoch: 8/100 | step: 14/422 | loss: 4.3724541664123535\n",
      "Epoch: 8/100 | step: 15/422 | loss: 4.455699443817139\n",
      "Epoch: 8/100 | step: 16/422 | loss: 4.352629661560059\n",
      "Epoch: 8/100 | step: 17/422 | loss: 4.443822860717773\n",
      "Epoch: 8/100 | step: 18/422 | loss: 4.402609348297119\n",
      "Epoch: 8/100 | step: 19/422 | loss: 4.2700042724609375\n",
      "Epoch: 8/100 | step: 20/422 | loss: 4.313773155212402\n",
      "Epoch: 8/100 | step: 21/422 | loss: 4.3330769538879395\n",
      "Epoch: 8/100 | step: 22/422 | loss: 4.367158889770508\n",
      "Epoch: 8/100 | step: 23/422 | loss: 4.2666215896606445\n",
      "Epoch: 8/100 | step: 24/422 | loss: 4.428206443786621\n",
      "Epoch: 8/100 | step: 25/422 | loss: 4.216589450836182\n",
      "Epoch: 8/100 | step: 26/422 | loss: 4.250133037567139\n",
      "Epoch: 8/100 | step: 27/422 | loss: 4.374468803405762\n",
      "Epoch: 8/100 | step: 28/422 | loss: 4.442684650421143\n",
      "Epoch: 8/100 | step: 29/422 | loss: 4.346920967102051\n",
      "Epoch: 8/100 | step: 30/422 | loss: 4.324154376983643\n",
      "Epoch: 8/100 | step: 31/422 | loss: 4.463871955871582\n",
      "Epoch: 8/100 | step: 32/422 | loss: 4.3550705909729\n",
      "Epoch: 8/100 | step: 33/422 | loss: 4.27120304107666\n",
      "Epoch: 8/100 | step: 34/422 | loss: 4.339234352111816\n",
      "Epoch: 8/100 | step: 35/422 | loss: 4.307175636291504\n",
      "Epoch: 8/100 | step: 36/422 | loss: 4.388983726501465\n",
      "Epoch: 8/100 | step: 37/422 | loss: 4.328815937042236\n",
      "Epoch: 8/100 | step: 38/422 | loss: 4.442344665527344\n",
      "Epoch: 8/100 | step: 39/422 | loss: 4.366292953491211\n",
      "Epoch: 8/100 | step: 40/422 | loss: 4.3632612228393555\n",
      "Epoch: 8/100 | step: 41/422 | loss: 4.3491363525390625\n",
      "Epoch: 8/100 | step: 42/422 | loss: 4.363049507141113\n",
      "Epoch: 8/100 | step: 43/422 | loss: 4.395004749298096\n",
      "Epoch: 8/100 | step: 44/422 | loss: 4.387459754943848\n",
      "Epoch: 8/100 | step: 45/422 | loss: 4.367187976837158\n",
      "Epoch: 8/100 | step: 46/422 | loss: 4.335874080657959\n",
      "Epoch: 8/100 | step: 47/422 | loss: 4.505383014678955\n",
      "Epoch: 8/100 | step: 48/422 | loss: 4.346530437469482\n",
      "Epoch: 8/100 | step: 49/422 | loss: 4.52341890335083\n",
      "Epoch: 8/100 | step: 50/422 | loss: 4.3604631423950195\n",
      "Epoch: 8/100 | step: 51/422 | loss: 4.365594863891602\n",
      "Epoch: 8/100 | step: 52/422 | loss: 4.435799598693848\n",
      "Epoch: 8/100 | step: 53/422 | loss: 4.396081924438477\n",
      "Epoch: 8/100 | step: 54/422 | loss: 4.304813861846924\n",
      "Epoch: 8/100 | step: 55/422 | loss: 4.349978446960449\n",
      "Epoch: 8/100 | step: 56/422 | loss: 4.311160087585449\n",
      "Epoch: 8/100 | step: 57/422 | loss: 4.369384288787842\n",
      "Epoch: 8/100 | step: 58/422 | loss: 4.488679885864258\n",
      "Epoch: 8/100 | step: 59/422 | loss: 4.321405410766602\n",
      "Epoch: 8/100 | step: 60/422 | loss: 4.273766040802002\n",
      "Epoch: 8/100 | step: 61/422 | loss: 4.435628414154053\n",
      "Epoch: 8/100 | step: 62/422 | loss: 4.379991054534912\n",
      "Epoch: 8/100 | step: 63/422 | loss: 4.351764678955078\n",
      "Epoch: 8/100 | step: 64/422 | loss: 4.336201190948486\n",
      "Epoch: 8/100 | step: 65/422 | loss: 4.373758792877197\n",
      "Epoch: 8/100 | step: 66/422 | loss: 4.3251495361328125\n",
      "Epoch: 8/100 | step: 67/422 | loss: 4.375509738922119\n",
      "Epoch: 8/100 | step: 68/422 | loss: 4.274876117706299\n",
      "Epoch: 8/100 | step: 69/422 | loss: 4.284247875213623\n",
      "Epoch: 8/100 | step: 70/422 | loss: 4.289239406585693\n",
      "Epoch: 8/100 | step: 71/422 | loss: 4.4064788818359375\n",
      "Epoch: 8/100 | step: 72/422 | loss: 4.335165977478027\n",
      "Epoch: 8/100 | step: 73/422 | loss: 4.346168518066406\n",
      "Epoch: 8/100 | step: 74/422 | loss: 4.224143981933594\n",
      "Epoch: 8/100 | step: 75/422 | loss: 4.247715950012207\n",
      "Epoch: 8/100 | step: 76/422 | loss: 4.466070652008057\n",
      "Epoch: 8/100 | step: 77/422 | loss: 4.410737037658691\n",
      "Epoch: 8/100 | step: 78/422 | loss: 4.353586673736572\n",
      "Epoch: 8/100 | step: 79/422 | loss: 4.214826583862305\n",
      "Epoch: 8/100 | step: 80/422 | loss: 4.446885585784912\n",
      "Epoch: 8/100 | step: 81/422 | loss: 4.300366401672363\n",
      "Epoch: 8/100 | step: 82/422 | loss: 4.34428071975708\n",
      "Epoch: 8/100 | step: 83/422 | loss: 4.407242774963379\n",
      "Epoch: 8/100 | step: 84/422 | loss: 4.444494247436523\n",
      "Epoch: 8/100 | step: 85/422 | loss: 4.308272361755371\n",
      "Epoch: 8/100 | step: 86/422 | loss: 4.431853771209717\n",
      "Epoch: 8/100 | step: 87/422 | loss: 4.279714107513428\n",
      "Epoch: 8/100 | step: 88/422 | loss: 4.293708324432373\n",
      "Epoch: 8/100 | step: 89/422 | loss: 4.355099678039551\n",
      "Epoch: 8/100 | step: 90/422 | loss: 4.393423080444336\n",
      "Epoch: 8/100 | step: 91/422 | loss: 4.376108646392822\n",
      "Epoch: 8/100 | step: 92/422 | loss: 4.29756498336792\n",
      "Epoch: 8/100 | step: 93/422 | loss: 4.343648433685303\n",
      "Epoch: 8/100 | step: 94/422 | loss: 4.34505558013916\n",
      "Epoch: 8/100 | step: 95/422 | loss: 4.362153053283691\n",
      "Epoch: 8/100 | step: 96/422 | loss: 4.284719944000244\n",
      "Epoch: 8/100 | step: 97/422 | loss: 4.369655132293701\n",
      "Epoch: 8/100 | step: 98/422 | loss: 4.2091145515441895\n",
      "Epoch: 8/100 | step: 99/422 | loss: 4.308991432189941\n",
      "Epoch: 8/100 | step: 100/422 | loss: 4.329877853393555\n",
      "Epoch: 8/100 | step: 101/422 | loss: 4.439093589782715\n",
      "Epoch: 8/100 | step: 102/422 | loss: 4.3741455078125\n",
      "Epoch: 8/100 | step: 103/422 | loss: 4.437753200531006\n",
      "Epoch: 8/100 | step: 104/422 | loss: 4.254707336425781\n",
      "Epoch: 8/100 | step: 105/422 | loss: 4.409026622772217\n",
      "Epoch: 8/100 | step: 106/422 | loss: 4.396193981170654\n",
      "Epoch: 8/100 | step: 107/422 | loss: 4.438876152038574\n",
      "Epoch: 8/100 | step: 108/422 | loss: 4.321191310882568\n",
      "Epoch: 8/100 | step: 109/422 | loss: 4.420307159423828\n",
      "Epoch: 8/100 | step: 110/422 | loss: 4.294264793395996\n",
      "Epoch: 8/100 | step: 111/422 | loss: 4.326744556427002\n",
      "Epoch: 8/100 | step: 112/422 | loss: 4.381046295166016\n",
      "Epoch: 8/100 | step: 113/422 | loss: 4.358524799346924\n",
      "Epoch: 8/100 | step: 114/422 | loss: 4.359152317047119\n",
      "Epoch: 8/100 | step: 115/422 | loss: 4.382079124450684\n",
      "Epoch: 8/100 | step: 116/422 | loss: 4.305619716644287\n",
      "Epoch: 8/100 | step: 117/422 | loss: 4.341775894165039\n",
      "Epoch: 8/100 | step: 118/422 | loss: 4.259615898132324\n",
      "Epoch: 8/100 | step: 119/422 | loss: 4.487411975860596\n",
      "Epoch: 8/100 | step: 120/422 | loss: 4.443330764770508\n",
      "Epoch: 8/100 | step: 121/422 | loss: 4.4315314292907715\n",
      "Epoch: 8/100 | step: 122/422 | loss: 4.351864814758301\n",
      "Epoch: 8/100 | step: 123/422 | loss: 4.2580342292785645\n",
      "Epoch: 8/100 | step: 124/422 | loss: 4.340677261352539\n",
      "Epoch: 8/100 | step: 125/422 | loss: 4.287315845489502\n",
      "Epoch: 8/100 | step: 126/422 | loss: 4.419370651245117\n",
      "Epoch: 8/100 | step: 127/422 | loss: 4.235934257507324\n",
      "Epoch: 8/100 | step: 128/422 | loss: 4.342258453369141\n",
      "Epoch: 8/100 | step: 129/422 | loss: 4.287062168121338\n",
      "Epoch: 8/100 | step: 130/422 | loss: 4.452432632446289\n",
      "Epoch: 8/100 | step: 131/422 | loss: 4.434822082519531\n",
      "Epoch: 8/100 | step: 132/422 | loss: 4.394482612609863\n",
      "Epoch: 8/100 | step: 133/422 | loss: 4.162013053894043\n",
      "Epoch: 8/100 | step: 134/422 | loss: 4.225292205810547\n",
      "Epoch: 8/100 | step: 135/422 | loss: 4.381479740142822\n",
      "Epoch: 8/100 | step: 136/422 | loss: 4.275856018066406\n",
      "Epoch: 8/100 | step: 137/422 | loss: 4.313572883605957\n",
      "Epoch: 8/100 | step: 138/422 | loss: 4.366997241973877\n",
      "Epoch: 8/100 | step: 139/422 | loss: 4.415463447570801\n",
      "Epoch: 8/100 | step: 140/422 | loss: 4.208996772766113\n",
      "Epoch: 8/100 | step: 141/422 | loss: 4.4325666427612305\n",
      "Epoch: 8/100 | step: 142/422 | loss: 4.205921649932861\n",
      "Epoch: 8/100 | step: 143/422 | loss: 4.128353118896484\n",
      "Epoch: 8/100 | step: 144/422 | loss: 4.175302028656006\n",
      "Epoch: 8/100 | step: 145/422 | loss: 4.304498672485352\n",
      "Epoch: 8/100 | step: 146/422 | loss: 4.282040596008301\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 9/100 | step: 1/422 | loss: 4.36706018447876\n",
      "Epoch: 9/100 | step: 2/422 | loss: 4.381975173950195\n",
      "Epoch: 9/100 | step: 3/422 | loss: 4.4266510009765625\n",
      "Epoch: 9/100 | step: 4/422 | loss: 4.418800354003906\n",
      "Epoch: 9/100 | step: 5/422 | loss: 4.2871174812316895\n",
      "Epoch: 9/100 | step: 6/422 | loss: 4.277524948120117\n",
      "Epoch: 9/100 | step: 7/422 | loss: 4.248569965362549\n",
      "Epoch: 9/100 | step: 8/422 | loss: 4.320768356323242\n",
      "Epoch: 9/100 | step: 9/422 | loss: 4.278039455413818\n",
      "Epoch: 9/100 | step: 10/422 | loss: 4.217113971710205\n",
      "Epoch: 9/100 | step: 11/422 | loss: 4.435337066650391\n",
      "Epoch: 9/100 | step: 12/422 | loss: 4.216942310333252\n",
      "Epoch: 9/100 | step: 13/422 | loss: 4.423590660095215\n",
      "Epoch: 9/100 | step: 14/422 | loss: 4.30472993850708\n",
      "Epoch: 9/100 | step: 15/422 | loss: 4.324524402618408\n",
      "Epoch: 9/100 | step: 16/422 | loss: 4.352388858795166\n",
      "Epoch: 9/100 | step: 17/422 | loss: 4.458618640899658\n",
      "Epoch: 9/100 | step: 18/422 | loss: 4.31044864654541\n",
      "Epoch: 9/100 | step: 19/422 | loss: 4.357092380523682\n",
      "Epoch: 9/100 | step: 20/422 | loss: 4.393783092498779\n",
      "Epoch: 9/100 | step: 21/422 | loss: 4.257293224334717\n",
      "Epoch: 9/100 | step: 22/422 | loss: 4.3438825607299805\n",
      "Epoch: 9/100 | step: 23/422 | loss: 4.348033428192139\n",
      "Epoch: 9/100 | step: 24/422 | loss: 4.106967926025391\n",
      "Epoch: 9/100 | step: 25/422 | loss: 4.386810302734375\n",
      "Epoch: 9/100 | step: 26/422 | loss: 4.286935806274414\n",
      "Epoch: 9/100 | step: 27/422 | loss: 4.139303207397461\n",
      "Epoch: 9/100 | step: 28/422 | loss: 4.490624904632568\n",
      "Epoch: 9/100 | step: 29/422 | loss: 4.345407962799072\n",
      "Epoch: 9/100 | step: 30/422 | loss: 4.234019756317139\n",
      "Epoch: 9/100 | step: 31/422 | loss: 4.2291364669799805\n",
      "Epoch: 9/100 | step: 32/422 | loss: 4.136170387268066\n",
      "Epoch: 9/100 | step: 33/422 | loss: 4.354228496551514\n",
      "Epoch: 9/100 | step: 34/422 | loss: 4.379752159118652\n",
      "Epoch: 9/100 | step: 35/422 | loss: 4.326882839202881\n",
      "Epoch: 9/100 | step: 36/422 | loss: 4.2899699211120605\n",
      "Epoch: 9/100 | step: 37/422 | loss: 4.268794536590576\n",
      "Epoch: 9/100 | step: 38/422 | loss: 4.276797771453857\n",
      "Epoch: 9/100 | step: 39/422 | loss: 4.268532752990723\n",
      "Epoch: 9/100 | step: 40/422 | loss: 4.416863918304443\n",
      "Epoch: 9/100 | step: 41/422 | loss: 4.256843566894531\n",
      "Epoch: 9/100 | step: 42/422 | loss: 4.201971054077148\n",
      "Epoch: 9/100 | step: 43/422 | loss: 4.252997875213623\n",
      "Epoch: 9/100 | step: 44/422 | loss: 4.521457672119141\n",
      "Epoch: 9/100 | step: 45/422 | loss: 4.3431220054626465\n",
      "Epoch: 9/100 | step: 46/422 | loss: 4.3568830490112305\n",
      "Epoch: 9/100 | step: 47/422 | loss: 4.299816131591797\n",
      "Epoch: 9/100 | step: 48/422 | loss: 4.472036361694336\n",
      "Epoch: 9/100 | step: 49/422 | loss: 4.305920600891113\n",
      "Epoch: 9/100 | step: 50/422 | loss: 4.319245338439941\n",
      "Epoch: 9/100 | step: 51/422 | loss: 4.300881862640381\n",
      "Epoch: 9/100 | step: 52/422 | loss: 4.4402689933776855\n",
      "Epoch: 9/100 | step: 53/422 | loss: 4.4124603271484375\n",
      "Epoch: 9/100 | step: 54/422 | loss: 4.328789234161377\n",
      "Epoch: 9/100 | step: 55/422 | loss: 4.258530616760254\n",
      "Epoch: 9/100 | step: 56/422 | loss: 4.37438440322876\n",
      "Epoch: 9/100 | step: 57/422 | loss: 4.3371710777282715\n",
      "Epoch: 9/100 | step: 58/422 | loss: 4.317788124084473\n",
      "Epoch: 9/100 | step: 59/422 | loss: 4.263359546661377\n",
      "Epoch: 9/100 | step: 60/422 | loss: 4.3579206466674805\n",
      "Epoch: 9/100 | step: 61/422 | loss: 4.327210903167725\n",
      "Epoch: 9/100 | step: 62/422 | loss: 4.401948928833008\n",
      "Epoch: 9/100 | step: 63/422 | loss: 4.249352931976318\n",
      "Epoch: 9/100 | step: 64/422 | loss: 4.170651912689209\n",
      "Epoch: 9/100 | step: 65/422 | loss: 4.191779613494873\n",
      "Epoch: 9/100 | step: 66/422 | loss: 4.208085536956787\n",
      "Epoch: 9/100 | step: 67/422 | loss: 4.22170352935791\n",
      "Epoch: 9/100 | step: 68/422 | loss: 4.3032050132751465\n",
      "Epoch: 9/100 | step: 69/422 | loss: 4.116766452789307\n",
      "Epoch: 9/100 | step: 70/422 | loss: 4.300405025482178\n",
      "Epoch: 9/100 | step: 71/422 | loss: 4.335290431976318\n",
      "Epoch: 9/100 | step: 72/422 | loss: 4.302337646484375\n",
      "Epoch: 9/100 | step: 73/422 | loss: 4.30389928817749\n",
      "Epoch: 9/100 | step: 74/422 | loss: 4.325700283050537\n",
      "Epoch: 9/100 | step: 75/422 | loss: 4.2914862632751465\n",
      "Epoch: 9/100 | step: 76/422 | loss: 4.056410789489746\n",
      "Epoch: 9/100 | step: 77/422 | loss: 4.294498920440674\n",
      "Epoch: 9/100 | step: 78/422 | loss: 4.343908309936523\n",
      "Epoch: 9/100 | step: 79/422 | loss: 4.270851135253906\n",
      "Epoch: 9/100 | step: 80/422 | loss: 4.424984931945801\n",
      "Epoch: 9/100 | step: 81/422 | loss: 4.476776123046875\n",
      "Epoch: 9/100 | step: 82/422 | loss: 4.2942023277282715\n",
      "Epoch: 9/100 | step: 83/422 | loss: 4.121899604797363\n",
      "Epoch: 9/100 | step: 84/422 | loss: 4.227546215057373\n",
      "Epoch: 9/100 | step: 85/422 | loss: 4.39927864074707\n",
      "Epoch: 9/100 | step: 86/422 | loss: 4.4355573654174805\n",
      "Epoch: 9/100 | step: 87/422 | loss: 4.2664642333984375\n",
      "Epoch: 9/100 | step: 88/422 | loss: 4.381264686584473\n",
      "Epoch: 9/100 | step: 89/422 | loss: 4.240525245666504\n",
      "Epoch: 9/100 | step: 90/422 | loss: 4.251696586608887\n",
      "Epoch: 9/100 | step: 91/422 | loss: 4.376908779144287\n",
      "Epoch: 9/100 | step: 92/422 | loss: 4.270694732666016\n",
      "Epoch: 9/100 | step: 93/422 | loss: 4.225622177124023\n",
      "Epoch: 9/100 | step: 94/422 | loss: 4.27510404586792\n",
      "Epoch: 9/100 | step: 95/422 | loss: 4.280415058135986\n",
      "Epoch: 9/100 | step: 96/422 | loss: 4.329916954040527\n",
      "Epoch: 9/100 | step: 97/422 | loss: 4.321110725402832\n",
      "Epoch: 9/100 | step: 98/422 | loss: 4.329733848571777\n",
      "Epoch: 9/100 | step: 99/422 | loss: 4.1626787185668945\n",
      "Epoch: 9/100 | step: 100/422 | loss: 4.367039203643799\n",
      "Epoch: 9/100 | step: 101/422 | loss: 4.255362510681152\n",
      "Epoch: 9/100 | step: 102/422 | loss: 4.37860631942749\n",
      "Epoch: 9/100 | step: 103/422 | loss: 4.209659576416016\n",
      "Epoch: 9/100 | step: 104/422 | loss: 4.379282474517822\n",
      "Epoch: 9/100 | step: 105/422 | loss: 4.319725513458252\n",
      "Epoch: 9/100 | step: 106/422 | loss: 4.233048915863037\n",
      "Epoch: 9/100 | step: 107/422 | loss: 4.187533378601074\n",
      "Epoch: 9/100 | step: 108/422 | loss: 4.303460597991943\n",
      "Epoch: 9/100 | step: 109/422 | loss: 4.417678356170654\n",
      "Epoch: 9/100 | step: 110/422 | loss: 4.390006065368652\n",
      "Epoch: 9/100 | step: 111/422 | loss: 4.22148323059082\n",
      "Epoch: 9/100 | step: 112/422 | loss: 4.326841831207275\n",
      "Epoch: 9/100 | step: 113/422 | loss: 4.0921430587768555\n",
      "Epoch: 9/100 | step: 114/422 | loss: 4.350367069244385\n",
      "Epoch: 9/100 | step: 115/422 | loss: 4.268645763397217\n",
      "Epoch: 9/100 | step: 116/422 | loss: 4.095479488372803\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 10/100 | step: 1/422 | loss: 4.197957992553711\n",
      "Epoch: 10/100 | step: 2/422 | loss: 4.1341705322265625\n",
      "Epoch: 10/100 | step: 3/422 | loss: 4.234309196472168\n",
      "Epoch: 10/100 | step: 4/422 | loss: 4.4163689613342285\n",
      "Epoch: 10/100 | step: 5/422 | loss: 4.306576728820801\n",
      "Epoch: 10/100 | step: 6/422 | loss: 4.275068759918213\n",
      "Epoch: 10/100 | step: 7/422 | loss: 4.32940149307251\n",
      "Epoch: 10/100 | step: 8/422 | loss: 4.210310459136963\n",
      "Epoch: 10/100 | step: 9/422 | loss: 4.294139862060547\n",
      "Epoch: 10/100 | step: 10/422 | loss: 4.235262393951416\n",
      "Epoch: 10/100 | step: 11/422 | loss: 4.2358527183532715\n",
      "Epoch: 10/100 | step: 12/422 | loss: 4.470856189727783\n",
      "Epoch: 10/100 | step: 13/422 | loss: 4.360165119171143\n",
      "Epoch: 10/100 | step: 14/422 | loss: 4.252589225769043\n",
      "Epoch: 10/100 | step: 15/422 | loss: 4.250362873077393\n",
      "Epoch: 10/100 | step: 16/422 | loss: 4.4133524894714355\n",
      "Epoch: 10/100 | step: 17/422 | loss: 4.324902057647705\n",
      "Epoch: 10/100 | step: 18/422 | loss: 4.283549785614014\n",
      "Epoch: 10/100 | step: 19/422 | loss: 4.265776634216309\n",
      "Epoch: 10/100 | step: 20/422 | loss: 4.148975849151611\n",
      "Epoch: 10/100 | step: 21/422 | loss: 4.2547221183776855\n",
      "Epoch: 10/100 | step: 22/422 | loss: 4.068090438842773\n",
      "Epoch: 10/100 | step: 23/422 | loss: 4.329795837402344\n",
      "Epoch: 10/100 | step: 24/422 | loss: 4.143238544464111\n",
      "Epoch: 10/100 | step: 25/422 | loss: 4.399337291717529\n",
      "Epoch: 10/100 | step: 26/422 | loss: 4.27882194519043\n",
      "Epoch: 10/100 | step: 27/422 | loss: 4.000360012054443\n",
      "Epoch: 10/100 | step: 28/422 | loss: 4.303552150726318\n",
      "Epoch: 10/100 | step: 29/422 | loss: 4.1873860359191895\n",
      "Epoch: 10/100 | step: 30/422 | loss: 4.215632438659668\n",
      "Epoch: 10/100 | step: 31/422 | loss: 4.31571102142334\n",
      "Epoch: 10/100 | step: 32/422 | loss: 4.272080898284912\n",
      "Epoch: 10/100 | step: 33/422 | loss: 4.184983730316162\n",
      "Epoch: 10/100 | step: 34/422 | loss: 4.231331825256348\n",
      "Epoch: 10/100 | step: 35/422 | loss: 4.268563747406006\n",
      "Epoch: 10/100 | step: 36/422 | loss: 4.405339241027832\n",
      "Epoch: 10/100 | step: 37/422 | loss: 4.174759387969971\n",
      "Epoch: 10/100 | step: 38/422 | loss: 4.327804088592529\n",
      "Epoch: 10/100 | step: 39/422 | loss: 4.115954399108887\n",
      "Epoch: 10/100 | step: 40/422 | loss: 4.266924858093262\n",
      "Epoch: 10/100 | step: 41/422 | loss: 4.217653751373291\n",
      "Epoch: 10/100 | step: 42/422 | loss: 4.031641483306885\n",
      "Epoch: 10/100 | step: 43/422 | loss: 4.187823295593262\n",
      "Epoch: 10/100 | step: 44/422 | loss: 4.400885105133057\n",
      "Epoch: 10/100 | step: 45/422 | loss: 4.311888217926025\n",
      "Epoch: 10/100 | step: 46/422 | loss: 4.441258907318115\n",
      "Epoch: 10/100 | step: 47/422 | loss: 4.316581726074219\n",
      "Epoch: 10/100 | step: 48/422 | loss: 4.167449951171875\n",
      "Epoch: 10/100 | step: 49/422 | loss: 4.31025505065918\n",
      "Epoch: 10/100 | step: 50/422 | loss: 4.332746982574463\n",
      "Epoch: 10/100 | step: 51/422 | loss: 4.229933738708496\n",
      "Epoch: 10/100 | step: 52/422 | loss: 4.1955389976501465\n",
      "Epoch: 10/100 | step: 53/422 | loss: 4.149856090545654\n",
      "Epoch: 10/100 | step: 54/422 | loss: 4.26391077041626\n",
      "Epoch: 10/100 | step: 55/422 | loss: 4.349862098693848\n",
      "Epoch: 10/100 | step: 56/422 | loss: 4.377016067504883\n",
      "Epoch: 10/100 | step: 57/422 | loss: 4.208994388580322\n",
      "Epoch: 10/100 | step: 58/422 | loss: 4.264533996582031\n",
      "Epoch: 10/100 | step: 59/422 | loss: 4.22623348236084\n",
      "Epoch: 10/100 | step: 60/422 | loss: 4.149969577789307\n",
      "Epoch: 10/100 | step: 61/422 | loss: 4.010001182556152\n",
      "Epoch: 10/100 | step: 62/422 | loss: 4.244925498962402\n",
      "Epoch: 10/100 | step: 63/422 | loss: 4.258291721343994\n",
      "Epoch: 10/100 | step: 64/422 | loss: 4.314860820770264\n",
      "Epoch: 10/100 | step: 65/422 | loss: 4.383401870727539\n",
      "Epoch: 10/100 | step: 66/422 | loss: 4.244720458984375\n",
      "Epoch: 10/100 | step: 67/422 | loss: 4.448202133178711\n",
      "Epoch: 10/100 | step: 68/422 | loss: 4.286422252655029\n",
      "Epoch: 10/100 | step: 69/422 | loss: 4.192808628082275\n",
      "Epoch: 10/100 | step: 70/422 | loss: 4.31568717956543\n",
      "Epoch: 10/100 | step: 71/422 | loss: 4.0592360496521\n",
      "Epoch: 10/100 | step: 72/422 | loss: 4.1937479972839355\n",
      "Epoch: 10/100 | step: 73/422 | loss: 4.266410827636719\n",
      "Epoch: 10/100 | step: 74/422 | loss: 4.363775253295898\n",
      "Epoch: 10/100 | step: 75/422 | loss: 4.201718807220459\n",
      "Epoch: 10/100 | step: 76/422 | loss: 4.189282417297363\n",
      "Epoch: 10/100 | step: 77/422 | loss: 4.481271266937256\n",
      "Epoch: 10/100 | step: 78/422 | loss: 4.33255672454834\n",
      "Epoch: 10/100 | step: 79/422 | loss: 4.155134677886963\n",
      "Epoch: 10/100 | step: 80/422 | loss: 4.181138038635254\n",
      "Epoch: 10/100 | step: 81/422 | loss: 4.337981224060059\n",
      "Epoch: 10/100 | step: 82/422 | loss: 4.312814712524414\n",
      "Epoch: 10/100 | step: 83/422 | loss: 4.282249450683594\n",
      "Epoch: 10/100 | step: 84/422 | loss: 4.256521701812744\n",
      "Epoch: 10/100 | step: 85/422 | loss: 4.222769260406494\n",
      "Epoch: 10/100 | step: 86/422 | loss: 4.218607425689697\n",
      "Epoch: 10/100 | step: 87/422 | loss: 4.233448028564453\n",
      "Epoch: 10/100 | step: 88/422 | loss: 4.2541093826293945\n",
      "Epoch: 10/100 | step: 89/422 | loss: 4.266813278198242\n",
      "Epoch: 10/100 | step: 90/422 | loss: 4.130919933319092\n",
      "Epoch: 10/100 | step: 91/422 | loss: 4.247701644897461\n",
      "Epoch: 10/100 | step: 92/422 | loss: 4.029073238372803\n",
      "Epoch: 10/100 | step: 93/422 | loss: 4.372655868530273\n",
      "Epoch: 10/100 | step: 94/422 | loss: 4.2233734130859375\n",
      "Epoch: 10/100 | step: 95/422 | loss: 4.3216633796691895\n",
      "Epoch: 10/100 | step: 96/422 | loss: 4.286911964416504\n",
      "Epoch: 10/100 | step: 97/422 | loss: 4.233312129974365\n",
      "Epoch: 10/100 | step: 98/422 | loss: 4.19972038269043\n",
      "Epoch: 10/100 | step: 99/422 | loss: 4.32341194152832\n",
      "Epoch: 10/100 | step: 100/422 | loss: 4.273740291595459\n",
      "Epoch: 10/100 | step: 101/422 | loss: 4.332709312438965\n",
      "Epoch: 10/100 | step: 102/422 | loss: 4.225566387176514\n",
      "Epoch: 10/100 | step: 103/422 | loss: 4.0899224281311035\n",
      "Epoch: 10/100 | step: 104/422 | loss: 4.2620134353637695\n",
      "Epoch: 10/100 | step: 105/422 | loss: 4.234411239624023\n",
      "Epoch: 10/100 | step: 106/422 | loss: 4.350174903869629\n",
      "Epoch: 10/100 | step: 107/422 | loss: 4.390758991241455\n",
      "Epoch: 10/100 | step: 108/422 | loss: 4.215117931365967\n",
      "Epoch: 10/100 | step: 109/422 | loss: 4.061330318450928\n",
      "Epoch: 10/100 | step: 110/422 | loss: 4.145753860473633\n",
      "Epoch: 10/100 | step: 111/422 | loss: 4.184002876281738\n",
      "Epoch: 10/100 | step: 112/422 | loss: 4.485312461853027\n",
      "Epoch: 10/100 | step: 113/422 | loss: 4.059051990509033\n",
      "Epoch: 10/100 | step: 114/422 | loss: 4.200788974761963\n",
      "Epoch: 10/100 | step: 115/422 | loss: 4.242806434631348\n",
      "Epoch: 10/100 | step: 116/422 | loss: 4.230153560638428\n",
      "Epoch: 10/100 | step: 117/422 | loss: 4.154512405395508\n",
      "Epoch: 10/100 | step: 118/422 | loss: 4.1705217361450195\n",
      "Epoch: 10/100 | step: 119/422 | loss: 4.16292142868042\n",
      "Epoch: 10/100 | step: 120/422 | loss: 4.091856956481934\n",
      "Epoch: 10/100 | step: 121/422 | loss: 4.210788249969482\n",
      "Epoch: 10/100 | step: 122/422 | loss: 4.164679050445557\n",
      "Epoch: 10/100 | step: 123/422 | loss: 4.160864353179932\n",
      "Epoch: 10/100 | step: 124/422 | loss: 4.249382019042969\n",
      "Epoch: 10/100 | step: 125/422 | loss: 4.327817916870117\n",
      "Epoch: 10/100 | step: 126/422 | loss: 4.349828243255615\n",
      "Epoch: 10/100 | step: 127/422 | loss: 4.221904277801514\n",
      "Epoch: 10/100 | step: 128/422 | loss: 4.268424987792969\n",
      "Epoch: 10/100 | step: 129/422 | loss: 4.073117256164551\n",
      "Epoch: 10/100 | step: 130/422 | loss: 4.1425580978393555\n",
      "Epoch: 10/100 | step: 131/422 | loss: 4.356889724731445\n",
      "Epoch: 10/100 | step: 132/422 | loss: 4.490213871002197\n",
      "Epoch: 10/100 | step: 133/422 | loss: 4.216712951660156\n",
      "Epoch: 10/100 | step: 134/422 | loss: 4.242774486541748\n",
      "Epoch: 10/100 | step: 135/422 | loss: 4.166494846343994\n",
      "Epoch: 10/100 | step: 136/422 | loss: 4.078826904296875\n",
      "Epoch: 10/100 | step: 137/422 | loss: 4.107144832611084\n",
      "Epoch: 10/100 | step: 138/422 | loss: 4.157864093780518\n",
      "Epoch: 10/100 | step: 139/422 | loss: 4.027461528778076\n",
      "Epoch: 10/100 | step: 140/422 | loss: 4.010647773742676\n",
      "Epoch: 10/100 | step: 141/422 | loss: 3.9622011184692383\n",
      "Epoch: 10/100 | step: 142/422 | loss: 4.468756675720215\n",
      "Epoch: 10/100 | step: 143/422 | loss: 4.16815185546875\n",
      "Epoch: 10/100 | step: 144/422 | loss: 3.982966184616089\n",
      "Epoch: 10/100 | step: 145/422 | loss: 4.246716499328613\n",
      "Epoch: 10/100 | step: 146/422 | loss: 4.072250843048096\n",
      "Epoch: 10/100 | step: 147/422 | loss: 4.131731986999512\n",
      "Epoch: 10/100 | step: 148/422 | loss: 4.3308515548706055\n",
      "Epoch: 10/100 | step: 149/422 | loss: 4.379270076751709\n",
      "Epoch: 10/100 | step: 150/422 | loss: 4.297706127166748\n",
      "Epoch: 10/100 | step: 151/422 | loss: 4.339995861053467\n",
      "Epoch: 10/100 | step: 152/422 | loss: 4.108955383300781\n",
      "Epoch: 10/100 | step: 153/422 | loss: 4.007125377655029\n",
      "Epoch: 10/100 | step: 154/422 | loss: 4.222497463226318\n",
      "Epoch: 10/100 | step: 155/422 | loss: 4.092298984527588\n",
      "Epoch: 10/100 | step: 156/422 | loss: 4.11731481552124\n",
      "Epoch: 10/100 | step: 157/422 | loss: 4.414690971374512\n",
      "Epoch: 10/100 | step: 158/422 | loss: 4.110805511474609\n",
      "Epoch: 10/100 | step: 159/422 | loss: 4.216090679168701\n",
      "Epoch: 10/100 | step: 160/422 | loss: 4.093822002410889\n",
      "Epoch: 10/100 | step: 161/422 | loss: 4.1876606941223145\n",
      "Epoch: 10/100 | step: 162/422 | loss: 4.237057209014893\n",
      "Epoch: 10/100 | step: 163/422 | loss: 4.412489891052246\n",
      "Epoch: 10/100 | step: 164/422 | loss: 4.213221549987793\n",
      "Epoch: 10/100 | step: 165/422 | loss: 4.287334442138672\n",
      "Epoch: 10/100 | step: 166/422 | loss: 4.270002841949463\n",
      "Epoch: 10/100 | step: 167/422 | loss: 4.254990577697754\n",
      "Epoch: 10/100 | step: 168/422 | loss: 4.328638553619385\n",
      "Epoch: 10/100 | step: 169/422 | loss: 4.199143409729004\n",
      "Epoch: 10/100 | step: 170/422 | loss: 4.028348922729492\n",
      "Epoch: 10/100 | step: 171/422 | loss: 4.2100348472595215\n",
      "Epoch: 10/100 | step: 172/422 | loss: 4.075399398803711\n",
      "Epoch: 10/100 | step: 173/422 | loss: 4.320068359375\n",
      "Epoch: 10/100 | step: 174/422 | loss: 4.041016101837158\n",
      "Epoch: 10/100 | step: 175/422 | loss: 4.080554962158203\n",
      "Epoch: 10/100 | step: 176/422 | loss: 4.172367095947266\n",
      "Epoch: 10/100 | step: 177/422 | loss: 3.9858765602111816\n",
      "Epoch: 10/100 | step: 178/422 | loss: 4.343221187591553\n",
      "Epoch: 10/100 | step: 179/422 | loss: 4.289373874664307\n",
      "Epoch: 10/100 | step: 180/422 | loss: 4.236188888549805\n",
      "Epoch: 10/100 | step: 181/422 | loss: 4.111297607421875\n",
      "Epoch: 10/100 | step: 182/422 | loss: 4.444238185882568\n",
      "Epoch: 10/100 | step: 183/422 | loss: 4.255894660949707\n",
      "Epoch: 10/100 | step: 184/422 | loss: 4.174600124359131\n",
      "Epoch: 10/100 | step: 185/422 | loss: 4.271239280700684\n",
      "Epoch: 10/100 | step: 186/422 | loss: 4.135311126708984\n",
      "Epoch: 10/100 | step: 187/422 | loss: 4.256050109863281\n",
      "Epoch: 10/100 | step: 188/422 | loss: 4.412769317626953\n",
      "Epoch: 10/100 | step: 189/422 | loss: 4.22504997253418\n",
      "Epoch: 10/100 | step: 190/422 | loss: 4.295289993286133\n",
      "Epoch: 10/100 | step: 191/422 | loss: 4.252535820007324\n",
      "Epoch: 10/100 | step: 192/422 | loss: 4.374398708343506\n",
      "Epoch: 10/100 | step: 193/422 | loss: 4.258611679077148\n",
      "Epoch: 10/100 | step: 194/422 | loss: 4.187269687652588\n",
      "Epoch: 10/100 | step: 195/422 | loss: 4.152351379394531\n",
      "Epoch: 10/100 | step: 196/422 | loss: 4.184299468994141\n",
      "Epoch: 10/100 | step: 197/422 | loss: 4.207686901092529\n",
      "Epoch: 10/100 | step: 198/422 | loss: 4.294370651245117\n",
      "Epoch: 10/100 | step: 199/422 | loss: 4.130495548248291\n",
      "Epoch: 10/100 | step: 200/422 | loss: 4.257874488830566\n",
      "Epoch: 10/100 | step: 201/422 | loss: 4.0698747634887695\n",
      "Epoch: 10/100 | step: 202/422 | loss: 4.08819055557251\n",
      "Epoch: 10/100 | step: 203/422 | loss: 4.008052349090576\n",
      "Epoch: 10/100 | step: 204/422 | loss: 4.164894104003906\n",
      "Epoch: 10/100 | step: 205/422 | loss: 3.883561372756958\n",
      "Epoch: 10/100 | step: 206/422 | loss: 4.13953971862793\n",
      "Epoch: 10/100 | step: 207/422 | loss: 4.303251266479492\n",
      "Epoch: 10/100 | step: 208/422 | loss: 4.336549282073975\n",
      "Epoch: 10/100 | step: 209/422 | loss: 4.211547374725342\n",
      "Epoch: 10/100 | step: 210/422 | loss: 4.222606658935547\n",
      "Epoch: 10/100 | step: 211/422 | loss: 4.253166675567627\n",
      "Epoch: 10/100 | step: 212/422 | loss: 4.080699920654297\n",
      "Epoch: 10/100 | step: 213/422 | loss: 4.260836601257324\n",
      "Epoch: 10/100 | step: 214/422 | loss: 4.159050941467285\n",
      "Epoch: 10/100 | step: 215/422 | loss: 4.078863143920898\n",
      "Epoch: 10/100 | step: 216/422 | loss: 4.442401885986328\n",
      "Epoch: 10/100 | step: 217/422 | loss: 4.198598384857178\n",
      "Epoch: 10/100 | step: 218/422 | loss: 4.306416988372803\n",
      "Epoch: 10/100 | step: 219/422 | loss: 4.0815110206604\n",
      "Epoch: 10/100 | step: 220/422 | loss: 4.095921516418457\n",
      "Epoch: 10/100 | step: 221/422 | loss: 4.214622497558594\n",
      "Epoch: 10/100 | step: 222/422 | loss: 4.119630813598633\n",
      "Epoch: 10/100 | step: 223/422 | loss: 4.2189040184021\n",
      "Epoch: 10/100 | step: 224/422 | loss: 4.226552486419678\n",
      "Epoch: 10/100 | step: 225/422 | loss: 4.033419609069824\n",
      "Epoch: 10/100 | step: 226/422 | loss: 4.235157012939453\n",
      "Epoch: 10/100 | step: 227/422 | loss: 4.249484539031982\n",
      "Epoch: 10/100 | step: 228/422 | loss: 4.15343713760376\n",
      "Epoch: 10/100 | step: 229/422 | loss: 4.050661087036133\n",
      "Epoch: 10/100 | step: 230/422 | loss: 3.9427690505981445\n",
      "Epoch: 10/100 | step: 231/422 | loss: 4.436581611633301\n",
      "Epoch: 10/100 | step: 232/422 | loss: 4.203304290771484\n",
      "Epoch: 10/100 | step: 233/422 | loss: 4.298277854919434\n",
      "Epoch: 10/100 | step: 234/422 | loss: 4.120968818664551\n",
      "Epoch: 10/100 | step: 235/422 | loss: 4.257950782775879\n",
      "Epoch: 10/100 | step: 236/422 | loss: 4.182460308074951\n",
      "Epoch: 10/100 | step: 237/422 | loss: 4.222938537597656\n",
      "Epoch: 10/100 | step: 238/422 | loss: 4.082413673400879\n",
      "Epoch: 10/100 | step: 239/422 | loss: 4.323790550231934\n",
      "Epoch: 10/100 | step: 240/422 | loss: 4.309487342834473\n",
      "Epoch: 10/100 | step: 241/422 | loss: 4.213790416717529\n",
      "Epoch: 10/100 | step: 242/422 | loss: 3.9770567417144775\n",
      "Epoch: 10/100 | step: 243/422 | loss: 4.227702617645264\n",
      "Epoch: 10/100 | step: 244/422 | loss: 4.3711161613464355\n",
      "Epoch: 10/100 | step: 245/422 | loss: 4.117525577545166\n",
      "Epoch: 10/100 | step: 246/422 | loss: 4.024580955505371\n",
      "Epoch: 10/100 | step: 247/422 | loss: 3.9862051010131836\n",
      "Epoch: 10/100 | step: 248/422 | loss: 4.021058559417725\n",
      "Epoch: 10/100 | step: 249/422 | loss: 4.271058082580566\n",
      "Epoch: 10/100 | step: 250/422 | loss: 4.199997425079346\n",
      "Epoch: 10/100 | step: 251/422 | loss: 4.2481231689453125\n",
      "Epoch: 10/100 | step: 252/422 | loss: 4.150486946105957\n",
      "Epoch: 10/100 | step: 253/422 | loss: 4.226556777954102\n",
      "Epoch: 10/100 | step: 254/422 | loss: 4.202941417694092\n",
      "Epoch: 10/100 | step: 255/422 | loss: 4.097005844116211\n",
      "Epoch: 10/100 | step: 256/422 | loss: 4.176923751831055\n",
      "Epoch: 10/100 | step: 257/422 | loss: 4.176206588745117\n",
      "Epoch: 10/100 | step: 258/422 | loss: 4.426813125610352\n",
      "Epoch: 10/100 | step: 259/422 | loss: 4.256875991821289\n",
      "Epoch: 10/100 | step: 260/422 | loss: 4.134819984436035\n",
      "Epoch: 10/100 | step: 261/422 | loss: 4.116650581359863\n",
      "Epoch: 10/100 | step: 262/422 | loss: 3.945650815963745\n",
      "Epoch: 10/100 | step: 263/422 | loss: 3.952709197998047\n",
      "Epoch: 10/100 | step: 264/422 | loss: 4.230433940887451\n",
      "Epoch: 10/100 | step: 265/422 | loss: 4.361654758453369\n",
      "Epoch: 10/100 | step: 266/422 | loss: 4.265318393707275\n",
      "Epoch: 10/100 | step: 267/422 | loss: 3.990936756134033\n",
      "Epoch: 10/100 | step: 268/422 | loss: 4.167334079742432\n",
      "Epoch: 10/100 | step: 269/422 | loss: 4.093569278717041\n",
      "Epoch: 10/100 | step: 270/422 | loss: 4.218230247497559\n",
      "Epoch: 10/100 | step: 271/422 | loss: 4.255800724029541\n",
      "Epoch: 10/100 | step: 272/422 | loss: 4.184637546539307\n",
      "Epoch: 10/100 | step: 273/422 | loss: 4.206748962402344\n",
      "Epoch: 10/100 | step: 274/422 | loss: 4.338648796081543\n",
      "Epoch: 10/100 | step: 275/422 | loss: 4.131081581115723\n",
      "Epoch: 10/100 | step: 276/422 | loss: 4.014902114868164\n",
      "Epoch: 10/100 | step: 277/422 | loss: 4.313854694366455\n",
      "Epoch: 10/100 | step: 278/422 | loss: 4.212367057800293\n",
      "Epoch: 10/100 | step: 279/422 | loss: 4.006810188293457\n",
      "Epoch: 10/100 | step: 280/422 | loss: 4.132610321044922\n",
      "Epoch: 10/100 | step: 281/422 | loss: 4.386507511138916\n",
      "Epoch: 10/100 | step: 282/422 | loss: 4.1633620262146\n",
      "Epoch: 10/100 | step: 283/422 | loss: 4.075666904449463\n",
      "Epoch: 10/100 | step: 284/422 | loss: 3.9856202602386475\n",
      "Epoch: 10/100 | step: 285/422 | loss: 4.42540979385376\n",
      "Epoch: 10/100 | step: 286/422 | loss: 4.2139129638671875\n",
      "Epoch: 10/100 | step: 287/422 | loss: 4.243070125579834\n",
      "Epoch: 10/100 | step: 288/422 | loss: 4.112749099731445\n",
      "Epoch: 10/100 | step: 289/422 | loss: 4.108095169067383\n",
      "Epoch: 10/100 | step: 290/422 | loss: 4.346188545227051\n",
      "Epoch: 10/100 | step: 291/422 | loss: 3.984684705734253\n",
      "Epoch: 10/100 | step: 292/422 | loss: 3.9127204418182373\n",
      "Epoch: 10/100 | step: 293/422 | loss: 4.121713161468506\n",
      "Epoch: 10/100 | step: 294/422 | loss: 4.025394439697266\n",
      "Epoch: 10/100 | step: 295/422 | loss: 4.129024982452393\n",
      "Epoch: 10/100 | step: 296/422 | loss: 4.222233295440674\n",
      "Epoch: 10/100 | step: 297/422 | loss: 4.209387302398682\n",
      "Epoch: 10/100 | step: 298/422 | loss: 4.107963562011719\n",
      "Epoch: 10/100 | step: 299/422 | loss: 4.110074520111084\n",
      "Epoch: 10/100 | step: 300/422 | loss: 4.18715238571167\n",
      "Epoch: 10/100 | step: 301/422 | loss: 4.209111213684082\n",
      "Epoch: 10/100 | step: 302/422 | loss: 4.244028568267822\n",
      "Epoch: 10/100 | step: 303/422 | loss: 4.1899871826171875\n",
      "Epoch: 10/100 | step: 304/422 | loss: 4.178154945373535\n",
      "Epoch: 10/100 | step: 305/422 | loss: 4.461073398590088\n",
      "Epoch: 10/100 | step: 306/422 | loss: 4.33687686920166\n",
      "Epoch: 10/100 | step: 307/422 | loss: 4.123465061187744\n",
      "Epoch: 10/100 | step: 308/422 | loss: 4.291261672973633\n",
      "Epoch: 10/100 | step: 309/422 | loss: 4.046199321746826\n",
      "Epoch: 10/100 | step: 310/422 | loss: 4.302618026733398\n",
      "Epoch: 10/100 | step: 311/422 | loss: 4.251491546630859\n",
      "Epoch: 10/100 | step: 312/422 | loss: 4.428748607635498\n",
      "Epoch: 10/100 | step: 313/422 | loss: 4.116684913635254\n",
      "Epoch: 10/100 | step: 314/422 | loss: 4.216310024261475\n",
      "Epoch: 10/100 | step: 315/422 | loss: 4.36013650894165\n",
      "Epoch: 10/100 | step: 316/422 | loss: 4.255753040313721\n",
      "Epoch: 10/100 | step: 317/422 | loss: 4.302622318267822\n",
      "Epoch: 10/100 | step: 318/422 | loss: 4.219277858734131\n",
      "Epoch: 10/100 | step: 319/422 | loss: 4.117038726806641\n",
      "Epoch: 10/100 | step: 320/422 | loss: 4.239790916442871\n",
      "Epoch: 10/100 | step: 321/422 | loss: 4.2386674880981445\n",
      "Epoch: 10/100 | step: 322/422 | loss: 4.201691627502441\n",
      "Epoch: 10/100 | step: 323/422 | loss: 4.251411437988281\n",
      "Epoch: 10/100 | step: 324/422 | loss: 4.127130508422852\n",
      "Epoch: 10/100 | step: 325/422 | loss: 4.250153541564941\n",
      "Epoch: 10/100 | step: 326/422 | loss: 4.072265625\n",
      "Epoch: 10/100 | step: 327/422 | loss: 4.277331352233887\n",
      "Epoch: 10/100 | step: 328/422 | loss: 3.9211971759796143\n",
      "Epoch: 10/100 | step: 329/422 | loss: 4.31477689743042\n",
      "Epoch: 10/100 | step: 330/422 | loss: 4.366064071655273\n",
      "Epoch: 10/100 | step: 331/422 | loss: 4.287206649780273\n",
      "Epoch: 10/100 | step: 332/422 | loss: 3.9979360103607178\n",
      "Epoch: 10/100 | step: 333/422 | loss: 4.262561321258545\n",
      "Epoch: 10/100 | step: 334/422 | loss: 4.165790557861328\n",
      "Epoch: 10/100 | step: 335/422 | loss: 4.008643627166748\n",
      "Epoch: 10/100 | step: 336/422 | loss: 4.369100570678711\n",
      "Epoch: 10/100 | step: 337/422 | loss: 4.004160404205322\n",
      "Epoch: 10/100 | step: 338/422 | loss: 4.3217363357543945\n",
      "Epoch: 10/100 | step: 339/422 | loss: 4.148255348205566\n",
      "Epoch: 10/100 | step: 340/422 | loss: 4.215499401092529\n",
      "Epoch: 10/100 | step: 341/422 | loss: 4.289135456085205\n",
      "Epoch: 10/100 | step: 342/422 | loss: 4.157201766967773\n",
      "Epoch: 10/100 | step: 343/422 | loss: 4.023701190948486\n",
      "Epoch: 10/100 | step: 344/422 | loss: 4.184299468994141\n",
      "Epoch: 10/100 | step: 345/422 | loss: 4.194301128387451\n",
      "Epoch: 10/100 | step: 346/422 | loss: 4.061513900756836\n",
      "Epoch: 10/100 | step: 347/422 | loss: 4.197368144989014\n",
      "Epoch: 10/100 | step: 348/422 | loss: 4.01927375793457\n",
      "Epoch: 10/100 | step: 349/422 | loss: 3.9809117317199707\n",
      "Epoch: 10/100 | step: 350/422 | loss: 4.065436840057373\n",
      "Epoch: 10/100 | step: 351/422 | loss: 4.026062488555908\n",
      "Epoch: 10/100 | step: 352/422 | loss: 4.28110933303833\n",
      "Epoch: 10/100 | step: 353/422 | loss: 4.024040222167969\n",
      "Epoch: 10/100 | step: 354/422 | loss: 4.290814399719238\n",
      "Epoch: 10/100 | step: 355/422 | loss: 3.94071102142334\n",
      "Epoch: 10/100 | step: 356/422 | loss: 4.071213722229004\n",
      "Epoch: 10/100 | step: 357/422 | loss: 4.0597758293151855\n",
      "Epoch: 10/100 | step: 358/422 | loss: 4.152763366699219\n",
      "Epoch: 10/100 | step: 359/422 | loss: 3.9866549968719482\n",
      "Epoch: 10/100 | step: 360/422 | loss: 4.131988525390625\n",
      "Epoch: 10/100 | step: 361/422 | loss: 4.234366416931152\n",
      "Epoch: 10/100 | step: 362/422 | loss: 3.9034149646759033\n",
      "Epoch: 10/100 | step: 363/422 | loss: 4.1703104972839355\n",
      "Epoch: 10/100 | step: 364/422 | loss: 4.230967998504639\n",
      "Epoch: 10/100 | step: 365/422 | loss: 4.152277946472168\n",
      "Epoch: 10/100 | step: 366/422 | loss: 3.9296555519104004\n",
      "Epoch: 10/100 | step: 367/422 | loss: 4.192833423614502\n",
      "Epoch: 10/100 | step: 368/422 | loss: 4.038347244262695\n",
      "Epoch: 10/100 | step: 369/422 | loss: 4.30866003036499\n",
      "Epoch: 10/100 | step: 370/422 | loss: 3.88276743888855\n",
      "Epoch: 10/100 | step: 371/422 | loss: 4.134171009063721\n",
      "Epoch: 10/100 | step: 372/422 | loss: 4.145092010498047\n",
      "Epoch: 10/100 | step: 373/422 | loss: 4.034939765930176\n",
      "Epoch: 10/100 | step: 374/422 | loss: 4.204823017120361\n",
      "Epoch: 10/100 | step: 375/422 | loss: 4.13482666015625\n",
      "Epoch: 10/100 | step: 376/422 | loss: 4.002781391143799\n",
      "Epoch: 10/100 | step: 377/422 | loss: 3.8105595111846924\n",
      "Epoch: 10/100 | step: 378/422 | loss: 4.229400634765625\n",
      "Epoch: 10/100 | step: 379/422 | loss: 4.003849029541016\n",
      "Epoch: 10/100 | step: 380/422 | loss: 4.095858573913574\n",
      "Epoch: 10/100 | step: 381/422 | loss: 4.135684967041016\n",
      "Epoch: 10/100 | step: 382/422 | loss: 4.067112445831299\n",
      "Epoch: 10/100 | step: 383/422 | loss: 4.0246405601501465\n",
      "Epoch: 10/100 | step: 384/422 | loss: 4.044185638427734\n",
      "Epoch: 10/100 | step: 385/422 | loss: 4.047786712646484\n",
      "Epoch: 10/100 | step: 386/422 | loss: 4.2133588790893555\n",
      "Epoch: 10/100 | step: 387/422 | loss: 4.037242889404297\n",
      "Epoch: 10/100 | step: 388/422 | loss: 4.110481262207031\n",
      "Epoch: 10/100 | step: 389/422 | loss: 4.114381790161133\n",
      "Epoch: 10/100 | step: 390/422 | loss: 4.1161417961120605\n",
      "Epoch: 10/100 | step: 391/422 | loss: 4.219355583190918\n",
      "Epoch: 10/100 | step: 392/422 | loss: 4.0507731437683105\n",
      "Epoch: 10/100 | step: 393/422 | loss: 4.101099014282227\n",
      "Epoch: 10/100 | step: 394/422 | loss: 4.290127277374268\n",
      "Epoch: 10/100 | step: 395/422 | loss: 4.182145595550537\n",
      "Epoch: 10/100 | step: 396/422 | loss: 4.087250709533691\n",
      "Epoch: 10/100 | step: 397/422 | loss: 4.1615400314331055\n",
      "Epoch: 10/100 | step: 398/422 | loss: 4.084580421447754\n",
      "Epoch: 10/100 | step: 399/422 | loss: 4.122470378875732\n",
      "Epoch: 10/100 | step: 400/422 | loss: 3.9728050231933594\n",
      "Epoch: 10/100 | step: 401/422 | loss: 4.2013163566589355\n",
      "Epoch: 10/100 | step: 402/422 | loss: 4.294390678405762\n",
      "Epoch: 10/100 | step: 403/422 | loss: 4.157450199127197\n",
      "Epoch: 10/100 | step: 404/422 | loss: 4.0404510498046875\n",
      "Epoch: 10/100 | step: 405/422 | loss: 4.02095365524292\n",
      "Epoch: 10/100 | step: 406/422 | loss: 4.009829998016357\n",
      "Epoch: 10/100 | step: 407/422 | loss: 4.0194597244262695\n",
      "Epoch: 10/100 | step: 408/422 | loss: 4.2825493812561035\n",
      "Epoch: 10/100 | step: 409/422 | loss: 4.151383399963379\n",
      "Epoch: 10/100 | step: 410/422 | loss: 4.3635382652282715\n",
      "Epoch: 10/100 | step: 411/422 | loss: 4.081110000610352\n",
      "Epoch: 10/100 | step: 412/422 | loss: 4.263169288635254\n",
      "Epoch: 10/100 | step: 413/422 | loss: 3.939711809158325\n",
      "Epoch: 10/100 | step: 414/422 | loss: 4.386307716369629\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 11/100 | step: 1/422 | loss: 4.123719692230225\n",
      "Epoch: 11/100 | step: 2/422 | loss: 4.1233720779418945\n",
      "Epoch: 11/100 | step: 3/422 | loss: 3.919640064239502\n",
      "Epoch: 11/100 | step: 4/422 | loss: 4.301988124847412\n",
      "Epoch: 11/100 | step: 5/422 | loss: 4.218045234680176\n",
      "Epoch: 11/100 | step: 6/422 | loss: 4.236569404602051\n",
      "Epoch: 11/100 | step: 7/422 | loss: 4.04465389251709\n",
      "Epoch: 11/100 | step: 8/422 | loss: 4.1682233810424805\n",
      "Epoch: 11/100 | step: 9/422 | loss: 4.130436420440674\n",
      "Epoch: 11/100 | step: 10/422 | loss: 4.031258583068848\n",
      "Epoch: 11/100 | step: 11/422 | loss: 4.183293342590332\n",
      "Epoch: 11/100 | step: 12/422 | loss: 4.034610271453857\n",
      "Epoch: 11/100 | step: 13/422 | loss: 3.9221274852752686\n",
      "Epoch: 11/100 | step: 14/422 | loss: 4.439118385314941\n",
      "Epoch: 11/100 | step: 15/422 | loss: 4.161259651184082\n",
      "Epoch: 11/100 | step: 16/422 | loss: 3.934217929840088\n",
      "Epoch: 11/100 | step: 17/422 | loss: 3.984642505645752\n",
      "Epoch: 11/100 | step: 18/422 | loss: 4.1590776443481445\n",
      "Epoch: 11/100 | step: 19/422 | loss: 4.072183132171631\n",
      "Epoch: 11/100 | step: 20/422 | loss: 4.01092529296875\n",
      "Epoch: 11/100 | step: 21/422 | loss: 4.100165843963623\n",
      "Epoch: 11/100 | step: 22/422 | loss: 4.28031587600708\n",
      "Epoch: 11/100 | step: 23/422 | loss: 3.9578936100006104\n",
      "Epoch: 11/100 | step: 24/422 | loss: 4.011295318603516\n",
      "Epoch: 11/100 | step: 25/422 | loss: 4.119135856628418\n",
      "Epoch: 11/100 | step: 26/422 | loss: 4.058789253234863\n",
      "Epoch: 11/100 | step: 27/422 | loss: 4.025638103485107\n",
      "Epoch: 11/100 | step: 28/422 | loss: 4.301177501678467\n",
      "Epoch: 11/100 | step: 29/422 | loss: 4.497519493103027\n",
      "Epoch: 11/100 | step: 30/422 | loss: 4.168495178222656\n",
      "Epoch: 11/100 | step: 31/422 | loss: 3.956557512283325\n",
      "Epoch: 11/100 | step: 32/422 | loss: 4.125270366668701\n",
      "Epoch: 11/100 | step: 33/422 | loss: 3.991211414337158\n",
      "Epoch: 11/100 | step: 34/422 | loss: 4.217746257781982\n",
      "Epoch: 11/100 | step: 35/422 | loss: 4.2143473625183105\n",
      "Epoch: 11/100 | step: 36/422 | loss: 4.0076823234558105\n",
      "Epoch: 11/100 | step: 37/422 | loss: 4.067212104797363\n",
      "Epoch: 11/100 | step: 38/422 | loss: 4.176268577575684\n",
      "Epoch: 11/100 | step: 39/422 | loss: 4.0871148109436035\n",
      "Epoch: 11/100 | step: 40/422 | loss: 3.963042974472046\n",
      "Epoch: 11/100 | step: 41/422 | loss: 3.8701975345611572\n",
      "Epoch: 11/100 | step: 42/422 | loss: 4.2234296798706055\n",
      "Epoch: 11/100 | step: 43/422 | loss: 4.093855381011963\n",
      "Epoch: 11/100 | step: 44/422 | loss: 4.129834175109863\n",
      "Epoch: 11/100 | step: 45/422 | loss: 4.029372215270996\n",
      "Epoch: 11/100 | step: 46/422 | loss: 4.181358337402344\n",
      "Epoch: 11/100 | step: 47/422 | loss: 3.7861292362213135\n",
      "Epoch: 11/100 | step: 48/422 | loss: 4.161838054656982\n",
      "Epoch: 11/100 | step: 49/422 | loss: 4.189702987670898\n",
      "Epoch: 11/100 | step: 50/422 | loss: 4.232239723205566\n",
      "Epoch: 11/100 | step: 51/422 | loss: 4.060062885284424\n",
      "Epoch: 11/100 | step: 52/422 | loss: 4.280701160430908\n",
      "Epoch: 11/100 | step: 53/422 | loss: 4.151700019836426\n",
      "Epoch: 11/100 | step: 54/422 | loss: 4.136575698852539\n",
      "Epoch: 11/100 | step: 55/422 | loss: 4.088572978973389\n",
      "Epoch: 11/100 | step: 56/422 | loss: 4.0258097648620605\n",
      "Epoch: 11/100 | step: 57/422 | loss: 4.074676513671875\n",
      "Epoch: 11/100 | step: 58/422 | loss: 3.9981484413146973\n",
      "Epoch: 11/100 | step: 59/422 | loss: 4.337688446044922\n",
      "Epoch: 11/100 | step: 60/422 | loss: 4.175233364105225\n",
      "Epoch: 11/100 | step: 61/422 | loss: 3.9681923389434814\n",
      "Epoch: 11/100 | step: 62/422 | loss: 4.183299541473389\n",
      "Epoch: 11/100 | step: 63/422 | loss: 4.120649337768555\n",
      "Epoch: 11/100 | step: 64/422 | loss: 4.008591175079346\n",
      "Epoch: 11/100 | step: 65/422 | loss: 3.9838240146636963\n",
      "Epoch: 11/100 | step: 66/422 | loss: 4.100432395935059\n",
      "Epoch: 11/100 | step: 67/422 | loss: 4.33427095413208\n",
      "Epoch: 11/100 | step: 68/422 | loss: 3.9468252658843994\n",
      "Epoch: 11/100 | step: 69/422 | loss: 3.8655953407287598\n",
      "Epoch: 11/100 | step: 70/422 | loss: 4.183530330657959\n",
      "Epoch: 11/100 | step: 71/422 | loss: 4.489608287811279\n",
      "Epoch: 11/100 | step: 72/422 | loss: 4.200415134429932\n",
      "Epoch: 11/100 | step: 73/422 | loss: 4.248331069946289\n",
      "Epoch: 11/100 | step: 74/422 | loss: 4.028337478637695\n",
      "Epoch: 11/100 | step: 75/422 | loss: 4.117128849029541\n",
      "Epoch: 11/100 | step: 76/422 | loss: 4.07826042175293\n",
      "Epoch: 11/100 | step: 77/422 | loss: 4.102702617645264\n",
      "Epoch: 11/100 | step: 78/422 | loss: 3.9112448692321777\n",
      "Epoch: 11/100 | step: 79/422 | loss: 3.92458438873291\n",
      "Epoch: 11/100 | step: 80/422 | loss: 4.211710453033447\n",
      "Epoch: 11/100 | step: 81/422 | loss: 4.191873073577881\n",
      "Epoch: 11/100 | step: 82/422 | loss: 3.9936654567718506\n",
      "Epoch: 11/100 | step: 83/422 | loss: 4.240896224975586\n",
      "Epoch: 11/100 | step: 84/422 | loss: 3.9345057010650635\n",
      "Epoch: 11/100 | step: 85/422 | loss: 4.048287868499756\n",
      "Epoch: 11/100 | step: 86/422 | loss: 4.001302242279053\n",
      "Epoch: 11/100 | step: 87/422 | loss: 4.152821063995361\n",
      "Epoch: 11/100 | step: 88/422 | loss: 4.07534646987915\n",
      "Epoch: 11/100 | step: 89/422 | loss: 4.23664665222168\n",
      "Epoch: 11/100 | step: 90/422 | loss: 4.007907867431641\n",
      "Epoch: 11/100 | step: 91/422 | loss: 3.8894107341766357\n",
      "Epoch: 11/100 | step: 92/422 | loss: 4.13438606262207\n",
      "Epoch: 11/100 | step: 93/422 | loss: 4.1762213706970215\n",
      "Epoch: 11/100 | step: 94/422 | loss: 4.082338333129883\n",
      "Epoch: 11/100 | step: 95/422 | loss: 3.7504913806915283\n",
      "Epoch: 11/100 | step: 96/422 | loss: 4.1336669921875\n",
      "Epoch: 11/100 | step: 97/422 | loss: 4.120368003845215\n",
      "Epoch: 11/100 | step: 98/422 | loss: 4.059373378753662\n",
      "Epoch: 11/100 | step: 99/422 | loss: 3.9984138011932373\n",
      "Epoch: 11/100 | step: 100/422 | loss: 4.074428558349609\n",
      "Epoch: 11/100 | step: 101/422 | loss: 3.8888211250305176\n",
      "Epoch: 11/100 | step: 102/422 | loss: 4.283559799194336\n",
      "Epoch: 11/100 | step: 103/422 | loss: 3.984198570251465\n",
      "Epoch: 11/100 | step: 104/422 | loss: 4.065879821777344\n",
      "Epoch: 11/100 | step: 105/422 | loss: 3.9292774200439453\n",
      "Epoch: 11/100 | step: 106/422 | loss: 3.9963557720184326\n",
      "Epoch: 11/100 | step: 107/422 | loss: 3.916855573654175\n",
      "Epoch: 11/100 | step: 108/422 | loss: 4.017777919769287\n",
      "Epoch: 11/100 | step: 109/422 | loss: 4.129662036895752\n",
      "Epoch: 11/100 | step: 110/422 | loss: 3.8600802421569824\n",
      "Epoch: 11/100 | step: 111/422 | loss: 4.018442153930664\n",
      "Epoch: 11/100 | step: 112/422 | loss: 4.038743495941162\n",
      "Epoch: 11/100 | step: 113/422 | loss: 3.9700844287872314\n",
      "Epoch: 11/100 | step: 114/422 | loss: 3.926302671432495\n",
      "Epoch: 11/100 | step: 115/422 | loss: 4.012453079223633\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 12/100 | step: 1/422 | loss: 3.8420279026031494\n",
      "Epoch: 12/100 | step: 2/422 | loss: 3.855905532836914\n",
      "Epoch: 12/100 | step: 3/422 | loss: 3.9691314697265625\n",
      "Epoch: 12/100 | step: 4/422 | loss: 4.112517356872559\n",
      "Epoch: 12/100 | step: 5/422 | loss: 4.224330902099609\n",
      "Epoch: 12/100 | step: 6/422 | loss: 3.9829812049865723\n",
      "Epoch: 12/100 | step: 7/422 | loss: 4.079927444458008\n",
      "Epoch: 12/100 | step: 8/422 | loss: 3.9982495307922363\n",
      "Epoch: 12/100 | step: 9/422 | loss: 3.863288402557373\n",
      "Epoch: 12/100 | step: 10/422 | loss: 3.8542873859405518\n",
      "Epoch: 12/100 | step: 11/422 | loss: 4.05422306060791\n",
      "Epoch: 12/100 | step: 12/422 | loss: 4.136980056762695\n",
      "Epoch: 12/100 | step: 13/422 | loss: 3.9562530517578125\n",
      "Epoch: 12/100 | step: 14/422 | loss: 3.9451680183410645\n",
      "Epoch: 12/100 | step: 15/422 | loss: 3.909149169921875\n",
      "Epoch: 12/100 | step: 16/422 | loss: 3.832547187805176\n",
      "Epoch: 12/100 | step: 17/422 | loss: 4.197348117828369\n",
      "Epoch: 12/100 | step: 18/422 | loss: 3.72411847114563\n",
      "Epoch: 12/100 | step: 19/422 | loss: 4.145082473754883\n",
      "Epoch: 12/100 | step: 20/422 | loss: 4.119194030761719\n",
      "Epoch: 12/100 | step: 21/422 | loss: 3.9646975994110107\n",
      "Epoch: 12/100 | step: 22/422 | loss: 4.088132858276367\n",
      "Epoch: 12/100 | step: 23/422 | loss: 4.29114294052124\n",
      "Epoch: 12/100 | step: 24/422 | loss: 3.9310662746429443\n",
      "Epoch: 12/100 | step: 25/422 | loss: 4.20544958114624\n",
      "Epoch: 12/100 | step: 26/422 | loss: 4.077554702758789\n",
      "Epoch: 12/100 | step: 27/422 | loss: 3.9857630729675293\n",
      "Epoch: 12/100 | step: 28/422 | loss: 3.9512763023376465\n",
      "Epoch: 12/100 | step: 29/422 | loss: 3.954742431640625\n",
      "Epoch: 12/100 | step: 30/422 | loss: 4.243836402893066\n",
      "Epoch: 12/100 | step: 31/422 | loss: 4.0453081130981445\n",
      "Epoch: 12/100 | step: 32/422 | loss: 4.1962361335754395\n",
      "Epoch: 12/100 | step: 33/422 | loss: 3.9512133598327637\n",
      "Epoch: 12/100 | step: 34/422 | loss: 3.790968894958496\n",
      "Epoch: 12/100 | step: 35/422 | loss: 3.90380859375\n",
      "Epoch: 12/100 | step: 36/422 | loss: 4.154569625854492\n",
      "Epoch: 12/100 | step: 37/422 | loss: 4.045154571533203\n",
      "Epoch: 12/100 | step: 38/422 | loss: 4.1064934730529785\n",
      "Epoch: 12/100 | step: 39/422 | loss: 4.363703727722168\n",
      "Epoch: 12/100 | step: 40/422 | loss: 4.1193718910217285\n",
      "Epoch: 12/100 | step: 41/422 | loss: 3.857252597808838\n",
      "Epoch: 12/100 | step: 42/422 | loss: 4.112536907196045\n",
      "Epoch: 12/100 | step: 43/422 | loss: 3.9791641235351562\n",
      "Epoch: 12/100 | step: 44/422 | loss: 4.158912181854248\n",
      "Epoch: 12/100 | step: 45/422 | loss: 4.055175304412842\n",
      "Epoch: 12/100 | step: 46/422 | loss: 4.003724575042725\n",
      "Epoch: 12/100 | step: 47/422 | loss: 4.117471694946289\n",
      "Epoch: 12/100 | step: 48/422 | loss: 3.9997928142547607\n",
      "Epoch: 12/100 | step: 49/422 | loss: 3.8702034950256348\n",
      "Epoch: 12/100 | step: 50/422 | loss: 4.116217613220215\n",
      "Epoch: 12/100 | step: 51/422 | loss: 3.8923544883728027\n",
      "Epoch: 12/100 | step: 52/422 | loss: 4.19550085067749\n",
      "Epoch: 12/100 | step: 53/422 | loss: 3.961308002471924\n",
      "Epoch: 12/100 | step: 54/422 | loss: 4.145184516906738\n",
      "Epoch: 12/100 | step: 55/422 | loss: 4.277335166931152\n",
      "Epoch: 12/100 | step: 56/422 | loss: 4.087116718292236\n",
      "Epoch: 12/100 | step: 57/422 | loss: 4.0841264724731445\n",
      "Epoch: 12/100 | step: 58/422 | loss: 4.169791221618652\n",
      "Epoch: 12/100 | step: 59/422 | loss: 4.100454807281494\n",
      "Epoch: 12/100 | step: 60/422 | loss: 3.8724191188812256\n",
      "Epoch: 12/100 | step: 61/422 | loss: 4.23849630355835\n",
      "Epoch: 12/100 | step: 62/422 | loss: 3.923025608062744\n",
      "Epoch: 12/100 | step: 63/422 | loss: 4.190550327301025\n",
      "Epoch: 12/100 | step: 64/422 | loss: 3.912341356277466\n",
      "Epoch: 12/100 | step: 65/422 | loss: 3.8421220779418945\n",
      "Epoch: 12/100 | step: 66/422 | loss: 4.100360870361328\n",
      "Epoch: 12/100 | step: 67/422 | loss: 3.8982093334198\n",
      "Epoch: 12/100 | step: 68/422 | loss: 4.170413970947266\n",
      "Epoch: 12/100 | step: 69/422 | loss: 4.197999000549316\n",
      "Epoch: 12/100 | step: 70/422 | loss: 3.810434341430664\n",
      "Epoch: 12/100 | step: 71/422 | loss: 4.133670330047607\n",
      "Epoch: 12/100 | step: 72/422 | loss: 4.202789783477783\n",
      "Epoch: 12/100 | step: 73/422 | loss: 4.039120674133301\n",
      "Epoch: 12/100 | step: 74/422 | loss: 4.222839832305908\n",
      "Epoch: 12/100 | step: 75/422 | loss: 4.052328586578369\n",
      "Epoch: 12/100 | step: 76/422 | loss: 3.918478012084961\n",
      "Epoch: 12/100 | step: 77/422 | loss: 3.9607651233673096\n",
      "Epoch: 12/100 | step: 78/422 | loss: 4.012763023376465\n",
      "Epoch: 12/100 | step: 79/422 | loss: 3.977759838104248\n",
      "Epoch: 12/100 | step: 80/422 | loss: 3.915111541748047\n",
      "Epoch: 12/100 | step: 81/422 | loss: 3.909200668334961\n",
      "Epoch: 12/100 | step: 82/422 | loss: 3.8778154850006104\n",
      "Epoch: 12/100 | step: 83/422 | loss: 4.177621841430664\n",
      "Epoch: 12/100 | step: 84/422 | loss: 4.027674198150635\n",
      "Epoch: 12/100 | step: 85/422 | loss: 3.9278488159179688\n",
      "Epoch: 12/100 | step: 86/422 | loss: 3.9586875438690186\n",
      "Epoch: 12/100 | step: 87/422 | loss: 3.930926561355591\n",
      "Epoch: 12/100 | step: 88/422 | loss: 4.146927833557129\n",
      "Epoch: 12/100 | step: 89/422 | loss: 4.419643878936768\n",
      "Epoch: 12/100 | step: 90/422 | loss: 3.927546501159668\n",
      "Epoch: 12/100 | step: 91/422 | loss: 4.229390621185303\n",
      "Epoch: 12/100 | step: 92/422 | loss: 4.19313907623291\n",
      "Epoch: 12/100 | step: 93/422 | loss: 4.2024245262146\n",
      "Epoch: 12/100 | step: 94/422 | loss: 3.941009283065796\n",
      "Epoch: 12/100 | step: 95/422 | loss: 3.9525818824768066\n",
      "Epoch: 12/100 | step: 96/422 | loss: 3.931155204772949\n",
      "Epoch: 12/100 | step: 97/422 | loss: 4.035433292388916\n",
      "Epoch: 12/100 | step: 98/422 | loss: 4.0302348136901855\n",
      "Epoch: 12/100 | step: 99/422 | loss: 4.066854953765869\n",
      "Epoch: 12/100 | step: 100/422 | loss: 4.120889186859131\n",
      "Epoch: 12/100 | step: 101/422 | loss: 3.92995285987854\n",
      "Epoch: 12/100 | step: 102/422 | loss: 4.12391996383667\n",
      "Epoch: 12/100 | step: 103/422 | loss: 4.060283184051514\n",
      "Epoch: 12/100 | step: 104/422 | loss: 3.9882826805114746\n",
      "Epoch: 12/100 | step: 105/422 | loss: 4.0129547119140625\n",
      "Epoch: 12/100 | step: 106/422 | loss: 3.910079002380371\n",
      "Epoch: 12/100 | step: 107/422 | loss: 3.8322322368621826\n",
      "Epoch: 12/100 | step: 108/422 | loss: 3.992971420288086\n",
      "Epoch: 12/100 | step: 109/422 | loss: 3.8794755935668945\n",
      "Epoch: 12/100 | step: 110/422 | loss: 4.022985458374023\n",
      "Epoch: 12/100 | step: 111/422 | loss: 3.9315426349639893\n",
      "Epoch: 12/100 | step: 112/422 | loss: 3.751938819885254\n",
      "Epoch: 12/100 | step: 113/422 | loss: 4.013073444366455\n",
      "Epoch: 12/100 | step: 114/422 | loss: 3.8985657691955566\n",
      "Epoch: 12/100 | step: 115/422 | loss: 3.9006378650665283\n",
      "Epoch: 12/100 | step: 116/422 | loss: 4.3783040046691895\n",
      "Epoch: 12/100 | step: 117/422 | loss: 3.8197262287139893\n",
      "Epoch: 12/100 | step: 118/422 | loss: 4.011786937713623\n",
      "Epoch: 12/100 | step: 119/422 | loss: 3.962782382965088\n",
      "Epoch: 12/100 | step: 120/422 | loss: 3.736600399017334\n",
      "Epoch: 12/100 | step: 121/422 | loss: 3.9245963096618652\n",
      "Epoch: 12/100 | step: 122/422 | loss: 4.126605033874512\n",
      "Epoch: 12/100 | step: 123/422 | loss: 3.907233476638794\n",
      "Epoch: 12/100 | step: 124/422 | loss: 3.929220676422119\n",
      "Epoch: 12/100 | step: 125/422 | loss: 3.9609215259552\n",
      "Epoch: 12/100 | step: 126/422 | loss: 4.105064392089844\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 13/100 | step: 1/422 | loss: 3.9537861347198486\n",
      "Epoch: 13/100 | step: 2/422 | loss: 3.9510440826416016\n",
      "Epoch: 13/100 | step: 3/422 | loss: 4.192933082580566\n",
      "Epoch: 13/100 | step: 4/422 | loss: 4.129909515380859\n",
      "Epoch: 13/100 | step: 5/422 | loss: 3.934154510498047\n",
      "Epoch: 13/100 | step: 6/422 | loss: 4.505836486816406\n",
      "Epoch: 13/100 | step: 7/422 | loss: 4.12885046005249\n",
      "Epoch: 13/100 | step: 8/422 | loss: 4.024509429931641\n",
      "Epoch: 13/100 | step: 9/422 | loss: 3.9845118522644043\n",
      "Epoch: 13/100 | step: 10/422 | loss: 4.084540367126465\n",
      "Epoch: 13/100 | step: 11/422 | loss: 4.221238136291504\n",
      "Epoch: 13/100 | step: 12/422 | loss: 3.8514790534973145\n",
      "Epoch: 13/100 | step: 13/422 | loss: 4.059826850891113\n",
      "Epoch: 13/100 | step: 14/422 | loss: 4.269871711730957\n",
      "Epoch: 13/100 | step: 15/422 | loss: 4.002750873565674\n",
      "Epoch: 13/100 | step: 16/422 | loss: 3.8133199214935303\n",
      "Epoch: 13/100 | step: 17/422 | loss: 4.047657489776611\n",
      "Epoch: 13/100 | step: 18/422 | loss: 4.080290794372559\n",
      "Epoch: 13/100 | step: 19/422 | loss: 3.7938313484191895\n",
      "Epoch: 13/100 | step: 20/422 | loss: 4.083681106567383\n",
      "Epoch: 13/100 | step: 21/422 | loss: 4.133006572723389\n",
      "Epoch: 13/100 | step: 22/422 | loss: 3.8964147567749023\n",
      "Epoch: 13/100 | step: 23/422 | loss: 3.9241347312927246\n",
      "Epoch: 13/100 | step: 24/422 | loss: 3.943577527999878\n",
      "Epoch: 13/100 | step: 25/422 | loss: 3.9478631019592285\n",
      "Epoch: 13/100 | step: 26/422 | loss: 3.927427291870117\n",
      "Epoch: 13/100 | step: 27/422 | loss: 4.021607875823975\n",
      "Epoch: 13/100 | step: 28/422 | loss: 3.963981866836548\n",
      "Epoch: 13/100 | step: 29/422 | loss: 3.874843120574951\n",
      "Epoch: 13/100 | step: 30/422 | loss: 3.7168211936950684\n",
      "Epoch: 13/100 | step: 31/422 | loss: 3.985640525817871\n",
      "Epoch: 13/100 | step: 32/422 | loss: 4.248470306396484\n",
      "Epoch: 13/100 | step: 33/422 | loss: 3.830864429473877\n",
      "Epoch: 13/100 | step: 34/422 | loss: 4.007577896118164\n",
      "Epoch: 13/100 | step: 35/422 | loss: 4.0858941078186035\n",
      "Epoch: 13/100 | step: 36/422 | loss: 3.9143335819244385\n",
      "Epoch: 13/100 | step: 37/422 | loss: 4.014462471008301\n",
      "Epoch: 13/100 | step: 38/422 | loss: 4.175849437713623\n",
      "Epoch: 13/100 | step: 39/422 | loss: 4.316525936126709\n",
      "Epoch: 13/100 | step: 40/422 | loss: 4.163830757141113\n",
      "Epoch: 13/100 | step: 41/422 | loss: 3.9819982051849365\n",
      "Epoch: 13/100 | step: 42/422 | loss: 4.071160316467285\n",
      "Epoch: 13/100 | step: 43/422 | loss: 4.0870280265808105\n",
      "Epoch: 13/100 | step: 44/422 | loss: 4.132707595825195\n",
      "Epoch: 13/100 | step: 45/422 | loss: 3.973599433898926\n",
      "Epoch: 13/100 | step: 46/422 | loss: 3.7934250831604004\n",
      "Epoch: 13/100 | step: 47/422 | loss: 4.059592247009277\n",
      "Epoch: 13/100 | step: 48/422 | loss: 3.737335681915283\n",
      "Epoch: 13/100 | step: 49/422 | loss: 3.8471155166625977\n",
      "Epoch: 13/100 | step: 50/422 | loss: 3.846851348876953\n",
      "Epoch: 13/100 | step: 51/422 | loss: 3.9887447357177734\n",
      "Epoch: 13/100 | step: 52/422 | loss: 3.843231201171875\n",
      "Epoch: 13/100 | step: 53/422 | loss: 3.9070565700531006\n",
      "Epoch: 13/100 | step: 54/422 | loss: 3.8583948612213135\n",
      "Epoch: 13/100 | step: 55/422 | loss: 3.959228515625\n",
      "Epoch: 13/100 | step: 56/422 | loss: 4.372995376586914\n",
      "Epoch: 13/100 | step: 57/422 | loss: 4.038166046142578\n",
      "Epoch: 13/100 | step: 58/422 | loss: 4.059264659881592\n",
      "Epoch: 13/100 | step: 59/422 | loss: 3.767998456954956\n",
      "Epoch: 13/100 | step: 60/422 | loss: 3.8608767986297607\n",
      "Epoch: 13/100 | step: 61/422 | loss: 3.7666006088256836\n",
      "Epoch: 13/100 | step: 62/422 | loss: 3.8366706371307373\n",
      "Epoch: 13/100 | step: 63/422 | loss: 4.039929389953613\n",
      "Epoch: 13/100 | step: 64/422 | loss: 4.170856952667236\n",
      "Epoch: 13/100 | step: 65/422 | loss: 4.123144626617432\n",
      "Epoch: 13/100 | step: 66/422 | loss: 3.9590096473693848\n",
      "Epoch: 13/100 | step: 67/422 | loss: 3.9951043128967285\n",
      "Epoch: 13/100 | step: 68/422 | loss: 4.073180675506592\n",
      "Epoch: 13/100 | step: 69/422 | loss: 4.256367206573486\n",
      "Epoch: 13/100 | step: 70/422 | loss: 3.828373908996582\n",
      "Epoch: 13/100 | step: 71/422 | loss: 4.139111042022705\n",
      "Epoch: 13/100 | step: 72/422 | loss: 4.051107883453369\n",
      "Epoch: 13/100 | step: 73/422 | loss: 3.9833531379699707\n",
      "Epoch: 13/100 | step: 74/422 | loss: 4.20008659362793\n",
      "Epoch: 13/100 | step: 75/422 | loss: 4.201434135437012\n",
      "Epoch: 13/100 | step: 76/422 | loss: 3.9546990394592285\n",
      "Epoch: 13/100 | step: 77/422 | loss: 4.0280585289001465\n",
      "Epoch: 13/100 | step: 78/422 | loss: 3.9414026737213135\n",
      "Epoch: 13/100 | step: 79/422 | loss: 4.074408054351807\n",
      "Epoch: 13/100 | step: 80/422 | loss: 3.865480899810791\n",
      "Epoch: 13/100 | step: 81/422 | loss: 4.197197914123535\n",
      "Epoch: 13/100 | step: 82/422 | loss: 3.956040620803833\n",
      "Epoch: 13/100 | step: 83/422 | loss: 4.029838562011719\n",
      "Epoch: 13/100 | step: 84/422 | loss: 3.514660120010376\n",
      "Epoch: 13/100 | step: 85/422 | loss: 4.000956058502197\n",
      "Epoch: 13/100 | step: 86/422 | loss: 4.045078277587891\n",
      "Epoch: 13/100 | step: 87/422 | loss: 3.968613862991333\n",
      "Epoch: 13/100 | step: 88/422 | loss: 4.137105941772461\n",
      "Epoch: 13/100 | step: 89/422 | loss: 3.5833652019500732\n",
      "Epoch: 13/100 | step: 90/422 | loss: 3.815640449523926\n",
      "Epoch: 13/100 | step: 91/422 | loss: 3.61978816986084\n",
      "Epoch: 13/100 | step: 92/422 | loss: 3.9696829319000244\n",
      "Epoch: 13/100 | step: 93/422 | loss: 3.7788894176483154\n",
      "Epoch: 13/100 | step: 94/422 | loss: 4.2227959632873535\n",
      "Epoch: 13/100 | step: 95/422 | loss: 4.23988151550293\n",
      "Epoch: 13/100 | step: 96/422 | loss: 3.788769245147705\n",
      "Epoch: 13/100 | step: 97/422 | loss: 4.103102207183838\n",
      "Epoch: 13/100 | step: 98/422 | loss: 4.012172698974609\n",
      "Epoch: 13/100 | step: 99/422 | loss: 4.167934894561768\n",
      "Epoch: 13/100 | step: 100/422 | loss: 3.8679893016815186\n",
      "Epoch: 13/100 | step: 101/422 | loss: 4.081063270568848\n",
      "Epoch: 13/100 | step: 102/422 | loss: 3.8648178577423096\n",
      "Epoch: 13/100 | step: 103/422 | loss: 4.115206718444824\n",
      "Epoch: 13/100 | step: 104/422 | loss: 4.129242420196533\n",
      "Epoch: 13/100 | step: 105/422 | loss: 4.21832799911499\n",
      "Epoch: 13/100 | step: 106/422 | loss: 4.123520851135254\n",
      "Epoch: 13/100 | step: 107/422 | loss: 3.94415020942688\n",
      "Epoch: 13/100 | step: 108/422 | loss: 3.9951791763305664\n",
      "Epoch: 13/100 | step: 109/422 | loss: 3.8081955909729004\n",
      "Epoch: 13/100 | step: 110/422 | loss: 3.7438931465148926\n",
      "Epoch: 13/100 | step: 111/422 | loss: 3.9136223793029785\n",
      "Epoch: 13/100 | step: 112/422 | loss: 3.933107376098633\n",
      "Epoch: 13/100 | step: 113/422 | loss: 3.9321136474609375\n",
      "Epoch: 13/100 | step: 114/422 | loss: 3.7744827270507812\n",
      "Epoch: 13/100 | step: 115/422 | loss: 4.004740238189697\n",
      "Epoch: 13/100 | step: 116/422 | loss: 3.848950147628784\n",
      "Epoch: 13/100 | step: 117/422 | loss: 3.720879077911377\n",
      "Epoch: 13/100 | step: 118/422 | loss: 3.9859976768493652\n",
      "Epoch: 13/100 | step: 119/422 | loss: 3.8652303218841553\n",
      "Epoch: 13/100 | step: 120/422 | loss: 4.035490989685059\n",
      "Epoch: 13/100 | step: 121/422 | loss: 4.209883213043213\n",
      "Epoch: 13/100 | step: 122/422 | loss: 4.1722331047058105\n",
      "Epoch: 13/100 | step: 123/422 | loss: 4.046798229217529\n",
      "Epoch: 13/100 | step: 124/422 | loss: 3.714416027069092\n",
      "Epoch: 13/100 | step: 125/422 | loss: 4.049350261688232\n",
      "Epoch: 13/100 | step: 126/422 | loss: 4.129138946533203\n",
      "Epoch: 13/100 | step: 127/422 | loss: 3.987914800643921\n",
      "Epoch: 13/100 | step: 128/422 | loss: 3.729771614074707\n",
      "Epoch: 13/100 | step: 129/422 | loss: 4.294590950012207\n",
      "Epoch: 13/100 | step: 130/422 | loss: 3.826058864593506\n",
      "Epoch: 13/100 | step: 131/422 | loss: 4.0745649337768555\n",
      "Epoch: 13/100 | step: 132/422 | loss: 4.281651020050049\n",
      "Epoch: 13/100 | step: 133/422 | loss: 4.0714850425720215\n",
      "Epoch: 13/100 | step: 134/422 | loss: 4.077463150024414\n",
      "Epoch: 13/100 | step: 135/422 | loss: 4.13631010055542\n",
      "Epoch: 13/100 | step: 136/422 | loss: 4.1787309646606445\n",
      "Epoch: 13/100 | step: 137/422 | loss: 3.8728115558624268\n",
      "Epoch: 13/100 | step: 138/422 | loss: 3.9139671325683594\n",
      "Epoch: 13/100 | step: 139/422 | loss: 4.154786109924316\n",
      "Epoch: 13/100 | step: 140/422 | loss: 3.8081657886505127\n",
      "Epoch: 13/100 | step: 141/422 | loss: 4.083494186401367\n",
      "Epoch: 13/100 | step: 142/422 | loss: 4.12793493270874\n",
      "Epoch: 13/100 | step: 143/422 | loss: 3.9368584156036377\n",
      "Epoch: 13/100 | step: 144/422 | loss: 3.923677444458008\n",
      "Epoch: 13/100 | step: 145/422 | loss: 3.9986822605133057\n",
      "Epoch: 13/100 | step: 146/422 | loss: 4.283287525177002\n",
      "Epoch: 13/100 | step: 147/422 | loss: 3.9461071491241455\n",
      "Epoch: 13/100 | step: 148/422 | loss: 4.110795021057129\n",
      "Epoch: 13/100 | step: 149/422 | loss: 3.930420160293579\n",
      "Epoch: 13/100 | step: 150/422 | loss: 3.7688896656036377\n",
      "Epoch: 13/100 | step: 151/422 | loss: 3.865483522415161\n",
      "Epoch: 13/100 | step: 152/422 | loss: 4.0042243003845215\n",
      "Epoch: 13/100 | step: 153/422 | loss: 3.9115617275238037\n",
      "Epoch: 13/100 | step: 154/422 | loss: 4.001040458679199\n",
      "Epoch: 13/100 | step: 155/422 | loss: 3.9035675525665283\n",
      "Epoch: 13/100 | step: 156/422 | loss: 4.311234474182129\n",
      "Epoch: 13/100 | step: 157/422 | loss: 3.814511299133301\n",
      "Epoch: 13/100 | step: 158/422 | loss: 4.029160976409912\n",
      "Epoch: 13/100 | step: 159/422 | loss: 4.1094770431518555\n",
      "Epoch: 13/100 | step: 160/422 | loss: 3.844425916671753\n",
      "Epoch: 13/100 | step: 161/422 | loss: 3.8377685546875\n",
      "Epoch: 13/100 | step: 162/422 | loss: 3.922628164291382\n",
      "Epoch: 13/100 | step: 163/422 | loss: 4.128882884979248\n",
      "Epoch: 13/100 | step: 164/422 | loss: 4.078903675079346\n",
      "Epoch: 13/100 | step: 165/422 | loss: 3.964555501937866\n",
      "Epoch: 13/100 | step: 166/422 | loss: 3.9754295349121094\n",
      "Epoch: 13/100 | step: 167/422 | loss: 4.032628059387207\n",
      "Epoch: 13/100 | step: 168/422 | loss: 4.030295372009277\n",
      "Epoch: 13/100 | step: 169/422 | loss: 4.072259902954102\n",
      "Epoch: 13/100 | step: 170/422 | loss: 3.9260311126708984\n",
      "Epoch: 13/100 | step: 171/422 | loss: 3.831207752227783\n",
      "Epoch: 13/100 | step: 172/422 | loss: 3.980621814727783\n",
      "Epoch: 13/100 | step: 173/422 | loss: 3.9702908992767334\n",
      "Epoch: 13/100 | step: 174/422 | loss: 3.9177887439727783\n",
      "Epoch: 13/100 | step: 175/422 | loss: 4.103806972503662\n",
      "Epoch: 13/100 | step: 176/422 | loss: 3.8380753993988037\n",
      "Epoch: 13/100 | step: 177/422 | loss: 3.940756320953369\n",
      "Epoch: 13/100 | step: 178/422 | loss: 3.937807083129883\n",
      "Epoch: 13/100 | step: 179/422 | loss: 3.9509835243225098\n",
      "Epoch: 13/100 | step: 180/422 | loss: 3.933002233505249\n",
      "Epoch: 13/100 | step: 181/422 | loss: 4.003023147583008\n",
      "Epoch: 13/100 | step: 182/422 | loss: 4.098654747009277\n",
      "Epoch: 13/100 | step: 183/422 | loss: 4.20321798324585\n",
      "Epoch: 13/100 | step: 184/422 | loss: 3.8983235359191895\n",
      "Epoch: 13/100 | step: 185/422 | loss: 3.793146848678589\n",
      "Epoch: 13/100 | step: 186/422 | loss: 4.051489353179932\n",
      "Epoch: 13/100 | step: 187/422 | loss: 3.915555238723755\n",
      "Epoch: 13/100 | step: 188/422 | loss: 4.036264896392822\n",
      "Epoch: 13/100 | step: 189/422 | loss: 3.9229776859283447\n",
      "Epoch: 13/100 | step: 190/422 | loss: 3.991957426071167\n",
      "Epoch: 13/100 | step: 191/422 | loss: 3.82995867729187\n",
      "Epoch: 13/100 | step: 192/422 | loss: 3.7066304683685303\n",
      "Epoch: 13/100 | step: 193/422 | loss: 3.7808117866516113\n",
      "Epoch: 13/100 | step: 194/422 | loss: 3.793414831161499\n",
      "Epoch: 13/100 | step: 195/422 | loss: 4.016016960144043\n",
      "Epoch: 13/100 | step: 196/422 | loss: 3.710087537765503\n",
      "Epoch: 13/100 | step: 197/422 | loss: 4.202450752258301\n",
      "Epoch: 13/100 | step: 198/422 | loss: 3.9310672283172607\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 14/100 | step: 1/422 | loss: 3.9343740940093994\n",
      "Epoch: 14/100 | step: 2/422 | loss: 3.8964476585388184\n",
      "Epoch: 14/100 | step: 3/422 | loss: 3.7074930667877197\n",
      "Epoch: 14/100 | step: 4/422 | loss: 4.108618259429932\n",
      "Epoch: 14/100 | step: 5/422 | loss: 4.066673755645752\n",
      "Epoch: 14/100 | step: 6/422 | loss: 4.188177585601807\n",
      "Epoch: 14/100 | step: 7/422 | loss: 3.9230053424835205\n",
      "Epoch: 14/100 | step: 8/422 | loss: 3.9288482666015625\n",
      "Epoch: 14/100 | step: 9/422 | loss: 3.8430402278900146\n",
      "Epoch: 14/100 | step: 10/422 | loss: 3.7845425605773926\n",
      "Epoch: 14/100 | step: 11/422 | loss: 4.081155776977539\n",
      "Epoch: 14/100 | step: 12/422 | loss: 3.632808208465576\n",
      "Epoch: 14/100 | step: 13/422 | loss: 3.9229960441589355\n",
      "Epoch: 14/100 | step: 14/422 | loss: 3.8801631927490234\n",
      "Epoch: 14/100 | step: 15/422 | loss: 4.132673263549805\n",
      "Epoch: 14/100 | step: 16/422 | loss: 3.710158348083496\n",
      "Epoch: 14/100 | step: 17/422 | loss: 3.9009668827056885\n",
      "Epoch: 14/100 | step: 18/422 | loss: 3.99068546295166\n",
      "Epoch: 14/100 | step: 19/422 | loss: 3.862860679626465\n",
      "Epoch: 14/100 | step: 20/422 | loss: 4.15133810043335\n",
      "Epoch: 14/100 | step: 21/422 | loss: 3.7765040397644043\n",
      "Epoch: 14/100 | step: 22/422 | loss: 3.7059073448181152\n",
      "Epoch: 14/100 | step: 23/422 | loss: 3.6582961082458496\n",
      "Epoch: 14/100 | step: 24/422 | loss: 3.8510026931762695\n",
      "Epoch: 14/100 | step: 25/422 | loss: 3.693842649459839\n",
      "Epoch: 14/100 | step: 26/422 | loss: 4.115480422973633\n",
      "Epoch: 14/100 | step: 27/422 | loss: 3.9686708450317383\n",
      "Epoch: 14/100 | step: 28/422 | loss: 3.9281694889068604\n",
      "Epoch: 14/100 | step: 29/422 | loss: 3.793144702911377\n",
      "Epoch: 14/100 | step: 30/422 | loss: 3.8977837562561035\n",
      "Epoch: 14/100 | step: 31/422 | loss: 3.772718667984009\n",
      "Epoch: 14/100 | step: 32/422 | loss: 3.942779541015625\n",
      "Epoch: 14/100 | step: 33/422 | loss: 3.9219987392425537\n",
      "Epoch: 14/100 | step: 34/422 | loss: 3.9242148399353027\n",
      "Epoch: 14/100 | step: 35/422 | loss: 3.6733803749084473\n",
      "Epoch: 14/100 | step: 36/422 | loss: 4.011824607849121\n",
      "Epoch: 14/100 | step: 37/422 | loss: 3.9885637760162354\n",
      "Epoch: 14/100 | step: 38/422 | loss: 3.768975257873535\n",
      "Epoch: 14/100 | step: 39/422 | loss: 4.068886756896973\n",
      "Epoch: 14/100 | step: 40/422 | loss: 3.868473768234253\n",
      "Epoch: 14/100 | step: 41/422 | loss: 3.5093038082122803\n",
      "Epoch: 14/100 | step: 42/422 | loss: 3.8181846141815186\n",
      "Epoch: 14/100 | step: 43/422 | loss: 3.9873721599578857\n",
      "Epoch: 14/100 | step: 44/422 | loss: 3.8323915004730225\n",
      "Epoch: 14/100 | step: 45/422 | loss: 3.6557376384735107\n",
      "Epoch: 14/100 | step: 46/422 | loss: 3.8264787197113037\n",
      "Epoch: 14/100 | step: 47/422 | loss: 3.5881004333496094\n",
      "Epoch: 14/100 | step: 48/422 | loss: 3.9574427604675293\n",
      "Epoch: 14/100 | step: 49/422 | loss: 3.718001365661621\n",
      "Epoch: 14/100 | step: 50/422 | loss: 3.8238182067871094\n",
      "Epoch: 14/100 | step: 51/422 | loss: 3.8708508014678955\n",
      "Epoch: 14/100 | step: 52/422 | loss: 4.1710357666015625\n",
      "Epoch: 14/100 | step: 53/422 | loss: 3.7816665172576904\n",
      "Epoch: 14/100 | step: 54/422 | loss: 3.951601028442383\n",
      "Epoch: 14/100 | step: 55/422 | loss: 3.970815658569336\n",
      "Epoch: 14/100 | step: 56/422 | loss: 4.05680513381958\n",
      "Epoch: 14/100 | step: 57/422 | loss: 3.8193359375\n",
      "Epoch: 14/100 | step: 58/422 | loss: 3.704972267150879\n",
      "Epoch: 14/100 | step: 59/422 | loss: 4.085003852844238\n",
      "Epoch: 14/100 | step: 60/422 | loss: 3.6750524044036865\n",
      "Epoch: 14/100 | step: 61/422 | loss: 3.9402198791503906\n",
      "Epoch: 14/100 | step: 62/422 | loss: 4.1545515060424805\n",
      "Epoch: 14/100 | step: 63/422 | loss: 3.941669464111328\n",
      "Epoch: 14/100 | step: 64/422 | loss: 3.85689377784729\n",
      "Epoch: 14/100 | step: 65/422 | loss: 3.8451945781707764\n",
      "Epoch: 14/100 | step: 66/422 | loss: 4.114038467407227\n",
      "Epoch: 14/100 | step: 67/422 | loss: 3.544189214706421\n",
      "Epoch: 14/100 | step: 68/422 | loss: 3.7996294498443604\n",
      "Epoch: 14/100 | step: 69/422 | loss: 3.8910481929779053\n",
      "Epoch: 14/100 | step: 70/422 | loss: 4.144412994384766\n",
      "Epoch: 14/100 | step: 71/422 | loss: 4.257165908813477\n",
      "Epoch: 14/100 | step: 72/422 | loss: 3.699962854385376\n",
      "Epoch: 14/100 | step: 73/422 | loss: 4.193957805633545\n",
      "Epoch: 14/100 | step: 74/422 | loss: 3.824259042739868\n",
      "Epoch: 14/100 | step: 75/422 | loss: 4.2558135986328125\n",
      "Epoch: 14/100 | step: 76/422 | loss: 3.7736542224884033\n",
      "Epoch: 14/100 | step: 77/422 | loss: 3.9314446449279785\n",
      "Epoch: 14/100 | step: 78/422 | loss: 3.922579526901245\n",
      "Epoch: 14/100 | step: 79/422 | loss: 3.8372814655303955\n",
      "Epoch: 14/100 | step: 80/422 | loss: 3.9350154399871826\n",
      "Epoch: 14/100 | step: 81/422 | loss: 4.243124961853027\n",
      "Epoch: 14/100 | step: 82/422 | loss: 3.8813650608062744\n",
      "Epoch: 14/100 | step: 83/422 | loss: 3.78719425201416\n",
      "Epoch: 14/100 | step: 84/422 | loss: 3.7341248989105225\n",
      "Epoch: 14/100 | step: 85/422 | loss: 4.076044082641602\n",
      "Epoch: 14/100 | step: 86/422 | loss: 3.8071272373199463\n",
      "Epoch: 14/100 | step: 87/422 | loss: 3.545785665512085\n",
      "Epoch: 14/100 | step: 88/422 | loss: 3.973217487335205\n",
      "Epoch: 14/100 | step: 89/422 | loss: 3.7874698638916016\n",
      "Epoch: 14/100 | step: 90/422 | loss: 3.7958738803863525\n",
      "Epoch: 14/100 | step: 91/422 | loss: 3.7998170852661133\n",
      "Epoch: 14/100 | step: 92/422 | loss: 4.076780319213867\n",
      "Epoch: 14/100 | step: 93/422 | loss: 3.8917174339294434\n",
      "Epoch: 14/100 | step: 94/422 | loss: 3.7684199810028076\n",
      "Epoch: 14/100 | step: 95/422 | loss: 4.193262100219727\n",
      "Epoch: 14/100 | step: 96/422 | loss: 3.8071680068969727\n",
      "Epoch: 14/100 | step: 97/422 | loss: 4.130784034729004\n",
      "Epoch: 14/100 | step: 98/422 | loss: 3.905184268951416\n",
      "Epoch: 14/100 | step: 99/422 | loss: 4.068572521209717\n",
      "Epoch: 14/100 | step: 100/422 | loss: 3.628351926803589\n",
      "Epoch: 14/100 | step: 101/422 | loss: 3.775089740753174\n",
      "Epoch: 14/100 | step: 102/422 | loss: 3.756150722503662\n",
      "Epoch: 14/100 | step: 103/422 | loss: 3.885526418685913\n",
      "Epoch: 14/100 | step: 104/422 | loss: 3.976447105407715\n",
      "Epoch: 14/100 | step: 105/422 | loss: 3.9555275440216064\n",
      "Epoch: 14/100 | step: 106/422 | loss: 3.911731481552124\n",
      "Epoch: 14/100 | step: 107/422 | loss: 4.270532131195068\n",
      "Epoch: 14/100 | step: 108/422 | loss: 3.8467516899108887\n",
      "Epoch: 14/100 | step: 109/422 | loss: 4.446717262268066\n",
      "Epoch: 14/100 | step: 110/422 | loss: 3.688188076019287\n",
      "Epoch: 14/100 | step: 111/422 | loss: 4.003064155578613\n",
      "Epoch: 14/100 | step: 112/422 | loss: 3.9941422939300537\n",
      "Epoch: 14/100 | step: 113/422 | loss: 3.9507153034210205\n",
      "Epoch: 14/100 | step: 114/422 | loss: 3.808769941329956\n",
      "Epoch: 14/100 | step: 115/422 | loss: 3.5589406490325928\n",
      "Epoch: 14/100 | step: 116/422 | loss: 4.1357340812683105\n",
      "Epoch: 14/100 | step: 117/422 | loss: 3.772472381591797\n",
      "Epoch: 14/100 | step: 118/422 | loss: 3.952568292617798\n",
      "Epoch: 14/100 | step: 119/422 | loss: 3.5701828002929688\n",
      "Epoch: 14/100 | step: 120/422 | loss: 3.627133846282959\n",
      "Epoch: 14/100 | step: 121/422 | loss: 3.9202704429626465\n",
      "Epoch: 14/100 | step: 122/422 | loss: 4.051529407501221\n",
      "Epoch: 14/100 | step: 123/422 | loss: 3.8458311557769775\n",
      "Epoch: 14/100 | step: 124/422 | loss: 3.93143892288208\n",
      "Epoch: 14/100 | step: 125/422 | loss: 3.956721782684326\n",
      "Epoch: 14/100 | step: 126/422 | loss: 3.548151731491089\n",
      "Epoch: 14/100 | step: 127/422 | loss: 3.8651130199432373\n",
      "Epoch: 14/100 | step: 128/422 | loss: 3.8576319217681885\n",
      "Epoch: 14/100 | step: 129/422 | loss: 3.7492454051971436\n",
      "Epoch: 14/100 | step: 130/422 | loss: 3.8819100856781006\n",
      "Epoch: 14/100 | step: 131/422 | loss: 3.8771238327026367\n",
      "Epoch: 14/100 | step: 132/422 | loss: 3.9350333213806152\n",
      "Epoch: 14/100 | step: 133/422 | loss: 4.332348823547363\n",
      "Epoch: 14/100 | step: 134/422 | loss: 3.6707053184509277\n",
      "Epoch: 14/100 | step: 135/422 | loss: 4.0032219886779785\n",
      "Epoch: 14/100 | step: 136/422 | loss: 3.6576850414276123\n",
      "Epoch: 14/100 | step: 137/422 | loss: 4.217832565307617\n",
      "Epoch: 14/100 | step: 138/422 | loss: 3.961132287979126\n",
      "Epoch: 14/100 | step: 139/422 | loss: 3.708317756652832\n",
      "Epoch: 14/100 | step: 140/422 | loss: 4.188239574432373\n",
      "Epoch: 14/100 | step: 141/422 | loss: 3.888993978500366\n",
      "Epoch: 14/100 | step: 142/422 | loss: 3.7978835105895996\n",
      "Epoch: 14/100 | step: 143/422 | loss: 4.282053470611572\n",
      "Epoch: 14/100 | step: 144/422 | loss: 3.9146151542663574\n",
      "Epoch: 14/100 | step: 145/422 | loss: 4.185340404510498\n",
      "Epoch: 14/100 | step: 146/422 | loss: 3.954721450805664\n",
      "Epoch: 14/100 | step: 147/422 | loss: 4.141866683959961\n",
      "Epoch: 14/100 | step: 148/422 | loss: 3.748354196548462\n",
      "Epoch: 14/100 | step: 149/422 | loss: 4.008370876312256\n",
      "Epoch: 14/100 | step: 150/422 | loss: 3.834707021713257\n",
      "Epoch: 14/100 | step: 151/422 | loss: 3.6985692977905273\n",
      "Epoch: 14/100 | step: 152/422 | loss: 3.94146990776062\n",
      "Epoch: 14/100 | step: 153/422 | loss: 3.9252548217773438\n",
      "Epoch: 14/100 | step: 154/422 | loss: 4.106352806091309\n",
      "Epoch: 14/100 | step: 155/422 | loss: 4.278177261352539\n",
      "Epoch: 14/100 | step: 156/422 | loss: 4.058562755584717\n",
      "Epoch: 14/100 | step: 157/422 | loss: 3.7866814136505127\n",
      "Epoch: 14/100 | step: 158/422 | loss: 3.701529026031494\n",
      "Epoch: 14/100 | step: 159/422 | loss: 3.743612766265869\n",
      "Epoch: 14/100 | step: 160/422 | loss: 4.014997482299805\n",
      "Epoch: 14/100 | step: 161/422 | loss: 4.017261028289795\n",
      "Epoch: 14/100 | step: 162/422 | loss: 3.858588933944702\n",
      "Epoch: 14/100 | step: 163/422 | loss: 3.8330299854278564\n",
      "Epoch: 14/100 | step: 164/422 | loss: 3.893486261367798\n",
      "Epoch: 14/100 | step: 165/422 | loss: 3.901064395904541\n",
      "Epoch: 14/100 | step: 166/422 | loss: 3.975754976272583\n",
      "Epoch: 14/100 | step: 167/422 | loss: 4.059582233428955\n",
      "Epoch: 14/100 | step: 168/422 | loss: 4.0356879234313965\n",
      "Epoch: 14/100 | step: 169/422 | loss: 3.860596179962158\n",
      "Epoch: 14/100 | step: 170/422 | loss: 4.235689163208008\n",
      "Epoch: 14/100 | step: 171/422 | loss: 4.225825309753418\n",
      "Epoch: 14/100 | step: 172/422 | loss: 3.8511435985565186\n",
      "Epoch: 14/100 | step: 173/422 | loss: 3.8664658069610596\n",
      "Epoch: 14/100 | step: 174/422 | loss: 3.7375574111938477\n",
      "Epoch: 14/100 | step: 175/422 | loss: 4.219298362731934\n",
      "Epoch: 14/100 | step: 176/422 | loss: 3.942054510116577\n",
      "Epoch: 14/100 | step: 177/422 | loss: 4.281931400299072\n",
      "Epoch: 14/100 | step: 178/422 | loss: 3.9790549278259277\n",
      "Epoch: 14/100 | step: 179/422 | loss: 3.632122755050659\n",
      "Epoch: 14/100 | step: 180/422 | loss: 4.158902168273926\n",
      "Epoch: 14/100 | step: 181/422 | loss: 3.6931939125061035\n",
      "Epoch: 14/100 | step: 182/422 | loss: 3.7949585914611816\n",
      "Epoch: 14/100 | step: 183/422 | loss: 3.648777961730957\n",
      "Epoch: 14/100 | step: 184/422 | loss: 3.931870222091675\n",
      "Epoch: 14/100 | step: 185/422 | loss: 3.6621339321136475\n",
      "Epoch: 14/100 | step: 186/422 | loss: 3.5576601028442383\n",
      "Epoch: 14/100 | step: 187/422 | loss: 4.056727886199951\n",
      "Epoch: 14/100 | step: 188/422 | loss: 3.912619113922119\n",
      "Epoch: 14/100 | step: 189/422 | loss: 4.068648338317871\n",
      "Epoch: 14/100 | step: 190/422 | loss: 4.213650703430176\n",
      "Epoch: 14/100 | step: 191/422 | loss: 4.141165733337402\n",
      "Epoch: 14/100 | step: 192/422 | loss: 4.041346073150635\n",
      "Epoch: 14/100 | step: 193/422 | loss: 3.6739370822906494\n",
      "Epoch: 14/100 | step: 194/422 | loss: 3.9016685485839844\n",
      "Epoch: 14/100 | step: 195/422 | loss: 4.01143741607666\n",
      "Epoch: 14/100 | step: 196/422 | loss: 3.8005123138427734\n",
      "Epoch: 14/100 | step: 197/422 | loss: 3.840797185897827\n",
      "Epoch: 14/100 | step: 198/422 | loss: 3.94113826751709\n",
      "Epoch: 14/100 | step: 199/422 | loss: 3.9865801334381104\n",
      "Epoch: 14/100 | step: 200/422 | loss: 3.7164597511291504\n",
      "Epoch: 14/100 | step: 201/422 | loss: 3.938568353652954\n",
      "Epoch: 14/100 | step: 202/422 | loss: 3.612147808074951\n",
      "Epoch: 14/100 | step: 203/422 | loss: 3.7651796340942383\n",
      "Epoch: 14/100 | step: 204/422 | loss: 3.793966293334961\n",
      "Epoch: 14/100 | step: 205/422 | loss: 4.232956886291504\n",
      "Epoch: 14/100 | step: 206/422 | loss: 3.792997360229492\n",
      "Epoch: 14/100 | step: 207/422 | loss: 3.742330312728882\n",
      "Epoch: 14/100 | step: 208/422 | loss: 3.9825072288513184\n",
      "Epoch: 14/100 | step: 209/422 | loss: 3.914483070373535\n",
      "Epoch: 14/100 | step: 210/422 | loss: 3.7707302570343018\n",
      "Epoch: 14/100 | step: 211/422 | loss: 3.6861343383789062\n",
      "Epoch: 14/100 | step: 212/422 | loss: 3.607024908065796\n",
      "Epoch: 14/100 | step: 213/422 | loss: 3.8204264640808105\n",
      "Epoch: 14/100 | step: 214/422 | loss: 3.747016191482544\n",
      "Epoch: 14/100 | step: 215/422 | loss: 3.977099895477295\n",
      "Epoch: 14/100 | step: 216/422 | loss: 4.029937744140625\n",
      "Epoch: 14/100 | step: 217/422 | loss: 3.7606313228607178\n",
      "Epoch: 14/100 | step: 218/422 | loss: 3.6711220741271973\n",
      "Epoch: 14/100 | step: 219/422 | loss: 3.6768529415130615\n",
      "Epoch: 14/100 | step: 220/422 | loss: 3.8865020275115967\n",
      "Epoch: 14/100 | step: 221/422 | loss: 4.055594444274902\n",
      "Epoch: 14/100 | step: 222/422 | loss: 3.939923048019409\n",
      "Epoch: 14/100 | step: 223/422 | loss: 3.816624879837036\n",
      "Epoch: 14/100 | step: 224/422 | loss: 3.354616403579712\n",
      "Epoch: 14/100 | step: 225/422 | loss: 3.8185627460479736\n",
      "Epoch: 14/100 | step: 226/422 | loss: 3.8222146034240723\n",
      "Epoch: 14/100 | step: 227/422 | loss: 3.6641745567321777\n",
      "Epoch: 14/100 | step: 228/422 | loss: 4.125342845916748\n",
      "Epoch: 14/100 | step: 229/422 | loss: 4.003876686096191\n",
      "Epoch: 14/100 | step: 230/422 | loss: 3.5375168323516846\n",
      "Epoch: 14/100 | step: 231/422 | loss: 4.0006842613220215\n",
      "Epoch: 14/100 | step: 232/422 | loss: 3.8479981422424316\n",
      "Epoch: 14/100 | step: 233/422 | loss: 3.9299590587615967\n",
      "Epoch: 14/100 | step: 234/422 | loss: 3.736253261566162\n",
      "Epoch: 14/100 | step: 235/422 | loss: 3.7605342864990234\n",
      "Epoch: 14/100 | step: 236/422 | loss: 4.0395283699035645\n",
      "Epoch: 14/100 | step: 237/422 | loss: 3.8514628410339355\n",
      "Epoch: 14/100 | step: 238/422 | loss: 3.810453414916992\n",
      "Epoch: 14/100 | step: 239/422 | loss: 3.823486566543579\n",
      "Epoch: 14/100 | step: 240/422 | loss: 3.6409106254577637\n",
      "Epoch: 14/100 | step: 241/422 | loss: 3.923182249069214\n",
      "Epoch: 14/100 | step: 242/422 | loss: 4.256489276885986\n",
      "Epoch: 14/100 | step: 243/422 | loss: 4.0967254638671875\n",
      "Epoch: 14/100 | step: 244/422 | loss: 3.8339855670928955\n",
      "Epoch: 14/100 | step: 245/422 | loss: 3.625749111175537\n",
      "Epoch: 14/100 | step: 246/422 | loss: 3.6282477378845215\n",
      "Epoch: 14/100 | step: 247/422 | loss: 3.8332033157348633\n",
      "Epoch: 14/100 | step: 248/422 | loss: 3.7523536682128906\n",
      "Epoch: 14/100 | step: 249/422 | loss: 3.878537178039551\n",
      "Epoch: 14/100 | step: 250/422 | loss: 4.073136329650879\n",
      "Epoch: 14/100 | step: 251/422 | loss: 3.736506938934326\n",
      "Epoch: 14/100 | step: 252/422 | loss: 4.045217037200928\n",
      "Epoch: 14/100 | step: 253/422 | loss: 3.9643361568450928\n",
      "Epoch: 14/100 | step: 254/422 | loss: 3.4587559700012207\n",
      "Epoch: 14/100 | step: 255/422 | loss: 3.778557062149048\n",
      "Epoch: 14/100 | step: 256/422 | loss: 3.56736159324646\n",
      "Epoch: 14/100 | step: 257/422 | loss: 3.942373752593994\n",
      "Epoch: 14/100 | step: 258/422 | loss: 3.6932904720306396\n",
      "Epoch: 14/100 | step: 259/422 | loss: 3.810883045196533\n",
      "Epoch: 14/100 | step: 260/422 | loss: 3.8844797611236572\n",
      "Epoch: 14/100 | step: 261/422 | loss: 3.847108840942383\n",
      "Epoch: 14/100 | step: 262/422 | loss: 3.7989838123321533\n",
      "Epoch: 14/100 | step: 263/422 | loss: 3.4136078357696533\n",
      "Epoch: 14/100 | step: 264/422 | loss: 3.892575740814209\n",
      "Epoch: 14/100 | step: 265/422 | loss: 3.447173595428467\n",
      "Epoch: 14/100 | step: 266/422 | loss: 3.4891107082366943\n",
      "Epoch: 14/100 | step: 267/422 | loss: 4.00428581237793\n",
      "Epoch: 14/100 | step: 268/422 | loss: 3.802872657775879\n",
      "Epoch: 14/100 | step: 269/422 | loss: 3.727710008621216\n",
      "Epoch: 14/100 | step: 270/422 | loss: 3.985944986343384\n",
      "Epoch: 14/100 | step: 271/422 | loss: 3.818549156188965\n",
      "Epoch: 14/100 | step: 272/422 | loss: 3.9385786056518555\n",
      "Epoch: 14/100 | step: 273/422 | loss: 3.8144547939300537\n",
      "Epoch: 14/100 | step: 274/422 | loss: 3.932959794998169\n",
      "Epoch: 14/100 | step: 275/422 | loss: 3.9820268154144287\n",
      "Epoch: 14/100 | step: 276/422 | loss: 3.5614566802978516\n",
      "Epoch: 14/100 | step: 277/422 | loss: 4.188241481781006\n",
      "Epoch: 14/100 | step: 278/422 | loss: 3.7983875274658203\n",
      "Epoch: 14/100 | step: 279/422 | loss: 3.8818464279174805\n",
      "Epoch: 14/100 | step: 280/422 | loss: 3.399261474609375\n",
      "Epoch: 14/100 | step: 281/422 | loss: 3.8076794147491455\n",
      "Epoch: 14/100 | step: 282/422 | loss: 3.6322181224823\n",
      "Epoch: 14/100 | step: 283/422 | loss: 3.8867127895355225\n",
      "Epoch: 14/100 | step: 284/422 | loss: 3.8424010276794434\n",
      "Epoch: 14/100 | step: 285/422 | loss: 3.791372060775757\n",
      "Epoch: 14/100 | step: 286/422 | loss: 4.034984588623047\n",
      "Epoch: 14/100 | step: 287/422 | loss: 3.7206802368164062\n",
      "Epoch: 14/100 | step: 288/422 | loss: 3.9321765899658203\n",
      "Epoch: 14/100 | step: 289/422 | loss: 3.3923988342285156\n",
      "Epoch: 14/100 | step: 290/422 | loss: 3.665546178817749\n",
      "Epoch: 14/100 | step: 291/422 | loss: 4.426942348480225\n",
      "Epoch: 14/100 | step: 292/422 | loss: 3.817533493041992\n",
      "Epoch: 14/100 | step: 293/422 | loss: 3.703279733657837\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 15/100 | step: 1/422 | loss: 3.801246404647827\n",
      "Epoch: 15/100 | step: 2/422 | loss: 3.467228651046753\n",
      "Epoch: 15/100 | step: 3/422 | loss: 3.600590705871582\n",
      "Epoch: 15/100 | step: 4/422 | loss: 3.655377149581909\n",
      "Epoch: 15/100 | step: 5/422 | loss: 4.051270008087158\n",
      "Epoch: 15/100 | step: 6/422 | loss: 3.9496922492980957\n",
      "Epoch: 15/100 | step: 7/422 | loss: 3.8023524284362793\n",
      "Epoch: 15/100 | step: 8/422 | loss: 4.011011600494385\n",
      "Epoch: 15/100 | step: 9/422 | loss: 3.6133224964141846\n",
      "Epoch: 15/100 | step: 10/422 | loss: 3.740790367126465\n",
      "Epoch: 15/100 | step: 11/422 | loss: 3.824260711669922\n",
      "Epoch: 15/100 | step: 12/422 | loss: 3.882455825805664\n",
      "Epoch: 15/100 | step: 13/422 | loss: 3.8062360286712646\n",
      "Epoch: 15/100 | step: 14/422 | loss: 3.7244324684143066\n",
      "Epoch: 15/100 | step: 15/422 | loss: 3.7968244552612305\n",
      "Epoch: 15/100 | step: 16/422 | loss: 3.567213535308838\n",
      "Epoch: 15/100 | step: 17/422 | loss: 3.9899394512176514\n",
      "Epoch: 15/100 | step: 18/422 | loss: 4.048783302307129\n",
      "Epoch: 15/100 | step: 19/422 | loss: 3.6923911571502686\n",
      "Epoch: 15/100 | step: 20/422 | loss: 3.9675183296203613\n",
      "Epoch: 15/100 | step: 21/422 | loss: 3.9636502265930176\n",
      "Epoch: 15/100 | step: 22/422 | loss: 3.9259531497955322\n",
      "Epoch: 15/100 | step: 23/422 | loss: 3.6839451789855957\n",
      "Epoch: 15/100 | step: 24/422 | loss: 3.684142589569092\n",
      "Epoch: 15/100 | step: 25/422 | loss: 3.504232406616211\n",
      "Epoch: 15/100 | step: 26/422 | loss: 3.5042929649353027\n",
      "Epoch: 15/100 | step: 27/422 | loss: 3.471824884414673\n",
      "Epoch: 15/100 | step: 28/422 | loss: 3.9629898071289062\n",
      "Epoch: 15/100 | step: 29/422 | loss: 3.8369007110595703\n",
      "Epoch: 15/100 | step: 30/422 | loss: 3.749467134475708\n",
      "Epoch: 15/100 | step: 31/422 | loss: 3.891497850418091\n",
      "Epoch: 15/100 | step: 32/422 | loss: 3.684413433074951\n",
      "Epoch: 15/100 | step: 33/422 | loss: 3.91304349899292\n",
      "Epoch: 15/100 | step: 34/422 | loss: 3.7407138347625732\n",
      "Epoch: 15/100 | step: 35/422 | loss: 4.151180267333984\n",
      "Epoch: 15/100 | step: 36/422 | loss: 3.669978380203247\n",
      "Epoch: 15/100 | step: 37/422 | loss: 3.7237062454223633\n",
      "Epoch: 15/100 | step: 38/422 | loss: 3.8348894119262695\n",
      "Epoch: 15/100 | step: 39/422 | loss: 3.8575851917266846\n",
      "Epoch: 15/100 | step: 40/422 | loss: 3.8532283306121826\n",
      "Epoch: 15/100 | step: 41/422 | loss: 3.6849887371063232\n",
      "Epoch: 15/100 | step: 42/422 | loss: 3.780376672744751\n",
      "Epoch: 15/100 | step: 43/422 | loss: 3.6950151920318604\n",
      "Epoch: 15/100 | step: 44/422 | loss: 3.6112611293792725\n",
      "Epoch: 15/100 | step: 45/422 | loss: 3.6693520545959473\n",
      "Epoch: 15/100 | step: 46/422 | loss: 3.7866530418395996\n",
      "Epoch: 15/100 | step: 47/422 | loss: 3.805299758911133\n",
      "Epoch: 15/100 | step: 48/422 | loss: 3.7756106853485107\n",
      "Epoch: 15/100 | step: 49/422 | loss: 3.810297727584839\n",
      "Epoch: 15/100 | step: 50/422 | loss: 3.9551868438720703\n",
      "Epoch: 15/100 | step: 51/422 | loss: 3.9707751274108887\n",
      "Epoch: 15/100 | step: 52/422 | loss: 3.928816080093384\n",
      "Epoch: 15/100 | step: 53/422 | loss: 3.8794901371002197\n",
      "Epoch: 15/100 | step: 54/422 | loss: 3.8822684288024902\n",
      "Epoch: 15/100 | step: 55/422 | loss: 3.3916962146759033\n",
      "Epoch: 15/100 | step: 56/422 | loss: 3.5758056640625\n",
      "Epoch: 15/100 | step: 57/422 | loss: 3.9909257888793945\n",
      "Epoch: 15/100 | step: 58/422 | loss: 4.17036247253418\n",
      "Epoch: 15/100 | step: 59/422 | loss: 3.7085888385772705\n",
      "Epoch: 15/100 | step: 60/422 | loss: 3.7388200759887695\n",
      "Epoch: 15/100 | step: 61/422 | loss: 3.945295810699463\n",
      "Epoch: 15/100 | step: 62/422 | loss: 3.9172487258911133\n",
      "Epoch: 15/100 | step: 63/422 | loss: 3.893893241882324\n",
      "Epoch: 15/100 | step: 64/422 | loss: 4.0493645668029785\n",
      "Epoch: 15/100 | step: 65/422 | loss: 3.795912504196167\n",
      "Epoch: 15/100 | step: 66/422 | loss: 3.7721877098083496\n",
      "Epoch: 15/100 | step: 67/422 | loss: 4.018061637878418\n",
      "Epoch: 15/100 | step: 68/422 | loss: 3.6367440223693848\n",
      "Epoch: 15/100 | step: 69/422 | loss: 4.059420585632324\n",
      "Epoch: 15/100 | step: 70/422 | loss: 3.640103816986084\n",
      "Epoch: 15/100 | step: 71/422 | loss: 3.7804195880889893\n",
      "Epoch: 15/100 | step: 72/422 | loss: 3.7293288707733154\n",
      "Epoch: 15/100 | step: 73/422 | loss: 3.6384193897247314\n",
      "Epoch: 15/100 | step: 74/422 | loss: 3.838810682296753\n",
      "Epoch: 15/100 | step: 75/422 | loss: 3.813784599304199\n",
      "Epoch: 15/100 | step: 76/422 | loss: 3.6636884212493896\n",
      "Epoch: 15/100 | step: 77/422 | loss: 3.85503888130188\n",
      "Epoch: 15/100 | step: 78/422 | loss: 3.645702362060547\n",
      "Epoch: 15/100 | step: 79/422 | loss: 3.745073080062866\n",
      "Epoch: 15/100 | step: 80/422 | loss: 3.9840168952941895\n",
      "Epoch: 15/100 | step: 81/422 | loss: 3.7467074394226074\n",
      "Epoch: 15/100 | step: 82/422 | loss: 3.9221742153167725\n",
      "Epoch: 15/100 | step: 83/422 | loss: 3.9142658710479736\n",
      "Epoch: 15/100 | step: 84/422 | loss: 3.869744062423706\n",
      "Epoch: 15/100 | step: 85/422 | loss: 3.8144612312316895\n",
      "Epoch: 15/100 | step: 86/422 | loss: 3.6565022468566895\n",
      "Epoch: 15/100 | step: 87/422 | loss: 3.6822354793548584\n",
      "Epoch: 15/100 | step: 88/422 | loss: 3.560654640197754\n",
      "Epoch: 15/100 | step: 89/422 | loss: 3.843458652496338\n",
      "Epoch: 15/100 | step: 90/422 | loss: 3.611572265625\n",
      "Epoch: 15/100 | step: 91/422 | loss: 3.7142181396484375\n",
      "Epoch: 15/100 | step: 92/422 | loss: 3.823188066482544\n",
      "Epoch: 15/100 | step: 93/422 | loss: 3.869605541229248\n",
      "Epoch: 15/100 | step: 94/422 | loss: 4.160714626312256\n",
      "Epoch: 15/100 | step: 95/422 | loss: 3.8774843215942383\n",
      "Epoch: 15/100 | step: 96/422 | loss: 3.7679426670074463\n",
      "Epoch: 15/100 | step: 97/422 | loss: 3.6078948974609375\n",
      "Epoch: 15/100 | step: 98/422 | loss: 3.4654455184936523\n",
      "Epoch: 15/100 | step: 99/422 | loss: 3.6615149974823\n",
      "Epoch: 15/100 | step: 100/422 | loss: 3.8377747535705566\n",
      "Epoch: 15/100 | step: 101/422 | loss: 3.746211051940918\n",
      "Epoch: 15/100 | step: 102/422 | loss: 3.6297831535339355\n",
      "Epoch: 15/100 | step: 103/422 | loss: 3.6764307022094727\n",
      "Epoch: 15/100 | step: 104/422 | loss: 3.75553297996521\n",
      "Epoch: 15/100 | step: 105/422 | loss: 3.836884021759033\n",
      "Epoch: 15/100 | step: 106/422 | loss: 3.7369158267974854\n",
      "Epoch: 15/100 | step: 107/422 | loss: 3.599667549133301\n",
      "Epoch: 15/100 | step: 108/422 | loss: 3.934631824493408\n",
      "Epoch: 15/100 | step: 109/422 | loss: 3.7369604110717773\n",
      "Epoch: 15/100 | step: 110/422 | loss: 3.658268928527832\n",
      "Epoch: 15/100 | step: 111/422 | loss: 3.6950557231903076\n",
      "Epoch: 15/100 | step: 112/422 | loss: 3.5020768642425537\n",
      "Epoch: 15/100 | step: 113/422 | loss: 3.656728744506836\n",
      "Epoch: 15/100 | step: 114/422 | loss: 3.7408697605133057\n",
      "Epoch: 15/100 | step: 115/422 | loss: 3.4875049591064453\n",
      "Epoch: 15/100 | step: 116/422 | loss: 3.6901767253875732\n",
      "Epoch: 15/100 | step: 117/422 | loss: 4.009174346923828\n",
      "Epoch: 15/100 | step: 118/422 | loss: 4.039788722991943\n",
      "Epoch: 15/100 | step: 119/422 | loss: 4.172530174255371\n",
      "Epoch: 15/100 | step: 120/422 | loss: 3.9594688415527344\n",
      "Epoch: 15/100 | step: 121/422 | loss: 3.6942012310028076\n",
      "Epoch: 15/100 | step: 122/422 | loss: 4.0319647789001465\n",
      "Epoch: 15/100 | step: 123/422 | loss: 3.8324832916259766\n",
      "Epoch: 15/100 | step: 124/422 | loss: 3.967391014099121\n",
      "Epoch: 15/100 | step: 125/422 | loss: 3.7479960918426514\n",
      "Epoch: 15/100 | step: 126/422 | loss: 3.719097137451172\n",
      "Epoch: 15/100 | step: 127/422 | loss: 4.126241683959961\n",
      "Epoch: 15/100 | step: 128/422 | loss: 3.5224666595458984\n",
      "Epoch: 15/100 | step: 129/422 | loss: 3.5722246170043945\n",
      "Epoch: 15/100 | step: 130/422 | loss: 3.5063743591308594\n",
      "Epoch: 15/100 | step: 131/422 | loss: 4.1833906173706055\n",
      "Epoch: 15/100 | step: 132/422 | loss: 3.908780813217163\n",
      "Epoch: 15/100 | step: 133/422 | loss: 3.867445707321167\n",
      "Epoch: 15/100 | step: 134/422 | loss: 3.697385311126709\n",
      "Epoch: 15/100 | step: 135/422 | loss: 3.5460364818573\n",
      "Epoch: 15/100 | step: 136/422 | loss: 3.7186083793640137\n",
      "Epoch: 15/100 | step: 137/422 | loss: 3.592327356338501\n",
      "Epoch: 15/100 | step: 138/422 | loss: 3.7816896438598633\n",
      "Epoch: 15/100 | step: 139/422 | loss: 3.710921049118042\n",
      "Epoch: 15/100 | step: 140/422 | loss: 3.733046531677246\n",
      "Epoch: 15/100 | step: 141/422 | loss: 3.964627504348755\n",
      "Epoch: 15/100 | step: 142/422 | loss: 4.195861339569092\n",
      "Epoch: 15/100 | step: 143/422 | loss: 3.7088496685028076\n",
      "Epoch: 15/100 | step: 144/422 | loss: 3.967090368270874\n",
      "Epoch: 15/100 | step: 145/422 | loss: 3.687300682067871\n",
      "Epoch: 15/100 | step: 146/422 | loss: 3.329860210418701\n",
      "Epoch: 15/100 | step: 147/422 | loss: 3.792127847671509\n",
      "Epoch: 15/100 | step: 148/422 | loss: 3.6911723613739014\n",
      "Epoch: 15/100 | step: 149/422 | loss: 4.126183032989502\n",
      "Epoch: 15/100 | step: 150/422 | loss: 3.9686310291290283\n",
      "Epoch: 15/100 | step: 151/422 | loss: 3.8635497093200684\n",
      "Epoch: 15/100 | step: 152/422 | loss: 3.580886125564575\n",
      "Epoch: 15/100 | step: 153/422 | loss: 3.7476277351379395\n",
      "Epoch: 15/100 | step: 154/422 | loss: 3.6687393188476562\n",
      "Epoch: 15/100 | step: 155/422 | loss: 3.737011671066284\n",
      "Epoch: 15/100 | step: 156/422 | loss: 3.670248508453369\n",
      "Epoch: 15/100 | step: 157/422 | loss: 3.669069290161133\n",
      "Epoch: 15/100 | step: 158/422 | loss: 3.8889851570129395\n",
      "Epoch: 15/100 | step: 159/422 | loss: 3.62178897857666\n",
      "Epoch: 15/100 | step: 160/422 | loss: 3.718712091445923\n",
      "Epoch: 15/100 | step: 161/422 | loss: 3.8006160259246826\n",
      "Epoch: 15/100 | step: 162/422 | loss: 3.8343687057495117\n",
      "Epoch: 15/100 | step: 163/422 | loss: 3.743605136871338\n",
      "Epoch: 15/100 | step: 164/422 | loss: 3.8403990268707275\n",
      "Epoch: 15/100 | step: 165/422 | loss: 3.8163042068481445\n",
      "Epoch: 15/100 | step: 166/422 | loss: 3.8615903854370117\n",
      "Epoch: 15/100 | step: 167/422 | loss: 4.149374008178711\n",
      "Epoch: 15/100 | step: 168/422 | loss: 3.787445306777954\n",
      "Epoch: 15/100 | step: 169/422 | loss: 3.7605879306793213\n",
      "Epoch: 15/100 | step: 170/422 | loss: 3.8334271907806396\n",
      "Epoch: 15/100 | step: 171/422 | loss: 3.6302907466888428\n",
      "Epoch: 15/100 | step: 172/422 | loss: 3.631675958633423\n",
      "Epoch: 15/100 | step: 173/422 | loss: 3.4491007328033447\n",
      "Epoch: 15/100 | step: 174/422 | loss: 3.77225661277771\n",
      "Epoch: 15/100 | step: 175/422 | loss: 3.692892551422119\n",
      "Epoch: 15/100 | step: 176/422 | loss: 3.751147985458374\n",
      "Epoch: 15/100 | step: 177/422 | loss: 3.5676097869873047\n",
      "Epoch: 15/100 | step: 178/422 | loss: 3.6960742473602295\n",
      "Epoch: 15/100 | step: 179/422 | loss: 4.135109901428223\n",
      "Epoch: 15/100 | step: 180/422 | loss: 3.6532392501831055\n",
      "Epoch: 15/100 | step: 181/422 | loss: 3.934985876083374\n",
      "Epoch: 15/100 | step: 182/422 | loss: 3.792172908782959\n",
      "Epoch: 15/100 | step: 183/422 | loss: 3.6953039169311523\n",
      "Epoch: 15/100 | step: 184/422 | loss: 3.9324488639831543\n",
      "Epoch: 15/100 | step: 185/422 | loss: 3.8557963371276855\n",
      "Epoch: 15/100 | step: 186/422 | loss: 3.5595321655273438\n",
      "Epoch: 15/100 | step: 187/422 | loss: 3.74393367767334\n",
      "Epoch: 15/100 | step: 188/422 | loss: 3.6893246173858643\n",
      "Epoch: 15/100 | step: 189/422 | loss: 3.897613286972046\n",
      "Epoch: 15/100 | step: 190/422 | loss: 3.875410556793213\n",
      "Epoch: 15/100 | step: 191/422 | loss: 3.6776509284973145\n",
      "Epoch: 15/100 | step: 192/422 | loss: 3.7077763080596924\n",
      "Epoch: 15/100 | step: 193/422 | loss: 3.832599401473999\n",
      "Epoch: 15/100 | step: 194/422 | loss: 3.867699146270752\n",
      "Epoch: 15/100 | step: 195/422 | loss: 3.8828048706054688\n",
      "Epoch: 15/100 | step: 196/422 | loss: 3.4400525093078613\n",
      "Epoch: 15/100 | step: 197/422 | loss: 3.8176536560058594\n",
      "Epoch: 15/100 | step: 198/422 | loss: 4.036314487457275\n",
      "Epoch: 15/100 | step: 199/422 | loss: 3.683760404586792\n",
      "Epoch: 15/100 | step: 200/422 | loss: 3.7551045417785645\n",
      "Epoch: 15/100 | step: 201/422 | loss: 4.120738983154297\n",
      "Epoch: 15/100 | step: 202/422 | loss: 3.837676763534546\n",
      "Epoch: 15/100 | step: 203/422 | loss: 3.898080825805664\n",
      "Epoch: 15/100 | step: 204/422 | loss: 3.7764334678649902\n",
      "Epoch: 15/100 | step: 205/422 | loss: 3.8835997581481934\n",
      "Epoch: 15/100 | step: 206/422 | loss: 3.5288684368133545\n",
      "Epoch: 15/100 | step: 207/422 | loss: 3.837672710418701\n",
      "Epoch: 15/100 | step: 208/422 | loss: 3.914733648300171\n",
      "Epoch: 15/100 | step: 209/422 | loss: 3.8390023708343506\n",
      "Epoch: 15/100 | step: 210/422 | loss: 3.39715838432312\n",
      "Epoch: 15/100 | step: 211/422 | loss: 3.8006889820098877\n",
      "Epoch: 15/100 | step: 212/422 | loss: 3.838130474090576\n",
      "Epoch: 15/100 | step: 213/422 | loss: 3.562264919281006\n",
      "Epoch: 15/100 | step: 214/422 | loss: 3.8693795204162598\n",
      "Epoch: 15/100 | step: 215/422 | loss: 3.670725107192993\n",
      "Epoch: 15/100 | step: 216/422 | loss: 3.7838306427001953\n",
      "Epoch: 15/100 | step: 217/422 | loss: 3.7175710201263428\n",
      "Epoch: 15/100 | step: 218/422 | loss: 3.8032126426696777\n",
      "Epoch: 15/100 | step: 219/422 | loss: 3.6393516063690186\n",
      "Epoch: 15/100 | step: 220/422 | loss: 3.8533997535705566\n",
      "Epoch: 15/100 | step: 221/422 | loss: 3.5308265686035156\n",
      "Epoch: 15/100 | step: 222/422 | loss: 3.5335988998413086\n",
      "Epoch: 15/100 | step: 223/422 | loss: 3.8108415603637695\n",
      "Epoch: 15/100 | step: 224/422 | loss: 3.7299180030822754\n",
      "Epoch: 15/100 | step: 225/422 | loss: 3.48038911819458\n",
      "Epoch: 15/100 | step: 226/422 | loss: 3.9735865592956543\n",
      "Epoch: 15/100 | step: 227/422 | loss: 3.5686731338500977\n",
      "Epoch: 15/100 | step: 228/422 | loss: 3.7377889156341553\n",
      "Epoch: 15/100 | step: 229/422 | loss: 3.5884859561920166\n",
      "Epoch: 15/100 | step: 230/422 | loss: 3.718320846557617\n",
      "Epoch: 15/100 | step: 231/422 | loss: 3.457460880279541\n",
      "Epoch: 15/100 | step: 232/422 | loss: 3.6259336471557617\n",
      "Epoch: 15/100 | step: 233/422 | loss: 3.4158895015716553\n",
      "Epoch: 15/100 | step: 234/422 | loss: 4.028179168701172\n",
      "Epoch: 15/100 | step: 235/422 | loss: 3.9431886672973633\n",
      "Epoch: 15/100 | step: 236/422 | loss: 3.6174683570861816\n",
      "Epoch: 15/100 | step: 237/422 | loss: 3.816884994506836\n",
      "Epoch: 15/100 | step: 238/422 | loss: 3.702639102935791\n",
      "Epoch: 15/100 | step: 239/422 | loss: 3.762972831726074\n",
      "Epoch: 15/100 | step: 240/422 | loss: 3.609924793243408\n",
      "Epoch: 15/100 | step: 241/422 | loss: 3.7540931701660156\n",
      "Epoch: 15/100 | step: 242/422 | loss: 3.79131817817688\n",
      "Epoch: 15/100 | step: 243/422 | loss: 3.9173152446746826\n",
      "Epoch: 15/100 | step: 244/422 | loss: 3.5904247760772705\n",
      "Epoch: 15/100 | step: 245/422 | loss: 3.8091816902160645\n",
      "Epoch: 15/100 | step: 246/422 | loss: 3.7444467544555664\n",
      "Epoch: 15/100 | step: 247/422 | loss: 3.7979085445404053\n",
      "Epoch: 15/100 | step: 248/422 | loss: 3.570126533508301\n",
      "Epoch: 15/100 | step: 249/422 | loss: 3.9672420024871826\n",
      "Epoch: 15/100 | step: 250/422 | loss: 3.7237820625305176\n",
      "Epoch: 15/100 | step: 251/422 | loss: 3.7744927406311035\n",
      "Epoch: 15/100 | step: 252/422 | loss: 3.7754719257354736\n",
      "Epoch: 15/100 | step: 253/422 | loss: 3.7580556869506836\n",
      "Epoch: 15/100 | step: 254/422 | loss: 3.967552423477173\n",
      "Epoch: 15/100 | step: 255/422 | loss: 3.7436587810516357\n",
      "Epoch: 15/100 | step: 256/422 | loss: 3.543416738510132\n",
      "Epoch: 15/100 | step: 257/422 | loss: 3.9626219272613525\n",
      "Epoch: 15/100 | step: 258/422 | loss: 4.033649921417236\n",
      "Epoch: 15/100 | step: 259/422 | loss: 3.661804676055908\n",
      "Epoch: 15/100 | step: 260/422 | loss: 3.8075451850891113\n",
      "Epoch: 15/100 | step: 261/422 | loss: 4.02680778503418\n",
      "Epoch: 15/100 | step: 262/422 | loss: 3.6216115951538086\n",
      "Epoch: 15/100 | step: 263/422 | loss: 3.8200795650482178\n",
      "Epoch: 15/100 | step: 264/422 | loss: 3.7829034328460693\n",
      "Epoch: 15/100 | step: 265/422 | loss: 3.576005697250366\n",
      "Epoch: 15/100 | step: 266/422 | loss: 3.634579658508301\n",
      "Epoch: 15/100 | step: 267/422 | loss: 3.7966082096099854\n",
      "Epoch: 15/100 | step: 268/422 | loss: 3.7255327701568604\n",
      "Epoch: 15/100 | step: 269/422 | loss: 3.6195740699768066\n",
      "Epoch: 15/100 | step: 270/422 | loss: 3.4877824783325195\n",
      "Epoch: 15/100 | step: 271/422 | loss: 3.9474172592163086\n",
      "Epoch: 15/100 | step: 272/422 | loss: 3.9179110527038574\n",
      "Epoch: 15/100 | step: 273/422 | loss: 3.5649564266204834\n",
      "Epoch: 15/100 | step: 274/422 | loss: 3.785111427307129\n",
      "Epoch: 15/100 | step: 275/422 | loss: 3.5008130073547363\n",
      "Epoch: 15/100 | step: 276/422 | loss: 3.8500280380249023\n",
      "Epoch: 15/100 | step: 277/422 | loss: 3.4579689502716064\n",
      "Epoch: 15/100 | step: 278/422 | loss: 3.6051323413848877\n",
      "Epoch: 15/100 | step: 279/422 | loss: 4.032108306884766\n",
      "Epoch: 15/100 | step: 280/422 | loss: 3.6545894145965576\n",
      "Epoch: 15/100 | step: 281/422 | loss: 3.7444145679473877\n",
      "Epoch: 15/100 | step: 282/422 | loss: 3.6405436992645264\n",
      "Epoch: 15/100 | step: 283/422 | loss: 3.779247283935547\n",
      "Epoch: 15/100 | step: 284/422 | loss: 3.330531120300293\n",
      "Epoch: 15/100 | step: 285/422 | loss: 3.935349225997925\n",
      "Epoch: 15/100 | step: 286/422 | loss: 3.85610294342041\n",
      "Epoch: 15/100 | step: 287/422 | loss: 3.6158158779144287\n",
      "Epoch: 15/100 | step: 288/422 | loss: 3.6032018661499023\n",
      "Epoch: 15/100 | step: 289/422 | loss: 3.585575819015503\n",
      "Epoch: 15/100 | step: 290/422 | loss: 3.8801116943359375\n",
      "Epoch: 15/100 | step: 291/422 | loss: 3.9929375648498535\n",
      "Epoch: 15/100 | step: 292/422 | loss: 3.64650297164917\n",
      "Epoch: 15/100 | step: 293/422 | loss: 3.9504122734069824\n",
      "Epoch: 15/100 | step: 294/422 | loss: 3.8777527809143066\n",
      "Epoch: 15/100 | step: 295/422 | loss: 4.019247055053711\n",
      "Epoch: 15/100 | step: 296/422 | loss: 3.5618746280670166\n",
      "Epoch: 15/100 | step: 297/422 | loss: 3.5836799144744873\n",
      "Epoch: 15/100 | step: 298/422 | loss: 3.7828075885772705\n",
      "Epoch: 15/100 | step: 299/422 | loss: 3.4652082920074463\n",
      "Epoch: 15/100 | step: 300/422 | loss: 3.490029811859131\n",
      "Epoch: 15/100 | step: 301/422 | loss: 3.4743025302886963\n",
      "Epoch: 15/100 | step: 302/422 | loss: 3.4155664443969727\n",
      "Epoch: 15/100 | step: 303/422 | loss: 4.202451705932617\n",
      "Epoch: 15/100 | step: 304/422 | loss: 3.7180798053741455\n",
      "Epoch: 15/100 | step: 305/422 | loss: 3.6879849433898926\n",
      "Epoch: 15/100 | step: 306/422 | loss: 3.7866368293762207\n",
      "Epoch: 15/100 | step: 307/422 | loss: 3.877866268157959\n",
      "Epoch: 15/100 | step: 308/422 | loss: 4.062192916870117\n",
      "Epoch: 15/100 | step: 309/422 | loss: 3.9270970821380615\n",
      "Epoch: 15/100 | step: 310/422 | loss: 3.494079828262329\n",
      "Epoch: 15/100 | step: 311/422 | loss: 3.4850566387176514\n",
      "Epoch: 15/100 | step: 312/422 | loss: 3.790541887283325\n",
      "Epoch: 15/100 | step: 313/422 | loss: 3.6464197635650635\n",
      "Epoch: 15/100 | step: 314/422 | loss: 3.6714603900909424\n",
      "Epoch: 15/100 | step: 315/422 | loss: 3.694000482559204\n",
      "Epoch: 15/100 | step: 316/422 | loss: 3.493722915649414\n",
      "Epoch: 15/100 | step: 317/422 | loss: 4.089241981506348\n",
      "Epoch: 15/100 | step: 318/422 | loss: 3.6243059635162354\n",
      "Epoch: 15/100 | step: 319/422 | loss: 3.3903162479400635\n",
      "Epoch: 15/100 | step: 320/422 | loss: 4.105849742889404\n",
      "Epoch: 15/100 | step: 321/422 | loss: 3.941540479660034\n",
      "Epoch: 15/100 | step: 322/422 | loss: 3.643714427947998\n",
      "Epoch: 15/100 | step: 323/422 | loss: 3.7159318923950195\n",
      "Epoch: 15/100 | step: 324/422 | loss: 3.6954705715179443\n",
      "Epoch: 15/100 | step: 325/422 | loss: 3.8878092765808105\n",
      "Epoch: 15/100 | step: 326/422 | loss: 3.6893670558929443\n",
      "Epoch: 15/100 | step: 327/422 | loss: 3.72647762298584\n",
      "Epoch: 15/100 | step: 328/422 | loss: 4.037879467010498\n",
      "Epoch: 15/100 | step: 329/422 | loss: 3.772446393966675\n",
      "Epoch: 15/100 | step: 330/422 | loss: 4.335756778717041\n",
      "Epoch: 15/100 | step: 331/422 | loss: 3.719259023666382\n",
      "Epoch: 15/100 | step: 332/422 | loss: 3.6283979415893555\n",
      "Epoch: 15/100 | step: 333/422 | loss: 3.623114585876465\n",
      "Epoch: 15/100 | step: 334/422 | loss: 3.748190402984619\n",
      "Epoch: 15/100 | step: 335/422 | loss: 3.5109481811523438\n",
      "Epoch: 15/100 | step: 336/422 | loss: 3.802675247192383\n",
      "Epoch: 15/100 | step: 337/422 | loss: 3.6140635013580322\n",
      "Epoch: 15/100 | step: 338/422 | loss: 3.823901891708374\n",
      "Epoch: 15/100 | step: 339/422 | loss: 3.8799118995666504\n",
      "Epoch: 15/100 | step: 340/422 | loss: 3.7212703227996826\n",
      "Epoch: 15/100 | step: 341/422 | loss: 3.5370259284973145\n",
      "Epoch: 15/100 | step: 342/422 | loss: 3.7635951042175293\n",
      "Epoch: 15/100 | step: 343/422 | loss: 3.831713914871216\n",
      "Epoch: 15/100 | step: 344/422 | loss: 3.993283271789551\n",
      "Epoch: 15/100 | step: 345/422 | loss: 3.745393753051758\n",
      "Epoch: 15/100 | step: 346/422 | loss: 3.608877658843994\n",
      "Epoch: 15/100 | step: 347/422 | loss: 3.6133029460906982\n",
      "Epoch: 15/100 | step: 348/422 | loss: 4.189298629760742\n",
      "Epoch: 15/100 | step: 349/422 | loss: 3.977614641189575\n",
      "Epoch: 15/100 | step: 350/422 | loss: 3.9257264137268066\n",
      "Epoch: 15/100 | step: 351/422 | loss: 3.723224401473999\n",
      "Epoch: 15/100 | step: 352/422 | loss: 3.481891632080078\n",
      "Epoch: 15/100 | step: 353/422 | loss: 3.764423131942749\n",
      "Epoch: 15/100 | step: 354/422 | loss: 3.503661870956421\n",
      "Epoch: 15/100 | step: 355/422 | loss: 4.008886337280273\n",
      "Epoch: 15/100 | step: 356/422 | loss: 3.593057870864868\n",
      "Epoch: 15/100 | step: 357/422 | loss: 4.020254135131836\n",
      "Epoch: 15/100 | step: 358/422 | loss: 3.9186670780181885\n",
      "Epoch: 15/100 | step: 359/422 | loss: 3.9952754974365234\n",
      "Epoch: 15/100 | step: 360/422 | loss: 3.7968196868896484\n",
      "Epoch: 15/100 | step: 361/422 | loss: 3.788252592086792\n",
      "Epoch: 15/100 | step: 362/422 | loss: 3.6491708755493164\n",
      "Epoch: 15/100 | step: 363/422 | loss: 3.806391716003418\n",
      "Epoch: 15/100 | step: 364/422 | loss: 4.028030872344971\n",
      "Epoch: 15/100 | step: 365/422 | loss: 3.58267879486084\n",
      "Epoch: 15/100 | step: 366/422 | loss: 3.9623634815216064\n",
      "Epoch: 15/100 | step: 367/422 | loss: 3.3774988651275635\n",
      "Epoch: 15/100 | step: 368/422 | loss: 3.8045461177825928\n",
      "Epoch: 15/100 | step: 369/422 | loss: 3.7019195556640625\n",
      "Epoch: 15/100 | step: 370/422 | loss: 3.5069234371185303\n",
      "Epoch: 15/100 | step: 371/422 | loss: 3.8226869106292725\n",
      "Epoch: 15/100 | step: 372/422 | loss: 3.7018074989318848\n",
      "Epoch: 15/100 | step: 373/422 | loss: 3.9123024940490723\n",
      "Epoch: 15/100 | step: 374/422 | loss: 3.768317937850952\n",
      "Epoch: 15/100 | step: 375/422 | loss: 3.54359769821167\n",
      "Epoch: 15/100 | step: 376/422 | loss: 3.597987651824951\n",
      "Epoch: 15/100 | step: 377/422 | loss: 3.8437418937683105\n",
      "Epoch: 15/100 | step: 378/422 | loss: 3.843721389770508\n",
      "Epoch: 15/100 | step: 379/422 | loss: 3.465805768966675\n",
      "Epoch: 15/100 | step: 380/422 | loss: 3.9385173320770264\n",
      "Epoch: 15/100 | step: 381/422 | loss: 3.9387364387512207\n",
      "Epoch: 15/100 | step: 382/422 | loss: 3.7631919384002686\n",
      "Epoch: 15/100 | step: 383/422 | loss: 3.6118507385253906\n",
      "Epoch: 15/100 | step: 384/422 | loss: 3.7975943088531494\n",
      "Epoch: 15/100 | step: 385/422 | loss: 3.9879190921783447\n",
      "Epoch: 15/100 | step: 386/422 | loss: 3.572627067565918\n",
      "Epoch: 15/100 | step: 387/422 | loss: 3.7951929569244385\n",
      "Epoch: 15/100 | step: 388/422 | loss: 3.593149423599243\n",
      "Epoch: 15/100 | step: 389/422 | loss: 3.5904366970062256\n",
      "Epoch: 15/100 | step: 390/422 | loss: 3.953434467315674\n",
      "Epoch: 15/100 | step: 391/422 | loss: 3.8434579372406006\n",
      "Epoch: 15/100 | step: 392/422 | loss: 3.518256187438965\n",
      "Epoch: 15/100 | step: 393/422 | loss: 3.7075884342193604\n",
      "Epoch: 15/100 | step: 394/422 | loss: 3.9803712368011475\n",
      "Epoch: 15/100 | step: 395/422 | loss: 3.732168674468994\n",
      "Epoch: 15/100 | step: 396/422 | loss: 3.9457485675811768\n",
      "Epoch: 15/100 | step: 397/422 | loss: 3.9499728679656982\n",
      "Epoch: 15/100 | step: 398/422 | loss: 3.9306139945983887\n",
      "Epoch: 15/100 | step: 399/422 | loss: 3.905886650085449\n",
      "Epoch: 15/100 | step: 400/422 | loss: 4.101764678955078\n",
      "Epoch: 15/100 | step: 401/422 | loss: 3.5794641971588135\n",
      "Epoch: 15/100 | step: 402/422 | loss: 4.236430644989014\n",
      "Epoch: 15/100 | step: 403/422 | loss: 3.8374440670013428\n",
      "Epoch: 15/100 | step: 404/422 | loss: 3.9997239112854004\n",
      "Epoch: 15/100 | step: 405/422 | loss: 3.9770143032073975\n",
      "Epoch: 15/100 | step: 406/422 | loss: 4.07940673828125\n",
      "Epoch: 15/100 | step: 407/422 | loss: 3.855201005935669\n",
      "Epoch: 15/100 | step: 408/422 | loss: 3.5889501571655273\n",
      "Epoch: 15/100 | step: 409/422 | loss: 3.5083696842193604\n",
      "Epoch: 15/100 | step: 410/422 | loss: 3.982300281524658\n",
      "Epoch: 15/100 | step: 411/422 | loss: 3.6979498863220215\n",
      "Epoch: 15/100 | step: 412/422 | loss: 3.8936924934387207\n",
      "Epoch: 15/100 | step: 413/422 | loss: 3.6728618144989014\n",
      "Epoch: 15/100 | step: 414/422 | loss: 3.8007657527923584\n",
      "Epoch: 15/100 | step: 415/422 | loss: 3.4327163696289062\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 16/100 | step: 1/422 | loss: 3.3416965007781982\n",
      "Epoch: 16/100 | step: 2/422 | loss: 3.8253872394561768\n",
      "Epoch: 16/100 | step: 3/422 | loss: 3.3812742233276367\n",
      "Epoch: 16/100 | step: 4/422 | loss: 3.7112326622009277\n",
      "Epoch: 16/100 | step: 5/422 | loss: 3.6993167400360107\n",
      "Epoch: 16/100 | step: 6/422 | loss: 3.362338066101074\n",
      "Epoch: 16/100 | step: 7/422 | loss: 3.659069538116455\n",
      "Epoch: 16/100 | step: 8/422 | loss: 3.493180751800537\n",
      "Epoch: 16/100 | step: 9/422 | loss: 4.04996395111084\n",
      "Epoch: 16/100 | step: 10/422 | loss: 4.013639450073242\n",
      "Epoch: 16/100 | step: 11/422 | loss: 3.6256299018859863\n",
      "Epoch: 16/100 | step: 12/422 | loss: 3.691239833831787\n",
      "Epoch: 16/100 | step: 13/422 | loss: 3.6796677112579346\n",
      "Epoch: 16/100 | step: 14/422 | loss: 3.9668185710906982\n",
      "Epoch: 16/100 | step: 15/422 | loss: 3.6315274238586426\n",
      "Epoch: 16/100 | step: 16/422 | loss: 3.910421848297119\n",
      "Epoch: 16/100 | step: 17/422 | loss: 3.5356979370117188\n",
      "Epoch: 16/100 | step: 18/422 | loss: 3.742570161819458\n",
      "Epoch: 16/100 | step: 19/422 | loss: 3.6652748584747314\n",
      "Epoch: 16/100 | step: 20/422 | loss: 3.946295738220215\n",
      "Epoch: 16/100 | step: 21/422 | loss: 3.472212076187134\n",
      "Epoch: 16/100 | step: 22/422 | loss: 3.641700267791748\n",
      "Epoch: 16/100 | step: 23/422 | loss: 3.5250701904296875\n",
      "Epoch: 16/100 | step: 24/422 | loss: 3.645390033721924\n",
      "Epoch: 16/100 | step: 25/422 | loss: 3.5278077125549316\n",
      "Epoch: 16/100 | step: 26/422 | loss: 3.6448044776916504\n",
      "Epoch: 16/100 | step: 27/422 | loss: 3.957843065261841\n",
      "Epoch: 16/100 | step: 28/422 | loss: 3.5669641494750977\n",
      "Epoch: 16/100 | step: 29/422 | loss: 3.5618062019348145\n",
      "Epoch: 16/100 | step: 30/422 | loss: 3.5669612884521484\n",
      "Epoch: 16/100 | step: 31/422 | loss: 3.681697368621826\n",
      "Epoch: 16/100 | step: 32/422 | loss: 3.4877960681915283\n",
      "Epoch: 16/100 | step: 33/422 | loss: 3.4556291103363037\n",
      "Epoch: 16/100 | step: 34/422 | loss: 4.1579694747924805\n",
      "Epoch: 16/100 | step: 35/422 | loss: 3.4234344959259033\n",
      "Epoch: 16/100 | step: 36/422 | loss: 3.84920334815979\n",
      "Epoch: 16/100 | step: 37/422 | loss: 4.170629978179932\n",
      "Epoch: 16/100 | step: 38/422 | loss: 3.591799736022949\n",
      "Epoch: 16/100 | step: 39/422 | loss: 3.6373343467712402\n",
      "Epoch: 16/100 | step: 40/422 | loss: 3.9102699756622314\n",
      "Epoch: 16/100 | step: 41/422 | loss: 3.987337112426758\n",
      "Epoch: 16/100 | step: 42/422 | loss: 3.6693036556243896\n",
      "Epoch: 16/100 | step: 43/422 | loss: 3.5335590839385986\n",
      "Epoch: 16/100 | step: 44/422 | loss: 3.3815271854400635\n",
      "Epoch: 16/100 | step: 45/422 | loss: 3.8467369079589844\n",
      "Epoch: 16/100 | step: 46/422 | loss: 3.5419507026672363\n",
      "Epoch: 16/100 | step: 47/422 | loss: 3.9669270515441895\n",
      "Epoch: 16/100 | step: 48/422 | loss: 3.5848560333251953\n",
      "Epoch: 16/100 | step: 49/422 | loss: 3.8513636589050293\n",
      "Epoch: 16/100 | step: 50/422 | loss: 3.733464479446411\n",
      "Epoch: 16/100 | step: 51/422 | loss: 3.5320191383361816\n",
      "Epoch: 16/100 | step: 52/422 | loss: 3.7078449726104736\n",
      "Epoch: 16/100 | step: 53/422 | loss: 3.884014368057251\n",
      "Epoch: 16/100 | step: 54/422 | loss: 3.5443994998931885\n",
      "Epoch: 16/100 | step: 55/422 | loss: 3.428279161453247\n",
      "Epoch: 16/100 | step: 56/422 | loss: 3.8627490997314453\n",
      "Epoch: 16/100 | step: 57/422 | loss: 3.482976198196411\n",
      "Epoch: 16/100 | step: 58/422 | loss: 3.6064960956573486\n",
      "Epoch: 16/100 | step: 59/422 | loss: 3.328233003616333\n",
      "Epoch: 16/100 | step: 60/422 | loss: 3.684537172317505\n",
      "Epoch: 16/100 | step: 61/422 | loss: 3.693164348602295\n",
      "Epoch: 16/100 | step: 62/422 | loss: 3.89123797416687\n",
      "Epoch: 16/100 | step: 63/422 | loss: 3.4525773525238037\n",
      "Epoch: 16/100 | step: 64/422 | loss: 3.787954568862915\n",
      "Epoch: 16/100 | step: 65/422 | loss: 3.7706298828125\n",
      "Epoch: 16/100 | step: 66/422 | loss: 4.159872055053711\n",
      "Epoch: 16/100 | step: 67/422 | loss: 3.637350559234619\n",
      "Epoch: 16/100 | step: 68/422 | loss: 3.694251537322998\n",
      "Epoch: 16/100 | step: 69/422 | loss: 3.6929619312286377\n",
      "Epoch: 16/100 | step: 70/422 | loss: 3.6095640659332275\n",
      "Epoch: 16/100 | step: 71/422 | loss: 4.092601299285889\n",
      "Epoch: 16/100 | step: 72/422 | loss: 3.476830005645752\n",
      "Epoch: 16/100 | step: 73/422 | loss: 3.721798896789551\n",
      "Epoch: 16/100 | step: 74/422 | loss: 3.3543784618377686\n",
      "Epoch: 16/100 | step: 75/422 | loss: 3.6224565505981445\n",
      "Epoch: 16/100 | step: 76/422 | loss: 3.874734401702881\n",
      "Epoch: 16/100 | step: 77/422 | loss: 3.7679247856140137\n",
      "Epoch: 16/100 | step: 78/422 | loss: 3.7608377933502197\n",
      "Epoch: 16/100 | step: 79/422 | loss: 3.840810537338257\n",
      "Epoch: 16/100 | step: 80/422 | loss: 3.8558802604675293\n",
      "Epoch: 16/100 | step: 81/422 | loss: 3.3505756855010986\n",
      "Epoch: 16/100 | step: 82/422 | loss: 3.8840065002441406\n",
      "Epoch: 16/100 | step: 83/422 | loss: 3.6546382904052734\n",
      "Epoch: 16/100 | step: 84/422 | loss: 3.1239984035491943\n",
      "Epoch: 16/100 | step: 85/422 | loss: 3.094762086868286\n",
      "Epoch: 16/100 | step: 86/422 | loss: 3.6943864822387695\n",
      "Epoch: 16/100 | step: 87/422 | loss: 3.685952663421631\n",
      "Epoch: 16/100 | step: 88/422 | loss: 3.3297548294067383\n",
      "Epoch: 16/100 | step: 89/422 | loss: 3.6300737857818604\n",
      "Epoch: 16/100 | step: 90/422 | loss: 3.7339794635772705\n",
      "Epoch: 16/100 | step: 91/422 | loss: 3.743298292160034\n",
      "Epoch: 16/100 | step: 92/422 | loss: 4.131536483764648\n",
      "Epoch: 16/100 | step: 93/422 | loss: 3.9920308589935303\n",
      "Epoch: 16/100 | step: 94/422 | loss: 3.6121108531951904\n",
      "Epoch: 16/100 | step: 95/422 | loss: 3.3968684673309326\n",
      "Epoch: 16/100 | step: 96/422 | loss: 3.4487996101379395\n",
      "Epoch: 16/100 | step: 97/422 | loss: 3.7152581214904785\n",
      "Epoch: 16/100 | step: 98/422 | loss: 3.3813276290893555\n",
      "Epoch: 16/100 | step: 99/422 | loss: 3.5867116451263428\n",
      "Epoch: 16/100 | step: 100/422 | loss: 3.536830186843872\n",
      "Epoch: 16/100 | step: 101/422 | loss: 3.746429204940796\n",
      "Epoch: 16/100 | step: 102/422 | loss: 3.6269426345825195\n",
      "Epoch: 16/100 | step: 103/422 | loss: 3.4519195556640625\n",
      "Epoch: 16/100 | step: 104/422 | loss: 3.5330801010131836\n",
      "Epoch: 16/100 | step: 105/422 | loss: 3.8968935012817383\n",
      "Epoch: 16/100 | step: 106/422 | loss: 3.4782419204711914\n",
      "Epoch: 16/100 | step: 107/422 | loss: 3.8414480686187744\n",
      "Epoch: 16/100 | step: 108/422 | loss: 3.554063320159912\n",
      "Epoch: 16/100 | step: 109/422 | loss: 3.4631545543670654\n",
      "Epoch: 16/100 | step: 110/422 | loss: 4.125776290893555\n",
      "Epoch: 16/100 | step: 111/422 | loss: 3.471363067626953\n",
      "Epoch: 16/100 | step: 112/422 | loss: 3.5485005378723145\n",
      "Epoch: 16/100 | step: 113/422 | loss: 3.4508066177368164\n",
      "Epoch: 16/100 | step: 114/422 | loss: 3.6310102939605713\n",
      "Epoch: 16/100 | step: 115/422 | loss: 3.835923910140991\n",
      "Epoch: 16/100 | step: 116/422 | loss: 4.155901908874512\n",
      "Epoch: 16/100 | step: 117/422 | loss: 3.405639171600342\n",
      "Epoch: 16/100 | step: 118/422 | loss: 3.462339401245117\n",
      "Epoch: 16/100 | step: 119/422 | loss: 3.4286346435546875\n",
      "Epoch: 16/100 | step: 120/422 | loss: 3.721676826477051\n",
      "Epoch: 16/100 | step: 121/422 | loss: 3.797114610671997\n",
      "Epoch: 16/100 | step: 122/422 | loss: 3.9286341667175293\n",
      "Epoch: 16/100 | step: 123/422 | loss: 3.4562249183654785\n",
      "Epoch: 16/100 | step: 124/422 | loss: 4.0362348556518555\n",
      "Epoch: 16/100 | step: 125/422 | loss: 3.6024043560028076\n",
      "Epoch: 16/100 | step: 126/422 | loss: 3.6849489212036133\n",
      "Epoch: 16/100 | step: 127/422 | loss: 3.452422857284546\n",
      "Epoch: 16/100 | step: 128/422 | loss: 3.622988224029541\n",
      "Epoch: 16/100 | step: 129/422 | loss: 3.8159537315368652\n",
      "Epoch: 16/100 | step: 130/422 | loss: 3.525420904159546\n",
      "Epoch: 16/100 | step: 131/422 | loss: 3.551565408706665\n",
      "Epoch: 16/100 | step: 132/422 | loss: 3.8909785747528076\n",
      "Epoch: 16/100 | step: 133/422 | loss: 3.4918668270111084\n",
      "Epoch: 16/100 | step: 134/422 | loss: 3.9500062465667725\n",
      "Epoch: 16/100 | step: 135/422 | loss: 3.935274362564087\n",
      "Epoch: 16/100 | step: 136/422 | loss: 3.9439008235931396\n",
      "Epoch: 16/100 | step: 137/422 | loss: 3.799896478652954\n",
      "Epoch: 16/100 | step: 138/422 | loss: 3.4492745399475098\n",
      "Epoch: 16/100 | step: 139/422 | loss: 3.6921796798706055\n",
      "Epoch: 16/100 | step: 140/422 | loss: 3.6668028831481934\n",
      "Epoch: 16/100 | step: 141/422 | loss: 3.325580596923828\n",
      "Epoch: 16/100 | step: 142/422 | loss: 3.4242537021636963\n",
      "Epoch: 16/100 | step: 143/422 | loss: 3.7595114707946777\n",
      "Epoch: 16/100 | step: 144/422 | loss: 3.2723546028137207\n",
      "Epoch: 16/100 | step: 145/422 | loss: 3.62606143951416\n",
      "Epoch: 16/100 | step: 146/422 | loss: 3.402254819869995\n",
      "Epoch: 16/100 | step: 147/422 | loss: 3.3843326568603516\n",
      "Epoch: 16/100 | step: 148/422 | loss: 3.568509101867676\n",
      "Epoch: 16/100 | step: 149/422 | loss: 3.587702751159668\n",
      "Epoch: 16/100 | step: 150/422 | loss: 3.459259033203125\n",
      "Epoch: 16/100 | step: 151/422 | loss: 3.558960437774658\n",
      "Epoch: 16/100 | step: 152/422 | loss: 3.634866714477539\n",
      "Epoch: 16/100 | step: 153/422 | loss: 3.960786819458008\n",
      "Epoch: 16/100 | step: 154/422 | loss: 3.6013290882110596\n",
      "Epoch: 16/100 | step: 155/422 | loss: 3.862691640853882\n",
      "Epoch: 16/100 | step: 156/422 | loss: 3.8464999198913574\n",
      "Epoch: 16/100 | step: 157/422 | loss: 3.6869561672210693\n",
      "Epoch: 16/100 | step: 158/422 | loss: 3.5447592735290527\n",
      "Epoch: 16/100 | step: 159/422 | loss: 4.003621578216553\n",
      "Epoch: 16/100 | step: 160/422 | loss: 3.802609443664551\n",
      "Epoch: 16/100 | step: 161/422 | loss: 3.7105965614318848\n",
      "Epoch: 16/100 | step: 162/422 | loss: 3.562911033630371\n",
      "Epoch: 16/100 | step: 163/422 | loss: 3.2487294673919678\n",
      "Epoch: 16/100 | step: 164/422 | loss: 3.4344823360443115\n",
      "Epoch: 16/100 | step: 165/422 | loss: 3.581429958343506\n",
      "Epoch: 16/100 | step: 166/422 | loss: 3.953357458114624\n",
      "Epoch: 16/100 | step: 167/422 | loss: 3.757629156112671\n",
      "Epoch: 16/100 | step: 168/422 | loss: 3.738830327987671\n",
      "Epoch: 16/100 | step: 169/422 | loss: 3.7203240394592285\n",
      "Epoch: 16/100 | step: 170/422 | loss: 3.489531993865967\n",
      "Epoch: 16/100 | step: 171/422 | loss: 3.6951637268066406\n",
      "Epoch: 16/100 | step: 172/422 | loss: 4.183093070983887\n",
      "Epoch: 16/100 | step: 173/422 | loss: 3.7149083614349365\n",
      "Epoch: 16/100 | step: 174/422 | loss: 3.5547804832458496\n",
      "Epoch: 16/100 | step: 175/422 | loss: 3.5605480670928955\n",
      "Epoch: 16/100 | step: 176/422 | loss: 3.500551223754883\n",
      "Epoch: 16/100 | step: 177/422 | loss: 3.629270553588867\n",
      "Epoch: 16/100 | step: 178/422 | loss: 3.5986268520355225\n",
      "Epoch: 16/100 | step: 179/422 | loss: 4.1121344566345215\n",
      "Epoch: 16/100 | step: 180/422 | loss: 3.2769033908843994\n",
      "Epoch: 16/100 | step: 181/422 | loss: 3.914341449737549\n",
      "Epoch: 16/100 | step: 182/422 | loss: 3.9491934776306152\n",
      "Epoch: 16/100 | step: 183/422 | loss: 3.7062089443206787\n",
      "Epoch: 16/100 | step: 184/422 | loss: 3.530184745788574\n",
      "Epoch: 16/100 | step: 185/422 | loss: 3.4965567588806152\n",
      "Epoch: 16/100 | step: 186/422 | loss: 3.8453259468078613\n",
      "Epoch: 16/100 | step: 187/422 | loss: 4.003077507019043\n",
      "Epoch: 16/100 | step: 188/422 | loss: 3.7525298595428467\n",
      "Epoch: 16/100 | step: 189/422 | loss: 3.548792839050293\n",
      "Epoch: 16/100 | step: 190/422 | loss: 3.6839358806610107\n",
      "Epoch: 16/100 | step: 191/422 | loss: 3.806151866912842\n",
      "Epoch: 16/100 | step: 192/422 | loss: 3.610106945037842\n",
      "Epoch: 16/100 | step: 193/422 | loss: 3.9611072540283203\n",
      "Epoch: 16/100 | step: 194/422 | loss: 3.5497443675994873\n",
      "Epoch: 16/100 | step: 195/422 | loss: 3.7635436058044434\n",
      "Epoch: 16/100 | step: 196/422 | loss: 3.6891446113586426\n",
      "Epoch: 16/100 | step: 197/422 | loss: 3.3358919620513916\n",
      "Epoch: 16/100 | step: 198/422 | loss: 3.4660072326660156\n",
      "Epoch: 16/100 | step: 199/422 | loss: 3.862947463989258\n",
      "Epoch: 16/100 | step: 200/422 | loss: 3.8732359409332275\n",
      "Epoch: 16/100 | step: 201/422 | loss: 3.655848264694214\n",
      "Epoch: 16/100 | step: 202/422 | loss: 3.8921337127685547\n",
      "Epoch: 16/100 | step: 203/422 | loss: 3.6393818855285645\n",
      "Epoch: 16/100 | step: 204/422 | loss: 4.053837776184082\n",
      "Epoch: 16/100 | step: 205/422 | loss: 3.590524435043335\n",
      "Epoch: 16/100 | step: 206/422 | loss: 3.692209005355835\n",
      "Epoch: 16/100 | step: 207/422 | loss: 3.6284074783325195\n",
      "Epoch: 16/100 | step: 208/422 | loss: 3.638456344604492\n",
      "Epoch: 16/100 | step: 209/422 | loss: 3.609421730041504\n",
      "Epoch: 16/100 | step: 210/422 | loss: 3.7647440433502197\n",
      "Epoch: 16/100 | step: 211/422 | loss: 4.085864543914795\n",
      "Epoch: 16/100 | step: 212/422 | loss: 3.5420114994049072\n",
      "Epoch: 16/100 | step: 213/422 | loss: 3.7990190982818604\n",
      "Epoch: 16/100 | step: 214/422 | loss: 3.7149288654327393\n",
      "Epoch: 16/100 | step: 215/422 | loss: 3.8354570865631104\n",
      "Epoch: 16/100 | step: 216/422 | loss: 3.350461006164551\n",
      "Epoch: 16/100 | step: 217/422 | loss: 3.694711923599243\n",
      "Epoch: 16/100 | step: 218/422 | loss: 3.7858150005340576\n",
      "Epoch: 16/100 | step: 219/422 | loss: 3.363924026489258\n",
      "Epoch: 16/100 | step: 220/422 | loss: 3.4827957153320312\n",
      "Epoch: 16/100 | step: 221/422 | loss: 3.6041290760040283\n",
      "Epoch: 16/100 | step: 222/422 | loss: 3.772747278213501\n",
      "Epoch: 16/100 | step: 223/422 | loss: 3.834042549133301\n",
      "Epoch: 16/100 | step: 224/422 | loss: 3.421602964401245\n",
      "Epoch: 16/100 | step: 225/422 | loss: 3.842076539993286\n",
      "Epoch: 16/100 | step: 226/422 | loss: 3.643672466278076\n",
      "Epoch: 16/100 | step: 227/422 | loss: 3.664365768432617\n",
      "Epoch: 16/100 | step: 228/422 | loss: 3.8049075603485107\n",
      "Epoch: 16/100 | step: 229/422 | loss: 3.7982826232910156\n",
      "Epoch: 16/100 | step: 230/422 | loss: 3.470008373260498\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 17/100 | step: 1/422 | loss: 3.6455631256103516\n",
      "Epoch: 17/100 | step: 2/422 | loss: 3.103327751159668\n",
      "Epoch: 17/100 | step: 3/422 | loss: 3.641589879989624\n",
      "Epoch: 17/100 | step: 4/422 | loss: 3.809448480606079\n",
      "Epoch: 17/100 | step: 5/422 | loss: 3.511554479598999\n",
      "Epoch: 17/100 | step: 6/422 | loss: 3.696317434310913\n",
      "Epoch: 17/100 | step: 7/422 | loss: 3.5234243869781494\n",
      "Epoch: 17/100 | step: 8/422 | loss: 3.1537976264953613\n",
      "Epoch: 17/100 | step: 9/422 | loss: 3.6196489334106445\n",
      "Epoch: 17/100 | step: 10/422 | loss: 3.7401607036590576\n",
      "Epoch: 17/100 | step: 11/422 | loss: 3.9078471660614014\n",
      "Epoch: 17/100 | step: 12/422 | loss: 3.554710865020752\n",
      "Epoch: 17/100 | step: 13/422 | loss: 3.688401460647583\n",
      "Epoch: 17/100 | step: 14/422 | loss: 3.4695723056793213\n",
      "Epoch: 17/100 | step: 15/422 | loss: 3.9752931594848633\n",
      "Epoch: 17/100 | step: 16/422 | loss: 4.027431964874268\n",
      "Epoch: 17/100 | step: 17/422 | loss: 3.148790121078491\n",
      "Epoch: 17/100 | step: 18/422 | loss: 3.9226315021514893\n",
      "Epoch: 17/100 | step: 19/422 | loss: 3.772935628890991\n",
      "Epoch: 17/100 | step: 20/422 | loss: 3.5017268657684326\n",
      "Epoch: 17/100 | step: 21/422 | loss: 3.4748053550720215\n",
      "Epoch: 17/100 | step: 22/422 | loss: 4.021190166473389\n",
      "Epoch: 17/100 | step: 23/422 | loss: 3.7341442108154297\n",
      "Epoch: 17/100 | step: 24/422 | loss: 3.7050061225891113\n",
      "Epoch: 17/100 | step: 25/422 | loss: 3.202805757522583\n",
      "Epoch: 17/100 | step: 26/422 | loss: 3.4320595264434814\n",
      "Epoch: 17/100 | step: 27/422 | loss: 3.6263670921325684\n",
      "Epoch: 17/100 | step: 28/422 | loss: 3.735463857650757\n",
      "Epoch: 17/100 | step: 29/422 | loss: 3.6154870986938477\n",
      "Epoch: 17/100 | step: 30/422 | loss: 3.8890187740325928\n",
      "Epoch: 17/100 | step: 31/422 | loss: 3.8680601119995117\n",
      "Epoch: 17/100 | step: 32/422 | loss: 3.631892681121826\n",
      "Epoch: 17/100 | step: 33/422 | loss: 3.769165515899658\n",
      "Epoch: 17/100 | step: 34/422 | loss: 3.374016046524048\n",
      "Epoch: 17/100 | step: 35/422 | loss: 3.798215627670288\n",
      "Epoch: 17/100 | step: 36/422 | loss: 3.448279619216919\n",
      "Epoch: 17/100 | step: 37/422 | loss: 3.2888593673706055\n",
      "Epoch: 17/100 | step: 38/422 | loss: 3.364394426345825\n",
      "Epoch: 17/100 | step: 39/422 | loss: 3.615375518798828\n",
      "Epoch: 17/100 | step: 40/422 | loss: 3.8371376991271973\n",
      "Epoch: 17/100 | step: 41/422 | loss: 3.768004894256592\n",
      "Epoch: 17/100 | step: 42/422 | loss: 3.5630805492401123\n",
      "Epoch: 17/100 | step: 43/422 | loss: 3.9283628463745117\n",
      "Epoch: 17/100 | step: 44/422 | loss: 3.253108501434326\n",
      "Epoch: 17/100 | step: 45/422 | loss: 3.1850574016571045\n",
      "Epoch: 17/100 | step: 46/422 | loss: 3.608896255493164\n",
      "Epoch: 17/100 | step: 47/422 | loss: 3.8726160526275635\n",
      "Epoch: 17/100 | step: 48/422 | loss: 3.761075258255005\n",
      "Epoch: 17/100 | step: 49/422 | loss: 3.3328957557678223\n",
      "Epoch: 17/100 | step: 50/422 | loss: 3.6749560832977295\n",
      "Epoch: 17/100 | step: 51/422 | loss: 3.321837902069092\n",
      "Epoch: 17/100 | step: 52/422 | loss: 3.333524227142334\n",
      "Epoch: 17/100 | step: 53/422 | loss: 3.3753178119659424\n",
      "Epoch: 17/100 | step: 54/422 | loss: 3.6295788288116455\n",
      "Epoch: 17/100 | step: 55/422 | loss: 3.401728868484497\n",
      "Epoch: 17/100 | step: 56/422 | loss: 3.555288076400757\n",
      "Epoch: 17/100 | step: 57/422 | loss: 3.0803275108337402\n",
      "Epoch: 17/100 | step: 58/422 | loss: 3.747668743133545\n",
      "Epoch: 17/100 | step: 59/422 | loss: 3.189810276031494\n",
      "Epoch: 17/100 | step: 60/422 | loss: 3.3122525215148926\n",
      "Epoch: 17/100 | step: 61/422 | loss: 4.1456499099731445\n",
      "Epoch: 17/100 | step: 62/422 | loss: 3.3399646282196045\n",
      "Epoch: 17/100 | step: 63/422 | loss: 3.6901872158050537\n",
      "Epoch: 17/100 | step: 64/422 | loss: 3.991497039794922\n",
      "Epoch: 17/100 | step: 65/422 | loss: 3.6797776222229004\n",
      "Epoch: 17/100 | step: 66/422 | loss: 3.7094879150390625\n",
      "Epoch: 17/100 | step: 67/422 | loss: 3.5650558471679688\n",
      "Epoch: 17/100 | step: 68/422 | loss: 3.6438679695129395\n",
      "Epoch: 17/100 | step: 69/422 | loss: 3.825063943862915\n",
      "Epoch: 17/100 | step: 70/422 | loss: 3.248883008956909\n",
      "Epoch: 17/100 | step: 71/422 | loss: 3.532332420349121\n",
      "Epoch: 17/100 | step: 72/422 | loss: 3.2543420791625977\n",
      "Epoch: 17/100 | step: 73/422 | loss: 3.4123597145080566\n",
      "Epoch: 17/100 | step: 74/422 | loss: 3.520535945892334\n",
      "Epoch: 17/100 | step: 75/422 | loss: 3.319239616394043\n",
      "Epoch: 17/100 | step: 76/422 | loss: 3.8970844745635986\n",
      "Epoch: 17/100 | step: 77/422 | loss: 3.6126201152801514\n",
      "Epoch: 17/100 | step: 78/422 | loss: 3.6277036666870117\n",
      "Epoch: 17/100 | step: 79/422 | loss: 3.3242979049682617\n",
      "Epoch: 17/100 | step: 80/422 | loss: 3.724184036254883\n",
      "Epoch: 17/100 | step: 81/422 | loss: 3.672029972076416\n",
      "Epoch: 17/100 | step: 82/422 | loss: 3.926375150680542\n",
      "Epoch: 17/100 | step: 83/422 | loss: 3.351092576980591\n",
      "Epoch: 17/100 | step: 84/422 | loss: 3.453641414642334\n",
      "Epoch: 17/100 | step: 85/422 | loss: 3.8591322898864746\n",
      "Epoch: 17/100 | step: 86/422 | loss: 3.3989834785461426\n",
      "Epoch: 17/100 | step: 87/422 | loss: 3.5254099369049072\n",
      "Epoch: 17/100 | step: 88/422 | loss: 3.6411354541778564\n",
      "Epoch: 17/100 | step: 89/422 | loss: 3.331204414367676\n",
      "Epoch: 17/100 | step: 90/422 | loss: 3.4266867637634277\n",
      "Epoch: 17/100 | step: 91/422 | loss: 3.589268922805786\n",
      "Epoch: 17/100 | step: 92/422 | loss: 3.710390090942383\n",
      "Epoch: 17/100 | step: 93/422 | loss: 3.7379794120788574\n",
      "Epoch: 17/100 | step: 94/422 | loss: 3.600719451904297\n",
      "Epoch: 17/100 | step: 95/422 | loss: 3.3981995582580566\n",
      "Epoch: 17/100 | step: 96/422 | loss: 3.7287538051605225\n",
      "Epoch: 17/100 | step: 97/422 | loss: 3.627642869949341\n",
      "Epoch: 17/100 | step: 98/422 | loss: 3.220954656600952\n",
      "Epoch: 17/100 | step: 99/422 | loss: 3.299238681793213\n",
      "Epoch: 17/100 | step: 100/422 | loss: 3.8033812046051025\n",
      "Epoch: 17/100 | step: 101/422 | loss: 3.2494304180145264\n",
      "Epoch: 17/100 | step: 102/422 | loss: 3.429617166519165\n",
      "Epoch: 17/100 | step: 103/422 | loss: 3.498424768447876\n",
      "Epoch: 17/100 | step: 104/422 | loss: 3.6431894302368164\n",
      "Epoch: 17/100 | step: 105/422 | loss: 3.499642848968506\n",
      "Epoch: 17/100 | step: 106/422 | loss: 3.4527945518493652\n",
      "Epoch: 17/100 | step: 107/422 | loss: 3.35183048248291\n",
      "Epoch: 17/100 | step: 108/422 | loss: 3.325281858444214\n",
      "Epoch: 17/100 | step: 109/422 | loss: 3.6145567893981934\n",
      "Epoch: 17/100 | step: 110/422 | loss: 3.690764904022217\n",
      "Epoch: 17/100 | step: 111/422 | loss: 3.638688087463379\n",
      "Epoch: 17/100 | step: 112/422 | loss: 3.516742706298828\n",
      "Epoch: 17/100 | step: 113/422 | loss: 3.736776113510132\n",
      "Epoch: 17/100 | step: 114/422 | loss: 3.4985604286193848\n",
      "Epoch: 17/100 | step: 115/422 | loss: 3.622967004776001\n",
      "Epoch: 17/100 | step: 116/422 | loss: 3.3277347087860107\n",
      "Epoch: 17/100 | step: 117/422 | loss: 3.5155136585235596\n",
      "Epoch: 17/100 | step: 118/422 | loss: 3.5897305011749268\n",
      "Epoch: 17/100 | step: 119/422 | loss: 3.331007242202759\n",
      "Epoch: 17/100 | step: 120/422 | loss: 3.5175750255584717\n",
      "Epoch: 17/100 | step: 121/422 | loss: 3.489907741546631\n",
      "Epoch: 17/100 | step: 122/422 | loss: 3.319331645965576\n",
      "Epoch: 17/100 | step: 123/422 | loss: 3.700685501098633\n",
      "Epoch: 17/100 | step: 124/422 | loss: 3.559051036834717\n",
      "Epoch: 17/100 | step: 125/422 | loss: 3.712968349456787\n",
      "Epoch: 17/100 | step: 126/422 | loss: 3.982771635055542\n",
      "Epoch: 17/100 | step: 127/422 | loss: 3.428868055343628\n",
      "Epoch: 17/100 | step: 128/422 | loss: 3.3786675930023193\n",
      "Epoch: 17/100 | step: 129/422 | loss: 3.898939371109009\n",
      "Epoch: 17/100 | step: 130/422 | loss: 3.5861425399780273\n",
      "Epoch: 17/100 | step: 131/422 | loss: 3.568845510482788\n",
      "Epoch: 17/100 | step: 132/422 | loss: 3.7884178161621094\n",
      "Epoch: 17/100 | step: 133/422 | loss: 3.6356143951416016\n",
      "Epoch: 17/100 | step: 134/422 | loss: 3.9578921794891357\n",
      "Epoch: 17/100 | step: 135/422 | loss: 3.9914638996124268\n",
      "Epoch: 17/100 | step: 136/422 | loss: 3.3708856105804443\n",
      "Epoch: 17/100 | step: 137/422 | loss: 3.5820930004119873\n",
      "Epoch: 17/100 | step: 138/422 | loss: 3.489029884338379\n",
      "Epoch: 17/100 | step: 139/422 | loss: 3.4693405628204346\n",
      "Epoch: 17/100 | step: 140/422 | loss: 3.93318510055542\n",
      "Epoch: 17/100 | step: 141/422 | loss: 3.595075845718384\n",
      "Epoch: 17/100 | step: 142/422 | loss: 3.4123878479003906\n",
      "Epoch: 17/100 | step: 143/422 | loss: 3.5802245140075684\n",
      "Epoch: 17/100 | step: 144/422 | loss: 3.021416187286377\n",
      "Epoch: 17/100 | step: 145/422 | loss: 3.6703152656555176\n",
      "Epoch: 17/100 | step: 146/422 | loss: 3.538461446762085\n",
      "Epoch: 17/100 | step: 147/422 | loss: 3.690524101257324\n",
      "Epoch: 17/100 | step: 148/422 | loss: 4.164371490478516\n",
      "Epoch: 17/100 | step: 149/422 | loss: 3.548612356185913\n",
      "Epoch: 17/100 | step: 150/422 | loss: 3.587918281555176\n",
      "Epoch: 17/100 | step: 151/422 | loss: 3.569361448287964\n",
      "Epoch: 17/100 | step: 152/422 | loss: 3.3770570755004883\n",
      "Epoch: 17/100 | step: 153/422 | loss: 3.578531503677368\n",
      "Epoch: 17/100 | step: 154/422 | loss: 3.6026477813720703\n",
      "Epoch: 17/100 | step: 155/422 | loss: 3.3309953212738037\n",
      "Epoch: 17/100 | step: 156/422 | loss: 3.296015977859497\n",
      "Epoch: 17/100 | step: 157/422 | loss: 3.589613914489746\n",
      "Epoch: 17/100 | step: 158/422 | loss: 3.5100767612457275\n",
      "Epoch: 17/100 | step: 159/422 | loss: 3.7149691581726074\n",
      "Epoch: 17/100 | step: 160/422 | loss: 3.9454803466796875\n",
      "Epoch: 17/100 | step: 161/422 | loss: 3.642428159713745\n",
      "Epoch: 17/100 | step: 162/422 | loss: 3.2572200298309326\n",
      "Epoch: 17/100 | step: 163/422 | loss: 3.6038711071014404\n",
      "Epoch: 17/100 | step: 164/422 | loss: 3.1750707626342773\n",
      "Epoch: 17/100 | step: 165/422 | loss: 3.5400307178497314\n",
      "Epoch: 17/100 | step: 166/422 | loss: 3.3697240352630615\n",
      "Epoch: 17/100 | step: 167/422 | loss: 3.7288808822631836\n",
      "Epoch: 17/100 | step: 168/422 | loss: 3.4284136295318604\n",
      "Epoch: 17/100 | step: 169/422 | loss: 3.3377628326416016\n",
      "Epoch: 17/100 | step: 170/422 | loss: 3.3594119548797607\n",
      "Epoch: 17/100 | step: 171/422 | loss: 3.623922348022461\n",
      "Epoch: 17/100 | step: 172/422 | loss: 3.6860527992248535\n",
      "Epoch: 17/100 | step: 173/422 | loss: 3.6450889110565186\n",
      "Epoch: 17/100 | step: 174/422 | loss: 3.5014901161193848\n",
      "Epoch: 17/100 | step: 175/422 | loss: 3.2052316665649414\n",
      "Epoch: 17/100 | step: 176/422 | loss: 3.443992853164673\n",
      "Epoch: 17/100 | step: 177/422 | loss: 4.0094380378723145\n",
      "Epoch: 17/100 | step: 178/422 | loss: 3.6828320026397705\n",
      "Epoch: 17/100 | step: 179/422 | loss: 3.049809694290161\n",
      "Epoch: 17/100 | step: 180/422 | loss: 3.595658540725708\n",
      "Epoch: 17/100 | step: 181/422 | loss: 3.775482654571533\n",
      "Epoch: 17/100 | step: 182/422 | loss: 3.5938596725463867\n",
      "Epoch: 17/100 | step: 183/422 | loss: 3.2743747234344482\n",
      "Epoch: 17/100 | step: 184/422 | loss: 3.802009344100952\n",
      "Epoch: 17/100 | step: 185/422 | loss: 4.044524192810059\n",
      "Epoch: 17/100 | step: 186/422 | loss: 3.141645669937134\n",
      "Epoch: 17/100 | step: 187/422 | loss: 3.5142688751220703\n",
      "Epoch: 17/100 | step: 188/422 | loss: 3.7226243019104004\n",
      "Epoch: 17/100 | step: 189/422 | loss: 3.395576000213623\n",
      "Epoch: 17/100 | step: 190/422 | loss: 3.39273738861084\n",
      "Epoch: 17/100 | step: 191/422 | loss: 3.6699345111846924\n",
      "Epoch: 17/100 | step: 192/422 | loss: 3.4005656242370605\n",
      "Epoch: 17/100 | step: 193/422 | loss: 3.904879570007324\n",
      "Epoch: 17/100 | step: 194/422 | loss: 3.6697189807891846\n",
      "Epoch: 17/100 | step: 195/422 | loss: 3.4450459480285645\n",
      "Epoch: 17/100 | step: 196/422 | loss: 3.728522777557373\n",
      "Epoch: 17/100 | step: 197/422 | loss: 3.3232390880584717\n",
      "Epoch: 17/100 | step: 198/422 | loss: 3.5603349208831787\n",
      "Epoch: 17/100 | step: 199/422 | loss: 4.191438674926758\n",
      "Epoch: 17/100 | step: 200/422 | loss: 3.2044594287872314\n",
      "Epoch: 17/100 | step: 201/422 | loss: 3.753279685974121\n",
      "Epoch: 17/100 | step: 202/422 | loss: 3.4155361652374268\n",
      "Epoch: 17/100 | step: 203/422 | loss: 3.7687435150146484\n",
      "Epoch: 17/100 | step: 204/422 | loss: 3.4896116256713867\n",
      "Epoch: 17/100 | step: 205/422 | loss: 3.6015782356262207\n",
      "Epoch: 17/100 | step: 206/422 | loss: 4.037205696105957\n",
      "Epoch: 17/100 | step: 207/422 | loss: 3.4514880180358887\n",
      "Epoch: 17/100 | step: 208/422 | loss: 3.022242546081543\n",
      "Epoch: 17/100 | step: 209/422 | loss: 3.324402093887329\n",
      "Epoch: 17/100 | step: 210/422 | loss: 3.542685031890869\n",
      "Epoch: 17/100 | step: 211/422 | loss: 3.461534023284912\n",
      "Epoch: 17/100 | step: 212/422 | loss: 3.5187482833862305\n",
      "Epoch: 17/100 | step: 213/422 | loss: 3.804527759552002\n",
      "Epoch: 17/100 | step: 214/422 | loss: 3.166571617126465\n",
      "Epoch: 17/100 | step: 215/422 | loss: 3.961731433868408\n",
      "Epoch: 17/100 | step: 216/422 | loss: 4.237417221069336\n",
      "Epoch: 17/100 | step: 217/422 | loss: 3.334153175354004\n",
      "Epoch: 17/100 | step: 218/422 | loss: 3.78411865234375\n",
      "Epoch: 17/100 | step: 219/422 | loss: 3.173959493637085\n",
      "Epoch: 17/100 | step: 220/422 | loss: 3.8656857013702393\n",
      "Epoch: 17/100 | step: 221/422 | loss: 3.5011775493621826\n",
      "Epoch: 17/100 | step: 222/422 | loss: 3.5430312156677246\n",
      "Epoch: 17/100 | step: 223/422 | loss: 4.070205211639404\n",
      "Epoch: 17/100 | step: 224/422 | loss: 3.8681223392486572\n",
      "Epoch: 17/100 | step: 225/422 | loss: 3.524447202682495\n",
      "Epoch: 17/100 | step: 226/422 | loss: 3.457853078842163\n",
      "Epoch: 17/100 | step: 227/422 | loss: 3.660757064819336\n",
      "Epoch: 17/100 | step: 228/422 | loss: 3.6508941650390625\n",
      "Epoch: 17/100 | step: 229/422 | loss: 4.188605308532715\n",
      "Epoch: 17/100 | step: 230/422 | loss: 3.771695852279663\n",
      "Epoch: 17/100 | step: 231/422 | loss: 3.663451910018921\n",
      "Epoch: 17/100 | step: 232/422 | loss: 3.448824644088745\n",
      "Epoch: 17/100 | step: 233/422 | loss: 3.31512713432312\n",
      "Epoch: 17/100 | step: 234/422 | loss: 3.6351888179779053\n",
      "Epoch: 17/100 | step: 235/422 | loss: 3.5900700092315674\n",
      "Epoch: 17/100 | step: 236/422 | loss: 3.2738921642303467\n",
      "Epoch: 17/100 | step: 237/422 | loss: 3.7685670852661133\n",
      "Epoch: 17/100 | step: 238/422 | loss: 3.5575037002563477\n",
      "Epoch: 17/100 | step: 239/422 | loss: 3.5940260887145996\n",
      "Epoch: 17/100 | step: 240/422 | loss: 3.4283556938171387\n",
      "Epoch: 17/100 | step: 241/422 | loss: 3.9406731128692627\n",
      "Epoch: 17/100 | step: 242/422 | loss: 3.118360757827759\n",
      "Epoch: 17/100 | step: 243/422 | loss: 3.398380994796753\n",
      "Epoch: 17/100 | step: 244/422 | loss: 3.786558151245117\n",
      "Epoch: 17/100 | step: 245/422 | loss: 3.3854074478149414\n",
      "Epoch: 17/100 | step: 246/422 | loss: 3.5317046642303467\n",
      "Epoch: 17/100 | step: 247/422 | loss: 3.494497537612915\n",
      "Epoch: 17/100 | step: 248/422 | loss: 4.033205509185791\n",
      "Epoch: 17/100 | step: 249/422 | loss: 3.3931944370269775\n",
      "Epoch: 17/100 | step: 250/422 | loss: 3.2545909881591797\n",
      "Epoch: 17/100 | step: 251/422 | loss: 3.8410627841949463\n",
      "Epoch: 17/100 | step: 252/422 | loss: 3.4957499504089355\n",
      "Epoch: 17/100 | step: 253/422 | loss: 3.3233461380004883\n",
      "Epoch: 17/100 | step: 254/422 | loss: 3.7151801586151123\n",
      "Epoch: 17/100 | step: 255/422 | loss: 3.514603853225708\n",
      "Epoch: 17/100 | step: 256/422 | loss: 3.3768160343170166\n",
      "Epoch: 17/100 | step: 257/422 | loss: 3.0829918384552\n",
      "Epoch: 17/100 | step: 258/422 | loss: 3.5158779621124268\n",
      "Epoch: 17/100 | step: 259/422 | loss: 3.848604679107666\n",
      "Epoch: 17/100 | step: 260/422 | loss: 3.6903581619262695\n",
      "Epoch: 17/100 | step: 261/422 | loss: 3.627941370010376\n",
      "Epoch: 17/100 | step: 262/422 | loss: 3.5065829753875732\n",
      "Epoch: 17/100 | step: 263/422 | loss: 3.754546642303467\n",
      "Epoch: 17/100 | step: 264/422 | loss: 4.2582526206970215\n",
      "Epoch: 17/100 | step: 265/422 | loss: 3.6588191986083984\n",
      "Epoch: 17/100 | step: 266/422 | loss: 3.637800693511963\n",
      "Epoch: 17/100 | step: 267/422 | loss: 3.4994890689849854\n",
      "Epoch: 17/100 | step: 268/422 | loss: 3.462277889251709\n",
      "Epoch: 17/100 | step: 269/422 | loss: 3.535055637359619\n",
      "Epoch: 17/100 | step: 270/422 | loss: 3.3936078548431396\n",
      "Epoch: 17/100 | step: 271/422 | loss: 3.48484206199646\n",
      "Epoch: 17/100 | step: 272/422 | loss: 3.1389594078063965\n",
      "Epoch: 17/100 | step: 273/422 | loss: 3.3671207427978516\n",
      "Epoch: 17/100 | step: 274/422 | loss: 3.7914950847625732\n",
      "Epoch: 17/100 | step: 275/422 | loss: 3.7377090454101562\n",
      "Epoch: 17/100 | step: 276/422 | loss: 3.4415130615234375\n",
      "Epoch: 17/100 | step: 277/422 | loss: 3.3128645420074463\n",
      "Epoch: 17/100 | step: 278/422 | loss: 4.239573001861572\n",
      "Epoch: 17/100 | step: 279/422 | loss: 3.6771092414855957\n",
      "Epoch: 17/100 | step: 280/422 | loss: 3.306931495666504\n",
      "Epoch: 17/100 | step: 281/422 | loss: 4.04014253616333\n",
      "Epoch: 17/100 | step: 282/422 | loss: 3.759336471557617\n",
      "Epoch: 17/100 | step: 283/422 | loss: 3.640005111694336\n",
      "Epoch: 17/100 | step: 284/422 | loss: 3.7350175380706787\n",
      "Epoch: 17/100 | step: 285/422 | loss: 3.7678613662719727\n",
      "Epoch: 17/100 | step: 286/422 | loss: 3.582880735397339\n",
      "Epoch: 17/100 | step: 287/422 | loss: 3.5475776195526123\n",
      "Epoch: 17/100 | step: 288/422 | loss: 3.5133700370788574\n",
      "Epoch: 17/100 | step: 289/422 | loss: 3.297635793685913\n",
      "Epoch: 17/100 | step: 290/422 | loss: 3.74434232711792\n",
      "Epoch: 17/100 | step: 291/422 | loss: 3.8288705348968506\n",
      "Epoch: 17/100 | step: 292/422 | loss: 3.545520067214966\n",
      "Epoch: 17/100 | step: 293/422 | loss: 3.697916030883789\n",
      "Epoch: 17/100 | step: 294/422 | loss: 3.256152629852295\n",
      "Epoch: 17/100 | step: 295/422 | loss: 4.143070220947266\n",
      "Epoch: 17/100 | step: 296/422 | loss: 3.5284602642059326\n",
      "Epoch: 17/100 | step: 297/422 | loss: 3.5648279190063477\n",
      "Epoch: 17/100 | step: 298/422 | loss: 3.609344959259033\n",
      "Epoch: 17/100 | step: 299/422 | loss: 3.549835681915283\n",
      "Epoch: 17/100 | step: 300/422 | loss: 3.5862789154052734\n",
      "Epoch: 17/100 | step: 301/422 | loss: 3.580859899520874\n",
      "Epoch: 17/100 | step: 302/422 | loss: 3.737184762954712\n",
      "Epoch: 17/100 | step: 303/422 | loss: 3.8434085845947266\n",
      "Epoch: 17/100 | step: 304/422 | loss: 3.725330114364624\n",
      "Epoch: 17/100 | step: 305/422 | loss: 3.4758596420288086\n",
      "Epoch: 17/100 | step: 306/422 | loss: 3.639552354812622\n",
      "Epoch: 17/100 | step: 307/422 | loss: 3.8465662002563477\n",
      "Epoch: 17/100 | step: 308/422 | loss: 3.3007514476776123\n",
      "Epoch: 17/100 | step: 309/422 | loss: 3.7598211765289307\n",
      "Epoch: 17/100 | step: 310/422 | loss: 3.4361348152160645\n",
      "Epoch: 17/100 | step: 311/422 | loss: 3.6475956439971924\n",
      "Epoch: 17/100 | step: 312/422 | loss: 3.8270392417907715\n",
      "Epoch: 17/100 | step: 313/422 | loss: 3.4574286937713623\n",
      "Epoch: 17/100 | step: 314/422 | loss: 3.3577144145965576\n",
      "Epoch: 17/100 | step: 315/422 | loss: 3.45465350151062\n",
      "Epoch: 17/100 | step: 316/422 | loss: 3.6123766899108887\n",
      "Epoch: 17/100 | step: 317/422 | loss: 3.7070388793945312\n",
      "Epoch: 17/100 | step: 318/422 | loss: 3.8103604316711426\n",
      "Epoch: 17/100 | step: 319/422 | loss: 3.616873264312744\n",
      "Epoch: 17/100 | step: 320/422 | loss: 3.8553085327148438\n",
      "Epoch: 17/100 | step: 321/422 | loss: 3.5390625\n",
      "Epoch: 17/100 | step: 322/422 | loss: 3.9493801593780518\n",
      "Epoch: 17/100 | step: 323/422 | loss: 3.6928465366363525\n",
      "Epoch: 17/100 | step: 324/422 | loss: 3.5383403301239014\n",
      "Epoch: 17/100 | step: 325/422 | loss: 3.5086653232574463\n",
      "Epoch: 17/100 | step: 326/422 | loss: 3.4366018772125244\n",
      "Epoch: 17/100 | step: 327/422 | loss: 3.655465841293335\n",
      "Epoch: 17/100 | step: 328/422 | loss: 3.70064640045166\n",
      "Epoch: 17/100 | step: 329/422 | loss: 3.656035900115967\n",
      "Epoch: 17/100 | step: 330/422 | loss: 3.4337847232818604\n",
      "Epoch: 17/100 | step: 331/422 | loss: 3.62819242477417\n",
      "Epoch: 17/100 | step: 332/422 | loss: 3.6590309143066406\n",
      "Epoch: 17/100 | step: 333/422 | loss: 3.4182941913604736\n",
      "Epoch: 17/100 | step: 334/422 | loss: 3.7857749462127686\n",
      "Epoch: 17/100 | step: 335/422 | loss: 3.266571283340454\n",
      "Epoch: 17/100 | step: 336/422 | loss: 3.3513331413269043\n",
      "Epoch: 17/100 | step: 337/422 | loss: 3.729668617248535\n",
      "Epoch: 17/100 | step: 338/422 | loss: 3.3935258388519287\n",
      "Epoch: 17/100 | step: 339/422 | loss: 3.430487632751465\n",
      "Epoch: 17/100 | step: 340/422 | loss: 3.432457685470581\n",
      "Epoch: 17/100 | step: 341/422 | loss: 3.9497244358062744\n",
      "Epoch: 17/100 | step: 342/422 | loss: 3.6449387073516846\n",
      "Epoch: 17/100 | step: 343/422 | loss: 3.284118175506592\n",
      "Epoch: 17/100 | step: 344/422 | loss: 3.768259286880493\n",
      "Epoch: 17/100 | step: 345/422 | loss: 3.4771676063537598\n",
      "Epoch: 17/100 | step: 346/422 | loss: 3.7156314849853516\n",
      "Epoch: 17/100 | step: 347/422 | loss: 3.137082576751709\n",
      "Epoch: 17/100 | step: 348/422 | loss: 4.100644588470459\n",
      "Epoch: 17/100 | step: 349/422 | loss: 3.5937509536743164\n",
      "Epoch: 17/100 | step: 350/422 | loss: 3.2721452713012695\n",
      "Epoch: 17/100 | step: 351/422 | loss: 3.6884567737579346\n",
      "Epoch: 17/100 | step: 352/422 | loss: 3.2881462574005127\n",
      "Epoch: 17/100 | step: 353/422 | loss: 3.578444480895996\n",
      "Epoch: 17/100 | step: 354/422 | loss: 3.8888323307037354\n",
      "Epoch: 17/100 | step: 355/422 | loss: 3.6785078048706055\n",
      "Epoch: 17/100 | step: 356/422 | loss: 3.5350303649902344\n",
      "Epoch: 17/100 | step: 357/422 | loss: 3.5443403720855713\n",
      "Epoch: 17/100 | step: 358/422 | loss: 3.394594192504883\n",
      "Epoch: 17/100 | step: 359/422 | loss: 3.364182233810425\n",
      "Epoch: 17/100 | step: 360/422 | loss: 3.3827953338623047\n",
      "Epoch: 17/100 | step: 361/422 | loss: 3.771322250366211\n",
      "Epoch: 17/100 | step: 362/422 | loss: 3.529330015182495\n",
      "Epoch: 17/100 | step: 363/422 | loss: 3.8454504013061523\n",
      "Epoch: 17/100 | step: 364/422 | loss: 3.578146457672119\n",
      "Epoch: 17/100 | step: 365/422 | loss: 3.3646907806396484\n",
      "Epoch: 17/100 | step: 366/422 | loss: 3.3691959381103516\n",
      "Epoch: 17/100 | step: 367/422 | loss: 3.255418062210083\n",
      "Epoch: 17/100 | step: 368/422 | loss: 3.5999207496643066\n",
      "Epoch: 17/100 | step: 369/422 | loss: 4.169922351837158\n",
      "Epoch: 17/100 | step: 370/422 | loss: 3.3365821838378906\n",
      "Epoch: 17/100 | step: 371/422 | loss: 3.448678970336914\n",
      "Epoch: 17/100 | step: 372/422 | loss: 3.5870096683502197\n",
      "Epoch: 17/100 | step: 373/422 | loss: 4.110249996185303\n",
      "Epoch: 17/100 | step: 374/422 | loss: 3.753286838531494\n",
      "Epoch: 17/100 | step: 375/422 | loss: 3.3831300735473633\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 18/100 | step: 1/422 | loss: 3.364595413208008\n",
      "Epoch: 18/100 | step: 2/422 | loss: 3.1884195804595947\n",
      "Epoch: 18/100 | step: 3/422 | loss: 3.719468355178833\n",
      "Epoch: 18/100 | step: 4/422 | loss: 3.5668411254882812\n",
      "Epoch: 18/100 | step: 5/422 | loss: 3.930783748626709\n",
      "Epoch: 18/100 | step: 6/422 | loss: 3.668278217315674\n",
      "Epoch: 18/100 | step: 7/422 | loss: 3.5148813724517822\n",
      "Epoch: 18/100 | step: 8/422 | loss: 3.3725128173828125\n",
      "Epoch: 18/100 | step: 9/422 | loss: 3.369331121444702\n",
      "Epoch: 18/100 | step: 10/422 | loss: 3.009080171585083\n",
      "Epoch: 18/100 | step: 11/422 | loss: 3.3840479850769043\n",
      "Epoch: 18/100 | step: 12/422 | loss: 3.651998281478882\n",
      "Epoch: 18/100 | step: 13/422 | loss: 3.5726988315582275\n",
      "Epoch: 18/100 | step: 14/422 | loss: 3.0921528339385986\n",
      "Epoch: 18/100 | step: 15/422 | loss: 3.914358615875244\n",
      "Epoch: 18/100 | step: 16/422 | loss: 2.7056069374084473\n",
      "Epoch: 18/100 | step: 17/422 | loss: 3.489891529083252\n",
      "Epoch: 18/100 | step: 18/422 | loss: 3.672126531600952\n",
      "Epoch: 18/100 | step: 19/422 | loss: 3.5169105529785156\n",
      "Epoch: 18/100 | step: 20/422 | loss: 3.625793218612671\n",
      "Epoch: 18/100 | step: 21/422 | loss: 3.4878783226013184\n",
      "Epoch: 18/100 | step: 22/422 | loss: 3.540374279022217\n",
      "Epoch: 18/100 | step: 23/422 | loss: 3.4252612590789795\n",
      "Epoch: 18/100 | step: 24/422 | loss: 3.639928102493286\n",
      "Epoch: 18/100 | step: 25/422 | loss: 3.707047939300537\n",
      "Epoch: 18/100 | step: 26/422 | loss: 3.5324716567993164\n",
      "Epoch: 18/100 | step: 27/422 | loss: 3.500439167022705\n",
      "Epoch: 18/100 | step: 28/422 | loss: 3.4399521350860596\n",
      "Epoch: 18/100 | step: 29/422 | loss: 3.5963029861450195\n",
      "Epoch: 18/100 | step: 30/422 | loss: 3.277033805847168\n",
      "Epoch: 18/100 | step: 31/422 | loss: 3.5077297687530518\n",
      "Epoch: 18/100 | step: 32/422 | loss: 3.492880344390869\n",
      "Epoch: 18/100 | step: 33/422 | loss: 3.518324851989746\n",
      "Epoch: 18/100 | step: 34/422 | loss: 3.873692035675049\n",
      "Epoch: 18/100 | step: 35/422 | loss: 3.298894166946411\n",
      "Epoch: 18/100 | step: 36/422 | loss: 3.631622314453125\n",
      "Epoch: 18/100 | step: 37/422 | loss: 3.601600408554077\n",
      "Epoch: 18/100 | step: 38/422 | loss: 3.0458626747131348\n",
      "Epoch: 18/100 | step: 39/422 | loss: 3.1308515071868896\n",
      "Epoch: 18/100 | step: 40/422 | loss: 3.7513725757598877\n",
      "Epoch: 18/100 | step: 41/422 | loss: 3.6140992641448975\n",
      "Epoch: 18/100 | step: 42/422 | loss: 3.445904016494751\n",
      "Epoch: 18/100 | step: 43/422 | loss: 3.7450623512268066\n",
      "Epoch: 18/100 | step: 44/422 | loss: 3.4035491943359375\n",
      "Epoch: 18/100 | step: 45/422 | loss: 3.473118305206299\n",
      "Epoch: 18/100 | step: 46/422 | loss: 3.1889116764068604\n",
      "Epoch: 18/100 | step: 47/422 | loss: 3.229083776473999\n",
      "Epoch: 18/100 | step: 48/422 | loss: 3.896491527557373\n",
      "Epoch: 18/100 | step: 49/422 | loss: 3.8828325271606445\n",
      "Epoch: 18/100 | step: 50/422 | loss: 3.9185500144958496\n",
      "Epoch: 18/100 | step: 51/422 | loss: 3.5503792762756348\n",
      "Epoch: 18/100 | step: 52/422 | loss: 3.169034481048584\n",
      "Epoch: 18/100 | step: 53/422 | loss: 3.810772180557251\n",
      "Epoch: 18/100 | step: 54/422 | loss: 3.861382484436035\n",
      "Epoch: 18/100 | step: 55/422 | loss: 3.4108922481536865\n",
      "Epoch: 18/100 | step: 56/422 | loss: 3.5643839836120605\n",
      "Epoch: 18/100 | step: 57/422 | loss: 3.3387463092803955\n",
      "Epoch: 18/100 | step: 58/422 | loss: 3.439344882965088\n",
      "Epoch: 18/100 | step: 59/422 | loss: 3.5111641883850098\n",
      "Epoch: 18/100 | step: 60/422 | loss: 3.733248710632324\n",
      "Epoch: 18/100 | step: 61/422 | loss: 3.2093021869659424\n",
      "Epoch: 18/100 | step: 62/422 | loss: 3.0881757736206055\n",
      "Epoch: 18/100 | step: 63/422 | loss: 3.3827695846557617\n",
      "Epoch: 18/100 | step: 64/422 | loss: 3.2524845600128174\n",
      "Epoch: 18/100 | step: 65/422 | loss: 3.4594924449920654\n",
      "Epoch: 18/100 | step: 66/422 | loss: 3.786759614944458\n",
      "Epoch: 18/100 | step: 67/422 | loss: 3.6965692043304443\n",
      "Epoch: 18/100 | step: 68/422 | loss: 3.3715009689331055\n",
      "Epoch: 18/100 | step: 69/422 | loss: 3.484130382537842\n",
      "Epoch: 18/100 | step: 70/422 | loss: 3.675649642944336\n",
      "Epoch: 18/100 | step: 71/422 | loss: 3.310208320617676\n",
      "Epoch: 18/100 | step: 72/422 | loss: 3.5675270557403564\n",
      "Epoch: 18/100 | step: 73/422 | loss: 3.538353204727173\n",
      "Epoch: 18/100 | step: 74/422 | loss: 3.350801467895508\n",
      "Epoch: 18/100 | step: 75/422 | loss: 3.571761131286621\n",
      "Epoch: 18/100 | step: 76/422 | loss: 3.3295986652374268\n",
      "Epoch: 18/100 | step: 77/422 | loss: 3.1833527088165283\n",
      "Epoch: 18/100 | step: 78/422 | loss: 3.550426721572876\n",
      "Epoch: 18/100 | step: 79/422 | loss: 3.6054742336273193\n",
      "Epoch: 18/100 | step: 80/422 | loss: 3.3327808380126953\n",
      "Epoch: 18/100 | step: 81/422 | loss: 3.140634298324585\n",
      "Epoch: 18/100 | step: 82/422 | loss: 3.972193717956543\n",
      "Epoch: 18/100 | step: 83/422 | loss: 3.251284122467041\n",
      "Epoch: 18/100 | step: 84/422 | loss: 2.9359803199768066\n",
      "Epoch: 18/100 | step: 85/422 | loss: 3.354483127593994\n",
      "Epoch: 18/100 | step: 86/422 | loss: 3.688034772872925\n",
      "Epoch: 18/100 | step: 87/422 | loss: 3.1579229831695557\n",
      "Epoch: 18/100 | step: 88/422 | loss: 3.7984259128570557\n",
      "Epoch: 18/100 | step: 89/422 | loss: 3.370300769805908\n",
      "Epoch: 18/100 | step: 90/422 | loss: 3.3257529735565186\n",
      "Epoch: 18/100 | step: 91/422 | loss: 3.1896955966949463\n",
      "Epoch: 18/100 | step: 92/422 | loss: 3.398468017578125\n",
      "Epoch: 18/100 | step: 93/422 | loss: 3.411461591720581\n",
      "Epoch: 18/100 | step: 94/422 | loss: 3.510424852371216\n",
      "Epoch: 18/100 | step: 95/422 | loss: 3.9765625\n",
      "Epoch: 18/100 | step: 96/422 | loss: 3.4654009342193604\n",
      "Epoch: 18/100 | step: 97/422 | loss: 3.3713157176971436\n",
      "Epoch: 18/100 | step: 98/422 | loss: 3.4400851726531982\n",
      "Epoch: 18/100 | step: 99/422 | loss: 3.3532392978668213\n",
      "Epoch: 18/100 | step: 100/422 | loss: 3.633436918258667\n",
      "Epoch: 18/100 | step: 101/422 | loss: 3.389453649520874\n",
      "Epoch: 18/100 | step: 102/422 | loss: 3.1592013835906982\n",
      "Epoch: 18/100 | step: 103/422 | loss: 3.320401191711426\n",
      "Epoch: 18/100 | step: 104/422 | loss: 3.7576804161071777\n",
      "Epoch: 18/100 | step: 105/422 | loss: 3.59120512008667\n",
      "Epoch: 18/100 | step: 106/422 | loss: 3.52939510345459\n",
      "Epoch: 18/100 | step: 107/422 | loss: 3.5428576469421387\n",
      "Epoch: 18/100 | step: 108/422 | loss: 3.659904718399048\n",
      "Epoch: 18/100 | step: 109/422 | loss: 3.8905441761016846\n",
      "Epoch: 18/100 | step: 110/422 | loss: 3.4784657955169678\n",
      "Epoch: 18/100 | step: 111/422 | loss: 3.0692195892333984\n",
      "Epoch: 18/100 | step: 112/422 | loss: 3.742105484008789\n",
      "Epoch: 18/100 | step: 113/422 | loss: 3.291918992996216\n",
      "Epoch: 18/100 | step: 114/422 | loss: 3.0994343757629395\n",
      "Epoch: 18/100 | step: 115/422 | loss: 3.670982599258423\n",
      "Epoch: 18/100 | step: 116/422 | loss: 3.3904223442077637\n",
      "Epoch: 18/100 | step: 117/422 | loss: 3.224914789199829\n",
      "Epoch: 18/100 | step: 118/422 | loss: 3.8817999362945557\n",
      "Epoch: 18/100 | step: 119/422 | loss: 3.5757761001586914\n",
      "Epoch: 18/100 | step: 120/422 | loss: 3.3639965057373047\n",
      "Epoch: 18/100 | step: 121/422 | loss: 3.2372167110443115\n",
      "Epoch: 18/100 | step: 122/422 | loss: 3.615814685821533\n",
      "Epoch: 18/100 | step: 123/422 | loss: 3.619781255722046\n",
      "Epoch: 18/100 | step: 124/422 | loss: 3.4602556228637695\n",
      "Epoch: 18/100 | step: 125/422 | loss: 3.978721857070923\n",
      "Epoch: 18/100 | step: 126/422 | loss: 3.273683786392212\n",
      "Epoch: 18/100 | step: 127/422 | loss: 3.0642340183258057\n",
      "Epoch: 18/100 | step: 128/422 | loss: 3.0505478382110596\n",
      "Epoch: 18/100 | step: 129/422 | loss: 3.7144651412963867\n",
      "Epoch: 18/100 | step: 130/422 | loss: 3.670497179031372\n",
      "Epoch: 18/100 | step: 131/422 | loss: 3.516045570373535\n",
      "Epoch: 18/100 | step: 132/422 | loss: 3.7721152305603027\n",
      "Epoch: 18/100 | step: 133/422 | loss: 3.420912981033325\n",
      "Epoch: 18/100 | step: 134/422 | loss: 3.5806565284729004\n",
      "Epoch: 18/100 | step: 135/422 | loss: 3.394137144088745\n",
      "Epoch: 18/100 | step: 136/422 | loss: 3.711937665939331\n",
      "Epoch: 18/100 | step: 137/422 | loss: 3.2696666717529297\n",
      "Epoch: 18/100 | step: 138/422 | loss: 3.75113844871521\n",
      "Epoch: 18/100 | step: 139/422 | loss: 3.151613235473633\n",
      "Epoch: 18/100 | step: 140/422 | loss: 3.5278077125549316\n",
      "Epoch: 18/100 | step: 141/422 | loss: 3.2871251106262207\n",
      "Epoch: 18/100 | step: 142/422 | loss: 3.3873226642608643\n",
      "Epoch: 18/100 | step: 143/422 | loss: 3.2693464756011963\n",
      "Epoch: 18/100 | step: 144/422 | loss: 3.6637802124023438\n",
      "Epoch: 18/100 | step: 145/422 | loss: 3.4897284507751465\n",
      "Epoch: 18/100 | step: 146/422 | loss: 3.0371415615081787\n",
      "Epoch: 18/100 | step: 147/422 | loss: 3.6539969444274902\n",
      "Epoch: 18/100 | step: 148/422 | loss: 3.5945146083831787\n",
      "Epoch: 18/100 | step: 149/422 | loss: 3.9373693466186523\n",
      "Epoch: 18/100 | step: 150/422 | loss: 3.66617488861084\n",
      "Epoch: 18/100 | step: 151/422 | loss: 3.8122568130493164\n",
      "Epoch: 18/100 | step: 152/422 | loss: 3.5651252269744873\n",
      "Epoch: 18/100 | step: 153/422 | loss: 3.3352606296539307\n",
      "Epoch: 18/100 | step: 154/422 | loss: 3.524341344833374\n",
      "Epoch: 18/100 | step: 155/422 | loss: 3.5386810302734375\n",
      "Epoch: 18/100 | step: 156/422 | loss: 2.9898886680603027\n",
      "Epoch: 18/100 | step: 157/422 | loss: 3.6522390842437744\n",
      "Epoch: 18/100 | step: 158/422 | loss: 3.5732972621917725\n",
      "Epoch: 18/100 | step: 159/422 | loss: 3.340435743331909\n",
      "Epoch: 18/100 | step: 160/422 | loss: 3.2675957679748535\n",
      "Epoch: 18/100 | step: 161/422 | loss: 3.8192663192749023\n",
      "Epoch: 18/100 | step: 162/422 | loss: 3.4146010875701904\n",
      "Epoch: 18/100 | step: 163/422 | loss: 3.3895835876464844\n",
      "Epoch: 18/100 | step: 164/422 | loss: 3.61454176902771\n",
      "Epoch: 18/100 | step: 165/422 | loss: 3.6204278469085693\n",
      "Epoch: 18/100 | step: 166/422 | loss: 3.5160207748413086\n",
      "Epoch: 18/100 | step: 167/422 | loss: 4.264193058013916\n",
      "Epoch: 18/100 | step: 168/422 | loss: 3.2013401985168457\n",
      "Epoch: 18/100 | step: 169/422 | loss: 3.4694578647613525\n",
      "Epoch: 18/100 | step: 170/422 | loss: 3.931962728500366\n",
      "Epoch: 18/100 | step: 171/422 | loss: 3.4757978916168213\n",
      "Epoch: 18/100 | step: 172/422 | loss: 3.252591133117676\n",
      "Epoch: 18/100 | step: 173/422 | loss: 3.1736221313476562\n",
      "Epoch: 18/100 | step: 174/422 | loss: 3.3430392742156982\n",
      "Epoch: 18/100 | step: 175/422 | loss: 3.383126974105835\n",
      "Epoch: 18/100 | step: 176/422 | loss: 3.47137713432312\n",
      "Epoch: 18/100 | step: 177/422 | loss: 3.4254069328308105\n",
      "Epoch: 18/100 | step: 178/422 | loss: 3.3943800926208496\n",
      "Epoch: 18/100 | step: 179/422 | loss: 4.034698963165283\n",
      "Epoch: 18/100 | step: 180/422 | loss: 3.3405723571777344\n",
      "Epoch: 18/100 | step: 181/422 | loss: 3.237910509109497\n",
      "Epoch: 18/100 | step: 182/422 | loss: 3.421372413635254\n",
      "Epoch: 18/100 | step: 183/422 | loss: 3.590050458908081\n",
      "Epoch: 18/100 | step: 184/422 | loss: 2.9631993770599365\n",
      "Epoch: 18/100 | step: 185/422 | loss: 3.455761432647705\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 19/100 | step: 1/422 | loss: 3.6621358394622803\n",
      "Epoch: 19/100 | step: 2/422 | loss: 3.0133004188537598\n",
      "Epoch: 19/100 | step: 3/422 | loss: 2.97434401512146\n",
      "Epoch: 19/100 | step: 4/422 | loss: 3.7561421394348145\n",
      "Epoch: 19/100 | step: 5/422 | loss: 3.6741809844970703\n",
      "Epoch: 19/100 | step: 6/422 | loss: 3.2103288173675537\n",
      "Epoch: 19/100 | step: 7/422 | loss: 3.1212894916534424\n",
      "Epoch: 19/100 | step: 8/422 | loss: 3.5167248249053955\n",
      "Epoch: 19/100 | step: 9/422 | loss: 3.204702377319336\n",
      "Epoch: 19/100 | step: 10/422 | loss: 3.2541046142578125\n",
      "Epoch: 19/100 | step: 11/422 | loss: 3.3231570720672607\n",
      "Epoch: 19/100 | step: 12/422 | loss: 2.984903335571289\n",
      "Epoch: 19/100 | step: 13/422 | loss: 3.4940638542175293\n",
      "Epoch: 19/100 | step: 14/422 | loss: 3.153075933456421\n",
      "Epoch: 19/100 | step: 15/422 | loss: 3.3828535079956055\n",
      "Epoch: 19/100 | step: 16/422 | loss: 3.5784318447113037\n",
      "Epoch: 19/100 | step: 17/422 | loss: 3.229128837585449\n",
      "Epoch: 19/100 | step: 18/422 | loss: 2.879647970199585\n",
      "Epoch: 19/100 | step: 19/422 | loss: 3.505189895629883\n",
      "Epoch: 19/100 | step: 20/422 | loss: 3.2928311824798584\n",
      "Epoch: 19/100 | step: 21/422 | loss: 3.50044322013855\n",
      "Epoch: 19/100 | step: 22/422 | loss: 3.5156335830688477\n",
      "Epoch: 19/100 | step: 23/422 | loss: 3.2492270469665527\n",
      "Epoch: 19/100 | step: 24/422 | loss: 2.9027178287506104\n",
      "Epoch: 19/100 | step: 25/422 | loss: 3.9672725200653076\n",
      "Epoch: 19/100 | step: 26/422 | loss: 2.9971330165863037\n",
      "Epoch: 19/100 | step: 27/422 | loss: 2.929001808166504\n",
      "Epoch: 19/100 | step: 28/422 | loss: 3.5262210369110107\n",
      "Epoch: 19/100 | step: 29/422 | loss: 3.63779878616333\n",
      "Epoch: 19/100 | step: 30/422 | loss: 3.1641461849212646\n",
      "Epoch: 19/100 | step: 31/422 | loss: 3.56644868850708\n",
      "Epoch: 19/100 | step: 32/422 | loss: 3.514612913131714\n",
      "Epoch: 19/100 | step: 33/422 | loss: 3.043628454208374\n",
      "Epoch: 19/100 | step: 34/422 | loss: 3.4852638244628906\n",
      "Epoch: 19/100 | step: 35/422 | loss: 3.225538730621338\n",
      "Epoch: 19/100 | step: 36/422 | loss: 3.42055606842041\n",
      "Epoch: 19/100 | step: 37/422 | loss: 3.869208812713623\n",
      "Epoch: 19/100 | step: 38/422 | loss: 3.6194167137145996\n",
      "Epoch: 19/100 | step: 39/422 | loss: 3.640713930130005\n",
      "Epoch: 19/100 | step: 40/422 | loss: 3.4041874408721924\n",
      "Epoch: 19/100 | step: 41/422 | loss: 3.2670934200286865\n",
      "Epoch: 19/100 | step: 42/422 | loss: 3.346954584121704\n",
      "Epoch: 19/100 | step: 43/422 | loss: 3.5660176277160645\n",
      "Epoch: 19/100 | step: 44/422 | loss: 3.2311923503875732\n",
      "Epoch: 19/100 | step: 45/422 | loss: 3.8358206748962402\n",
      "Epoch: 19/100 | step: 46/422 | loss: 3.115621566772461\n",
      "Epoch: 19/100 | step: 47/422 | loss: 3.724313497543335\n",
      "Epoch: 19/100 | step: 48/422 | loss: 3.4913594722747803\n",
      "Epoch: 19/100 | step: 49/422 | loss: 3.17254376411438\n",
      "Epoch: 19/100 | step: 50/422 | loss: 3.1942453384399414\n",
      "Epoch: 19/100 | step: 51/422 | loss: 2.935089588165283\n",
      "Epoch: 19/100 | step: 52/422 | loss: 2.7889535427093506\n",
      "Epoch: 19/100 | step: 53/422 | loss: 3.3954877853393555\n",
      "Epoch: 19/100 | step: 54/422 | loss: 3.301375389099121\n",
      "Epoch: 19/100 | step: 55/422 | loss: 3.2967982292175293\n",
      "Epoch: 19/100 | step: 56/422 | loss: 3.5575520992279053\n",
      "Epoch: 19/100 | step: 57/422 | loss: 3.713179111480713\n",
      "Epoch: 19/100 | step: 58/422 | loss: 3.3211660385131836\n",
      "Epoch: 19/100 | step: 59/422 | loss: 3.3855273723602295\n",
      "Epoch: 19/100 | step: 60/422 | loss: 3.663689136505127\n",
      "Epoch: 19/100 | step: 61/422 | loss: 3.344619035720825\n",
      "Epoch: 19/100 | step: 62/422 | loss: 2.8614754676818848\n",
      "Epoch: 19/100 | step: 63/422 | loss: 3.1365487575531006\n",
      "Epoch: 19/100 | step: 64/422 | loss: 3.746833324432373\n",
      "Epoch: 19/100 | step: 65/422 | loss: 3.389610528945923\n",
      "Epoch: 19/100 | step: 66/422 | loss: 3.1307013034820557\n",
      "Epoch: 19/100 | step: 67/422 | loss: 3.180065393447876\n",
      "Epoch: 19/100 | step: 68/422 | loss: 3.5534913539886475\n",
      "Epoch: 19/100 | step: 69/422 | loss: 3.3809566497802734\n",
      "Epoch: 19/100 | step: 70/422 | loss: 3.7285382747650146\n",
      "Epoch: 19/100 | step: 71/422 | loss: 3.761622190475464\n",
      "Epoch: 19/100 | step: 72/422 | loss: 3.639240264892578\n",
      "Epoch: 19/100 | step: 73/422 | loss: 3.548269033432007\n",
      "Epoch: 19/100 | step: 74/422 | loss: 3.2284698486328125\n",
      "Epoch: 19/100 | step: 75/422 | loss: 3.909027576446533\n",
      "Epoch: 19/100 | step: 76/422 | loss: 3.395082950592041\n",
      "Epoch: 19/100 | step: 77/422 | loss: 3.7008159160614014\n",
      "Epoch: 19/100 | step: 78/422 | loss: 3.4157233238220215\n",
      "Epoch: 19/100 | step: 79/422 | loss: 3.110708713531494\n",
      "Epoch: 19/100 | step: 80/422 | loss: 3.840538263320923\n",
      "Epoch: 19/100 | step: 81/422 | loss: 3.6249234676361084\n",
      "Epoch: 19/100 | step: 82/422 | loss: 3.628903865814209\n",
      "Epoch: 19/100 | step: 83/422 | loss: 3.726127862930298\n",
      "Epoch: 19/100 | step: 84/422 | loss: 3.158020257949829\n",
      "Epoch: 19/100 | step: 85/422 | loss: 3.5069210529327393\n",
      "Epoch: 19/100 | step: 86/422 | loss: 3.7590572834014893\n",
      "Epoch: 19/100 | step: 87/422 | loss: 3.1696267127990723\n",
      "Epoch: 19/100 | step: 88/422 | loss: 3.6565299034118652\n",
      "Epoch: 19/100 | step: 89/422 | loss: 3.3030848503112793\n",
      "Epoch: 19/100 | step: 90/422 | loss: 2.9963090419769287\n",
      "Epoch: 19/100 | step: 91/422 | loss: 3.3975720405578613\n",
      "Epoch: 19/100 | step: 92/422 | loss: 3.388784646987915\n",
      "Epoch: 19/100 | step: 93/422 | loss: 3.7187371253967285\n",
      "Epoch: 19/100 | step: 94/422 | loss: 3.0250701904296875\n",
      "Epoch: 19/100 | step: 95/422 | loss: 3.406341314315796\n",
      "Epoch: 19/100 | step: 96/422 | loss: 3.2894186973571777\n",
      "Epoch: 19/100 | step: 97/422 | loss: 3.507756471633911\n",
      "Epoch: 19/100 | step: 98/422 | loss: 3.4133658409118652\n",
      "Epoch: 19/100 | step: 99/422 | loss: 3.298128843307495\n",
      "Epoch: 19/100 | step: 100/422 | loss: 3.4707577228546143\n",
      "Epoch: 19/100 | step: 101/422 | loss: 3.248288154602051\n",
      "Epoch: 19/100 | step: 102/422 | loss: 3.781846284866333\n",
      "Epoch: 19/100 | step: 103/422 | loss: 3.4648795127868652\n",
      "Epoch: 19/100 | step: 104/422 | loss: 3.469330310821533\n",
      "Epoch: 19/100 | step: 105/422 | loss: 3.5162885189056396\n",
      "Epoch: 19/100 | step: 106/422 | loss: 3.2597572803497314\n",
      "Epoch: 19/100 | step: 107/422 | loss: 3.3849356174468994\n",
      "Epoch: 19/100 | step: 108/422 | loss: 3.4713656902313232\n",
      "Epoch: 19/100 | step: 109/422 | loss: 3.44053316116333\n",
      "Epoch: 19/100 | step: 110/422 | loss: 3.3501479625701904\n",
      "Epoch: 19/100 | step: 111/422 | loss: 3.3117945194244385\n",
      "Epoch: 19/100 | step: 112/422 | loss: 3.3165721893310547\n",
      "Epoch: 19/100 | step: 113/422 | loss: 4.014798164367676\n",
      "Epoch: 19/100 | step: 114/422 | loss: 3.375612735748291\n",
      "Epoch: 19/100 | step: 115/422 | loss: 3.148557662963867\n",
      "Epoch: 19/100 | step: 116/422 | loss: 3.676975727081299\n",
      "Epoch: 19/100 | step: 117/422 | loss: 3.315218448638916\n",
      "Epoch: 19/100 | step: 118/422 | loss: 3.043982982635498\n",
      "Epoch: 19/100 | step: 119/422 | loss: 3.6686620712280273\n",
      "Epoch: 19/100 | step: 120/422 | loss: 3.6040196418762207\n",
      "Epoch: 19/100 | step: 121/422 | loss: 3.331118583679199\n",
      "Epoch: 19/100 | step: 122/422 | loss: 3.9385945796966553\n",
      "Epoch: 19/100 | step: 123/422 | loss: 3.3310463428497314\n",
      "Epoch: 19/100 | step: 124/422 | loss: 3.225446939468384\n",
      "Epoch: 19/100 | step: 125/422 | loss: 3.566518545150757\n",
      "Epoch: 19/100 | step: 126/422 | loss: 3.480142593383789\n",
      "Epoch: 19/100 | step: 127/422 | loss: 3.436250686645508\n",
      "Epoch: 19/100 | step: 128/422 | loss: 3.611912727355957\n",
      "Epoch: 19/100 | step: 129/422 | loss: 3.159475326538086\n",
      "Epoch: 19/100 | step: 130/422 | loss: 3.1983842849731445\n",
      "Epoch: 19/100 | step: 131/422 | loss: 3.3512203693389893\n",
      "Epoch: 19/100 | step: 132/422 | loss: 3.574967861175537\n",
      "Epoch: 19/100 | step: 133/422 | loss: 3.524353265762329\n",
      "Epoch: 19/100 | step: 134/422 | loss: 3.3325133323669434\n",
      "Epoch: 19/100 | step: 135/422 | loss: 3.2212746143341064\n",
      "Epoch: 19/100 | step: 136/422 | loss: 3.26263689994812\n",
      "Epoch: 19/100 | step: 137/422 | loss: 3.5398752689361572\n",
      "Epoch: 19/100 | step: 138/422 | loss: 3.145528793334961\n",
      "Epoch: 19/100 | step: 139/422 | loss: 3.6051700115203857\n",
      "Epoch: 19/100 | step: 140/422 | loss: 3.3092684745788574\n",
      "Epoch: 19/100 | step: 141/422 | loss: 2.8952226638793945\n",
      "Epoch: 19/100 | step: 142/422 | loss: 3.960894823074341\n",
      "Epoch: 19/100 | step: 143/422 | loss: 3.787635564804077\n",
      "Epoch: 19/100 | step: 144/422 | loss: 3.4774043560028076\n",
      "Epoch: 19/100 | step: 145/422 | loss: 3.8027396202087402\n",
      "Epoch: 19/100 | step: 146/422 | loss: 3.5516982078552246\n",
      "Epoch: 19/100 | step: 147/422 | loss: 3.3171560764312744\n",
      "Epoch: 19/100 | step: 148/422 | loss: 3.1509547233581543\n",
      "Epoch: 19/100 | step: 149/422 | loss: 3.672027826309204\n",
      "Epoch: 19/100 | step: 150/422 | loss: 3.7805299758911133\n",
      "Epoch: 19/100 | step: 151/422 | loss: 3.502117395401001\n",
      "Epoch: 19/100 | step: 152/422 | loss: 3.3762619495391846\n",
      "Epoch: 19/100 | step: 153/422 | loss: 3.5733845233917236\n",
      "Epoch: 19/100 | step: 154/422 | loss: 3.729780912399292\n",
      "Epoch: 19/100 | step: 155/422 | loss: 3.4767000675201416\n",
      "Epoch: 19/100 | step: 156/422 | loss: 3.2344961166381836\n",
      "Epoch: 19/100 | step: 157/422 | loss: 3.136901378631592\n",
      "Epoch: 19/100 | step: 158/422 | loss: 3.457005262374878\n",
      "Epoch: 19/100 | step: 159/422 | loss: 3.3295650482177734\n",
      "Epoch: 19/100 | step: 160/422 | loss: 3.7580792903900146\n",
      "Epoch: 19/100 | step: 161/422 | loss: 3.797046184539795\n",
      "Epoch: 19/100 | step: 162/422 | loss: 3.49505877494812\n",
      "Epoch: 19/100 | step: 163/422 | loss: 3.4186134338378906\n",
      "Epoch: 19/100 | step: 164/422 | loss: 3.136791229248047\n",
      "Epoch: 19/100 | step: 165/422 | loss: 3.4500925540924072\n",
      "Epoch: 19/100 | step: 166/422 | loss: 3.972792387008667\n",
      "Epoch: 19/100 | step: 167/422 | loss: 3.4918901920318604\n",
      "Epoch: 19/100 | step: 168/422 | loss: 2.966426372528076\n",
      "Epoch: 19/100 | step: 169/422 | loss: 3.5387113094329834\n",
      "Epoch: 19/100 | step: 170/422 | loss: 3.634880542755127\n",
      "Epoch: 19/100 | step: 171/422 | loss: 3.2889597415924072\n",
      "Epoch: 19/100 | step: 172/422 | loss: 3.379251718521118\n",
      "Epoch: 19/100 | step: 173/422 | loss: 3.1162068843841553\n",
      "Epoch: 19/100 | step: 174/422 | loss: 3.05855393409729\n",
      "Epoch: 19/100 | step: 175/422 | loss: 3.6467065811157227\n",
      "Epoch: 19/100 | step: 176/422 | loss: 3.5896005630493164\n",
      "Epoch: 19/100 | step: 177/422 | loss: 3.554244041442871\n",
      "Epoch: 19/100 | step: 178/422 | loss: 3.915364980697632\n",
      "Epoch: 19/100 | step: 179/422 | loss: 3.6700491905212402\n",
      "Epoch: 19/100 | step: 180/422 | loss: 3.5491206645965576\n",
      "Epoch: 19/100 | step: 181/422 | loss: 3.375108003616333\n",
      "Epoch: 19/100 | step: 182/422 | loss: 3.3573250770568848\n",
      "Epoch: 19/100 | step: 183/422 | loss: 3.354992628097534\n",
      "Epoch: 19/100 | step: 184/422 | loss: 2.852008819580078\n",
      "Epoch: 19/100 | step: 185/422 | loss: 3.6564486026763916\n",
      "Epoch: 19/100 | step: 186/422 | loss: 3.519632339477539\n",
      "Epoch: 19/100 | step: 187/422 | loss: 3.733170509338379\n",
      "Epoch: 19/100 | step: 188/422 | loss: 3.3846611976623535\n",
      "Epoch: 19/100 | step: 189/422 | loss: 3.483703851699829\n",
      "Epoch: 19/100 | step: 190/422 | loss: 3.209730386734009\n",
      "Epoch: 19/100 | step: 191/422 | loss: 3.184299945831299\n",
      "Epoch: 19/100 | step: 192/422 | loss: 3.494920492172241\n",
      "Epoch: 19/100 | step: 193/422 | loss: 3.2802207469940186\n",
      "Epoch: 19/100 | step: 194/422 | loss: 3.4972915649414062\n",
      "Epoch: 19/100 | step: 195/422 | loss: 3.7098915576934814\n",
      "Epoch: 19/100 | step: 196/422 | loss: 3.3599324226379395\n",
      "Epoch: 19/100 | step: 197/422 | loss: 3.7009799480438232\n",
      "Epoch: 19/100 | step: 198/422 | loss: 3.1142327785491943\n",
      "Epoch: 19/100 | step: 199/422 | loss: 3.1875319480895996\n",
      "Epoch: 19/100 | step: 200/422 | loss: 3.2705509662628174\n",
      "Epoch: 19/100 | step: 201/422 | loss: 3.4906716346740723\n",
      "Epoch: 19/100 | step: 202/422 | loss: 3.3746440410614014\n",
      "Epoch: 19/100 | step: 203/422 | loss: 3.1443278789520264\n",
      "Epoch: 19/100 | step: 204/422 | loss: 3.452465772628784\n",
      "Epoch: 19/100 | step: 205/422 | loss: 3.318925619125366\n",
      "Epoch: 19/100 | step: 206/422 | loss: 3.607527732849121\n",
      "Epoch: 19/100 | step: 207/422 | loss: 3.4861979484558105\n",
      "Epoch: 19/100 | step: 208/422 | loss: 3.7421391010284424\n",
      "Epoch: 19/100 | step: 209/422 | loss: 3.4659111499786377\n",
      "Epoch: 19/100 | step: 210/422 | loss: 3.2367756366729736\n",
      "Epoch: 19/100 | step: 211/422 | loss: 3.601161003112793\n",
      "Epoch: 19/100 | step: 212/422 | loss: 3.5878868103027344\n",
      "Epoch: 19/100 | step: 213/422 | loss: 3.776866912841797\n",
      "Epoch: 19/100 | step: 214/422 | loss: 3.2911832332611084\n",
      "Epoch: 19/100 | step: 215/422 | loss: 3.378985643386841\n",
      "Epoch: 19/100 | step: 216/422 | loss: 3.3224401473999023\n",
      "Epoch: 19/100 | step: 217/422 | loss: 3.6093192100524902\n",
      "Epoch: 19/100 | step: 218/422 | loss: 3.3727614879608154\n",
      "Epoch: 19/100 | step: 219/422 | loss: 3.4889931678771973\n",
      "Epoch: 19/100 | step: 220/422 | loss: 3.239762544631958\n",
      "Epoch: 19/100 | step: 221/422 | loss: 3.394805431365967\n",
      "Epoch: 19/100 | step: 222/422 | loss: 3.4627318382263184\n",
      "Epoch: 19/100 | step: 223/422 | loss: 3.160038471221924\n",
      "Epoch: 19/100 | step: 224/422 | loss: 3.8119399547576904\n",
      "Epoch: 19/100 | step: 225/422 | loss: 3.757319688796997\n",
      "Epoch: 19/100 | step: 226/422 | loss: 3.4673097133636475\n",
      "Epoch: 19/100 | step: 227/422 | loss: 3.11141037940979\n",
      "Epoch: 19/100 | step: 228/422 | loss: 3.348789691925049\n",
      "Epoch: 19/100 | step: 229/422 | loss: 3.1874563694000244\n",
      "Epoch: 19/100 | step: 230/422 | loss: 3.6894690990448\n",
      "Epoch: 19/100 | step: 231/422 | loss: 3.5135552883148193\n",
      "Epoch: 19/100 | step: 232/422 | loss: 3.5590357780456543\n",
      "Epoch: 19/100 | step: 233/422 | loss: 3.3397157192230225\n",
      "Epoch: 19/100 | step: 234/422 | loss: 3.417538642883301\n",
      "Epoch: 19/100 | step: 235/422 | loss: 3.442530870437622\n",
      "Epoch: 19/100 | step: 236/422 | loss: 3.517270088195801\n",
      "Epoch: 19/100 | step: 237/422 | loss: 3.065235137939453\n",
      "Epoch: 19/100 | step: 238/422 | loss: 3.348417043685913\n",
      "Epoch: 19/100 | step: 239/422 | loss: 3.4852213859558105\n",
      "Epoch: 19/100 | step: 240/422 | loss: 3.3186869621276855\n",
      "Epoch: 19/100 | step: 241/422 | loss: 3.6070454120635986\n",
      "Epoch: 19/100 | step: 242/422 | loss: 3.08007550239563\n",
      "Epoch: 19/100 | step: 243/422 | loss: 3.467460870742798\n",
      "Epoch: 19/100 | step: 244/422 | loss: 3.4458136558532715\n",
      "Epoch: 19/100 | step: 245/422 | loss: 3.984055519104004\n",
      "Epoch: 19/100 | step: 246/422 | loss: 3.4498541355133057\n",
      "Epoch: 19/100 | step: 247/422 | loss: 3.372258424758911\n",
      "Epoch: 19/100 | step: 248/422 | loss: 3.341470718383789\n",
      "Epoch: 19/100 | step: 249/422 | loss: 3.647695541381836\n",
      "Epoch: 19/100 | step: 250/422 | loss: 3.466531991958618\n",
      "Epoch: 19/100 | step: 251/422 | loss: 3.168142557144165\n",
      "Epoch: 19/100 | step: 252/422 | loss: 3.769334316253662\n",
      "Epoch: 19/100 | step: 253/422 | loss: 3.382436513900757\n",
      "Epoch: 19/100 | step: 254/422 | loss: 3.299802780151367\n",
      "Epoch: 19/100 | step: 255/422 | loss: 3.243638038635254\n",
      "Epoch: 19/100 | step: 256/422 | loss: 3.2228410243988037\n",
      "Epoch: 19/100 | step: 257/422 | loss: 3.5609447956085205\n",
      "Epoch: 19/100 | step: 258/422 | loss: 3.556317090988159\n",
      "Epoch: 19/100 | step: 259/422 | loss: 3.0435447692871094\n",
      "Epoch: 19/100 | step: 260/422 | loss: 3.9365954399108887\n",
      "Epoch: 19/100 | step: 261/422 | loss: 3.6251943111419678\n",
      "Epoch: 19/100 | step: 262/422 | loss: 3.5282461643218994\n",
      "Epoch: 19/100 | step: 263/422 | loss: 3.4503965377807617\n",
      "Epoch: 19/100 | step: 264/422 | loss: 3.543221950531006\n",
      "Epoch: 19/100 | step: 265/422 | loss: 3.3083393573760986\n",
      "Epoch: 19/100 | step: 266/422 | loss: 3.320554494857788\n",
      "Epoch: 19/100 | step: 267/422 | loss: 3.3089447021484375\n",
      "Epoch: 19/100 | step: 268/422 | loss: 3.389408588409424\n",
      "Epoch: 19/100 | step: 269/422 | loss: 3.5487074851989746\n",
      "Epoch: 19/100 | step: 270/422 | loss: 3.300715446472168\n",
      "Epoch: 19/100 | step: 271/422 | loss: 3.325039863586426\n",
      "Epoch: 19/100 | step: 272/422 | loss: 3.288717031478882\n",
      "Epoch: 19/100 | step: 273/422 | loss: 3.0766403675079346\n",
      "Epoch: 19/100 | step: 274/422 | loss: 3.275141954421997\n",
      "Epoch: 19/100 | step: 275/422 | loss: 2.989328622817993\n",
      "Epoch: 19/100 | step: 276/422 | loss: 3.8192126750946045\n",
      "Epoch: 19/100 | step: 277/422 | loss: 3.5296363830566406\n",
      "Epoch: 19/100 | step: 278/422 | loss: 3.2850232124328613\n",
      "Epoch: 19/100 | step: 279/422 | loss: 3.5367705821990967\n",
      "Epoch: 19/100 | step: 280/422 | loss: 3.1008200645446777\n",
      "Epoch: 19/100 | step: 281/422 | loss: 3.311985731124878\n",
      "Epoch: 19/100 | step: 282/422 | loss: 3.24202561378479\n",
      "Epoch: 19/100 | step: 283/422 | loss: 3.7691445350646973\n",
      "Epoch: 19/100 | step: 284/422 | loss: 3.5834527015686035\n",
      "Epoch: 19/100 | step: 285/422 | loss: 3.614323616027832\n",
      "Epoch: 19/100 | step: 286/422 | loss: 3.2130796909332275\n",
      "Epoch: 19/100 | step: 287/422 | loss: 3.387455701828003\n",
      "Epoch: 19/100 | step: 288/422 | loss: 3.4085352420806885\n",
      "Epoch: 19/100 | step: 289/422 | loss: 3.0950517654418945\n",
      "Epoch: 19/100 | step: 290/422 | loss: 3.3602490425109863\n",
      "Epoch: 19/100 | step: 291/422 | loss: 3.2675533294677734\n",
      "Epoch: 19/100 | step: 292/422 | loss: 3.200258731842041\n",
      "Epoch: 19/100 | step: 293/422 | loss: 3.604679822921753\n",
      "Epoch: 19/100 | step: 294/422 | loss: 3.2268033027648926\n",
      "Epoch: 19/100 | step: 295/422 | loss: 3.631758213043213\n",
      "Epoch: 19/100 | step: 296/422 | loss: 3.185861349105835\n",
      "Epoch: 19/100 | step: 297/422 | loss: 3.309443473815918\n",
      "Epoch: 19/100 | step: 298/422 | loss: 3.1964468955993652\n",
      "Epoch: 19/100 | step: 299/422 | loss: 3.4321045875549316\n",
      "Epoch: 19/100 | step: 300/422 | loss: 3.38987398147583\n",
      "Epoch: 19/100 | step: 301/422 | loss: 3.748460292816162\n",
      "Epoch: 19/100 | step: 302/422 | loss: 3.809134006500244\n",
      "Epoch: 19/100 | step: 303/422 | loss: 3.53332257270813\n",
      "Epoch: 19/100 | step: 304/422 | loss: 2.9574313163757324\n",
      "Epoch: 19/100 | step: 305/422 | loss: 3.4788644313812256\n",
      "Epoch: 19/100 | step: 306/422 | loss: 3.338364362716675\n",
      "Epoch: 19/100 | step: 307/422 | loss: 3.472813606262207\n",
      "Epoch: 19/100 | step: 308/422 | loss: 3.352039337158203\n",
      "Epoch: 19/100 | step: 309/422 | loss: 3.7826576232910156\n",
      "Epoch: 19/100 | step: 310/422 | loss: 3.4185118675231934\n",
      "Epoch: 19/100 | step: 311/422 | loss: 3.225498676300049\n",
      "Epoch: 19/100 | step: 312/422 | loss: 3.4499804973602295\n",
      "Epoch: 19/100 | step: 313/422 | loss: 3.181356906890869\n",
      "Epoch: 19/100 | step: 314/422 | loss: 3.3087353706359863\n",
      "Epoch: 19/100 | step: 315/422 | loss: 3.7698960304260254\n",
      "Epoch: 19/100 | step: 316/422 | loss: 3.5268452167510986\n",
      "Epoch: 19/100 | step: 317/422 | loss: 3.3286497592926025\n",
      "Epoch: 19/100 | step: 318/422 | loss: 3.3050503730773926\n",
      "Epoch: 19/100 | step: 319/422 | loss: 3.22017240524292\n",
      "Epoch: 19/100 | step: 320/422 | loss: 3.3766586780548096\n",
      "Epoch: 19/100 | step: 321/422 | loss: 3.349360227584839\n",
      "Epoch: 19/100 | step: 322/422 | loss: 3.2382290363311768\n",
      "Epoch: 19/100 | step: 323/422 | loss: 3.627060651779175\n",
      "Epoch: 19/100 | step: 324/422 | loss: 3.8067939281463623\n",
      "Epoch: 19/100 | step: 325/422 | loss: 3.6996028423309326\n",
      "Epoch: 19/100 | step: 326/422 | loss: 3.615147113800049\n",
      "Epoch: 19/100 | step: 327/422 | loss: 3.132812976837158\n",
      "Epoch: 19/100 | step: 328/422 | loss: 3.7665255069732666\n",
      "Epoch: 19/100 | step: 329/422 | loss: 2.956707000732422\n",
      "Epoch: 19/100 | step: 330/422 | loss: 3.4893040657043457\n",
      "Epoch: 19/100 | step: 331/422 | loss: 3.5816221237182617\n",
      "Epoch: 19/100 | step: 332/422 | loss: 2.9963393211364746\n",
      "Epoch: 19/100 | step: 333/422 | loss: 3.1733686923980713\n",
      "Epoch: 19/100 | step: 334/422 | loss: 3.4449706077575684\n",
      "Epoch: 19/100 | step: 335/422 | loss: 3.5375025272369385\n",
      "Epoch: 19/100 | step: 336/422 | loss: 3.701439619064331\n",
      "Epoch: 19/100 | step: 337/422 | loss: 3.0052974224090576\n",
      "Epoch: 19/100 | step: 338/422 | loss: 3.9667789936065674\n",
      "Epoch: 19/100 | step: 339/422 | loss: 3.2129406929016113\n",
      "Epoch: 19/100 | step: 340/422 | loss: 3.9205284118652344\n",
      "Epoch: 19/100 | step: 341/422 | loss: 3.0506207942962646\n",
      "Epoch: 19/100 | step: 342/422 | loss: 3.9208292961120605\n",
      "Epoch: 19/100 | step: 343/422 | loss: 3.599482297897339\n",
      "Epoch: 19/100 | step: 344/422 | loss: 3.494961738586426\n",
      "Epoch: 19/100 | step: 345/422 | loss: 3.4803760051727295\n",
      "Epoch: 19/100 | step: 346/422 | loss: 3.070681571960449\n",
      "Epoch: 19/100 | step: 347/422 | loss: 2.9232516288757324\n",
      "Epoch: 19/100 | step: 348/422 | loss: 3.644585609436035\n",
      "Epoch: 19/100 | step: 349/422 | loss: 3.302284002304077\n",
      "Epoch: 19/100 | step: 350/422 | loss: 3.132199287414551\n",
      "Epoch: 19/100 | step: 351/422 | loss: 3.289027214050293\n",
      "Epoch: 19/100 | step: 352/422 | loss: 3.807478189468384\n",
      "Epoch: 19/100 | step: 353/422 | loss: 3.436800956726074\n",
      "Epoch: 19/100 | step: 354/422 | loss: 3.3194639682769775\n",
      "Epoch: 19/100 | step: 355/422 | loss: 3.1072170734405518\n",
      "Epoch: 19/100 | step: 356/422 | loss: 2.9994049072265625\n",
      "Epoch: 19/100 | step: 357/422 | loss: 3.2812485694885254\n",
      "Epoch: 19/100 | step: 358/422 | loss: 2.9446167945861816\n",
      "Epoch: 19/100 | step: 359/422 | loss: 2.984248399734497\n",
      "Epoch: 19/100 | step: 360/422 | loss: 3.598050355911255\n",
      "Epoch: 19/100 | step: 361/422 | loss: 3.453277587890625\n",
      "Epoch: 19/100 | step: 362/422 | loss: 3.37809681892395\n",
      "Epoch: 19/100 | step: 363/422 | loss: 3.758589267730713\n",
      "Epoch: 19/100 | step: 364/422 | loss: 3.592312812805176\n",
      "Epoch: 19/100 | step: 365/422 | loss: 3.574841260910034\n",
      "Epoch: 19/100 | step: 366/422 | loss: 3.2602672576904297\n",
      "Epoch: 19/100 | step: 367/422 | loss: 3.479034900665283\n",
      "Epoch: 19/100 | step: 368/422 | loss: 3.2208385467529297\n",
      "Epoch: 19/100 | step: 369/422 | loss: 3.135293483734131\n",
      "Epoch: 19/100 | step: 370/422 | loss: 3.853667736053467\n",
      "Epoch: 19/100 | step: 371/422 | loss: 3.2317636013031006\n",
      "Epoch: 19/100 | step: 372/422 | loss: 3.4424567222595215\n",
      "Epoch: 19/100 | step: 373/422 | loss: 3.6367483139038086\n",
      "Epoch: 19/100 | step: 374/422 | loss: 3.770852565765381\n",
      "Epoch: 19/100 | step: 375/422 | loss: 3.502166271209717\n",
      "Epoch: 19/100 | step: 376/422 | loss: 3.197791337966919\n",
      "Epoch: 19/100 | step: 377/422 | loss: 3.691739082336426\n",
      "Epoch: 19/100 | step: 378/422 | loss: 3.1785035133361816\n",
      "Epoch: 19/100 | step: 379/422 | loss: 3.89386248588562\n",
      "Epoch: 19/100 | step: 380/422 | loss: 3.479595899581909\n",
      "Epoch: 19/100 | step: 381/422 | loss: 3.1108155250549316\n",
      "Epoch: 19/100 | step: 382/422 | loss: 3.3837342262268066\n",
      "Epoch: 19/100 | step: 383/422 | loss: 3.207064151763916\n",
      "Epoch: 19/100 | step: 384/422 | loss: 3.3345534801483154\n",
      "Epoch: 19/100 | step: 385/422 | loss: 3.973958969116211\n",
      "Epoch: 19/100 | step: 386/422 | loss: 3.485382318496704\n",
      "Epoch: 19/100 | step: 387/422 | loss: 3.5034561157226562\n",
      "Epoch: 19/100 | step: 388/422 | loss: 3.483456611633301\n",
      "Epoch: 19/100 | step: 389/422 | loss: 3.3322997093200684\n",
      "Epoch: 19/100 | step: 390/422 | loss: 3.406126022338867\n",
      "Epoch: 19/100 | step: 391/422 | loss: 3.5727603435516357\n",
      "Epoch: 19/100 | step: 392/422 | loss: 3.6091415882110596\n",
      "Epoch: 19/100 | step: 393/422 | loss: 3.5218405723571777\n",
      "Epoch: 19/100 | step: 394/422 | loss: 3.4067273139953613\n",
      "Epoch: 19/100 | step: 395/422 | loss: 3.4215071201324463\n",
      "Epoch: 19/100 | step: 396/422 | loss: 3.548021078109741\n",
      "Epoch: 19/100 | step: 397/422 | loss: 3.591076135635376\n",
      "Epoch: 19/100 | step: 398/422 | loss: 3.684065580368042\n",
      "Epoch: 19/100 | step: 399/422 | loss: 3.913583517074585\n",
      "Epoch: 19/100 | step: 400/422 | loss: 3.4572513103485107\n",
      "Epoch: 19/100 | step: 401/422 | loss: 3.5508036613464355\n",
      "Epoch: 19/100 | step: 402/422 | loss: 3.264061689376831\n",
      "Epoch: 19/100 | step: 403/422 | loss: 3.3552136421203613\n",
      "Epoch: 19/100 | step: 404/422 | loss: 3.500004529953003\n",
      "Epoch: 19/100 | step: 405/422 | loss: 3.3621609210968018\n",
      "Epoch: 19/100 | step: 406/422 | loss: 3.2931973934173584\n",
      "Epoch: 19/100 | step: 407/422 | loss: 3.5332565307617188\n",
      "Epoch: 19/100 | step: 408/422 | loss: 3.665879249572754\n",
      "Epoch: 19/100 | step: 409/422 | loss: 3.5573511123657227\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 20/100 | step: 1/422 | loss: 3.0526931285858154\n",
      "Epoch: 20/100 | step: 2/422 | loss: 3.328657627105713\n",
      "Epoch: 20/100 | step: 3/422 | loss: 3.4487977027893066\n",
      "Epoch: 20/100 | step: 4/422 | loss: 3.393367290496826\n",
      "Epoch: 20/100 | step: 5/422 | loss: 3.083407163619995\n",
      "Epoch: 20/100 | step: 6/422 | loss: 3.4244768619537354\n",
      "Epoch: 20/100 | step: 7/422 | loss: 3.2977612018585205\n",
      "Epoch: 20/100 | step: 8/422 | loss: 3.113711357116699\n",
      "Epoch: 20/100 | step: 9/422 | loss: 3.386449098587036\n",
      "Epoch: 20/100 | step: 10/422 | loss: 3.2942981719970703\n",
      "Epoch: 20/100 | step: 11/422 | loss: 3.567767381668091\n",
      "Epoch: 20/100 | step: 12/422 | loss: 3.083618640899658\n",
      "Epoch: 20/100 | step: 13/422 | loss: 3.2250630855560303\n",
      "Epoch: 20/100 | step: 14/422 | loss: 3.4083261489868164\n",
      "Epoch: 20/100 | step: 15/422 | loss: 3.4654040336608887\n",
      "Epoch: 20/100 | step: 16/422 | loss: 3.0431771278381348\n",
      "Epoch: 20/100 | step: 17/422 | loss: 3.533240795135498\n",
      "Epoch: 20/100 | step: 18/422 | loss: 3.652407646179199\n",
      "Epoch: 20/100 | step: 19/422 | loss: 3.033836841583252\n",
      "Epoch: 20/100 | step: 20/422 | loss: 3.135356903076172\n",
      "Epoch: 20/100 | step: 21/422 | loss: 3.2064387798309326\n",
      "Epoch: 20/100 | step: 22/422 | loss: 2.915436029434204\n",
      "Epoch: 20/100 | step: 23/422 | loss: 3.8087992668151855\n",
      "Epoch: 20/100 | step: 24/422 | loss: 3.1554501056671143\n",
      "Epoch: 20/100 | step: 25/422 | loss: 3.322235107421875\n",
      "Epoch: 20/100 | step: 26/422 | loss: 3.6798388957977295\n",
      "Epoch: 20/100 | step: 27/422 | loss: 3.6969470977783203\n",
      "Epoch: 20/100 | step: 28/422 | loss: 3.422173500061035\n",
      "Epoch: 20/100 | step: 29/422 | loss: 2.944129467010498\n",
      "Epoch: 20/100 | step: 30/422 | loss: 3.4159045219421387\n",
      "Epoch: 20/100 | step: 31/422 | loss: 3.007704257965088\n",
      "Epoch: 20/100 | step: 32/422 | loss: 3.13075852394104\n",
      "Epoch: 20/100 | step: 33/422 | loss: 3.3921449184417725\n",
      "Epoch: 20/100 | step: 34/422 | loss: 3.038912534713745\n",
      "Epoch: 20/100 | step: 35/422 | loss: 2.934283494949341\n",
      "Epoch: 20/100 | step: 36/422 | loss: 3.5098330974578857\n",
      "Epoch: 20/100 | step: 37/422 | loss: 3.4698047637939453\n",
      "Epoch: 20/100 | step: 38/422 | loss: 3.3699135780334473\n",
      "Epoch: 20/100 | step: 39/422 | loss: 3.3428726196289062\n",
      "Epoch: 20/100 | step: 40/422 | loss: 3.0760254859924316\n",
      "Epoch: 20/100 | step: 41/422 | loss: 3.598020315170288\n",
      "Epoch: 20/100 | step: 42/422 | loss: 3.038445472717285\n",
      "Epoch: 20/100 | step: 43/422 | loss: 3.6115331649780273\n",
      "Epoch: 20/100 | step: 44/422 | loss: 3.111588716506958\n",
      "Epoch: 20/100 | step: 45/422 | loss: 3.1711981296539307\n",
      "Epoch: 20/100 | step: 46/422 | loss: 3.676248073577881\n",
      "Epoch: 20/100 | step: 47/422 | loss: 3.1556620597839355\n",
      "Epoch: 20/100 | step: 48/422 | loss: 3.2088606357574463\n",
      "Epoch: 20/100 | step: 49/422 | loss: 3.30197811126709\n",
      "Epoch: 20/100 | step: 50/422 | loss: 3.2822091579437256\n",
      "Epoch: 20/100 | step: 51/422 | loss: 3.0759811401367188\n",
      "Epoch: 20/100 | step: 52/422 | loss: 3.286839246749878\n",
      "Epoch: 20/100 | step: 53/422 | loss: 3.2220230102539062\n",
      "Epoch: 20/100 | step: 54/422 | loss: 3.3423550128936768\n",
      "Epoch: 20/100 | step: 55/422 | loss: 3.35267972946167\n",
      "Epoch: 20/100 | step: 56/422 | loss: 2.8969759941101074\n",
      "Epoch: 20/100 | step: 57/422 | loss: 3.2994091510772705\n",
      "Epoch: 20/100 | step: 58/422 | loss: 2.986318588256836\n",
      "Epoch: 20/100 | step: 59/422 | loss: 2.7651729583740234\n",
      "Epoch: 20/100 | step: 60/422 | loss: 3.273751735687256\n",
      "Epoch: 20/100 | step: 61/422 | loss: 3.3852388858795166\n",
      "Epoch: 20/100 | step: 62/422 | loss: 3.4366567134857178\n",
      "Epoch: 20/100 | step: 63/422 | loss: 3.293921947479248\n",
      "Epoch: 20/100 | step: 64/422 | loss: 3.5468790531158447\n",
      "Epoch: 20/100 | step: 65/422 | loss: 3.5185623168945312\n",
      "Epoch: 20/100 | step: 66/422 | loss: 3.1952035427093506\n",
      "Epoch: 20/100 | step: 67/422 | loss: 3.1745071411132812\n",
      "Epoch: 20/100 | step: 68/422 | loss: 3.1510727405548096\n",
      "Epoch: 20/100 | step: 69/422 | loss: 3.307898998260498\n",
      "Epoch: 20/100 | step: 70/422 | loss: 3.3391220569610596\n",
      "Epoch: 20/100 | step: 71/422 | loss: 3.53501033782959\n",
      "Epoch: 20/100 | step: 72/422 | loss: 3.3844950199127197\n",
      "Epoch: 20/100 | step: 73/422 | loss: 2.8662495613098145\n",
      "Epoch: 20/100 | step: 74/422 | loss: 2.9605419635772705\n",
      "Epoch: 20/100 | step: 75/422 | loss: 3.8086600303649902\n",
      "Epoch: 20/100 | step: 76/422 | loss: 3.706963062286377\n",
      "Epoch: 20/100 | step: 77/422 | loss: 3.5493698120117188\n",
      "Epoch: 20/100 | step: 78/422 | loss: 3.469062328338623\n",
      "Epoch: 20/100 | step: 79/422 | loss: 3.2728590965270996\n",
      "Epoch: 20/100 | step: 80/422 | loss: 3.9399569034576416\n",
      "Epoch: 20/100 | step: 81/422 | loss: 4.016897678375244\n",
      "Epoch: 20/100 | step: 82/422 | loss: 3.3654770851135254\n",
      "Epoch: 20/100 | step: 83/422 | loss: 3.299192428588867\n",
      "Epoch: 20/100 | step: 84/422 | loss: 3.07951021194458\n",
      "Epoch: 20/100 | step: 85/422 | loss: 3.156609535217285\n",
      "Epoch: 20/100 | step: 86/422 | loss: 3.1265878677368164\n",
      "Epoch: 20/100 | step: 87/422 | loss: 3.3407886028289795\n",
      "Epoch: 20/100 | step: 88/422 | loss: 3.00456166267395\n",
      "Epoch: 20/100 | step: 89/422 | loss: 3.00724196434021\n",
      "Epoch: 20/100 | step: 90/422 | loss: 3.4765472412109375\n",
      "Epoch: 20/100 | step: 91/422 | loss: 3.102048873901367\n",
      "Epoch: 20/100 | step: 92/422 | loss: 3.8930041790008545\n",
      "Epoch: 20/100 | step: 93/422 | loss: 3.2529940605163574\n",
      "Epoch: 20/100 | step: 94/422 | loss: 3.1917872428894043\n",
      "Epoch: 20/100 | step: 95/422 | loss: 3.276237964630127\n",
      "Epoch: 20/100 | step: 96/422 | loss: 3.251174211502075\n",
      "Epoch: 20/100 | step: 97/422 | loss: 3.443434238433838\n",
      "Epoch: 20/100 | step: 98/422 | loss: 3.4257185459136963\n",
      "Epoch: 20/100 | step: 99/422 | loss: 3.0294718742370605\n",
      "Epoch: 20/100 | step: 100/422 | loss: 3.9057507514953613\n",
      "Epoch: 20/100 | step: 101/422 | loss: 3.6083381175994873\n",
      "Epoch: 20/100 | step: 102/422 | loss: 3.3683996200561523\n",
      "Epoch: 20/100 | step: 103/422 | loss: 2.9549477100372314\n",
      "Epoch: 20/100 | step: 104/422 | loss: 3.492612838745117\n",
      "Epoch: 20/100 | step: 105/422 | loss: 3.318124771118164\n",
      "Epoch: 20/100 | step: 106/422 | loss: 3.6613094806671143\n",
      "Epoch: 20/100 | step: 107/422 | loss: 3.2565550804138184\n",
      "Epoch: 20/100 | step: 108/422 | loss: 2.953860282897949\n",
      "Epoch: 20/100 | step: 109/422 | loss: 3.178274154663086\n",
      "Epoch: 20/100 | step: 110/422 | loss: 3.1158289909362793\n",
      "Epoch: 20/100 | step: 111/422 | loss: 3.4834837913513184\n",
      "Epoch: 20/100 | step: 112/422 | loss: 3.0742719173431396\n",
      "Epoch: 20/100 | step: 113/422 | loss: 3.23952054977417\n",
      "Epoch: 20/100 | step: 114/422 | loss: 3.5788614749908447\n",
      "Epoch: 20/100 | step: 115/422 | loss: 3.457946538925171\n",
      "Epoch: 20/100 | step: 116/422 | loss: 3.585184335708618\n",
      "Epoch: 20/100 | step: 117/422 | loss: 3.613647699356079\n",
      "Epoch: 20/100 | step: 118/422 | loss: 3.5774941444396973\n",
      "Epoch: 20/100 | step: 119/422 | loss: 2.9143102169036865\n",
      "Epoch: 20/100 | step: 120/422 | loss: 3.4764323234558105\n",
      "Epoch: 20/100 | step: 121/422 | loss: 2.9302072525024414\n",
      "Epoch: 20/100 | step: 122/422 | loss: 3.3801066875457764\n",
      "Epoch: 20/100 | step: 123/422 | loss: 3.410353660583496\n",
      "Epoch: 20/100 | step: 124/422 | loss: 3.192166328430176\n",
      "Epoch: 20/100 | step: 125/422 | loss: 3.1357414722442627\n",
      "Epoch: 20/100 | step: 126/422 | loss: 3.113039255142212\n",
      "Epoch: 20/100 | step: 127/422 | loss: 3.3802902698516846\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 21/100 | step: 1/422 | loss: 3.0878398418426514\n",
      "Epoch: 21/100 | step: 2/422 | loss: 3.1394574642181396\n",
      "Epoch: 21/100 | step: 3/422 | loss: 3.6517863273620605\n",
      "Epoch: 21/100 | step: 4/422 | loss: 3.1391539573669434\n",
      "Epoch: 21/100 | step: 5/422 | loss: 3.3830437660217285\n",
      "Epoch: 21/100 | step: 6/422 | loss: 3.1225523948669434\n",
      "Epoch: 21/100 | step: 7/422 | loss: 3.2716689109802246\n",
      "Epoch: 21/100 | step: 8/422 | loss: 3.5929317474365234\n",
      "Epoch: 21/100 | step: 9/422 | loss: 3.317227602005005\n",
      "Epoch: 21/100 | step: 10/422 | loss: 3.41825532913208\n",
      "Epoch: 21/100 | step: 11/422 | loss: 3.077991247177124\n",
      "Epoch: 21/100 | step: 12/422 | loss: 3.077684164047241\n",
      "Epoch: 21/100 | step: 13/422 | loss: 2.94726824760437\n",
      "Epoch: 21/100 | step: 14/422 | loss: 2.902813196182251\n",
      "Epoch: 21/100 | step: 15/422 | loss: 3.3977842330932617\n",
      "Epoch: 21/100 | step: 16/422 | loss: 3.377490997314453\n",
      "Epoch: 21/100 | step: 17/422 | loss: 3.2134060859680176\n",
      "Epoch: 21/100 | step: 18/422 | loss: 3.141040086746216\n",
      "Epoch: 21/100 | step: 19/422 | loss: 3.245353937149048\n",
      "Epoch: 21/100 | step: 20/422 | loss: 3.4549026489257812\n",
      "Epoch: 21/100 | step: 21/422 | loss: 3.108067512512207\n",
      "Epoch: 21/100 | step: 22/422 | loss: 3.4360013008117676\n",
      "Epoch: 21/100 | step: 23/422 | loss: 3.4108357429504395\n",
      "Epoch: 21/100 | step: 24/422 | loss: 2.8662326335906982\n",
      "Epoch: 21/100 | step: 25/422 | loss: 3.539764165878296\n",
      "Epoch: 21/100 | step: 26/422 | loss: 3.3339920043945312\n",
      "Epoch: 21/100 | step: 27/422 | loss: 3.538989305496216\n",
      "Epoch: 21/100 | step: 28/422 | loss: 3.1728878021240234\n",
      "Epoch: 21/100 | step: 29/422 | loss: 3.1606338024139404\n",
      "Epoch: 21/100 | step: 30/422 | loss: 3.218066692352295\n",
      "Epoch: 21/100 | step: 31/422 | loss: 3.4925060272216797\n",
      "Epoch: 21/100 | step: 32/422 | loss: 3.5339062213897705\n",
      "Epoch: 21/100 | step: 33/422 | loss: 3.3264074325561523\n",
      "Epoch: 21/100 | step: 34/422 | loss: 3.2317941188812256\n",
      "Epoch: 21/100 | step: 35/422 | loss: 3.0565390586853027\n",
      "Epoch: 21/100 | step: 36/422 | loss: 3.0872344970703125\n",
      "Epoch: 21/100 | step: 37/422 | loss: 3.282266616821289\n",
      "Epoch: 21/100 | step: 38/422 | loss: 3.2984917163848877\n",
      "Epoch: 21/100 | step: 39/422 | loss: 3.071960687637329\n",
      "Epoch: 21/100 | step: 40/422 | loss: 3.7603116035461426\n",
      "Epoch: 21/100 | step: 41/422 | loss: 3.4859745502471924\n",
      "Epoch: 21/100 | step: 42/422 | loss: 3.1708059310913086\n",
      "Epoch: 21/100 | step: 43/422 | loss: 3.248837471008301\n",
      "Epoch: 21/100 | step: 44/422 | loss: 3.1157355308532715\n",
      "Epoch: 21/100 | step: 45/422 | loss: 3.17368483543396\n",
      "Epoch: 21/100 | step: 46/422 | loss: 3.1147501468658447\n",
      "Epoch: 21/100 | step: 47/422 | loss: 3.25103497505188\n",
      "Epoch: 21/100 | step: 48/422 | loss: 3.007258653640747\n",
      "Epoch: 21/100 | step: 49/422 | loss: 3.352034330368042\n",
      "Epoch: 21/100 | step: 50/422 | loss: 3.344132423400879\n",
      "Epoch: 21/100 | step: 51/422 | loss: 3.6661856174468994\n",
      "Epoch: 21/100 | step: 52/422 | loss: 3.441629648208618\n",
      "Epoch: 21/100 | step: 53/422 | loss: 3.222909688949585\n",
      "Epoch: 21/100 | step: 54/422 | loss: 3.436803102493286\n",
      "Epoch: 21/100 | step: 55/422 | loss: 3.6086721420288086\n",
      "Epoch: 21/100 | step: 56/422 | loss: 3.2504241466522217\n",
      "Epoch: 21/100 | step: 57/422 | loss: 4.021390914916992\n",
      "Epoch: 21/100 | step: 58/422 | loss: 3.2919161319732666\n",
      "Epoch: 21/100 | step: 59/422 | loss: 3.0127620697021484\n",
      "Epoch: 21/100 | step: 60/422 | loss: 2.9621126651763916\n",
      "Epoch: 21/100 | step: 61/422 | loss: 3.5077297687530518\n",
      "Epoch: 21/100 | step: 62/422 | loss: 3.040705919265747\n",
      "Epoch: 21/100 | step: 63/422 | loss: 3.441964626312256\n",
      "Epoch: 21/100 | step: 64/422 | loss: 3.503150701522827\n",
      "Epoch: 21/100 | step: 65/422 | loss: 3.312413454055786\n",
      "Epoch: 21/100 | step: 66/422 | loss: 3.473162889480591\n",
      "Epoch: 21/100 | step: 67/422 | loss: 3.103038787841797\n",
      "Epoch: 21/100 | step: 68/422 | loss: 3.5588154792785645\n",
      "Epoch: 21/100 | step: 69/422 | loss: 3.133432149887085\n",
      "Epoch: 21/100 | step: 70/422 | loss: 3.198021650314331\n",
      "Epoch: 21/100 | step: 71/422 | loss: 3.5929110050201416\n",
      "Epoch: 21/100 | step: 72/422 | loss: 3.234433889389038\n",
      "Epoch: 21/100 | step: 73/422 | loss: 3.176570415496826\n",
      "Epoch: 21/100 | step: 74/422 | loss: 3.1660101413726807\n",
      "Epoch: 21/100 | step: 75/422 | loss: 3.0763769149780273\n",
      "Epoch: 21/100 | step: 76/422 | loss: 3.154646635055542\n",
      "Epoch: 21/100 | step: 77/422 | loss: 3.153262138366699\n",
      "Epoch: 21/100 | step: 78/422 | loss: 3.133159875869751\n",
      "Epoch: 21/100 | step: 79/422 | loss: 2.955091714859009\n",
      "Epoch: 21/100 | step: 80/422 | loss: 3.1959588527679443\n",
      "Epoch: 21/100 | step: 81/422 | loss: 3.422886610031128\n",
      "Epoch: 21/100 | step: 82/422 | loss: 3.0069401264190674\n",
      "Epoch: 21/100 | step: 83/422 | loss: 3.1785054206848145\n",
      "Epoch: 21/100 | step: 84/422 | loss: 3.465026378631592\n",
      "Epoch: 21/100 | step: 85/422 | loss: 3.183126449584961\n",
      "Epoch: 21/100 | step: 86/422 | loss: 3.282695770263672\n",
      "Epoch: 21/100 | step: 87/422 | loss: 3.613037347793579\n",
      "Epoch: 21/100 | step: 88/422 | loss: 3.175241470336914\n",
      "Epoch: 21/100 | step: 89/422 | loss: 3.0886828899383545\n",
      "Epoch: 21/100 | step: 90/422 | loss: 3.431910514831543\n",
      "Epoch: 21/100 | step: 91/422 | loss: 3.5005009174346924\n",
      "Epoch: 21/100 | step: 92/422 | loss: 3.3080785274505615\n",
      "Epoch: 21/100 | step: 93/422 | loss: 3.4630379676818848\n",
      "Epoch: 21/100 | step: 94/422 | loss: 3.052218198776245\n",
      "Epoch: 21/100 | step: 95/422 | loss: 3.434535026550293\n",
      "Epoch: 21/100 | step: 96/422 | loss: 3.1554770469665527\n",
      "Epoch: 21/100 | step: 97/422 | loss: 3.3778202533721924\n",
      "Epoch: 21/100 | step: 98/422 | loss: 3.038884162902832\n",
      "Epoch: 21/100 | step: 99/422 | loss: 2.782548666000366\n",
      "Epoch: 21/100 | step: 100/422 | loss: 3.2576444149017334\n",
      "Epoch: 21/100 | step: 101/422 | loss: 3.323309898376465\n",
      "Epoch: 21/100 | step: 102/422 | loss: 3.5213897228240967\n",
      "Epoch: 21/100 | step: 103/422 | loss: 3.0114941596984863\n",
      "Epoch: 21/100 | step: 104/422 | loss: 3.410336494445801\n",
      "Epoch: 21/100 | step: 105/422 | loss: 3.005253314971924\n",
      "Epoch: 21/100 | step: 106/422 | loss: 3.0729756355285645\n",
      "Epoch: 21/100 | step: 107/422 | loss: 3.3901054859161377\n",
      "Epoch: 21/100 | step: 108/422 | loss: 3.380211591720581\n",
      "Epoch: 21/100 | step: 109/422 | loss: 3.018739700317383\n",
      "Epoch: 21/100 | step: 110/422 | loss: 2.8937177658081055\n",
      "Epoch: 21/100 | step: 111/422 | loss: 3.3300106525421143\n",
      "Epoch: 21/100 | step: 112/422 | loss: 3.031224489212036\n",
      "Epoch: 21/100 | step: 113/422 | loss: 3.428591012954712\n",
      "Epoch: 21/100 | step: 114/422 | loss: 3.651700019836426\n",
      "Epoch: 21/100 | step: 115/422 | loss: 3.157212257385254\n",
      "Epoch: 21/100 | step: 116/422 | loss: 3.7052698135375977\n",
      "Epoch: 21/100 | step: 117/422 | loss: 2.8756284713745117\n",
      "Epoch: 21/100 | step: 118/422 | loss: 3.1769416332244873\n",
      "Epoch: 21/100 | step: 119/422 | loss: 2.9573187828063965\n",
      "Epoch: 21/100 | step: 120/422 | loss: 3.5133423805236816\n",
      "Epoch: 21/100 | step: 121/422 | loss: 3.3151352405548096\n",
      "Epoch: 21/100 | step: 122/422 | loss: 3.553744077682495\n",
      "Epoch: 21/100 | step: 123/422 | loss: 3.0866684913635254\n",
      "Epoch: 21/100 | step: 124/422 | loss: 2.9194936752319336\n",
      "Epoch: 21/100 | step: 125/422 | loss: 3.4011054039001465\n",
      "Epoch: 21/100 | step: 126/422 | loss: 3.0113399028778076\n",
      "Epoch: 21/100 | step: 127/422 | loss: 3.6652071475982666\n",
      "Epoch: 21/100 | step: 128/422 | loss: 3.563122272491455\n",
      "Epoch: 21/100 | step: 129/422 | loss: 3.4509356021881104\n",
      "Epoch: 21/100 | step: 130/422 | loss: 3.7273736000061035\n",
      "Epoch: 21/100 | step: 131/422 | loss: 3.658879041671753\n",
      "Epoch: 21/100 | step: 132/422 | loss: 3.025560140609741\n",
      "Epoch: 21/100 | step: 133/422 | loss: 2.9254589080810547\n",
      "Epoch: 21/100 | step: 134/422 | loss: 3.470324754714966\n",
      "Epoch: 21/100 | step: 135/422 | loss: 3.234575033187866\n",
      "Epoch: 21/100 | step: 136/422 | loss: 3.5660104751586914\n",
      "Epoch: 21/100 | step: 137/422 | loss: 3.174687147140503\n",
      "Epoch: 21/100 | step: 138/422 | loss: 3.4942967891693115\n",
      "Epoch: 21/100 | step: 139/422 | loss: 3.5821290016174316\n",
      "Epoch: 21/100 | step: 140/422 | loss: 3.6235032081604004\n",
      "Epoch: 21/100 | step: 141/422 | loss: 3.3202059268951416\n",
      "Epoch: 21/100 | step: 142/422 | loss: 3.3864197731018066\n",
      "Epoch: 21/100 | step: 143/422 | loss: 3.1515371799468994\n",
      "Epoch: 21/100 | step: 144/422 | loss: 3.2053515911102295\n",
      "Epoch: 21/100 | step: 145/422 | loss: 3.0084781646728516\n",
      "Epoch: 21/100 | step: 146/422 | loss: 3.2604458332061768\n",
      "Epoch: 21/100 | step: 147/422 | loss: 2.5822482109069824\n",
      "Epoch: 21/100 | step: 148/422 | loss: 3.247921943664551\n",
      "Epoch: 21/100 | step: 149/422 | loss: 3.3286349773406982\n",
      "Epoch: 21/100 | step: 150/422 | loss: 3.5611634254455566\n",
      "Epoch: 21/100 | step: 151/422 | loss: 3.5538175106048584\n",
      "Epoch: 21/100 | step: 152/422 | loss: 3.2349488735198975\n",
      "Epoch: 21/100 | step: 153/422 | loss: 3.037128210067749\n",
      "Epoch: 21/100 | step: 154/422 | loss: 3.466507911682129\n",
      "Epoch: 21/100 | step: 155/422 | loss: 2.9537384510040283\n",
      "Epoch: 21/100 | step: 156/422 | loss: 3.136866807937622\n",
      "Epoch: 21/100 | step: 157/422 | loss: 3.104783058166504\n",
      "Epoch: 21/100 | step: 158/422 | loss: 3.1057040691375732\n",
      "Epoch: 21/100 | step: 159/422 | loss: 3.392960786819458\n",
      "Epoch: 21/100 | step: 160/422 | loss: 3.075347423553467\n",
      "Epoch: 21/100 | step: 161/422 | loss: 3.1576809883117676\n",
      "Epoch: 21/100 | step: 162/422 | loss: 3.3114430904388428\n",
      "Epoch: 21/100 | step: 163/422 | loss: 2.9147679805755615\n",
      "Epoch: 21/100 | step: 164/422 | loss: 3.319558620452881\n",
      "Epoch: 21/100 | step: 165/422 | loss: 2.7295989990234375\n",
      "Epoch: 21/100 | step: 166/422 | loss: 3.320991039276123\n",
      "Epoch: 21/100 | step: 167/422 | loss: 3.0143957138061523\n",
      "Epoch: 21/100 | step: 168/422 | loss: 3.434154510498047\n",
      "Epoch: 21/100 | step: 169/422 | loss: 3.4880385398864746\n",
      "Epoch: 21/100 | step: 170/422 | loss: 3.2275779247283936\n",
      "Epoch: 21/100 | step: 171/422 | loss: 3.3331873416900635\n",
      "Epoch: 21/100 | step: 172/422 | loss: 3.13177752494812\n",
      "Epoch: 21/100 | step: 173/422 | loss: 3.5539538860321045\n",
      "Epoch: 21/100 | step: 174/422 | loss: 3.2839813232421875\n",
      "Epoch: 21/100 | step: 175/422 | loss: 3.685544967651367\n",
      "Epoch: 21/100 | step: 176/422 | loss: 2.86921763420105\n",
      "Epoch: 21/100 | step: 177/422 | loss: 3.458178997039795\n",
      "Epoch: 21/100 | step: 178/422 | loss: 3.6461880207061768\n",
      "Epoch: 21/100 | step: 179/422 | loss: 3.386537551879883\n",
      "Epoch: 21/100 | step: 180/422 | loss: 3.633122444152832\n",
      "Epoch: 21/100 | step: 181/422 | loss: 3.094712495803833\n",
      "Epoch: 21/100 | step: 182/422 | loss: 3.194896697998047\n",
      "Epoch: 21/100 | step: 183/422 | loss: 3.8216183185577393\n",
      "Epoch: 21/100 | step: 184/422 | loss: 3.361586570739746\n",
      "Epoch: 21/100 | step: 185/422 | loss: 3.205570697784424\n",
      "Epoch: 21/100 | step: 186/422 | loss: 3.5129504203796387\n",
      "Epoch: 21/100 | step: 187/422 | loss: 3.3550422191619873\n",
      "Epoch: 21/100 | step: 188/422 | loss: 2.905550003051758\n",
      "Epoch: 21/100 | step: 189/422 | loss: 3.174689531326294\n",
      "Epoch: 21/100 | step: 190/422 | loss: 3.270897388458252\n",
      "Epoch: 21/100 | step: 191/422 | loss: 3.2084572315216064\n",
      "Epoch: 21/100 | step: 192/422 | loss: 3.8807520866394043\n",
      "Epoch: 21/100 | step: 193/422 | loss: 3.28027081489563\n",
      "Epoch: 21/100 | step: 194/422 | loss: 3.8117642402648926\n",
      "Epoch: 21/100 | step: 195/422 | loss: 3.152343273162842\n",
      "Epoch: 21/100 | step: 196/422 | loss: 3.679905414581299\n",
      "Epoch: 21/100 | step: 197/422 | loss: 3.521869659423828\n",
      "Epoch: 21/100 | step: 198/422 | loss: 3.4513490200042725\n",
      "Epoch: 21/100 | step: 199/422 | loss: 3.555314779281616\n",
      "Epoch: 21/100 | step: 200/422 | loss: 3.5099050998687744\n",
      "Epoch: 21/100 | step: 201/422 | loss: 3.6295113563537598\n",
      "Epoch: 21/100 | step: 202/422 | loss: 3.1809465885162354\n",
      "Epoch: 21/100 | step: 203/422 | loss: 3.336594581604004\n",
      "Epoch: 21/100 | step: 204/422 | loss: 3.854292869567871\n",
      "Epoch: 21/100 | step: 205/422 | loss: 3.427736759185791\n",
      "Epoch: 21/100 | step: 206/422 | loss: 2.8408234119415283\n",
      "Epoch: 21/100 | step: 207/422 | loss: 3.1817753314971924\n",
      "Epoch: 21/100 | step: 208/422 | loss: 3.309262275695801\n",
      "Epoch: 21/100 | step: 209/422 | loss: 2.9551351070404053\n",
      "Epoch: 21/100 | step: 210/422 | loss: 3.6115756034851074\n",
      "Epoch: 21/100 | step: 211/422 | loss: 3.5492756366729736\n",
      "Epoch: 21/100 | step: 212/422 | loss: 3.479602336883545\n",
      "Epoch: 21/100 | step: 213/422 | loss: 3.2885992527008057\n",
      "Epoch: 21/100 | step: 214/422 | loss: 3.159183979034424\n",
      "Epoch: 21/100 | step: 215/422 | loss: 3.1052443981170654\n",
      "Epoch: 21/100 | step: 216/422 | loss: 3.019213914871216\n",
      "Epoch: 21/100 | step: 217/422 | loss: 3.2819061279296875\n",
      "Epoch: 21/100 | step: 218/422 | loss: 3.734113931655884\n",
      "Epoch: 21/100 | step: 219/422 | loss: 3.187525987625122\n",
      "Epoch: 21/100 | step: 220/422 | loss: 2.7460622787475586\n",
      "Epoch: 21/100 | step: 221/422 | loss: 3.2942047119140625\n",
      "Epoch: 21/100 | step: 222/422 | loss: 2.949589729309082\n",
      "Epoch: 21/100 | step: 223/422 | loss: 2.915498733520508\n",
      "Epoch: 21/100 | step: 224/422 | loss: 3.288217544555664\n",
      "Epoch: 21/100 | step: 225/422 | loss: 3.2639832496643066\n",
      "Epoch: 21/100 | step: 226/422 | loss: 3.020474910736084\n",
      "Epoch: 21/100 | step: 227/422 | loss: 3.1045615673065186\n",
      "Epoch: 21/100 | step: 228/422 | loss: 3.216428756713867\n",
      "Epoch: 21/100 | step: 229/422 | loss: 3.1278226375579834\n",
      "Epoch: 21/100 | step: 230/422 | loss: 3.4143905639648438\n",
      "Epoch: 21/100 | step: 231/422 | loss: 2.8360707759857178\n",
      "Epoch: 21/100 | step: 232/422 | loss: 3.083144187927246\n",
      "Epoch: 21/100 | step: 233/422 | loss: 2.918611526489258\n",
      "Epoch: 21/100 | step: 234/422 | loss: 3.1883490085601807\n",
      "Epoch: 21/100 | step: 235/422 | loss: 3.140507459640503\n",
      "Epoch: 21/100 | step: 236/422 | loss: 2.9043729305267334\n",
      "Epoch: 21/100 | step: 237/422 | loss: 3.7103164196014404\n",
      "Epoch: 21/100 | step: 238/422 | loss: 3.691739082336426\n",
      "Epoch: 21/100 | step: 239/422 | loss: 3.615769863128662\n",
      "Epoch: 21/100 | step: 240/422 | loss: 3.2633585929870605\n",
      "Epoch: 21/100 | step: 241/422 | loss: 3.5286271572113037\n",
      "Epoch: 21/100 | step: 242/422 | loss: 3.117091655731201\n",
      "Epoch: 21/100 | step: 243/422 | loss: 2.8258330821990967\n",
      "Epoch: 21/100 | step: 244/422 | loss: 3.438746452331543\n",
      "Epoch: 21/100 | step: 245/422 | loss: 3.4627089500427246\n",
      "Epoch: 21/100 | step: 246/422 | loss: 3.0065627098083496\n",
      "Epoch: 21/100 | step: 247/422 | loss: 3.5374388694763184\n",
      "Epoch: 21/100 | step: 248/422 | loss: 2.8149895668029785\n",
      "Epoch: 21/100 | step: 249/422 | loss: 3.2449429035186768\n",
      "Epoch: 21/100 | step: 250/422 | loss: 3.007465362548828\n",
      "Epoch: 21/100 | step: 251/422 | loss: 3.0137693881988525\n",
      "Epoch: 21/100 | step: 252/422 | loss: 2.9444198608398438\n",
      "Epoch: 21/100 | step: 253/422 | loss: 3.1139726638793945\n",
      "Epoch: 21/100 | step: 254/422 | loss: 2.8165700435638428\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 22/100 | step: 1/422 | loss: 3.642016649246216\n",
      "Epoch: 22/100 | step: 2/422 | loss: 2.982926607131958\n",
      "Epoch: 22/100 | step: 3/422 | loss: 3.4282608032226562\n",
      "Epoch: 22/100 | step: 4/422 | loss: 3.210491895675659\n",
      "Epoch: 22/100 | step: 5/422 | loss: 3.228729486465454\n",
      "Epoch: 22/100 | step: 6/422 | loss: 2.969999074935913\n",
      "Epoch: 22/100 | step: 7/422 | loss: 3.2825310230255127\n",
      "Epoch: 22/100 | step: 8/422 | loss: 3.015092372894287\n",
      "Epoch: 22/100 | step: 9/422 | loss: 3.2605087757110596\n",
      "Epoch: 22/100 | step: 10/422 | loss: 3.4159035682678223\n",
      "Epoch: 22/100 | step: 11/422 | loss: 3.545151710510254\n",
      "Epoch: 22/100 | step: 12/422 | loss: 2.9067752361297607\n",
      "Epoch: 22/100 | step: 13/422 | loss: 3.0127532482147217\n",
      "Epoch: 22/100 | step: 14/422 | loss: 2.9344286918640137\n",
      "Epoch: 22/100 | step: 15/422 | loss: 3.043416976928711\n",
      "Epoch: 22/100 | step: 16/422 | loss: 3.15960693359375\n",
      "Epoch: 22/100 | step: 17/422 | loss: 3.3653626441955566\n",
      "Epoch: 22/100 | step: 18/422 | loss: 3.1257336139678955\n",
      "Epoch: 22/100 | step: 19/422 | loss: 3.0264151096343994\n",
      "Epoch: 22/100 | step: 20/422 | loss: 3.1364150047302246\n",
      "Epoch: 22/100 | step: 21/422 | loss: 2.9673798084259033\n",
      "Epoch: 22/100 | step: 22/422 | loss: 3.34456467628479\n",
      "Epoch: 22/100 | step: 23/422 | loss: 3.058154821395874\n",
      "Epoch: 22/100 | step: 24/422 | loss: 3.2326860427856445\n",
      "Epoch: 22/100 | step: 25/422 | loss: 3.585987091064453\n",
      "Epoch: 22/100 | step: 26/422 | loss: 3.148425579071045\n",
      "Epoch: 22/100 | step: 27/422 | loss: 3.202152729034424\n",
      "Epoch: 22/100 | step: 28/422 | loss: 3.5710299015045166\n",
      "Epoch: 22/100 | step: 29/422 | loss: 2.6504321098327637\n",
      "Epoch: 22/100 | step: 30/422 | loss: 3.2367656230926514\n",
      "Epoch: 22/100 | step: 31/422 | loss: 3.653083324432373\n",
      "Epoch: 22/100 | step: 32/422 | loss: 3.0475032329559326\n",
      "Epoch: 22/100 | step: 33/422 | loss: 2.169137716293335\n",
      "Epoch: 22/100 | step: 34/422 | loss: 2.865788698196411\n",
      "Epoch: 22/100 | step: 35/422 | loss: 3.1294827461242676\n",
      "Epoch: 22/100 | step: 36/422 | loss: 3.0583043098449707\n",
      "Epoch: 22/100 | step: 37/422 | loss: 3.3044896125793457\n",
      "Epoch: 22/100 | step: 38/422 | loss: 3.4229695796966553\n",
      "Epoch: 22/100 | step: 39/422 | loss: 2.8641552925109863\n",
      "Epoch: 22/100 | step: 40/422 | loss: 3.173280715942383\n",
      "Epoch: 22/100 | step: 41/422 | loss: 2.979462146759033\n",
      "Epoch: 22/100 | step: 42/422 | loss: 2.912108898162842\n",
      "Epoch: 22/100 | step: 43/422 | loss: 3.279155969619751\n",
      "Epoch: 22/100 | step: 44/422 | loss: 3.1558499336242676\n",
      "Epoch: 22/100 | step: 45/422 | loss: 3.53228497505188\n",
      "Epoch: 22/100 | step: 46/422 | loss: 3.0955004692077637\n",
      "Epoch: 22/100 | step: 47/422 | loss: 3.2562007904052734\n",
      "Epoch: 22/100 | step: 48/422 | loss: 3.032707929611206\n",
      "Epoch: 22/100 | step: 49/422 | loss: 3.0835788249969482\n",
      "Epoch: 22/100 | step: 50/422 | loss: 2.9674243927001953\n",
      "Epoch: 22/100 | step: 51/422 | loss: 3.9234695434570312\n",
      "Epoch: 22/100 | step: 52/422 | loss: 3.161876678466797\n",
      "Epoch: 22/100 | step: 53/422 | loss: 3.143047571182251\n",
      "Epoch: 22/100 | step: 54/422 | loss: 3.2045938968658447\n",
      "Epoch: 22/100 | step: 55/422 | loss: 3.676351547241211\n",
      "Epoch: 22/100 | step: 56/422 | loss: 2.935723066329956\n",
      "Epoch: 22/100 | step: 57/422 | loss: 3.30148983001709\n",
      "Epoch: 22/100 | step: 58/422 | loss: 3.5879287719726562\n",
      "Epoch: 22/100 | step: 59/422 | loss: 3.5395052433013916\n",
      "Epoch: 22/100 | step: 60/422 | loss: 3.256139039993286\n",
      "Epoch: 22/100 | step: 61/422 | loss: 2.7393431663513184\n",
      "Epoch: 22/100 | step: 62/422 | loss: 2.64194917678833\n",
      "Epoch: 22/100 | step: 63/422 | loss: 3.349863052368164\n",
      "Epoch: 22/100 | step: 64/422 | loss: 3.1240453720092773\n",
      "Epoch: 22/100 | step: 65/422 | loss: 3.108212947845459\n",
      "Epoch: 22/100 | step: 66/422 | loss: 3.083383083343506\n",
      "Epoch: 22/100 | step: 67/422 | loss: 3.2414426803588867\n",
      "Epoch: 22/100 | step: 68/422 | loss: 3.5702149868011475\n",
      "Epoch: 22/100 | step: 69/422 | loss: 2.742030620574951\n",
      "Epoch: 22/100 | step: 70/422 | loss: 3.799901008605957\n",
      "Epoch: 22/100 | step: 71/422 | loss: 3.328263759613037\n",
      "Epoch: 22/100 | step: 72/422 | loss: 2.934953212738037\n",
      "Epoch: 22/100 | step: 73/422 | loss: 3.098339319229126\n",
      "Epoch: 22/100 | step: 74/422 | loss: 3.13232421875\n",
      "Epoch: 22/100 | step: 75/422 | loss: 2.261918544769287\n",
      "Epoch: 22/100 | step: 76/422 | loss: 3.5470330715179443\n",
      "Epoch: 22/100 | step: 77/422 | loss: 2.972931385040283\n",
      "Epoch: 22/100 | step: 78/422 | loss: 3.6213219165802\n",
      "Epoch: 22/100 | step: 79/422 | loss: 3.4064676761627197\n",
      "Epoch: 22/100 | step: 80/422 | loss: 3.1582374572753906\n",
      "Epoch: 22/100 | step: 81/422 | loss: 3.1506404876708984\n",
      "Epoch: 22/100 | step: 82/422 | loss: 3.289846658706665\n",
      "Epoch: 22/100 | step: 83/422 | loss: 3.1660869121551514\n",
      "Epoch: 22/100 | step: 84/422 | loss: 3.5682461261749268\n",
      "Epoch: 22/100 | step: 85/422 | loss: 3.035149335861206\n",
      "Epoch: 22/100 | step: 86/422 | loss: 3.079768180847168\n",
      "Epoch: 22/100 | step: 87/422 | loss: 3.1774284839630127\n",
      "Epoch: 22/100 | step: 88/422 | loss: 2.9965832233428955\n",
      "Epoch: 22/100 | step: 89/422 | loss: 3.014023542404175\n",
      "Epoch: 22/100 | step: 90/422 | loss: 2.9287097454071045\n",
      "Epoch: 22/100 | step: 91/422 | loss: 3.583616018295288\n",
      "Epoch: 22/100 | step: 92/422 | loss: 3.2054457664489746\n",
      "Epoch: 22/100 | step: 93/422 | loss: 3.00681471824646\n",
      "Epoch: 22/100 | step: 94/422 | loss: 3.0590691566467285\n",
      "Epoch: 22/100 | step: 95/422 | loss: 3.1005327701568604\n",
      "Epoch: 22/100 | step: 96/422 | loss: 3.527297258377075\n",
      "Epoch: 22/100 | step: 97/422 | loss: 2.927765130996704\n",
      "Epoch: 22/100 | step: 98/422 | loss: 3.0439860820770264\n",
      "Epoch: 22/100 | step: 99/422 | loss: 3.0704708099365234\n",
      "Epoch: 22/100 | step: 100/422 | loss: 3.5487236976623535\n",
      "Epoch: 22/100 | step: 101/422 | loss: 2.8623297214508057\n",
      "Epoch: 22/100 | step: 102/422 | loss: 3.3372433185577393\n",
      "Epoch: 22/100 | step: 103/422 | loss: 2.9950716495513916\n",
      "Epoch: 22/100 | step: 104/422 | loss: 3.0599937438964844\n",
      "Epoch: 22/100 | step: 105/422 | loss: 3.2807509899139404\n",
      "Epoch: 22/100 | step: 106/422 | loss: 3.1916394233703613\n",
      "Epoch: 22/100 | step: 107/422 | loss: 3.6545920372009277\n",
      "Epoch: 22/100 | step: 108/422 | loss: 3.3304083347320557\n",
      "Epoch: 22/100 | step: 109/422 | loss: 3.25834321975708\n",
      "Epoch: 22/100 | step: 110/422 | loss: 2.876664876937866\n",
      "Epoch: 22/100 | step: 111/422 | loss: 3.3226852416992188\n",
      "Epoch: 22/100 | step: 112/422 | loss: 2.96988582611084\n",
      "Epoch: 22/100 | step: 113/422 | loss: 3.110309362411499\n",
      "Epoch: 22/100 | step: 114/422 | loss: 3.4601824283599854\n",
      "Epoch: 22/100 | step: 115/422 | loss: 3.089017629623413\n",
      "Epoch: 22/100 | step: 116/422 | loss: 3.292646884918213\n",
      "Epoch: 22/100 | step: 117/422 | loss: 2.9177777767181396\n",
      "Epoch: 22/100 | step: 118/422 | loss: 3.6109914779663086\n",
      "Epoch: 22/100 | step: 119/422 | loss: 3.1131463050842285\n",
      "Epoch: 22/100 | step: 120/422 | loss: 3.2116260528564453\n",
      "Epoch: 22/100 | step: 121/422 | loss: 2.9593982696533203\n",
      "Epoch: 22/100 | step: 122/422 | loss: 3.122349739074707\n",
      "Epoch: 22/100 | step: 123/422 | loss: 3.4458351135253906\n",
      "Epoch: 22/100 | step: 124/422 | loss: 3.5692050457000732\n",
      "Epoch: 22/100 | step: 125/422 | loss: 3.6348559856414795\n",
      "Epoch: 22/100 | step: 126/422 | loss: 3.137995958328247\n",
      "Epoch: 22/100 | step: 127/422 | loss: 3.3419837951660156\n",
      "Epoch: 22/100 | step: 128/422 | loss: 2.815748929977417\n",
      "Epoch: 22/100 | step: 129/422 | loss: 3.4105947017669678\n",
      "Epoch: 22/100 | step: 130/422 | loss: 3.054657459259033\n",
      "Epoch: 22/100 | step: 131/422 | loss: 3.537501096725464\n",
      "Epoch: 22/100 | step: 132/422 | loss: 3.2922401428222656\n",
      "Epoch: 22/100 | step: 133/422 | loss: 3.129246234893799\n",
      "Epoch: 22/100 | step: 134/422 | loss: 3.2726118564605713\n",
      "Epoch: 22/100 | step: 135/422 | loss: 3.3919880390167236\n",
      "Epoch: 22/100 | step: 136/422 | loss: 2.806480646133423\n",
      "Epoch: 22/100 | step: 137/422 | loss: 3.4854366779327393\n",
      "Epoch: 22/100 | step: 138/422 | loss: 3.0357766151428223\n",
      "Epoch: 22/100 | step: 139/422 | loss: 3.084641695022583\n",
      "Epoch: 22/100 | step: 140/422 | loss: 3.4938228130340576\n",
      "Epoch: 22/100 | step: 141/422 | loss: 3.1074717044830322\n",
      "Epoch: 22/100 | step: 142/422 | loss: 3.4126157760620117\n",
      "Epoch: 22/100 | step: 143/422 | loss: 3.1920952796936035\n",
      "Epoch: 22/100 | step: 144/422 | loss: 3.014590263366699\n",
      "Epoch: 22/100 | step: 145/422 | loss: 2.615480422973633\n",
      "Epoch: 22/100 | step: 146/422 | loss: 3.1452107429504395\n",
      "Epoch: 22/100 | step: 147/422 | loss: 3.118422269821167\n",
      "Epoch: 22/100 | step: 148/422 | loss: 3.193402051925659\n",
      "Epoch: 22/100 | step: 149/422 | loss: 3.0799546241760254\n",
      "Epoch: 22/100 | step: 150/422 | loss: 3.175553560256958\n",
      "Epoch: 22/100 | step: 151/422 | loss: 3.4094231128692627\n",
      "Epoch: 22/100 | step: 152/422 | loss: 3.859834671020508\n",
      "Epoch: 22/100 | step: 153/422 | loss: 3.3051915168762207\n",
      "Epoch: 22/100 | step: 154/422 | loss: 2.8927924633026123\n",
      "Epoch: 22/100 | step: 155/422 | loss: 3.768259048461914\n",
      "Epoch: 22/100 | step: 156/422 | loss: 2.7478187084198\n",
      "Epoch: 22/100 | step: 157/422 | loss: 3.694378137588501\n",
      "Epoch: 22/100 | step: 158/422 | loss: 2.7128334045410156\n",
      "Epoch: 22/100 | step: 159/422 | loss: 3.349416732788086\n",
      "Epoch: 22/100 | step: 160/422 | loss: 3.8167872428894043\n",
      "Epoch: 22/100 | step: 161/422 | loss: 3.172290325164795\n",
      "Epoch: 22/100 | step: 162/422 | loss: 2.4962618350982666\n",
      "Epoch: 22/100 | step: 163/422 | loss: 3.4552700519561768\n",
      "Epoch: 22/100 | step: 164/422 | loss: 3.3824195861816406\n",
      "Epoch: 22/100 | step: 165/422 | loss: 3.330479145050049\n",
      "Epoch: 22/100 | step: 166/422 | loss: 3.2419824600219727\n",
      "Epoch: 22/100 | step: 167/422 | loss: 2.9988739490509033\n",
      "Epoch: 22/100 | step: 168/422 | loss: 2.3949594497680664\n",
      "Epoch: 22/100 | step: 169/422 | loss: 3.2011051177978516\n",
      "Epoch: 22/100 | step: 170/422 | loss: 3.1723110675811768\n",
      "Epoch: 22/100 | step: 171/422 | loss: 3.0976967811584473\n",
      "Epoch: 22/100 | step: 172/422 | loss: 3.1758668422698975\n",
      "Epoch: 22/100 | step: 173/422 | loss: 2.9765963554382324\n",
      "Epoch: 22/100 | step: 174/422 | loss: 3.064774990081787\n",
      "Epoch: 22/100 | step: 175/422 | loss: 3.23148775100708\n",
      "Epoch: 22/100 | step: 176/422 | loss: 2.6990535259246826\n",
      "Epoch: 22/100 | step: 177/422 | loss: 3.128276824951172\n",
      "Epoch: 22/100 | step: 178/422 | loss: 2.984189510345459\n",
      "Epoch: 22/100 | step: 179/422 | loss: 3.0544650554656982\n",
      "Epoch: 22/100 | step: 180/422 | loss: 2.9996511936187744\n",
      "Epoch: 22/100 | step: 181/422 | loss: 2.7324390411376953\n",
      "Epoch: 22/100 | step: 182/422 | loss: 3.0237207412719727\n",
      "Epoch: 22/100 | step: 183/422 | loss: 3.0107040405273438\n",
      "Epoch: 22/100 | step: 184/422 | loss: 3.3394687175750732\n",
      "Epoch: 22/100 | step: 185/422 | loss: 3.6451144218444824\n",
      "Epoch: 22/100 | step: 186/422 | loss: 2.9946365356445312\n",
      "Epoch: 22/100 | step: 187/422 | loss: 2.820697784423828\n",
      "Epoch: 22/100 | step: 188/422 | loss: 2.829495668411255\n",
      "Epoch: 22/100 | step: 189/422 | loss: 3.2520172595977783\n",
      "Epoch: 22/100 | step: 190/422 | loss: 3.1619997024536133\n",
      "Epoch: 22/100 | step: 191/422 | loss: 3.3415367603302\n",
      "Epoch: 22/100 | step: 192/422 | loss: 3.034799098968506\n",
      "Epoch: 22/100 | step: 193/422 | loss: 3.0039215087890625\n",
      "Epoch: 22/100 | step: 194/422 | loss: 3.9043803215026855\n",
      "Epoch: 22/100 | step: 195/422 | loss: 3.2053232192993164\n",
      "Epoch: 22/100 | step: 196/422 | loss: 3.5049264430999756\n",
      "Epoch: 22/100 | step: 197/422 | loss: 3.511489152908325\n",
      "Epoch: 22/100 | step: 198/422 | loss: 3.474430561065674\n",
      "Epoch: 22/100 | step: 199/422 | loss: 3.4993629455566406\n",
      "Epoch: 22/100 | step: 200/422 | loss: 3.1289358139038086\n",
      "Epoch: 22/100 | step: 201/422 | loss: 2.877664804458618\n",
      "Epoch: 22/100 | step: 202/422 | loss: 3.4599993228912354\n",
      "Epoch: 22/100 | step: 203/422 | loss: 3.2627835273742676\n",
      "Epoch: 22/100 | step: 204/422 | loss: 3.2091591358184814\n",
      "Epoch: 22/100 | step: 205/422 | loss: 3.1140806674957275\n",
      "Epoch: 22/100 | step: 206/422 | loss: 2.9320871829986572\n",
      "Epoch: 22/100 | step: 207/422 | loss: 3.3639371395111084\n",
      "Epoch: 22/100 | step: 208/422 | loss: 2.914555788040161\n",
      "Epoch: 22/100 | step: 209/422 | loss: 2.8156864643096924\n",
      "Epoch: 22/100 | step: 210/422 | loss: 3.2207424640655518\n",
      "Epoch: 22/100 | step: 211/422 | loss: 3.450458288192749\n",
      "Epoch: 22/100 | step: 212/422 | loss: 3.1912124156951904\n",
      "Epoch: 22/100 | step: 213/422 | loss: 3.264047384262085\n",
      "Epoch: 22/100 | step: 214/422 | loss: 4.005786895751953\n",
      "Epoch: 22/100 | step: 215/422 | loss: 3.171901226043701\n",
      "Epoch: 22/100 | step: 216/422 | loss: 3.398623466491699\n",
      "Epoch: 22/100 | step: 217/422 | loss: 3.2246768474578857\n",
      "Epoch: 22/100 | step: 218/422 | loss: 3.2936034202575684\n",
      "Epoch: 22/100 | step: 219/422 | loss: 3.122292995452881\n",
      "Epoch: 22/100 | step: 220/422 | loss: 3.016646146774292\n",
      "Epoch: 22/100 | step: 221/422 | loss: 3.355440378189087\n",
      "Epoch: 22/100 | step: 222/422 | loss: 3.310908317565918\n",
      "Epoch: 22/100 | step: 223/422 | loss: 3.359819173812866\n",
      "Epoch: 22/100 | step: 224/422 | loss: 3.6712749004364014\n",
      "Epoch: 22/100 | step: 225/422 | loss: 3.5911941528320312\n",
      "Epoch: 22/100 | step: 226/422 | loss: 3.4996325969696045\n",
      "Epoch: 22/100 | step: 227/422 | loss: 3.36077880859375\n",
      "Epoch: 22/100 | step: 228/422 | loss: 3.3170714378356934\n",
      "Epoch: 22/100 | step: 229/422 | loss: 3.192312479019165\n",
      "Epoch: 22/100 | step: 230/422 | loss: 3.451798915863037\n",
      "Epoch: 22/100 | step: 231/422 | loss: 3.1285736560821533\n",
      "Epoch: 22/100 | step: 232/422 | loss: 2.779092788696289\n",
      "Epoch: 22/100 | step: 233/422 | loss: 3.180554151535034\n",
      "Epoch: 22/100 | step: 234/422 | loss: 2.9063477516174316\n",
      "Epoch: 22/100 | step: 235/422 | loss: 3.309791326522827\n",
      "Epoch: 22/100 | step: 236/422 | loss: 3.813875198364258\n",
      "Epoch: 22/100 | step: 237/422 | loss: 2.931119680404663\n",
      "Epoch: 22/100 | step: 238/422 | loss: 3.5143327713012695\n",
      "Epoch: 22/100 | step: 239/422 | loss: 3.2973461151123047\n",
      "Epoch: 22/100 | step: 240/422 | loss: 2.9769206047058105\n",
      "Epoch: 22/100 | step: 241/422 | loss: 3.2040016651153564\n",
      "Epoch: 22/100 | step: 242/422 | loss: 2.776169538497925\n",
      "Epoch: 22/100 | step: 243/422 | loss: 3.0394279956817627\n",
      "Epoch: 22/100 | step: 244/422 | loss: 3.0669727325439453\n",
      "Epoch: 22/100 | step: 245/422 | loss: 3.2526423931121826\n",
      "Epoch: 22/100 | step: 246/422 | loss: 3.29757022857666\n",
      "Epoch: 22/100 | step: 247/422 | loss: 3.3761603832244873\n",
      "Epoch: 22/100 | step: 248/422 | loss: 3.2590043544769287\n",
      "Epoch: 22/100 | step: 249/422 | loss: 2.9219141006469727\n",
      "Epoch: 22/100 | step: 250/422 | loss: 3.472991943359375\n",
      "Epoch: 22/100 | step: 251/422 | loss: 3.201245069503784\n",
      "Epoch: 22/100 | step: 252/422 | loss: 3.159071683883667\n",
      "Epoch: 22/100 | step: 253/422 | loss: 3.080254077911377\n",
      "Epoch: 22/100 | step: 254/422 | loss: 3.532684803009033\n",
      "Epoch: 22/100 | step: 255/422 | loss: 3.135244131088257\n",
      "Epoch: 22/100 | step: 256/422 | loss: 3.542156934738159\n",
      "Epoch: 22/100 | step: 257/422 | loss: 2.764160633087158\n",
      "Epoch: 22/100 | step: 258/422 | loss: 3.0610673427581787\n",
      "Epoch: 22/100 | step: 259/422 | loss: 3.5204644203186035\n",
      "Epoch: 22/100 | step: 260/422 | loss: 2.8630712032318115\n",
      "Epoch: 22/100 | step: 261/422 | loss: 2.830929756164551\n",
      "Epoch: 22/100 | step: 262/422 | loss: 3.0652904510498047\n",
      "Epoch: 22/100 | step: 263/422 | loss: 3.0677034854888916\n",
      "Epoch: 22/100 | step: 264/422 | loss: 2.755141019821167\n",
      "Epoch: 22/100 | step: 265/422 | loss: 3.1142306327819824\n",
      "Epoch: 22/100 | step: 266/422 | loss: 3.1529366970062256\n",
      "Epoch: 22/100 | step: 267/422 | loss: 3.9894704818725586\n",
      "Epoch: 22/100 | step: 268/422 | loss: 3.236250638961792\n",
      "Epoch: 22/100 | step: 269/422 | loss: 3.5111424922943115\n",
      "Epoch: 22/100 | step: 270/422 | loss: 3.456345319747925\n",
      "Epoch: 22/100 | step: 271/422 | loss: 3.4704434871673584\n",
      "Epoch: 22/100 | step: 272/422 | loss: 2.973728656768799\n",
      "Epoch: 22/100 | step: 273/422 | loss: 3.3566346168518066\n",
      "Epoch: 22/100 | step: 274/422 | loss: 2.7435431480407715\n",
      "Epoch: 22/100 | step: 275/422 | loss: 3.514522075653076\n",
      "Epoch: 22/100 | step: 276/422 | loss: 3.8554699420928955\n",
      "Epoch: 22/100 | step: 277/422 | loss: 3.7240231037139893\n",
      "Epoch: 22/100 | step: 278/422 | loss: 2.9097931385040283\n",
      "Epoch: 22/100 | step: 279/422 | loss: 3.190352439880371\n",
      "Epoch: 22/100 | step: 280/422 | loss: 3.0214855670928955\n",
      "Epoch: 22/100 | step: 281/422 | loss: 3.127969264984131\n",
      "Epoch: 22/100 | step: 282/422 | loss: 3.386319637298584\n",
      "Epoch: 22/100 | step: 283/422 | loss: 3.0830562114715576\n",
      "Epoch: 22/100 | step: 284/422 | loss: 3.1260416507720947\n",
      "Epoch: 22/100 | step: 285/422 | loss: 2.701289653778076\n",
      "Epoch: 22/100 | step: 286/422 | loss: 2.9757821559906006\n",
      "Epoch: 22/100 | step: 287/422 | loss: 2.7532260417938232\n",
      "Epoch: 22/100 | step: 288/422 | loss: 2.5220797061920166\n",
      "Epoch: 22/100 | step: 289/422 | loss: 3.2394745349884033\n",
      "Epoch: 22/100 | step: 290/422 | loss: 3.505263566970825\n",
      "Epoch: 22/100 | step: 291/422 | loss: 3.300930976867676\n",
      "Epoch: 22/100 | step: 292/422 | loss: 3.1321170330047607\n",
      "Epoch: 22/100 | step: 293/422 | loss: 2.7037160396575928\n",
      "Epoch: 22/100 | step: 294/422 | loss: 2.716181516647339\n",
      "Epoch: 22/100 | step: 295/422 | loss: 3.4397504329681396\n",
      "Epoch: 22/100 | step: 296/422 | loss: 3.2588467597961426\n",
      "Epoch: 22/100 | step: 297/422 | loss: 2.7740938663482666\n",
      "Epoch: 22/100 | step: 298/422 | loss: 2.8680801391601562\n",
      "Epoch: 22/100 | step: 299/422 | loss: 3.036203622817993\n",
      "Epoch: 22/100 | step: 300/422 | loss: 3.2345633506774902\n",
      "Epoch: 22/100 | step: 301/422 | loss: 3.332597255706787\n",
      "Epoch: 22/100 | step: 302/422 | loss: 2.8525660037994385\n",
      "Epoch: 22/100 | step: 303/422 | loss: 3.175436496734619\n",
      "Epoch: 22/100 | step: 304/422 | loss: 3.044614315032959\n",
      "Epoch: 22/100 | step: 305/422 | loss: 3.236011505126953\n",
      "Epoch: 22/100 | step: 306/422 | loss: 3.427665948867798\n",
      "Epoch: 22/100 | step: 307/422 | loss: 3.0728209018707275\n",
      "Epoch: 22/100 | step: 308/422 | loss: 3.4297938346862793\n",
      "Epoch: 22/100 | step: 309/422 | loss: 3.4496469497680664\n",
      "Epoch: 22/100 | step: 310/422 | loss: 3.2099695205688477\n",
      "Epoch: 22/100 | step: 311/422 | loss: 3.1399426460266113\n",
      "Epoch: 22/100 | step: 312/422 | loss: 3.509556293487549\n",
      "Epoch: 22/100 | step: 313/422 | loss: 3.1901051998138428\n",
      "Epoch: 22/100 | step: 314/422 | loss: 2.762882709503174\n",
      "Epoch: 22/100 | step: 315/422 | loss: 3.158484935760498\n",
      "Epoch: 22/100 | step: 316/422 | loss: 2.916754961013794\n",
      "Epoch: 22/100 | step: 317/422 | loss: 2.9176220893859863\n",
      "Epoch: 22/100 | step: 318/422 | loss: 3.1261346340179443\n",
      "Epoch: 22/100 | step: 319/422 | loss: 2.8438425064086914\n",
      "Epoch: 22/100 | step: 320/422 | loss: 3.1261579990386963\n",
      "Epoch: 22/100 | step: 321/422 | loss: 2.9679059982299805\n",
      "Epoch: 22/100 | step: 322/422 | loss: 3.1124374866485596\n",
      "Epoch: 22/100 | step: 323/422 | loss: 3.4548118114471436\n",
      "Epoch: 22/100 | step: 324/422 | loss: 3.0520551204681396\n",
      "Epoch: 22/100 | step: 325/422 | loss: 2.888784408569336\n",
      "Epoch: 22/100 | step: 326/422 | loss: 2.968660831451416\n",
      "Epoch: 22/100 | step: 327/422 | loss: 2.849719524383545\n",
      "Epoch: 22/100 | step: 328/422 | loss: 3.1279468536376953\n",
      "Epoch: 22/100 | step: 329/422 | loss: 3.3367230892181396\n",
      "Epoch: 22/100 | step: 330/422 | loss: 3.1261394023895264\n",
      "Epoch: 22/100 | step: 331/422 | loss: 3.1526787281036377\n",
      "Epoch: 22/100 | step: 332/422 | loss: 3.03975248336792\n",
      "Epoch: 22/100 | step: 333/422 | loss: 3.4402103424072266\n",
      "Epoch: 22/100 | step: 334/422 | loss: 3.2692816257476807\n",
      "Epoch: 22/100 | step: 335/422 | loss: 2.9168701171875\n",
      "Epoch: 22/100 | step: 336/422 | loss: 3.27315354347229\n",
      "Epoch: 22/100 | step: 337/422 | loss: 3.7380576133728027\n",
      "Epoch: 22/100 | step: 338/422 | loss: 3.2342960834503174\n",
      "Epoch: 22/100 | step: 339/422 | loss: 3.3008322715759277\n",
      "Epoch: 22/100 | step: 340/422 | loss: 2.7735579013824463\n",
      "Epoch: 22/100 | step: 341/422 | loss: 3.295665979385376\n",
      "Epoch: 22/100 | step: 342/422 | loss: 3.295811414718628\n",
      "Epoch: 22/100 | step: 343/422 | loss: 2.9310379028320312\n",
      "Epoch: 22/100 | step: 344/422 | loss: 3.0029006004333496\n",
      "Epoch: 22/100 | step: 345/422 | loss: 2.777679204940796\n",
      "Epoch: 22/100 | step: 346/422 | loss: 3.138123035430908\n",
      "Epoch: 22/100 | step: 347/422 | loss: 3.8978404998779297\n",
      "Epoch: 22/100 | step: 348/422 | loss: 3.3526158332824707\n",
      "Epoch: 22/100 | step: 349/422 | loss: 3.2286856174468994\n",
      "Epoch: 22/100 | step: 350/422 | loss: 3.0354840755462646\n",
      "Epoch: 22/100 | step: 351/422 | loss: 3.314906597137451\n",
      "Epoch: 22/100 | step: 352/422 | loss: 3.181241035461426\n",
      "Epoch: 22/100 | step: 353/422 | loss: 3.141629457473755\n",
      "Epoch: 22/100 | step: 354/422 | loss: 2.705997943878174\n",
      "Epoch: 22/100 | step: 355/422 | loss: 3.157569169998169\n",
      "Epoch: 22/100 | step: 356/422 | loss: 3.175473213195801\n",
      "Epoch: 22/100 | step: 357/422 | loss: 3.3275256156921387\n",
      "Epoch: 22/100 | step: 358/422 | loss: 3.2520980834960938\n",
      "Epoch: 22/100 | step: 359/422 | loss: 3.087858200073242\n",
      "Epoch: 22/100 | step: 360/422 | loss: 2.930283546447754\n",
      "Epoch: 22/100 | step: 361/422 | loss: 3.0165646076202393\n",
      "Epoch: 22/100 | step: 362/422 | loss: 2.9235000610351562\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 23/100 | step: 1/422 | loss: 3.3238401412963867\n",
      "Epoch: 23/100 | step: 2/422 | loss: 2.824319362640381\n",
      "Epoch: 23/100 | step: 3/422 | loss: 3.3266589641571045\n",
      "Epoch: 23/100 | step: 4/422 | loss: 2.8801276683807373\n",
      "Epoch: 23/100 | step: 5/422 | loss: 2.7530272006988525\n",
      "Epoch: 23/100 | step: 6/422 | loss: 3.1922614574432373\n",
      "Epoch: 23/100 | step: 7/422 | loss: 3.7761340141296387\n",
      "Epoch: 23/100 | step: 8/422 | loss: 3.2334797382354736\n",
      "Epoch: 23/100 | step: 9/422 | loss: 3.3254995346069336\n",
      "Epoch: 23/100 | step: 10/422 | loss: 2.8154265880584717\n",
      "Epoch: 23/100 | step: 11/422 | loss: 2.9986531734466553\n",
      "Epoch: 23/100 | step: 12/422 | loss: 3.064610242843628\n",
      "Epoch: 23/100 | step: 13/422 | loss: 3.279694080352783\n",
      "Epoch: 23/100 | step: 14/422 | loss: 2.9638750553131104\n",
      "Epoch: 23/100 | step: 15/422 | loss: 2.990302801132202\n",
      "Epoch: 23/100 | step: 16/422 | loss: 2.701608419418335\n",
      "Epoch: 23/100 | step: 17/422 | loss: 3.1866652965545654\n",
      "Epoch: 23/100 | step: 18/422 | loss: 3.117753744125366\n",
      "Epoch: 23/100 | step: 19/422 | loss: 2.9139599800109863\n",
      "Epoch: 23/100 | step: 20/422 | loss: 3.133669376373291\n",
      "Epoch: 23/100 | step: 21/422 | loss: 2.896221160888672\n",
      "Epoch: 23/100 | step: 22/422 | loss: 3.122636556625366\n",
      "Epoch: 23/100 | step: 23/422 | loss: 2.845383882522583\n",
      "Epoch: 23/100 | step: 24/422 | loss: 3.003000020980835\n",
      "Epoch: 23/100 | step: 25/422 | loss: 3.0274572372436523\n",
      "Epoch: 23/100 | step: 26/422 | loss: 2.6486597061157227\n",
      "Epoch: 23/100 | step: 27/422 | loss: 3.064753770828247\n",
      "Epoch: 23/100 | step: 28/422 | loss: 3.0550808906555176\n",
      "Epoch: 23/100 | step: 29/422 | loss: 2.762605667114258\n",
      "Epoch: 23/100 | step: 30/422 | loss: 3.5555217266082764\n",
      "Epoch: 23/100 | step: 31/422 | loss: 3.006091594696045\n",
      "Epoch: 23/100 | step: 32/422 | loss: 3.019106864929199\n",
      "Epoch: 23/100 | step: 33/422 | loss: 3.391037702560425\n",
      "Epoch: 23/100 | step: 34/422 | loss: 2.7078022956848145\n",
      "Epoch: 23/100 | step: 35/422 | loss: 3.0487442016601562\n",
      "Epoch: 23/100 | step: 36/422 | loss: 2.7378017902374268\n",
      "Epoch: 23/100 | step: 37/422 | loss: 2.8808910846710205\n",
      "Epoch: 23/100 | step: 38/422 | loss: 3.2758405208587646\n",
      "Epoch: 23/100 | step: 39/422 | loss: 3.275233507156372\n",
      "Epoch: 23/100 | step: 40/422 | loss: 2.9988017082214355\n",
      "Epoch: 23/100 | step: 41/422 | loss: 3.228095293045044\n",
      "Epoch: 23/100 | step: 42/422 | loss: 3.369910478591919\n",
      "Epoch: 23/100 | step: 43/422 | loss: 3.197767972946167\n",
      "Epoch: 23/100 | step: 44/422 | loss: 3.08061146736145\n",
      "Epoch: 23/100 | step: 45/422 | loss: 3.2686548233032227\n",
      "Epoch: 23/100 | step: 46/422 | loss: 2.8042378425598145\n",
      "Epoch: 23/100 | step: 47/422 | loss: 3.1347763538360596\n",
      "Epoch: 23/100 | step: 48/422 | loss: 3.498964786529541\n",
      "Epoch: 23/100 | step: 49/422 | loss: 3.089979887008667\n",
      "Epoch: 23/100 | step: 50/422 | loss: 2.8953638076782227\n",
      "Epoch: 23/100 | step: 51/422 | loss: 3.316160202026367\n",
      "Epoch: 23/100 | step: 52/422 | loss: 3.380540132522583\n",
      "Epoch: 23/100 | step: 53/422 | loss: 3.1428489685058594\n",
      "Epoch: 23/100 | step: 54/422 | loss: 2.9664199352264404\n",
      "Epoch: 23/100 | step: 55/422 | loss: 2.5794076919555664\n",
      "Epoch: 23/100 | step: 56/422 | loss: 3.335191249847412\n",
      "Epoch: 23/100 | step: 57/422 | loss: 2.7744274139404297\n",
      "Epoch: 23/100 | step: 58/422 | loss: 2.7814536094665527\n",
      "Epoch: 23/100 | step: 59/422 | loss: 3.260437488555908\n",
      "Epoch: 23/100 | step: 60/422 | loss: 3.014392614364624\n",
      "Epoch: 23/100 | step: 61/422 | loss: 2.695081949234009\n",
      "Epoch: 23/100 | step: 62/422 | loss: 2.8523528575897217\n",
      "Epoch: 23/100 | step: 63/422 | loss: 2.9318907260894775\n",
      "Epoch: 23/100 | step: 64/422 | loss: 2.5697693824768066\n",
      "Epoch: 23/100 | step: 65/422 | loss: 3.1858487129211426\n",
      "Epoch: 23/100 | step: 66/422 | loss: 3.7011640071868896\n",
      "Epoch: 23/100 | step: 67/422 | loss: 3.511240243911743\n",
      "Epoch: 23/100 | step: 68/422 | loss: 2.8218116760253906\n",
      "Epoch: 23/100 | step: 69/422 | loss: 3.4377334117889404\n",
      "Epoch: 23/100 | step: 70/422 | loss: 3.12593150138855\n",
      "Epoch: 23/100 | step: 71/422 | loss: 2.5763535499572754\n",
      "Epoch: 23/100 | step: 72/422 | loss: 2.7527530193328857\n",
      "Epoch: 23/100 | step: 73/422 | loss: 3.128767490386963\n",
      "Epoch: 23/100 | step: 74/422 | loss: 2.6539390087127686\n",
      "Epoch: 23/100 | step: 75/422 | loss: 3.4230356216430664\n",
      "Epoch: 23/100 | step: 76/422 | loss: 3.015695810317993\n",
      "Epoch: 23/100 | step: 77/422 | loss: 2.459958791732788\n",
      "Epoch: 23/100 | step: 78/422 | loss: 3.168222665786743\n",
      "Epoch: 23/100 | step: 79/422 | loss: 2.865706205368042\n",
      "Epoch: 23/100 | step: 80/422 | loss: 3.5177321434020996\n",
      "Epoch: 23/100 | step: 81/422 | loss: 3.4802069664001465\n",
      "Epoch: 23/100 | step: 82/422 | loss: 2.9647767543792725\n",
      "Epoch: 23/100 | step: 83/422 | loss: 3.0772783756256104\n",
      "Epoch: 23/100 | step: 84/422 | loss: 3.0666491985321045\n",
      "Epoch: 23/100 | step: 85/422 | loss: 2.7787508964538574\n",
      "Epoch: 23/100 | step: 86/422 | loss: 3.022446393966675\n",
      "Epoch: 23/100 | step: 87/422 | loss: 2.676516056060791\n",
      "Epoch: 23/100 | step: 88/422 | loss: 3.011789560317993\n",
      "Epoch: 23/100 | step: 89/422 | loss: 3.2026712894439697\n",
      "Epoch: 23/100 | step: 90/422 | loss: 3.027874708175659\n",
      "Epoch: 23/100 | step: 91/422 | loss: 2.929330825805664\n",
      "Epoch: 23/100 | step: 92/422 | loss: 3.1088485717773438\n",
      "Epoch: 23/100 | step: 93/422 | loss: 2.9774715900421143\n",
      "Epoch: 23/100 | step: 94/422 | loss: 2.9146249294281006\n",
      "Epoch: 23/100 | step: 95/422 | loss: 3.0297698974609375\n",
      "Epoch: 23/100 | step: 96/422 | loss: 3.0535149574279785\n",
      "Epoch: 23/100 | step: 97/422 | loss: 2.9101130962371826\n",
      "Epoch: 23/100 | step: 98/422 | loss: 3.1256802082061768\n",
      "Epoch: 23/100 | step: 99/422 | loss: 3.176738739013672\n",
      "Epoch: 23/100 | step: 100/422 | loss: 3.303457021713257\n",
      "Epoch: 23/100 | step: 101/422 | loss: 2.8960890769958496\n",
      "Epoch: 23/100 | step: 102/422 | loss: 2.686129093170166\n",
      "Epoch: 23/100 | step: 103/422 | loss: 3.1695549488067627\n",
      "Epoch: 23/100 | step: 104/422 | loss: 2.9936141967773438\n",
      "Epoch: 23/100 | step: 105/422 | loss: 2.8959100246429443\n",
      "Epoch: 23/100 | step: 106/422 | loss: 2.7241315841674805\n",
      "Epoch: 23/100 | step: 107/422 | loss: 3.025914430618286\n",
      "Epoch: 23/100 | step: 108/422 | loss: 2.762803316116333\n",
      "Epoch: 23/100 | step: 109/422 | loss: 2.6209490299224854\n",
      "Epoch: 23/100 | step: 110/422 | loss: 3.0356650352478027\n",
      "Epoch: 23/100 | step: 111/422 | loss: 3.0502731800079346\n",
      "Epoch: 23/100 | step: 112/422 | loss: 3.55692195892334\n",
      "Epoch: 23/100 | step: 113/422 | loss: 2.860346555709839\n",
      "Epoch: 23/100 | step: 114/422 | loss: 3.1575489044189453\n",
      "Epoch: 23/100 | step: 115/422 | loss: 2.696004867553711\n",
      "Epoch: 23/100 | step: 116/422 | loss: 2.7917702198028564\n",
      "Epoch: 23/100 | step: 117/422 | loss: 3.006540060043335\n",
      "Epoch: 23/100 | step: 118/422 | loss: 3.006312131881714\n",
      "Epoch: 23/100 | step: 119/422 | loss: 2.8909451961517334\n",
      "Epoch: 23/100 | step: 120/422 | loss: 3.157457113265991\n",
      "Epoch: 23/100 | step: 121/422 | loss: 2.952455997467041\n",
      "Epoch: 23/100 | step: 122/422 | loss: 2.941664218902588\n",
      "Epoch: 23/100 | step: 123/422 | loss: 3.013416290283203\n",
      "Epoch: 23/100 | step: 124/422 | loss: 2.574799060821533\n",
      "Epoch: 23/100 | step: 125/422 | loss: 3.72851824760437\n",
      "Epoch: 23/100 | step: 126/422 | loss: 3.6771039962768555\n",
      "Epoch: 23/100 | step: 127/422 | loss: 2.9612624645233154\n",
      "Epoch: 23/100 | step: 128/422 | loss: 3.5307881832122803\n",
      "Epoch: 23/100 | step: 129/422 | loss: 3.3558154106140137\n",
      "Epoch: 23/100 | step: 130/422 | loss: 3.035435199737549\n",
      "Epoch: 23/100 | step: 131/422 | loss: 3.4360737800598145\n",
      "Epoch: 23/100 | step: 132/422 | loss: 3.066199779510498\n",
      "Epoch: 23/100 | step: 133/422 | loss: 2.829590082168579\n",
      "Epoch: 23/100 | step: 134/422 | loss: 3.5079257488250732\n",
      "Epoch: 23/100 | step: 135/422 | loss: 3.5485098361968994\n",
      "Epoch: 23/100 | step: 136/422 | loss: 2.951036214828491\n",
      "Epoch: 23/100 | step: 137/422 | loss: 3.5969021320343018\n",
      "Epoch: 23/100 | step: 138/422 | loss: 3.1984057426452637\n",
      "Epoch: 23/100 | step: 139/422 | loss: 2.888449192047119\n",
      "Epoch: 23/100 | step: 140/422 | loss: 2.960390329360962\n",
      "Epoch: 23/100 | step: 141/422 | loss: 3.632934808731079\n",
      "Epoch: 23/100 | step: 142/422 | loss: 3.3112692832946777\n",
      "Epoch: 23/100 | step: 143/422 | loss: 2.9974052906036377\n",
      "Epoch: 23/100 | step: 144/422 | loss: 2.9759554862976074\n",
      "Epoch: 23/100 | step: 145/422 | loss: 3.33015775680542\n",
      "Epoch: 23/100 | step: 146/422 | loss: 2.8749969005584717\n",
      "Epoch: 23/100 | step: 147/422 | loss: 3.1202356815338135\n",
      "Epoch: 23/100 | step: 148/422 | loss: 2.9622902870178223\n",
      "Epoch: 23/100 | step: 149/422 | loss: 3.187079906463623\n",
      "Epoch: 23/100 | step: 150/422 | loss: 2.7792911529541016\n",
      "Epoch: 23/100 | step: 151/422 | loss: 3.0459187030792236\n",
      "Epoch: 23/100 | step: 152/422 | loss: 2.850555658340454\n",
      "Epoch: 23/100 | step: 153/422 | loss: 2.841648817062378\n",
      "Epoch: 23/100 | step: 154/422 | loss: 3.3306610584259033\n",
      "Epoch: 23/100 | step: 155/422 | loss: 2.9280261993408203\n",
      "Epoch: 23/100 | step: 156/422 | loss: 3.3137896060943604\n",
      "Epoch: 23/100 | step: 157/422 | loss: 2.8060476779937744\n",
      "Epoch: 23/100 | step: 158/422 | loss: 2.9745895862579346\n",
      "Epoch: 23/100 | step: 159/422 | loss: 2.8705408573150635\n",
      "Epoch: 23/100 | step: 160/422 | loss: 3.3945186138153076\n",
      "Epoch: 23/100 | step: 161/422 | loss: 4.044861793518066\n",
      "Epoch: 23/100 | step: 162/422 | loss: 2.992948055267334\n",
      "Epoch: 23/100 | step: 163/422 | loss: 3.0257487297058105\n",
      "Epoch: 23/100 | step: 164/422 | loss: 2.9668428897857666\n",
      "Epoch: 23/100 | step: 165/422 | loss: 4.048243522644043\n",
      "Epoch: 23/100 | step: 166/422 | loss: 2.859889507293701\n",
      "Epoch: 23/100 | step: 167/422 | loss: 2.6889712810516357\n",
      "Epoch: 23/100 | step: 168/422 | loss: 2.558171033859253\n",
      "Epoch: 23/100 | step: 169/422 | loss: 2.8593735694885254\n",
      "Epoch: 23/100 | step: 170/422 | loss: 3.2321841716766357\n",
      "Epoch: 23/100 | step: 171/422 | loss: 3.196310520172119\n",
      "Epoch: 23/100 | step: 172/422 | loss: 3.0400125980377197\n",
      "Epoch: 23/100 | step: 173/422 | loss: 2.713090181350708\n",
      "Epoch: 23/100 | step: 174/422 | loss: 3.4286327362060547\n",
      "Epoch: 23/100 | step: 175/422 | loss: 3.098818302154541\n",
      "Epoch: 23/100 | step: 176/422 | loss: 3.1615114212036133\n",
      "Epoch: 23/100 | step: 177/422 | loss: 2.776707649230957\n",
      "Epoch: 23/100 | step: 178/422 | loss: 2.7196543216705322\n",
      "Epoch: 23/100 | step: 179/422 | loss: 3.00565242767334\n",
      "Epoch: 23/100 | step: 180/422 | loss: 2.9472861289978027\n",
      "Epoch: 23/100 | step: 181/422 | loss: 2.723010540008545\n",
      "Epoch: 23/100 | step: 182/422 | loss: 3.244199514389038\n",
      "Epoch: 23/100 | step: 183/422 | loss: 3.0275959968566895\n",
      "Epoch: 23/100 | step: 184/422 | loss: 3.2884247303009033\n",
      "Epoch: 23/100 | step: 185/422 | loss: 3.414360523223877\n",
      "Epoch: 23/100 | step: 186/422 | loss: 3.128889322280884\n",
      "Epoch: 23/100 | step: 187/422 | loss: 2.836108684539795\n",
      "Epoch: 23/100 | step: 188/422 | loss: 2.6996235847473145\n",
      "Epoch: 23/100 | step: 189/422 | loss: 3.3826193809509277\n",
      "Epoch: 23/100 | step: 190/422 | loss: 2.778421401977539\n",
      "Epoch: 23/100 | step: 191/422 | loss: 2.675896167755127\n",
      "Epoch: 23/100 | step: 192/422 | loss: 2.960296154022217\n",
      "Epoch: 23/100 | step: 193/422 | loss: 3.397221326828003\n",
      "Epoch: 23/100 | step: 194/422 | loss: 2.818455696105957\n",
      "Epoch: 23/100 | step: 195/422 | loss: 3.520547389984131\n",
      "Epoch: 23/100 | step: 196/422 | loss: 3.2614381313323975\n",
      "Epoch: 23/100 | step: 197/422 | loss: 2.7737321853637695\n",
      "Epoch: 23/100 | step: 198/422 | loss: 3.1340291500091553\n",
      "Epoch: 23/100 | step: 199/422 | loss: 2.8970398902893066\n",
      "Epoch: 23/100 | step: 200/422 | loss: 3.424717426300049\n",
      "Epoch: 23/100 | step: 201/422 | loss: 2.900494337081909\n",
      "Epoch: 23/100 | step: 202/422 | loss: 3.036505937576294\n",
      "Epoch: 23/100 | step: 203/422 | loss: 3.342543125152588\n",
      "Epoch: 23/100 | step: 204/422 | loss: 3.094191312789917\n",
      "Epoch: 23/100 | step: 205/422 | loss: 3.129228115081787\n",
      "Epoch: 23/100 | step: 206/422 | loss: 3.0982701778411865\n",
      "Epoch: 23/100 | step: 207/422 | loss: 3.313322067260742\n",
      "Epoch: 23/100 | step: 208/422 | loss: 2.8961172103881836\n",
      "Epoch: 23/100 | step: 209/422 | loss: 3.4387309551239014\n",
      "Epoch: 23/100 | step: 210/422 | loss: 3.139835834503174\n",
      "Epoch: 23/100 | step: 211/422 | loss: 3.080064535140991\n",
      "Epoch: 23/100 | step: 212/422 | loss: 2.7803006172180176\n",
      "Epoch: 23/100 | step: 213/422 | loss: 3.0670557022094727\n",
      "Epoch: 23/100 | step: 214/422 | loss: 3.533297538757324\n",
      "Epoch: 23/100 | step: 215/422 | loss: 3.055678129196167\n",
      "Epoch: 23/100 | step: 216/422 | loss: 3.4132659435272217\n",
      "Epoch: 23/100 | step: 217/422 | loss: 2.912977695465088\n",
      "Epoch: 23/100 | step: 218/422 | loss: 2.7978315353393555\n",
      "Epoch: 23/100 | step: 219/422 | loss: 2.8815600872039795\n",
      "Epoch: 23/100 | step: 220/422 | loss: 2.5226051807403564\n",
      "Epoch: 23/100 | step: 221/422 | loss: 3.1752848625183105\n",
      "Epoch: 23/100 | step: 222/422 | loss: 3.106384038925171\n",
      "Epoch: 23/100 | step: 223/422 | loss: 3.042332649230957\n",
      "Epoch: 23/100 | step: 224/422 | loss: 3.4859514236450195\n",
      "Epoch: 23/100 | step: 225/422 | loss: 2.917539596557617\n",
      "Epoch: 23/100 | step: 226/422 | loss: 2.7085201740264893\n",
      "Epoch: 23/100 | step: 227/422 | loss: 3.061793327331543\n",
      "Epoch: 23/100 | step: 228/422 | loss: 3.054659128189087\n",
      "Epoch: 23/100 | step: 229/422 | loss: 3.0544257164001465\n",
      "Epoch: 23/100 | step: 230/422 | loss: 2.5356366634368896\n",
      "Epoch: 23/100 | step: 231/422 | loss: 3.2513985633850098\n",
      "Epoch: 23/100 | step: 232/422 | loss: 3.114103078842163\n",
      "Epoch: 23/100 | step: 233/422 | loss: 3.0146055221557617\n",
      "Epoch: 23/100 | step: 234/422 | loss: 3.8457324504852295\n",
      "Epoch: 23/100 | step: 235/422 | loss: 3.066093683242798\n",
      "Epoch: 23/100 | step: 236/422 | loss: 3.235522985458374\n",
      "Epoch: 23/100 | step: 237/422 | loss: 2.714219570159912\n",
      "Epoch: 23/100 | step: 238/422 | loss: 2.8244919776916504\n",
      "Epoch: 23/100 | step: 239/422 | loss: 3.135378122329712\n",
      "Epoch: 23/100 | step: 240/422 | loss: 2.884319305419922\n",
      "Epoch: 23/100 | step: 241/422 | loss: 3.328043222427368\n",
      "Epoch: 23/100 | step: 242/422 | loss: 2.8629345893859863\n",
      "Epoch: 23/100 | step: 243/422 | loss: 3.47670578956604\n",
      "Epoch: 23/100 | step: 244/422 | loss: 3.4141898155212402\n",
      "Epoch: 23/100 | step: 245/422 | loss: 3.3303797245025635\n",
      "Epoch: 23/100 | step: 246/422 | loss: 3.303184747695923\n",
      "Epoch: 23/100 | step: 247/422 | loss: 2.9654080867767334\n",
      "Epoch: 23/100 | step: 248/422 | loss: 3.107365608215332\n",
      "Epoch: 23/100 | step: 249/422 | loss: 3.1735548973083496\n",
      "Epoch: 23/100 | step: 250/422 | loss: 3.1480891704559326\n",
      "Epoch: 23/100 | step: 251/422 | loss: 2.8609979152679443\n",
      "Epoch: 23/100 | step: 252/422 | loss: 2.938836097717285\n",
      "Epoch: 23/100 | step: 253/422 | loss: 3.028916120529175\n",
      "Epoch: 23/100 | step: 254/422 | loss: 2.9815266132354736\n",
      "Epoch: 23/100 | step: 255/422 | loss: 3.1723310947418213\n",
      "Epoch: 23/100 | step: 256/422 | loss: 2.8902578353881836\n",
      "Epoch: 23/100 | step: 257/422 | loss: 3.2168784141540527\n",
      "Epoch: 23/100 | step: 258/422 | loss: 3.254638195037842\n",
      "Epoch: 23/100 | step: 259/422 | loss: 3.0261483192443848\n",
      "Epoch: 23/100 | step: 260/422 | loss: 3.297945499420166\n",
      "Epoch: 23/100 | step: 261/422 | loss: 3.0326921939849854\n",
      "Epoch: 23/100 | step: 262/422 | loss: 3.3626952171325684\n",
      "Epoch: 23/100 | step: 263/422 | loss: 3.0655977725982666\n",
      "Epoch: 23/100 | step: 264/422 | loss: 2.9086270332336426\n",
      "Epoch: 23/100 | step: 265/422 | loss: 3.239940643310547\n",
      "Epoch: 23/100 | step: 266/422 | loss: 3.329960584640503\n",
      "Epoch: 23/100 | step: 267/422 | loss: 2.820706367492676\n",
      "Epoch: 23/100 | step: 268/422 | loss: 3.320568561553955\n",
      "Epoch: 23/100 | step: 269/422 | loss: 2.9017179012298584\n",
      "Epoch: 23/100 | step: 270/422 | loss: 3.4559152126312256\n",
      "Epoch: 23/100 | step: 271/422 | loss: 3.196439504623413\n",
      "Epoch: 23/100 | step: 272/422 | loss: 3.073697090148926\n",
      "Epoch: 23/100 | step: 273/422 | loss: 3.0145034790039062\n",
      "Epoch: 23/100 | step: 274/422 | loss: 2.939575433731079\n",
      "Epoch: 23/100 | step: 275/422 | loss: 2.570892333984375\n",
      "Epoch: 23/100 | step: 276/422 | loss: 3.0297064781188965\n",
      "Epoch: 23/100 | step: 277/422 | loss: 2.8379783630371094\n",
      "Epoch: 23/100 | step: 278/422 | loss: 2.893235683441162\n",
      "Epoch: 23/100 | step: 279/422 | loss: 3.2327446937561035\n",
      "Epoch: 23/100 | step: 280/422 | loss: 3.168398380279541\n",
      "Epoch: 23/100 | step: 281/422 | loss: 2.44264554977417\n",
      "Epoch: 23/100 | step: 282/422 | loss: 3.0124502182006836\n",
      "Epoch: 23/100 | step: 283/422 | loss: 2.960191249847412\n",
      "Epoch: 23/100 | step: 284/422 | loss: 3.392131805419922\n",
      "Epoch: 23/100 | step: 285/422 | loss: 3.292907953262329\n",
      "Epoch: 23/100 | step: 286/422 | loss: 2.9199378490448\n",
      "Epoch: 23/100 | step: 287/422 | loss: 3.4131722450256348\n",
      "Epoch: 23/100 | step: 288/422 | loss: 3.7041327953338623\n",
      "Epoch: 23/100 | step: 289/422 | loss: 3.058497190475464\n",
      "Epoch: 23/100 | step: 290/422 | loss: 3.2060394287109375\n",
      "Epoch: 23/100 | step: 291/422 | loss: 3.185631036758423\n",
      "Epoch: 23/100 | step: 292/422 | loss: 2.8923566341400146\n",
      "Epoch: 23/100 | step: 293/422 | loss: 2.960704803466797\n",
      "Epoch: 23/100 | step: 294/422 | loss: 2.7018725872039795\n",
      "Epoch: 23/100 | step: 295/422 | loss: 2.809093713760376\n",
      "Epoch: 23/100 | step: 296/422 | loss: 2.5807409286499023\n",
      "Epoch: 23/100 | step: 297/422 | loss: 3.1616134643554688\n",
      "Epoch: 23/100 | step: 298/422 | loss: 3.456902503967285\n",
      "Epoch: 23/100 | step: 299/422 | loss: 2.9504613876342773\n",
      "Epoch: 23/100 | step: 300/422 | loss: 2.9217278957366943\n",
      "Epoch: 23/100 | step: 301/422 | loss: 3.0977015495300293\n",
      "Epoch: 23/100 | step: 302/422 | loss: 2.857654571533203\n",
      "Epoch: 23/100 | step: 303/422 | loss: 2.813847780227661\n",
      "Epoch: 23/100 | step: 304/422 | loss: 2.839623212814331\n",
      "Epoch: 23/100 | step: 305/422 | loss: 3.0937790870666504\n",
      "Epoch: 23/100 | step: 306/422 | loss: 2.899792194366455\n",
      "Epoch: 23/100 | step: 307/422 | loss: 3.112548828125\n",
      "Epoch: 23/100 | step: 308/422 | loss: 3.1650640964508057\n",
      "Epoch: 23/100 | step: 309/422 | loss: 3.3299293518066406\n",
      "Epoch: 23/100 | step: 310/422 | loss: 2.5889923572540283\n",
      "Epoch: 23/100 | step: 311/422 | loss: 3.2961294651031494\n",
      "Epoch: 23/100 | step: 312/422 | loss: 2.884836196899414\n",
      "Epoch: 23/100 | step: 313/422 | loss: 3.0724315643310547\n",
      "Epoch: 23/100 | step: 314/422 | loss: 2.7960188388824463\n",
      "Epoch: 23/100 | step: 315/422 | loss: 3.0223045349121094\n",
      "Epoch: 23/100 | step: 316/422 | loss: 2.554354667663574\n",
      "Epoch: 23/100 | step: 317/422 | loss: 2.6196491718292236\n",
      "Epoch: 23/100 | step: 318/422 | loss: 3.316030263900757\n",
      "Epoch: 23/100 | step: 319/422 | loss: 3.1805341243743896\n",
      "Epoch: 23/100 | step: 320/422 | loss: 3.0370700359344482\n",
      "Epoch: 23/100 | step: 321/422 | loss: 3.0617260932922363\n",
      "Epoch: 23/100 | step: 322/422 | loss: 3.289109945297241\n",
      "Epoch: 23/100 | step: 323/422 | loss: 2.79925537109375\n",
      "Epoch: 23/100 | step: 324/422 | loss: 3.372100353240967\n",
      "Epoch: 23/100 | step: 325/422 | loss: 3.2027363777160645\n",
      "Epoch: 23/100 | step: 326/422 | loss: 3.305630922317505\n",
      "Epoch: 23/100 | step: 327/422 | loss: 2.8785932064056396\n",
      "Epoch: 23/100 | step: 328/422 | loss: 3.400520086288452\n",
      "Epoch: 23/100 | step: 329/422 | loss: 2.74423885345459\n",
      "Epoch: 23/100 | step: 330/422 | loss: 3.0601019859313965\n",
      "Epoch: 23/100 | step: 331/422 | loss: 3.040271043777466\n",
      "Epoch: 23/100 | step: 332/422 | loss: 3.0496835708618164\n",
      "Epoch: 23/100 | step: 333/422 | loss: 2.8741345405578613\n",
      "Epoch: 23/100 | step: 334/422 | loss: 3.0621018409729004\n",
      "Epoch: 23/100 | step: 335/422 | loss: 2.6572463512420654\n",
      "Epoch: 23/100 | step: 336/422 | loss: 2.83209490776062\n",
      "Epoch: 23/100 | step: 337/422 | loss: 3.1571621894836426\n",
      "Epoch: 23/100 | step: 338/422 | loss: 3.17031192779541\n",
      "Epoch: 23/100 | step: 339/422 | loss: 3.343656063079834\n",
      "Epoch: 23/100 | step: 340/422 | loss: 2.9910309314727783\n",
      "Epoch: 23/100 | step: 341/422 | loss: 3.5387215614318848\n",
      "Epoch: 23/100 | step: 342/422 | loss: 3.0082671642303467\n",
      "Epoch: 23/100 | step: 343/422 | loss: 2.7635624408721924\n",
      "Epoch: 23/100 | step: 344/422 | loss: 3.2297990322113037\n",
      "Epoch: 23/100 | step: 345/422 | loss: 2.9981210231781006\n",
      "Epoch: 23/100 | step: 346/422 | loss: 2.7565178871154785\n",
      "Epoch: 23/100 | step: 347/422 | loss: 2.937098503112793\n",
      "Epoch: 23/100 | step: 348/422 | loss: 2.8768603801727295\n",
      "Epoch: 23/100 | step: 349/422 | loss: 2.9901750087738037\n",
      "Epoch: 23/100 | step: 350/422 | loss: 3.203805446624756\n",
      "Epoch: 23/100 | step: 351/422 | loss: 3.016551971435547\n",
      "Epoch: 23/100 | step: 352/422 | loss: 2.8862292766571045\n",
      "Epoch: 23/100 | step: 353/422 | loss: 3.017061948776245\n",
      "Epoch: 23/100 | step: 354/422 | loss: 3.1497390270233154\n",
      "Epoch: 23/100 | step: 355/422 | loss: 2.6121766567230225\n",
      "Epoch: 23/100 | step: 356/422 | loss: 3.4666941165924072\n",
      "Epoch: 23/100 | step: 357/422 | loss: 2.779008388519287\n",
      "Epoch: 23/100 | step: 358/422 | loss: 3.438218832015991\n",
      "Epoch: 23/100 | step: 359/422 | loss: 3.6071019172668457\n",
      "Epoch: 23/100 | step: 360/422 | loss: 2.903064489364624\n",
      "Epoch: 23/100 | step: 361/422 | loss: 2.7928736209869385\n",
      "Epoch: 23/100 | step: 362/422 | loss: 3.402768611907959\n",
      "Epoch: 23/100 | step: 363/422 | loss: 3.4290289878845215\n",
      "Epoch: 23/100 | step: 364/422 | loss: 3.2537572383880615\n",
      "Epoch: 23/100 | step: 365/422 | loss: 2.7610225677490234\n",
      "Epoch: 23/100 | step: 366/422 | loss: 3.3990907669067383\n",
      "Epoch: 23/100 | step: 367/422 | loss: 3.12728214263916\n",
      "Epoch: 23/100 | step: 368/422 | loss: 3.422470808029175\n",
      "Epoch: 23/100 | step: 369/422 | loss: 3.1931586265563965\n",
      "Epoch: 23/100 | step: 370/422 | loss: 3.268064260482788\n",
      "Epoch: 23/100 | step: 371/422 | loss: 3.1500463485717773\n",
      "Epoch: 23/100 | step: 372/422 | loss: 2.9788269996643066\n",
      "Epoch: 23/100 | step: 373/422 | loss: 2.745880603790283\n",
      "Epoch: 23/100 | step: 374/422 | loss: 3.127066135406494\n",
      "Epoch: 23/100 | step: 375/422 | loss: 3.2286219596862793\n",
      "Epoch: 23/100 | step: 376/422 | loss: 3.6630256175994873\n",
      "Epoch: 23/100 | step: 377/422 | loss: 2.555158853530884\n",
      "Epoch: 23/100 | step: 378/422 | loss: 3.0106446743011475\n",
      "Epoch: 23/100 | step: 379/422 | loss: 3.133096218109131\n",
      "Epoch: 23/100 | step: 380/422 | loss: 3.107738733291626\n",
      "Epoch: 23/100 | step: 381/422 | loss: 3.0967891216278076\n",
      "Epoch: 23/100 | step: 382/422 | loss: 2.916001558303833\n",
      "Epoch: 23/100 | step: 383/422 | loss: 2.8072617053985596\n",
      "Epoch: 23/100 | step: 384/422 | loss: 3.108757734298706\n",
      "Epoch: 23/100 | step: 385/422 | loss: 3.297119379043579\n",
      "Epoch: 23/100 | step: 386/422 | loss: 2.804433822631836\n",
      "Epoch: 23/100 | step: 387/422 | loss: 2.881831645965576\n",
      "Epoch: 23/100 | step: 388/422 | loss: 3.517853021621704\n",
      "Epoch: 23/100 | step: 389/422 | loss: 3.6803972721099854\n",
      "Epoch: 23/100 | step: 390/422 | loss: 3.063127040863037\n",
      "Epoch: 23/100 | step: 391/422 | loss: 2.8664402961730957\n",
      "Epoch: 23/100 | step: 392/422 | loss: 3.100395917892456\n",
      "Epoch: 23/100 | step: 393/422 | loss: 3.464494466781616\n",
      "Epoch: 23/100 | step: 394/422 | loss: 2.9489593505859375\n",
      "Epoch: 23/100 | step: 395/422 | loss: 2.7574071884155273\n",
      "Epoch: 23/100 | step: 396/422 | loss: 2.8187241554260254\n",
      "Epoch: 23/100 | step: 397/422 | loss: 3.307001829147339\n",
      "Epoch: 23/100 | step: 398/422 | loss: 2.7881433963775635\n",
      "Epoch: 23/100 | step: 399/422 | loss: 3.0021581649780273\n",
      "Epoch: 23/100 | step: 400/422 | loss: 2.785261869430542\n",
      "Epoch: 23/100 | step: 401/422 | loss: 2.944291591644287\n",
      "Epoch: 23/100 | step: 402/422 | loss: 3.004856586456299\n",
      "Epoch: 23/100 | step: 403/422 | loss: 2.5292863845825195\n",
      "Epoch: 23/100 | step: 404/422 | loss: 2.7473862171173096\n",
      "Epoch: 23/100 | step: 405/422 | loss: 3.2300164699554443\n",
      "Epoch: 23/100 | step: 406/422 | loss: 3.365875720977783\n",
      "Epoch: 23/100 | step: 407/422 | loss: 2.7299160957336426\n",
      "Epoch: 23/100 | step: 408/422 | loss: 3.104597568511963\n",
      "Epoch: 23/100 | step: 409/422 | loss: 3.059807777404785\n",
      "Epoch: 23/100 | step: 410/422 | loss: 2.935239553451538\n",
      "Epoch: 23/100 | step: 411/422 | loss: 3.0165770053863525\n",
      "Epoch: 23/100 | step: 412/422 | loss: 3.41804838180542\n",
      "Epoch: 23/100 | step: 413/422 | loss: 2.952895164489746\n",
      "Epoch: 23/100 | step: 414/422 | loss: 2.974329948425293\n",
      "Epoch: 23/100 | step: 415/422 | loss: 3.324429988861084\n",
      "Epoch: 23/100 | step: 416/422 | loss: 3.083662509918213\n",
      "Epoch: 23/100 | step: 417/422 | loss: 3.2914209365844727\n",
      "Epoch: 23/100 | step: 418/422 | loss: 2.9294955730438232\n",
      "Epoch: 23/100 | step: 419/422 | loss: 3.1556918621063232\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 24/100 | step: 1/422 | loss: 2.8471500873565674\n",
      "Epoch: 24/100 | step: 2/422 | loss: 2.7819058895111084\n",
      "Epoch: 24/100 | step: 3/422 | loss: 3.079740524291992\n",
      "Epoch: 24/100 | step: 4/422 | loss: 2.9309141635894775\n",
      "Epoch: 24/100 | step: 5/422 | loss: 2.2588846683502197\n",
      "Epoch: 24/100 | step: 6/422 | loss: 2.674776315689087\n",
      "Epoch: 24/100 | step: 7/422 | loss: 3.142615795135498\n",
      "Epoch: 24/100 | step: 8/422 | loss: 2.677114248275757\n",
      "Epoch: 24/100 | step: 9/422 | loss: 2.5891761779785156\n",
      "Epoch: 24/100 | step: 10/422 | loss: 3.14149808883667\n",
      "Epoch: 24/100 | step: 11/422 | loss: 2.671630382537842\n",
      "Epoch: 24/100 | step: 12/422 | loss: 2.7854864597320557\n",
      "Epoch: 24/100 | step: 13/422 | loss: 2.466486692428589\n",
      "Epoch: 24/100 | step: 14/422 | loss: 3.4348156452178955\n",
      "Epoch: 24/100 | step: 15/422 | loss: 3.098984479904175\n",
      "Epoch: 24/100 | step: 16/422 | loss: 2.6334524154663086\n",
      "Epoch: 24/100 | step: 17/422 | loss: 3.4252593517303467\n",
      "Epoch: 24/100 | step: 18/422 | loss: 2.9257724285125732\n",
      "Epoch: 24/100 | step: 19/422 | loss: 2.603959560394287\n",
      "Epoch: 24/100 | step: 20/422 | loss: 3.008204221725464\n",
      "Epoch: 24/100 | step: 21/422 | loss: 3.1401617527008057\n",
      "Epoch: 24/100 | step: 22/422 | loss: 2.9722399711608887\n",
      "Epoch: 24/100 | step: 23/422 | loss: 2.432370185852051\n",
      "Epoch: 24/100 | step: 24/422 | loss: 3.4313747882843018\n",
      "Epoch: 24/100 | step: 25/422 | loss: 3.0734565258026123\n",
      "Epoch: 24/100 | step: 26/422 | loss: 3.1211302280426025\n",
      "Epoch: 24/100 | step: 27/422 | loss: 3.20700740814209\n",
      "Epoch: 24/100 | step: 28/422 | loss: 3.398587942123413\n",
      "Epoch: 24/100 | step: 29/422 | loss: 2.6033692359924316\n",
      "Epoch: 24/100 | step: 30/422 | loss: 2.7854957580566406\n",
      "Epoch: 24/100 | step: 31/422 | loss: 2.5194919109344482\n",
      "Epoch: 24/100 | step: 32/422 | loss: 2.8471732139587402\n",
      "Epoch: 24/100 | step: 33/422 | loss: 2.720106601715088\n",
      "Epoch: 24/100 | step: 34/422 | loss: 2.574636459350586\n",
      "Epoch: 24/100 | step: 35/422 | loss: 3.114844799041748\n",
      "Epoch: 24/100 | step: 36/422 | loss: 3.1269872188568115\n",
      "Epoch: 24/100 | step: 37/422 | loss: 2.52262806892395\n",
      "Epoch: 24/100 | step: 38/422 | loss: 2.6782782077789307\n",
      "Epoch: 24/100 | step: 39/422 | loss: 3.1196367740631104\n",
      "Epoch: 24/100 | step: 40/422 | loss: 2.7192025184631348\n",
      "Epoch: 24/100 | step: 41/422 | loss: 2.759040594100952\n",
      "Epoch: 24/100 | step: 42/422 | loss: 3.0289087295532227\n",
      "Epoch: 24/100 | step: 43/422 | loss: 2.892638683319092\n",
      "Epoch: 24/100 | step: 44/422 | loss: 3.123842239379883\n",
      "Epoch: 24/100 | step: 45/422 | loss: 2.701953172683716\n",
      "Epoch: 24/100 | step: 46/422 | loss: 2.9236502647399902\n",
      "Epoch: 24/100 | step: 47/422 | loss: 2.7476820945739746\n",
      "Epoch: 24/100 | step: 48/422 | loss: 3.1220104694366455\n",
      "Epoch: 24/100 | step: 49/422 | loss: 2.436378240585327\n",
      "Epoch: 24/100 | step: 50/422 | loss: 3.4912970066070557\n",
      "Epoch: 24/100 | step: 51/422 | loss: 2.9654557704925537\n",
      "Epoch: 24/100 | step: 52/422 | loss: 3.1238067150115967\n",
      "Epoch: 24/100 | step: 53/422 | loss: 2.648756742477417\n",
      "Epoch: 24/100 | step: 54/422 | loss: 2.6940858364105225\n",
      "Epoch: 24/100 | step: 55/422 | loss: 2.7327682971954346\n",
      "Epoch: 24/100 | step: 56/422 | loss: 3.651627540588379\n",
      "Epoch: 24/100 | step: 57/422 | loss: 2.722428321838379\n",
      "Epoch: 24/100 | step: 58/422 | loss: 3.1006197929382324\n",
      "Epoch: 24/100 | step: 59/422 | loss: 2.40921688079834\n",
      "Epoch: 24/100 | step: 60/422 | loss: 3.265110969543457\n",
      "Epoch: 24/100 | step: 61/422 | loss: 3.0416998863220215\n",
      "Epoch: 24/100 | step: 62/422 | loss: 2.9597744941711426\n",
      "Epoch: 24/100 | step: 63/422 | loss: 2.8384206295013428\n",
      "Epoch: 24/100 | step: 64/422 | loss: 2.987556219100952\n",
      "Epoch: 24/100 | step: 65/422 | loss: 2.8106212615966797\n",
      "Epoch: 24/100 | step: 66/422 | loss: 2.97283935546875\n",
      "Epoch: 24/100 | step: 67/422 | loss: 2.9072091579437256\n",
      "Epoch: 24/100 | step: 68/422 | loss: 2.835779905319214\n",
      "Epoch: 24/100 | step: 69/422 | loss: 2.71976375579834\n",
      "Epoch: 24/100 | step: 70/422 | loss: 2.533519983291626\n",
      "Epoch: 24/100 | step: 71/422 | loss: 2.428666353225708\n",
      "Epoch: 24/100 | step: 72/422 | loss: 2.244154930114746\n",
      "Epoch: 24/100 | step: 73/422 | loss: 2.722935914993286\n",
      "Epoch: 24/100 | step: 74/422 | loss: 3.105445623397827\n",
      "Epoch: 24/100 | step: 75/422 | loss: 3.3051280975341797\n",
      "Epoch: 24/100 | step: 76/422 | loss: 2.88191556930542\n",
      "Epoch: 24/100 | step: 77/422 | loss: 2.9699504375457764\n",
      "Epoch: 24/100 | step: 78/422 | loss: 2.572842597961426\n",
      "Epoch: 24/100 | step: 79/422 | loss: 2.770892381668091\n",
      "Epoch: 24/100 | step: 80/422 | loss: 3.131321907043457\n",
      "Epoch: 24/100 | step: 81/422 | loss: 3.284026622772217\n",
      "Epoch: 24/100 | step: 82/422 | loss: 2.6973750591278076\n",
      "Epoch: 24/100 | step: 83/422 | loss: 2.731391191482544\n",
      "Epoch: 24/100 | step: 84/422 | loss: 3.025242328643799\n",
      "Epoch: 24/100 | step: 85/422 | loss: 3.134734630584717\n",
      "Epoch: 24/100 | step: 86/422 | loss: 3.344433307647705\n",
      "Epoch: 24/100 | step: 87/422 | loss: 3.1812851428985596\n",
      "Epoch: 24/100 | step: 88/422 | loss: 2.685886859893799\n",
      "Epoch: 24/100 | step: 89/422 | loss: 2.892958641052246\n",
      "Epoch: 24/100 | step: 90/422 | loss: 3.4620940685272217\n",
      "Epoch: 24/100 | step: 91/422 | loss: 2.5611793994903564\n",
      "Epoch: 24/100 | step: 92/422 | loss: 2.8546290397644043\n",
      "Epoch: 24/100 | step: 93/422 | loss: 2.240948438644409\n",
      "Epoch: 24/100 | step: 94/422 | loss: 3.145474672317505\n",
      "Epoch: 24/100 | step: 95/422 | loss: 3.226264238357544\n",
      "Epoch: 24/100 | step: 96/422 | loss: 2.7003660202026367\n",
      "Epoch: 24/100 | step: 97/422 | loss: 2.8232791423797607\n",
      "Epoch: 24/100 | step: 98/422 | loss: 2.551276445388794\n",
      "Epoch: 24/100 | step: 99/422 | loss: 3.024733543395996\n",
      "Epoch: 24/100 | step: 100/422 | loss: 3.2960708141326904\n",
      "Epoch: 24/100 | step: 101/422 | loss: 3.0369672775268555\n",
      "Epoch: 24/100 | step: 102/422 | loss: 2.977607250213623\n",
      "Epoch: 24/100 | step: 103/422 | loss: 2.755394458770752\n",
      "Epoch: 24/100 | step: 104/422 | loss: 3.023362874984741\n",
      "Epoch: 24/100 | step: 105/422 | loss: 3.3484554290771484\n",
      "Epoch: 24/100 | step: 106/422 | loss: 3.0664620399475098\n",
      "Epoch: 24/100 | step: 107/422 | loss: 2.6816513538360596\n",
      "Epoch: 24/100 | step: 108/422 | loss: 2.9833061695098877\n",
      "Epoch: 24/100 | step: 109/422 | loss: 2.997037887573242\n",
      "Epoch: 24/100 | step: 110/422 | loss: 2.283918857574463\n",
      "Epoch: 24/100 | step: 111/422 | loss: 2.9754767417907715\n",
      "Epoch: 24/100 | step: 112/422 | loss: 2.9051883220672607\n",
      "Epoch: 24/100 | step: 113/422 | loss: 2.8860554695129395\n",
      "Epoch: 24/100 | step: 114/422 | loss: 3.222693681716919\n",
      "Epoch: 24/100 | step: 115/422 | loss: 2.8357417583465576\n",
      "Epoch: 24/100 | step: 116/422 | loss: 3.1514785289764404\n",
      "Epoch: 24/100 | step: 117/422 | loss: 2.9818942546844482\n",
      "Epoch: 24/100 | step: 118/422 | loss: 2.980834722518921\n",
      "Epoch: 24/100 | step: 119/422 | loss: 2.968477964401245\n",
      "Epoch: 24/100 | step: 120/422 | loss: 2.7959210872650146\n",
      "Epoch: 24/100 | step: 121/422 | loss: 3.2461113929748535\n",
      "Epoch: 24/100 | step: 122/422 | loss: 2.904665231704712\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 25/100 | step: 1/422 | loss: 3.042882204055786\n",
      "Epoch: 25/100 | step: 2/422 | loss: 3.021981954574585\n",
      "Epoch: 25/100 | step: 3/422 | loss: 2.631253242492676\n",
      "Epoch: 25/100 | step: 4/422 | loss: 2.992269277572632\n",
      "Epoch: 25/100 | step: 5/422 | loss: 2.678507089614868\n",
      "Epoch: 25/100 | step: 6/422 | loss: 2.56391978263855\n",
      "Epoch: 25/100 | step: 7/422 | loss: 3.097999334335327\n",
      "Epoch: 25/100 | step: 8/422 | loss: 2.807075262069702\n",
      "Epoch: 25/100 | step: 9/422 | loss: 3.107442617416382\n",
      "Epoch: 25/100 | step: 10/422 | loss: 2.4521069526672363\n",
      "Epoch: 25/100 | step: 11/422 | loss: 2.5946149826049805\n",
      "Epoch: 25/100 | step: 12/422 | loss: 2.7648067474365234\n",
      "Epoch: 25/100 | step: 13/422 | loss: 2.2394680976867676\n",
      "Epoch: 25/100 | step: 14/422 | loss: 2.98525333404541\n",
      "Epoch: 25/100 | step: 15/422 | loss: 2.630445957183838\n",
      "Epoch: 25/100 | step: 16/422 | loss: 2.7709898948669434\n",
      "Epoch: 25/100 | step: 17/422 | loss: 2.965178966522217\n",
      "Epoch: 25/100 | step: 18/422 | loss: 3.1017353534698486\n",
      "Epoch: 25/100 | step: 19/422 | loss: 2.8148791790008545\n",
      "Epoch: 25/100 | step: 20/422 | loss: 2.833301544189453\n",
      "Epoch: 25/100 | step: 21/422 | loss: 2.5103189945220947\n",
      "Epoch: 25/100 | step: 22/422 | loss: 3.1207942962646484\n",
      "Epoch: 25/100 | step: 23/422 | loss: 2.7248950004577637\n",
      "Epoch: 25/100 | step: 24/422 | loss: 3.0064597129821777\n",
      "Epoch: 25/100 | step: 25/422 | loss: 2.716747999191284\n",
      "Epoch: 25/100 | step: 26/422 | loss: 3.077904462814331\n",
      "Epoch: 25/100 | step: 27/422 | loss: 2.9798362255096436\n",
      "Epoch: 25/100 | step: 28/422 | loss: 3.1074562072753906\n",
      "Epoch: 25/100 | step: 29/422 | loss: 2.6692230701446533\n",
      "Epoch: 25/100 | step: 30/422 | loss: 2.989584445953369\n",
      "Epoch: 25/100 | step: 31/422 | loss: 2.8730409145355225\n",
      "Epoch: 25/100 | step: 32/422 | loss: 2.979243278503418\n",
      "Epoch: 25/100 | step: 33/422 | loss: 2.8051373958587646\n",
      "Epoch: 25/100 | step: 34/422 | loss: 3.2163901329040527\n",
      "Epoch: 25/100 | step: 35/422 | loss: 2.9762659072875977\n",
      "Epoch: 25/100 | step: 36/422 | loss: 3.0738863945007324\n",
      "Epoch: 25/100 | step: 37/422 | loss: 2.6660521030426025\n",
      "Epoch: 25/100 | step: 38/422 | loss: 2.655179500579834\n",
      "Epoch: 25/100 | step: 39/422 | loss: 2.8352162837982178\n",
      "Epoch: 25/100 | step: 40/422 | loss: 2.947819232940674\n",
      "Epoch: 25/100 | step: 41/422 | loss: 2.7032992839813232\n",
      "Epoch: 25/100 | step: 42/422 | loss: 3.0693233013153076\n",
      "Epoch: 25/100 | step: 43/422 | loss: 3.0756773948669434\n",
      "Epoch: 25/100 | step: 44/422 | loss: 3.096513271331787\n",
      "Epoch: 25/100 | step: 45/422 | loss: 2.6400656700134277\n",
      "Epoch: 25/100 | step: 46/422 | loss: 2.7641079425811768\n",
      "Epoch: 25/100 | step: 47/422 | loss: 2.962306261062622\n",
      "Epoch: 25/100 | step: 48/422 | loss: 3.1171345710754395\n",
      "Epoch: 25/100 | step: 49/422 | loss: 2.736753225326538\n",
      "Epoch: 25/100 | step: 50/422 | loss: 2.6789114475250244\n",
      "Epoch: 25/100 | step: 51/422 | loss: 3.3043360710144043\n",
      "Epoch: 25/100 | step: 52/422 | loss: 2.6448376178741455\n",
      "Epoch: 25/100 | step: 53/422 | loss: 2.7205631732940674\n",
      "Epoch: 25/100 | step: 54/422 | loss: 3.6676862239837646\n",
      "Epoch: 25/100 | step: 55/422 | loss: 3.2342002391815186\n",
      "Epoch: 25/100 | step: 56/422 | loss: 2.895493984222412\n",
      "Epoch: 25/100 | step: 57/422 | loss: 2.9181551933288574\n",
      "Epoch: 25/100 | step: 58/422 | loss: 3.2453157901763916\n",
      "Epoch: 25/100 | step: 59/422 | loss: 2.5277490615844727\n",
      "Epoch: 25/100 | step: 60/422 | loss: 3.048215866088867\n",
      "Epoch: 25/100 | step: 61/422 | loss: 3.377072811126709\n",
      "Epoch: 25/100 | step: 62/422 | loss: 2.655629873275757\n",
      "Epoch: 25/100 | step: 63/422 | loss: 2.7591018676757812\n",
      "Epoch: 25/100 | step: 64/422 | loss: 2.910982131958008\n",
      "Epoch: 25/100 | step: 65/422 | loss: 2.572542190551758\n",
      "Epoch: 25/100 | step: 66/422 | loss: 3.0668840408325195\n",
      "Epoch: 25/100 | step: 67/422 | loss: 2.541372299194336\n",
      "Epoch: 25/100 | step: 68/422 | loss: 3.032583713531494\n",
      "Epoch: 25/100 | step: 69/422 | loss: 2.8205552101135254\n",
      "Epoch: 25/100 | step: 70/422 | loss: 2.9779746532440186\n",
      "Epoch: 25/100 | step: 71/422 | loss: 2.5092363357543945\n",
      "Epoch: 25/100 | step: 72/422 | loss: 2.977860689163208\n",
      "Epoch: 25/100 | step: 73/422 | loss: 3.2933905124664307\n",
      "Epoch: 25/100 | step: 74/422 | loss: 2.974976062774658\n",
      "Epoch: 25/100 | step: 75/422 | loss: 2.6911792755126953\n",
      "Epoch: 25/100 | step: 76/422 | loss: 3.092348337173462\n",
      "Epoch: 25/100 | step: 77/422 | loss: 3.1605911254882812\n",
      "Epoch: 25/100 | step: 78/422 | loss: 3.3085451126098633\n",
      "Epoch: 25/100 | step: 79/422 | loss: 3.0781164169311523\n",
      "Epoch: 25/100 | step: 80/422 | loss: 3.3227553367614746\n",
      "Epoch: 25/100 | step: 81/422 | loss: 2.7469921112060547\n",
      "Epoch: 25/100 | step: 82/422 | loss: 2.5723628997802734\n",
      "Epoch: 25/100 | step: 83/422 | loss: 2.863267421722412\n",
      "Epoch: 25/100 | step: 84/422 | loss: 2.676124334335327\n",
      "Epoch: 25/100 | step: 85/422 | loss: 3.215237855911255\n",
      "Epoch: 25/100 | step: 86/422 | loss: 2.9764060974121094\n",
      "Epoch: 25/100 | step: 87/422 | loss: 2.6500651836395264\n",
      "Epoch: 25/100 | step: 88/422 | loss: 3.185508966445923\n",
      "Epoch: 25/100 | step: 89/422 | loss: 2.7164394855499268\n",
      "Epoch: 25/100 | step: 90/422 | loss: 2.7706093788146973\n",
      "Epoch: 25/100 | step: 91/422 | loss: 2.9561192989349365\n",
      "Epoch: 25/100 | step: 92/422 | loss: 2.834843158721924\n",
      "Epoch: 25/100 | step: 93/422 | loss: 3.0376973152160645\n",
      "Epoch: 25/100 | step: 94/422 | loss: 2.8374011516571045\n",
      "Epoch: 25/100 | step: 95/422 | loss: 2.8448619842529297\n",
      "Epoch: 25/100 | step: 96/422 | loss: 3.225314140319824\n",
      "Epoch: 25/100 | step: 97/422 | loss: 2.608557939529419\n",
      "Epoch: 25/100 | step: 98/422 | loss: 3.1062066555023193\n",
      "Epoch: 25/100 | step: 99/422 | loss: 2.860135316848755\n",
      "Epoch: 25/100 | step: 100/422 | loss: 3.149507522583008\n",
      "Epoch: 25/100 | step: 101/422 | loss: 2.8830413818359375\n",
      "Epoch: 25/100 | step: 102/422 | loss: 2.3849503993988037\n",
      "Epoch: 25/100 | step: 103/422 | loss: 2.947791814804077\n",
      "Epoch: 25/100 | step: 104/422 | loss: 3.1216073036193848\n",
      "Epoch: 25/100 | step: 105/422 | loss: 2.716761589050293\n",
      "Epoch: 25/100 | step: 106/422 | loss: 2.561497688293457\n",
      "Epoch: 25/100 | step: 107/422 | loss: 3.1185996532440186\n",
      "Epoch: 25/100 | step: 108/422 | loss: 2.931550979614258\n",
      "Epoch: 25/100 | step: 109/422 | loss: 2.645059823989868\n",
      "Epoch: 25/100 | step: 110/422 | loss: 2.6923859119415283\n",
      "Epoch: 25/100 | step: 111/422 | loss: 2.858170986175537\n",
      "Epoch: 25/100 | step: 112/422 | loss: 2.6368489265441895\n",
      "Epoch: 25/100 | step: 113/422 | loss: 2.887477159500122\n",
      "Epoch: 25/100 | step: 114/422 | loss: 2.6056559085845947\n",
      "Epoch: 25/100 | step: 115/422 | loss: 2.952727794647217\n",
      "Epoch: 25/100 | step: 116/422 | loss: 2.968562364578247\n",
      "Epoch: 25/100 | step: 117/422 | loss: 3.379973888397217\n",
      "Epoch: 25/100 | step: 118/422 | loss: 2.830315113067627\n",
      "Epoch: 25/100 | step: 119/422 | loss: 2.787771224975586\n",
      "Epoch: 25/100 | step: 120/422 | loss: 2.5202877521514893\n",
      "Epoch: 25/100 | step: 121/422 | loss: 2.874307870864868\n",
      "Epoch: 25/100 | step: 122/422 | loss: 3.130669355392456\n",
      "Epoch: 25/100 | step: 123/422 | loss: 3.1129496097564697\n",
      "Epoch: 25/100 | step: 124/422 | loss: 2.7203145027160645\n",
      "Epoch: 25/100 | step: 125/422 | loss: 3.2273201942443848\n",
      "Epoch: 25/100 | step: 126/422 | loss: 2.649177312850952\n",
      "Epoch: 25/100 | step: 127/422 | loss: 3.0946950912475586\n",
      "Epoch: 25/100 | step: 128/422 | loss: 3.091231107711792\n",
      "Epoch: 25/100 | step: 129/422 | loss: 2.9627606868743896\n",
      "Epoch: 25/100 | step: 130/422 | loss: 2.986889362335205\n",
      "Epoch: 25/100 | step: 131/422 | loss: 3.236327886581421\n",
      "Epoch: 25/100 | step: 132/422 | loss: 2.925311326980591\n",
      "Epoch: 25/100 | step: 133/422 | loss: 3.369636058807373\n",
      "Epoch: 25/100 | step: 134/422 | loss: 2.8513455390930176\n",
      "Epoch: 25/100 | step: 135/422 | loss: 2.8949620723724365\n",
      "Epoch: 25/100 | step: 136/422 | loss: 2.979301929473877\n",
      "Epoch: 25/100 | step: 137/422 | loss: 2.71773099899292\n",
      "Epoch: 25/100 | step: 138/422 | loss: 2.4660348892211914\n",
      "Epoch: 25/100 | step: 139/422 | loss: 3.2575573921203613\n",
      "Epoch: 25/100 | step: 140/422 | loss: 2.948082208633423\n",
      "Epoch: 25/100 | step: 141/422 | loss: 2.6368541717529297\n",
      "Epoch: 25/100 | step: 142/422 | loss: 2.8940255641937256\n",
      "Epoch: 25/100 | step: 143/422 | loss: 2.83062744140625\n",
      "Epoch: 25/100 | step: 144/422 | loss: 3.3689916133880615\n",
      "Epoch: 25/100 | step: 145/422 | loss: 3.0832223892211914\n",
      "Epoch: 25/100 | step: 146/422 | loss: 3.10276460647583\n",
      "Epoch: 25/100 | step: 147/422 | loss: 3.0005650520324707\n",
      "Epoch: 25/100 | step: 148/422 | loss: 3.5802712440490723\n",
      "Epoch: 25/100 | step: 149/422 | loss: 2.0469274520874023\n",
      "Epoch: 25/100 | step: 150/422 | loss: 2.506246328353882\n",
      "Epoch: 25/100 | step: 151/422 | loss: 2.7870585918426514\n",
      "Epoch: 25/100 | step: 152/422 | loss: 2.7192909717559814\n",
      "Epoch: 25/100 | step: 153/422 | loss: 2.724865436553955\n",
      "Epoch: 25/100 | step: 154/422 | loss: 2.9824907779693604\n",
      "Epoch: 25/100 | step: 155/422 | loss: 2.6750991344451904\n",
      "Epoch: 25/100 | step: 156/422 | loss: 3.0631461143493652\n",
      "Epoch: 25/100 | step: 157/422 | loss: 2.9221742153167725\n",
      "Epoch: 25/100 | step: 158/422 | loss: 3.3058784008026123\n",
      "Epoch: 25/100 | step: 159/422 | loss: 2.9226438999176025\n",
      "Epoch: 25/100 | step: 160/422 | loss: 2.809577465057373\n",
      "Epoch: 25/100 | step: 161/422 | loss: 2.900914192199707\n",
      "Epoch: 25/100 | step: 162/422 | loss: 2.7151968479156494\n",
      "Epoch: 25/100 | step: 163/422 | loss: 2.96882700920105\n",
      "Epoch: 25/100 | step: 164/422 | loss: 3.0545947551727295\n",
      "Epoch: 25/100 | step: 165/422 | loss: 3.0188088417053223\n",
      "Epoch: 25/100 | step: 166/422 | loss: 2.5692341327667236\n",
      "Epoch: 25/100 | step: 167/422 | loss: 3.2630646228790283\n",
      "Epoch: 25/100 | step: 168/422 | loss: 2.7772860527038574\n",
      "Epoch: 25/100 | step: 169/422 | loss: 2.839935541152954\n",
      "Epoch: 25/100 | step: 170/422 | loss: 2.5436506271362305\n",
      "Epoch: 25/100 | step: 171/422 | loss: 2.947725296020508\n",
      "Epoch: 25/100 | step: 172/422 | loss: 3.1599433422088623\n",
      "Epoch: 25/100 | step: 173/422 | loss: 3.016071319580078\n",
      "Epoch: 25/100 | step: 174/422 | loss: 3.235384941101074\n",
      "Epoch: 25/100 | step: 175/422 | loss: 2.9924304485321045\n",
      "Epoch: 25/100 | step: 176/422 | loss: 3.403377056121826\n",
      "Epoch: 25/100 | step: 177/422 | loss: 3.1164803504943848\n",
      "Epoch: 25/100 | step: 178/422 | loss: 2.690309762954712\n",
      "Epoch: 25/100 | step: 179/422 | loss: 2.3756775856018066\n",
      "Epoch: 25/100 | step: 180/422 | loss: 2.660593271255493\n",
      "Epoch: 25/100 | step: 181/422 | loss: 3.2749557495117188\n",
      "Epoch: 25/100 | step: 182/422 | loss: 2.55255389213562\n",
      "Epoch: 25/100 | step: 183/422 | loss: 2.324188470840454\n",
      "Epoch: 25/100 | step: 184/422 | loss: 2.624990940093994\n",
      "Epoch: 25/100 | step: 185/422 | loss: 3.1327788829803467\n",
      "Epoch: 25/100 | step: 186/422 | loss: 2.934419870376587\n",
      "Epoch: 25/100 | step: 187/422 | loss: 2.8747074604034424\n",
      "Epoch: 25/100 | step: 188/422 | loss: 2.814448356628418\n",
      "Epoch: 25/100 | step: 189/422 | loss: 3.4558773040771484\n",
      "Epoch: 25/100 | step: 190/422 | loss: 2.6038098335266113\n",
      "Epoch: 25/100 | step: 191/422 | loss: 3.3880012035369873\n",
      "Epoch: 25/100 | step: 192/422 | loss: 2.580040693283081\n",
      "Epoch: 25/100 | step: 193/422 | loss: 2.928767442703247\n",
      "Epoch: 25/100 | step: 194/422 | loss: 2.706608533859253\n",
      "Epoch: 25/100 | step: 195/422 | loss: 3.6523075103759766\n",
      "Epoch: 25/100 | step: 196/422 | loss: 3.237630844116211\n",
      "Epoch: 25/100 | step: 197/422 | loss: 2.924576759338379\n",
      "Epoch: 25/100 | step: 198/422 | loss: 2.686246395111084\n",
      "Epoch: 25/100 | step: 199/422 | loss: 2.6179604530334473\n",
      "Epoch: 25/100 | step: 200/422 | loss: 2.7466025352478027\n",
      "Epoch: 25/100 | step: 201/422 | loss: 2.9623403549194336\n",
      "Epoch: 25/100 | step: 202/422 | loss: 2.973775863647461\n",
      "Epoch: 25/100 | step: 203/422 | loss: 2.981928825378418\n",
      "Epoch: 25/100 | step: 204/422 | loss: 2.883695602416992\n",
      "Epoch: 25/100 | step: 205/422 | loss: 3.0820958614349365\n",
      "Epoch: 25/100 | step: 206/422 | loss: 2.6673977375030518\n",
      "Epoch: 25/100 | step: 207/422 | loss: 2.804900884628296\n",
      "Epoch: 25/100 | step: 208/422 | loss: 2.281698703765869\n",
      "Epoch: 25/100 | step: 209/422 | loss: 2.998070478439331\n",
      "Epoch: 25/100 | step: 210/422 | loss: 3.171276807785034\n",
      "Epoch: 25/100 | step: 211/422 | loss: 2.864706039428711\n",
      "Epoch: 25/100 | step: 212/422 | loss: 2.884958505630493\n",
      "Epoch: 25/100 | step: 213/422 | loss: 2.9108288288116455\n",
      "Epoch: 25/100 | step: 214/422 | loss: 2.840691089630127\n",
      "Epoch: 25/100 | step: 215/422 | loss: 2.461717367172241\n",
      "Epoch: 25/100 | step: 216/422 | loss: 2.472360372543335\n",
      "Epoch: 25/100 | step: 217/422 | loss: 2.537583589553833\n",
      "Epoch: 25/100 | step: 218/422 | loss: 3.3426151275634766\n",
      "Epoch: 25/100 | step: 219/422 | loss: 2.7047929763793945\n",
      "Epoch: 25/100 | step: 220/422 | loss: 2.4306375980377197\n",
      "Epoch: 25/100 | step: 221/422 | loss: 2.8153209686279297\n",
      "Epoch: 25/100 | step: 222/422 | loss: 2.6081438064575195\n",
      "Epoch: 25/100 | step: 223/422 | loss: 2.863720655441284\n",
      "Epoch: 25/100 | step: 224/422 | loss: 3.090968608856201\n",
      "Epoch: 25/100 | step: 225/422 | loss: 3.3548600673675537\n",
      "Epoch: 25/100 | step: 226/422 | loss: 3.023306131362915\n",
      "Epoch: 25/100 | step: 227/422 | loss: 3.345798969268799\n",
      "Epoch: 25/100 | step: 228/422 | loss: 3.5279741287231445\n",
      "Epoch: 25/100 | step: 229/422 | loss: 2.841602325439453\n",
      "Epoch: 25/100 | step: 230/422 | loss: 3.0428273677825928\n",
      "Epoch: 25/100 | step: 231/422 | loss: 2.863086223602295\n",
      "Epoch: 25/100 | step: 232/422 | loss: 2.4928231239318848\n",
      "Epoch: 25/100 | step: 233/422 | loss: 2.741730213165283\n",
      "Epoch: 25/100 | step: 234/422 | loss: 3.0840330123901367\n",
      "Epoch: 25/100 | step: 235/422 | loss: 3.2877743244171143\n",
      "Epoch: 25/100 | step: 236/422 | loss: 2.476985454559326\n",
      "Epoch: 25/100 | step: 237/422 | loss: 3.3144173622131348\n",
      "Epoch: 25/100 | step: 238/422 | loss: 2.9417030811309814\n",
      "Epoch: 25/100 | step: 239/422 | loss: 3.11702823638916\n",
      "Epoch: 25/100 | step: 240/422 | loss: 2.7851951122283936\n",
      "Epoch: 25/100 | step: 241/422 | loss: 2.9599342346191406\n",
      "Epoch: 25/100 | step: 242/422 | loss: 3.0641937255859375\n",
      "Epoch: 25/100 | step: 243/422 | loss: 2.999547243118286\n",
      "Epoch: 25/100 | step: 244/422 | loss: 2.5996744632720947\n",
      "Epoch: 25/100 | step: 245/422 | loss: 2.6867928504943848\n",
      "Epoch: 25/100 | step: 246/422 | loss: 3.032574415206909\n",
      "Epoch: 25/100 | step: 247/422 | loss: 2.7398455142974854\n",
      "Epoch: 25/100 | step: 248/422 | loss: 2.6442489624023438\n",
      "Epoch: 25/100 | step: 249/422 | loss: 2.536771774291992\n",
      "Epoch: 25/100 | step: 250/422 | loss: 3.0125551223754883\n",
      "Epoch: 25/100 | step: 251/422 | loss: 2.7275779247283936\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 26/100 | step: 1/422 | loss: 3.0219202041625977\n",
      "Epoch: 26/100 | step: 2/422 | loss: 2.3972039222717285\n",
      "Epoch: 26/100 | step: 3/422 | loss: 3.3173739910125732\n",
      "Epoch: 26/100 | step: 4/422 | loss: 3.2062814235687256\n",
      "Epoch: 26/100 | step: 5/422 | loss: 3.0501630306243896\n",
      "Epoch: 26/100 | step: 6/422 | loss: 3.0256848335266113\n",
      "Epoch: 26/100 | step: 7/422 | loss: 2.4795782566070557\n",
      "Epoch: 26/100 | step: 8/422 | loss: 2.872602939605713\n",
      "Epoch: 26/100 | step: 9/422 | loss: 3.047459125518799\n",
      "Epoch: 26/100 | step: 10/422 | loss: 3.2434163093566895\n",
      "Epoch: 26/100 | step: 11/422 | loss: 2.6816439628601074\n",
      "Epoch: 26/100 | step: 12/422 | loss: 3.1138594150543213\n",
      "Epoch: 26/100 | step: 13/422 | loss: 2.632162570953369\n",
      "Epoch: 26/100 | step: 14/422 | loss: 2.6690733432769775\n",
      "Epoch: 26/100 | step: 15/422 | loss: 2.430912971496582\n",
      "Epoch: 26/100 | step: 16/422 | loss: 2.6153290271759033\n",
      "Epoch: 26/100 | step: 17/422 | loss: 2.9217045307159424\n",
      "Epoch: 26/100 | step: 18/422 | loss: 3.05348539352417\n",
      "Epoch: 26/100 | step: 19/422 | loss: 2.6181559562683105\n",
      "Epoch: 26/100 | step: 20/422 | loss: 3.145728349685669\n",
      "Epoch: 26/100 | step: 21/422 | loss: 2.2832188606262207\n",
      "Epoch: 26/100 | step: 22/422 | loss: 3.3372342586517334\n",
      "Epoch: 26/100 | step: 23/422 | loss: 2.9827523231506348\n",
      "Epoch: 26/100 | step: 24/422 | loss: 2.6386756896972656\n",
      "Epoch: 26/100 | step: 25/422 | loss: 3.1641845703125\n",
      "Epoch: 26/100 | step: 26/422 | loss: 2.2777719497680664\n",
      "Epoch: 26/100 | step: 27/422 | loss: 3.2265539169311523\n",
      "Epoch: 26/100 | step: 28/422 | loss: 2.897113084793091\n",
      "Epoch: 26/100 | step: 29/422 | loss: 2.682267904281616\n",
      "Epoch: 26/100 | step: 30/422 | loss: 2.837730884552002\n",
      "Epoch: 26/100 | step: 31/422 | loss: 2.719968795776367\n",
      "Epoch: 26/100 | step: 32/422 | loss: 2.9861533641815186\n",
      "Epoch: 26/100 | step: 33/422 | loss: 2.4506397247314453\n",
      "Epoch: 26/100 | step: 34/422 | loss: 2.827364921569824\n",
      "Epoch: 26/100 | step: 35/422 | loss: 3.1503398418426514\n",
      "Epoch: 26/100 | step: 36/422 | loss: 2.796936511993408\n",
      "Epoch: 26/100 | step: 37/422 | loss: 2.701037883758545\n",
      "Epoch: 26/100 | step: 38/422 | loss: 2.689858913421631\n",
      "Epoch: 26/100 | step: 39/422 | loss: 2.8457376956939697\n",
      "Epoch: 26/100 | step: 40/422 | loss: 2.367400646209717\n",
      "Epoch: 26/100 | step: 41/422 | loss: 2.7587125301361084\n",
      "Epoch: 26/100 | step: 42/422 | loss: 1.8465510606765747\n",
      "Epoch: 26/100 | step: 43/422 | loss: 2.596524715423584\n",
      "Epoch: 26/100 | step: 44/422 | loss: 2.586183786392212\n",
      "Epoch: 26/100 | step: 45/422 | loss: 2.6202139854431152\n",
      "Epoch: 26/100 | step: 46/422 | loss: 3.0126118659973145\n",
      "Epoch: 26/100 | step: 47/422 | loss: 2.433204412460327\n",
      "Epoch: 26/100 | step: 48/422 | loss: 2.7445313930511475\n",
      "Epoch: 26/100 | step: 49/422 | loss: 2.7494466304779053\n",
      "Epoch: 26/100 | step: 50/422 | loss: 2.6821250915527344\n",
      "Epoch: 26/100 | step: 51/422 | loss: 3.0193355083465576\n",
      "Epoch: 26/100 | step: 52/422 | loss: 3.141960382461548\n",
      "Epoch: 26/100 | step: 53/422 | loss: 2.365269660949707\n",
      "Epoch: 26/100 | step: 54/422 | loss: 3.139763116836548\n",
      "Epoch: 26/100 | step: 55/422 | loss: 2.633326530456543\n",
      "Epoch: 26/100 | step: 56/422 | loss: 2.990344762802124\n",
      "Epoch: 26/100 | step: 57/422 | loss: 2.330113410949707\n",
      "Epoch: 26/100 | step: 58/422 | loss: 2.8191566467285156\n",
      "Epoch: 26/100 | step: 59/422 | loss: 2.52488112449646\n",
      "Epoch: 26/100 | step: 60/422 | loss: 2.4467039108276367\n",
      "Epoch: 26/100 | step: 61/422 | loss: 3.234591007232666\n",
      "Epoch: 26/100 | step: 62/422 | loss: 2.4842758178710938\n",
      "Epoch: 26/100 | step: 63/422 | loss: 2.9640865325927734\n",
      "Epoch: 26/100 | step: 64/422 | loss: 2.6916372776031494\n",
      "Epoch: 26/100 | step: 65/422 | loss: 2.6777894496917725\n",
      "Epoch: 26/100 | step: 66/422 | loss: 3.0896193981170654\n",
      "Epoch: 26/100 | step: 67/422 | loss: 3.1077117919921875\n",
      "Epoch: 26/100 | step: 68/422 | loss: 3.025495767593384\n",
      "Epoch: 26/100 | step: 69/422 | loss: 2.8884806632995605\n",
      "Epoch: 26/100 | step: 70/422 | loss: 3.0665464401245117\n",
      "Epoch: 26/100 | step: 71/422 | loss: 2.236417531967163\n",
      "Epoch: 26/100 | step: 72/422 | loss: 3.066911458969116\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 27/100 | step: 1/422 | loss: 2.8389675617218018\n",
      "Epoch: 27/100 | step: 2/422 | loss: 2.460498809814453\n",
      "Epoch: 27/100 | step: 3/422 | loss: 2.206550121307373\n",
      "Epoch: 27/100 | step: 4/422 | loss: 2.7897896766662598\n",
      "Epoch: 27/100 | step: 5/422 | loss: 2.5631983280181885\n",
      "Epoch: 27/100 | step: 6/422 | loss: 3.1993350982666016\n",
      "Epoch: 27/100 | step: 7/422 | loss: 2.9397780895233154\n",
      "Epoch: 27/100 | step: 8/422 | loss: 2.65446400642395\n",
      "Epoch: 27/100 | step: 9/422 | loss: 3.4420275688171387\n",
      "Epoch: 27/100 | step: 10/422 | loss: 2.579296112060547\n",
      "Epoch: 27/100 | step: 11/422 | loss: 2.8815863132476807\n",
      "Epoch: 27/100 | step: 12/422 | loss: 2.999011516571045\n",
      "Epoch: 27/100 | step: 13/422 | loss: 2.9723305702209473\n",
      "Epoch: 27/100 | step: 14/422 | loss: 2.639660596847534\n",
      "Epoch: 27/100 | step: 15/422 | loss: 3.1166024208068848\n",
      "Epoch: 27/100 | step: 16/422 | loss: 2.8591299057006836\n",
      "Epoch: 27/100 | step: 17/422 | loss: 2.4401469230651855\n",
      "Epoch: 27/100 | step: 18/422 | loss: 2.8841922283172607\n",
      "Epoch: 27/100 | step: 19/422 | loss: 2.6817338466644287\n",
      "Epoch: 27/100 | step: 20/422 | loss: 2.538430690765381\n",
      "Epoch: 27/100 | step: 21/422 | loss: 2.741194486618042\n",
      "Epoch: 27/100 | step: 22/422 | loss: 3.1513376235961914\n",
      "Epoch: 27/100 | step: 23/422 | loss: 2.9308152198791504\n",
      "Epoch: 27/100 | step: 24/422 | loss: 3.01576828956604\n",
      "Epoch: 27/100 | step: 25/422 | loss: 2.784818172454834\n",
      "Epoch: 27/100 | step: 26/422 | loss: 2.5266551971435547\n",
      "Epoch: 27/100 | step: 27/422 | loss: 2.7170090675354004\n",
      "Epoch: 27/100 | step: 28/422 | loss: 3.0395078659057617\n",
      "Epoch: 27/100 | step: 29/422 | loss: 2.9365501403808594\n",
      "Epoch: 27/100 | step: 30/422 | loss: 2.121041774749756\n",
      "Epoch: 27/100 | step: 31/422 | loss: 2.841249942779541\n",
      "Epoch: 27/100 | step: 32/422 | loss: 2.3525612354278564\n",
      "Epoch: 27/100 | step: 33/422 | loss: 2.5068867206573486\n",
      "Epoch: 27/100 | step: 34/422 | loss: 3.0632669925689697\n",
      "Epoch: 27/100 | step: 35/422 | loss: 2.498577117919922\n",
      "Epoch: 27/100 | step: 36/422 | loss: 2.938275098800659\n",
      "Epoch: 27/100 | step: 37/422 | loss: 2.7739555835723877\n",
      "Epoch: 27/100 | step: 38/422 | loss: 3.014526605606079\n",
      "Epoch: 27/100 | step: 39/422 | loss: 2.6847522258758545\n",
      "Epoch: 27/100 | step: 40/422 | loss: 2.9889605045318604\n",
      "Epoch: 27/100 | step: 41/422 | loss: 2.3601577281951904\n",
      "Epoch: 27/100 | step: 42/422 | loss: 2.7621428966522217\n",
      "Epoch: 27/100 | step: 43/422 | loss: 2.7636983394622803\n",
      "Epoch: 27/100 | step: 44/422 | loss: 2.4140820503234863\n",
      "Epoch: 27/100 | step: 45/422 | loss: 3.0950663089752197\n",
      "Epoch: 27/100 | step: 46/422 | loss: 3.145681381225586\n",
      "Epoch: 27/100 | step: 47/422 | loss: 3.2928311824798584\n",
      "Epoch: 27/100 | step: 48/422 | loss: 2.7533721923828125\n",
      "Epoch: 27/100 | step: 49/422 | loss: 2.619983434677124\n",
      "Epoch: 27/100 | step: 50/422 | loss: 2.44287109375\n",
      "Epoch: 27/100 | step: 51/422 | loss: 2.753462553024292\n",
      "Epoch: 27/100 | step: 52/422 | loss: 3.0192060470581055\n",
      "Epoch: 27/100 | step: 53/422 | loss: 2.5041749477386475\n",
      "Epoch: 27/100 | step: 54/422 | loss: 3.184971332550049\n",
      "Epoch: 27/100 | step: 55/422 | loss: 2.745713710784912\n",
      "Epoch: 27/100 | step: 56/422 | loss: 3.415602922439575\n",
      "Epoch: 27/100 | step: 57/422 | loss: 2.817866563796997\n",
      "Epoch: 27/100 | step: 58/422 | loss: 2.6162426471710205\n",
      "Epoch: 27/100 | step: 59/422 | loss: 2.649963855743408\n",
      "Epoch: 27/100 | step: 60/422 | loss: 2.6036899089813232\n",
      "Epoch: 27/100 | step: 61/422 | loss: 3.1269140243530273\n",
      "Epoch: 27/100 | step: 62/422 | loss: 2.6667873859405518\n",
      "Epoch: 27/100 | step: 63/422 | loss: 2.823127508163452\n",
      "Epoch: 27/100 | step: 64/422 | loss: 2.858288288116455\n",
      "Epoch: 27/100 | step: 65/422 | loss: 2.7953763008117676\n",
      "Epoch: 27/100 | step: 66/422 | loss: 2.611558198928833\n",
      "Epoch: 27/100 | step: 67/422 | loss: 3.0733795166015625\n",
      "Epoch: 27/100 | step: 68/422 | loss: 3.295119524002075\n",
      "Epoch: 27/100 | step: 69/422 | loss: 2.7952215671539307\n",
      "Epoch: 27/100 | step: 70/422 | loss: 2.5756659507751465\n",
      "Epoch: 27/100 | step: 71/422 | loss: 3.0863444805145264\n",
      "Epoch: 27/100 | step: 72/422 | loss: 2.837583065032959\n",
      "Epoch: 27/100 | step: 73/422 | loss: 2.2784030437469482\n",
      "Epoch: 27/100 | step: 74/422 | loss: 2.662604808807373\n",
      "Epoch: 27/100 | step: 75/422 | loss: 2.9567346572875977\n",
      "Epoch: 27/100 | step: 76/422 | loss: 3.0989465713500977\n",
      "Epoch: 27/100 | step: 77/422 | loss: 2.949599027633667\n",
      "Epoch: 27/100 | step: 78/422 | loss: 2.7782938480377197\n",
      "Epoch: 27/100 | step: 79/422 | loss: 2.5514204502105713\n",
      "Epoch: 27/100 | step: 80/422 | loss: 2.50673770904541\n",
      "Epoch: 27/100 | step: 81/422 | loss: 3.4201743602752686\n",
      "Epoch: 27/100 | step: 82/422 | loss: 3.184884548187256\n",
      "Epoch: 27/100 | step: 83/422 | loss: 2.725324869155884\n",
      "Epoch: 27/100 | step: 84/422 | loss: 2.7163915634155273\n",
      "Epoch: 27/100 | step: 85/422 | loss: 3.509549379348755\n",
      "Epoch: 27/100 | step: 86/422 | loss: 2.988687038421631\n",
      "Epoch: 27/100 | step: 87/422 | loss: 2.6842870712280273\n",
      "Epoch: 27/100 | step: 88/422 | loss: 3.0639259815216064\n",
      "Epoch: 27/100 | step: 89/422 | loss: 2.7901370525360107\n",
      "Epoch: 27/100 | step: 90/422 | loss: 2.9763760566711426\n",
      "Epoch: 27/100 | step: 91/422 | loss: 2.7745277881622314\n",
      "Epoch: 27/100 | step: 92/422 | loss: 2.9431843757629395\n",
      "Epoch: 27/100 | step: 93/422 | loss: 3.224396228790283\n",
      "Epoch: 27/100 | step: 94/422 | loss: 2.6920738220214844\n",
      "Epoch: 27/100 | step: 95/422 | loss: 2.3672735691070557\n",
      "Epoch: 27/100 | step: 96/422 | loss: 3.0135765075683594\n",
      "Epoch: 27/100 | step: 97/422 | loss: 3.2795324325561523\n",
      "Epoch: 27/100 | step: 98/422 | loss: 3.144333600997925\n",
      "Epoch: 27/100 | step: 99/422 | loss: 2.341905355453491\n",
      "Epoch: 27/100 | step: 100/422 | loss: 3.087810516357422\n",
      "Epoch: 27/100 | step: 101/422 | loss: 2.9900777339935303\n",
      "Epoch: 27/100 | step: 102/422 | loss: 2.3569631576538086\n",
      "Epoch: 27/100 | step: 103/422 | loss: 2.5793161392211914\n",
      "Epoch: 27/100 | step: 104/422 | loss: 2.3771841526031494\n",
      "Epoch: 27/100 | step: 105/422 | loss: 2.628138780593872\n",
      "Epoch: 27/100 | step: 106/422 | loss: 2.973090410232544\n",
      "Epoch: 27/100 | step: 107/422 | loss: 2.6803393363952637\n",
      "Epoch: 27/100 | step: 108/422 | loss: 2.4439406394958496\n",
      "Epoch: 27/100 | step: 109/422 | loss: 2.829984426498413\n",
      "Epoch: 27/100 | step: 110/422 | loss: 2.758074998855591\n",
      "Epoch: 27/100 | step: 111/422 | loss: 2.3120150566101074\n",
      "Epoch: 27/100 | step: 112/422 | loss: 3.1717281341552734\n",
      "Epoch: 27/100 | step: 113/422 | loss: 2.994184732437134\n",
      "Epoch: 27/100 | step: 114/422 | loss: 2.260540246963501\n",
      "Epoch: 27/100 | step: 115/422 | loss: 2.532378911972046\n",
      "Epoch: 27/100 | step: 116/422 | loss: 2.8540642261505127\n",
      "Epoch: 27/100 | step: 117/422 | loss: 2.8184192180633545\n",
      "Epoch: 27/100 | step: 118/422 | loss: 2.7802278995513916\n",
      "Epoch: 27/100 | step: 119/422 | loss: 2.3594882488250732\n",
      "Epoch: 27/100 | step: 120/422 | loss: 3.1057612895965576\n",
      "Epoch: 27/100 | step: 121/422 | loss: 2.9415154457092285\n",
      "Epoch: 27/100 | step: 122/422 | loss: 2.666686534881592\n",
      "Epoch: 27/100 | step: 123/422 | loss: 3.343393087387085\n",
      "Epoch: 27/100 | step: 124/422 | loss: 2.743330717086792\n",
      "Epoch: 27/100 | step: 125/422 | loss: 2.5905821323394775\n",
      "Epoch: 27/100 | step: 126/422 | loss: 2.7112276554107666\n",
      "Epoch: 27/100 | step: 127/422 | loss: 2.2481000423431396\n",
      "Epoch: 27/100 | step: 128/422 | loss: 2.454563617706299\n",
      "Epoch: 27/100 | step: 129/422 | loss: 2.413715362548828\n",
      "Epoch: 27/100 | step: 130/422 | loss: 3.1542232036590576\n",
      "Epoch: 27/100 | step: 131/422 | loss: 2.877054452896118\n",
      "Epoch: 27/100 | step: 132/422 | loss: 2.4558188915252686\n",
      "Epoch: 27/100 | step: 133/422 | loss: 3.0486464500427246\n",
      "Epoch: 27/100 | step: 134/422 | loss: 2.5673699378967285\n",
      "Epoch: 27/100 | step: 135/422 | loss: 2.213174343109131\n",
      "Epoch: 27/100 | step: 136/422 | loss: 2.802598237991333\n",
      "Epoch: 27/100 | step: 137/422 | loss: 2.535717248916626\n",
      "Epoch: 27/100 | step: 138/422 | loss: 2.8667447566986084\n",
      "Epoch: 27/100 | step: 139/422 | loss: 2.7866859436035156\n",
      "Epoch: 27/100 | step: 140/422 | loss: 2.7047183513641357\n",
      "Epoch: 27/100 | step: 141/422 | loss: 2.7129101753234863\n",
      "Epoch: 27/100 | step: 142/422 | loss: 3.235968828201294\n",
      "Epoch: 27/100 | step: 143/422 | loss: 3.0983424186706543\n",
      "Epoch: 27/100 | step: 144/422 | loss: 3.090459108352661\n",
      "Epoch: 27/100 | step: 145/422 | loss: 2.966484546661377\n",
      "Epoch: 27/100 | step: 146/422 | loss: 2.6128482818603516\n",
      "Epoch: 27/100 | step: 147/422 | loss: 2.8033111095428467\n",
      "Epoch: 27/100 | step: 148/422 | loss: 3.013570785522461\n",
      "Epoch: 27/100 | step: 149/422 | loss: 2.87848162651062\n",
      "Epoch: 27/100 | step: 150/422 | loss: 2.773111343383789\n",
      "Epoch: 27/100 | step: 151/422 | loss: 2.735642433166504\n",
      "Epoch: 27/100 | step: 152/422 | loss: 2.922776222229004\n",
      "Epoch: 27/100 | step: 153/422 | loss: 3.1638681888580322\n",
      "Epoch: 27/100 | step: 154/422 | loss: 3.3848109245300293\n",
      "Epoch: 27/100 | step: 155/422 | loss: 2.4813454151153564\n",
      "Epoch: 27/100 | step: 156/422 | loss: 2.5733020305633545\n",
      "Epoch: 27/100 | step: 157/422 | loss: 2.593291759490967\n",
      "Epoch: 27/100 | step: 158/422 | loss: 3.2082903385162354\n",
      "Epoch: 27/100 | step: 159/422 | loss: 2.594339609146118\n",
      "Epoch: 27/100 | step: 160/422 | loss: 3.15474271774292\n",
      "Epoch: 27/100 | step: 161/422 | loss: 2.5731277465820312\n",
      "Epoch: 27/100 | step: 162/422 | loss: 2.7544217109680176\n",
      "Epoch: 27/100 | step: 163/422 | loss: 2.422858238220215\n",
      "Epoch: 27/100 | step: 164/422 | loss: 2.845491409301758\n",
      "Epoch: 27/100 | step: 165/422 | loss: 2.910442352294922\n",
      "Epoch: 27/100 | step: 166/422 | loss: 3.1336052417755127\n",
      "Epoch: 27/100 | step: 167/422 | loss: 2.86124587059021\n",
      "Epoch: 27/100 | step: 168/422 | loss: 3.032041072845459\n",
      "Epoch: 27/100 | step: 169/422 | loss: 2.3144707679748535\n",
      "Epoch: 27/100 | step: 170/422 | loss: 2.5132837295532227\n",
      "Epoch: 27/100 | step: 171/422 | loss: 2.7193803787231445\n",
      "Epoch: 27/100 | step: 172/422 | loss: 2.964001178741455\n",
      "Epoch: 27/100 | step: 173/422 | loss: 2.867432117462158\n",
      "Epoch: 27/100 | step: 174/422 | loss: 2.85672664642334\n",
      "Epoch: 27/100 | step: 175/422 | loss: 3.0376100540161133\n",
      "Epoch: 27/100 | step: 176/422 | loss: 3.1463921070098877\n",
      "Epoch: 27/100 | step: 177/422 | loss: 3.126344919204712\n",
      "Epoch: 27/100 | step: 178/422 | loss: 2.497588872909546\n",
      "Epoch: 27/100 | step: 179/422 | loss: 3.0543129444122314\n",
      "Epoch: 27/100 | step: 180/422 | loss: 2.915346622467041\n",
      "Epoch: 27/100 | step: 181/422 | loss: 2.6194629669189453\n",
      "Epoch: 27/100 | step: 182/422 | loss: 3.047685384750366\n",
      "Epoch: 27/100 | step: 183/422 | loss: 2.7028558254241943\n",
      "Epoch: 27/100 | step: 184/422 | loss: 3.138420820236206\n",
      "Epoch: 27/100 | step: 185/422 | loss: 2.7561144828796387\n",
      "Epoch: 27/100 | step: 186/422 | loss: 2.6059458255767822\n",
      "Epoch: 27/100 | step: 187/422 | loss: 3.0484559535980225\n",
      "Epoch: 27/100 | step: 188/422 | loss: 2.9019088745117188\n",
      "Epoch: 27/100 | step: 189/422 | loss: 2.599961280822754\n",
      "Epoch: 27/100 | step: 190/422 | loss: 3.1475446224212646\n",
      "Epoch: 27/100 | step: 191/422 | loss: 2.6449036598205566\n",
      "Epoch: 27/100 | step: 192/422 | loss: 3.3242125511169434\n",
      "Epoch: 27/100 | step: 193/422 | loss: 2.97405743598938\n",
      "Epoch: 27/100 | step: 194/422 | loss: 2.5848071575164795\n",
      "Epoch: 27/100 | step: 195/422 | loss: 2.7727155685424805\n",
      "Epoch: 27/100 | step: 196/422 | loss: 2.9196457862854004\n",
      "Epoch: 27/100 | step: 197/422 | loss: 2.777066469192505\n",
      "Epoch: 27/100 | step: 198/422 | loss: 3.3118767738342285\n",
      "Epoch: 27/100 | step: 199/422 | loss: 2.675786018371582\n",
      "Epoch: 27/100 | step: 200/422 | loss: 2.7333731651306152\n",
      "Epoch: 27/100 | step: 201/422 | loss: 3.320208787918091\n",
      "Epoch: 27/100 | step: 202/422 | loss: 3.1878318786621094\n",
      "Epoch: 27/100 | step: 203/422 | loss: 3.006176233291626\n",
      "Epoch: 27/100 | step: 204/422 | loss: 3.2582688331604004\n",
      "Epoch: 27/100 | step: 205/422 | loss: 2.786127805709839\n",
      "Epoch: 27/100 | step: 206/422 | loss: 2.935619354248047\n",
      "Epoch: 27/100 | step: 207/422 | loss: 3.304424285888672\n",
      "Epoch: 27/100 | step: 208/422 | loss: 2.725584030151367\n",
      "Epoch: 27/100 | step: 209/422 | loss: 2.47016978263855\n",
      "Epoch: 27/100 | step: 210/422 | loss: 2.7512824535369873\n",
      "Epoch: 27/100 | step: 211/422 | loss: 2.8147730827331543\n",
      "Epoch: 27/100 | step: 212/422 | loss: 3.14613938331604\n",
      "Epoch: 27/100 | step: 213/422 | loss: 2.708434581756592\n",
      "Epoch: 27/100 | step: 214/422 | loss: 2.862339496612549\n",
      "Epoch: 27/100 | step: 215/422 | loss: 3.039740562438965\n",
      "Epoch: 27/100 | step: 216/422 | loss: 2.782214641571045\n",
      "Epoch: 27/100 | step: 217/422 | loss: 2.885051965713501\n",
      "Epoch: 27/100 | step: 218/422 | loss: 2.7047441005706787\n",
      "Epoch: 27/100 | step: 219/422 | loss: 2.5484087467193604\n",
      "Epoch: 27/100 | step: 220/422 | loss: 2.7961676120758057\n",
      "Epoch: 27/100 | step: 221/422 | loss: 2.5183956623077393\n",
      "Epoch: 27/100 | step: 222/422 | loss: 2.9185807704925537\n",
      "Epoch: 27/100 | step: 223/422 | loss: 2.947056531906128\n",
      "Epoch: 27/100 | step: 224/422 | loss: 2.4284424781799316\n",
      "Epoch: 27/100 | step: 225/422 | loss: 2.9048774242401123\n",
      "Epoch: 27/100 | step: 226/422 | loss: 2.759747266769409\n",
      "Epoch: 27/100 | step: 227/422 | loss: 2.9967832565307617\n",
      "Epoch: 27/100 | step: 228/422 | loss: 2.7726292610168457\n",
      "Epoch: 27/100 | step: 229/422 | loss: 2.6247146129608154\n",
      "Epoch: 27/100 | step: 230/422 | loss: 2.8401548862457275\n",
      "Epoch: 27/100 | step: 231/422 | loss: 2.846597671508789\n",
      "Epoch: 27/100 | step: 232/422 | loss: 3.457000732421875\n",
      "Epoch: 27/100 | step: 233/422 | loss: 2.6151466369628906\n",
      "Epoch: 27/100 | step: 234/422 | loss: 2.455709934234619\n",
      "Epoch: 27/100 | step: 235/422 | loss: 3.2691750526428223\n",
      "Epoch: 27/100 | step: 236/422 | loss: 2.7327330112457275\n",
      "Epoch: 27/100 | step: 237/422 | loss: 3.063521146774292\n",
      "Epoch: 27/100 | step: 238/422 | loss: 2.9761693477630615\n",
      "Epoch: 27/100 | step: 239/422 | loss: 3.2620108127593994\n",
      "Epoch: 27/100 | step: 240/422 | loss: 3.050222635269165\n",
      "Epoch: 27/100 | step: 241/422 | loss: 2.9789822101593018\n",
      "Epoch: 27/100 | step: 242/422 | loss: 2.2989463806152344\n",
      "Epoch: 27/100 | step: 243/422 | loss: 2.385460376739502\n",
      "Epoch: 27/100 | step: 244/422 | loss: 2.931824207305908\n",
      "Epoch: 27/100 | step: 245/422 | loss: 2.8850317001342773\n",
      "Epoch: 27/100 | step: 246/422 | loss: 3.178520441055298\n",
      "Epoch: 27/100 | step: 247/422 | loss: 3.0088095664978027\n",
      "Epoch: 27/100 | step: 248/422 | loss: 2.6503827571868896\n",
      "Epoch: 27/100 | step: 249/422 | loss: 2.534438371658325\n",
      "Epoch: 27/100 | step: 250/422 | loss: 3.23271107673645\n",
      "Epoch: 27/100 | step: 251/422 | loss: 3.070345878601074\n",
      "Epoch: 27/100 | step: 252/422 | loss: 2.7587199211120605\n",
      "Epoch: 27/100 | step: 253/422 | loss: 2.8555386066436768\n",
      "Epoch: 27/100 | step: 254/422 | loss: 3.146031618118286\n",
      "Epoch: 27/100 | step: 255/422 | loss: 2.848010778427124\n",
      "Epoch: 27/100 | step: 256/422 | loss: 3.140378475189209\n",
      "Epoch: 27/100 | step: 257/422 | loss: 2.8741605281829834\n",
      "Epoch: 27/100 | step: 258/422 | loss: 2.948467969894409\n",
      "Epoch: 27/100 | step: 259/422 | loss: 2.606254816055298\n",
      "Epoch: 27/100 | step: 260/422 | loss: 3.211503744125366\n",
      "Epoch: 27/100 | step: 261/422 | loss: 2.489309549331665\n",
      "Epoch: 27/100 | step: 262/422 | loss: 2.587639093399048\n",
      "Epoch: 27/100 | step: 263/422 | loss: 2.271573543548584\n",
      "Epoch: 27/100 | step: 264/422 | loss: 3.045503854751587\n",
      "Epoch: 27/100 | step: 265/422 | loss: 2.847747564315796\n",
      "Epoch: 27/100 | step: 266/422 | loss: 2.6994950771331787\n",
      "Epoch: 27/100 | step: 267/422 | loss: 2.7836430072784424\n",
      "Epoch: 27/100 | step: 268/422 | loss: 2.8308095932006836\n",
      "Epoch: 27/100 | step: 269/422 | loss: 2.807847499847412\n",
      "Epoch: 27/100 | step: 270/422 | loss: 3.028930187225342\n",
      "Epoch: 27/100 | step: 271/422 | loss: 2.9346373081207275\n",
      "Epoch: 27/100 | step: 272/422 | loss: 2.805697202682495\n",
      "Epoch: 27/100 | step: 273/422 | loss: 3.003859043121338\n",
      "Epoch: 27/100 | step: 274/422 | loss: 2.9024498462677\n",
      "Epoch: 27/100 | step: 275/422 | loss: 3.1794676780700684\n",
      "Epoch: 27/100 | step: 276/422 | loss: 2.981055736541748\n",
      "Epoch: 27/100 | step: 277/422 | loss: 2.670670509338379\n",
      "Epoch: 27/100 | step: 278/422 | loss: 2.7296314239501953\n",
      "Epoch: 27/100 | step: 279/422 | loss: 2.6043710708618164\n",
      "Epoch: 27/100 | step: 280/422 | loss: 2.574979066848755\n",
      "Epoch: 27/100 | step: 281/422 | loss: 2.3298873901367188\n",
      "Epoch: 27/100 | step: 282/422 | loss: 2.4678874015808105\n",
      "Epoch: 27/100 | step: 283/422 | loss: 2.5302176475524902\n",
      "Epoch: 27/100 | step: 284/422 | loss: 2.9810378551483154\n",
      "Epoch: 27/100 | step: 285/422 | loss: 2.7232494354248047\n",
      "Epoch: 27/100 | step: 286/422 | loss: 2.9011223316192627\n",
      "Epoch: 27/100 | step: 287/422 | loss: 2.556901454925537\n",
      "Epoch: 27/100 | step: 288/422 | loss: 2.7561662197113037\n",
      "Epoch: 27/100 | step: 289/422 | loss: 2.785719394683838\n",
      "Epoch: 27/100 | step: 290/422 | loss: 3.0276825428009033\n",
      "Epoch: 27/100 | step: 291/422 | loss: 2.649014711380005\n",
      "Epoch: 27/100 | step: 292/422 | loss: 3.0824928283691406\n",
      "Epoch: 27/100 | step: 293/422 | loss: 3.1675686836242676\n",
      "Epoch: 27/100 | step: 294/422 | loss: 3.103118658065796\n",
      "Epoch: 27/100 | step: 295/422 | loss: 2.234469413757324\n",
      "Epoch: 27/100 | step: 296/422 | loss: 2.6413559913635254\n",
      "Epoch: 27/100 | step: 297/422 | loss: 2.628312826156616\n",
      "Epoch: 27/100 | step: 298/422 | loss: 2.71760630607605\n",
      "Epoch: 27/100 | step: 299/422 | loss: 3.032210111618042\n",
      "Epoch: 27/100 | step: 300/422 | loss: 2.5939292907714844\n",
      "Epoch: 27/100 | step: 301/422 | loss: 2.5439765453338623\n",
      "Epoch: 27/100 | step: 302/422 | loss: 2.86981463432312\n",
      "Epoch: 27/100 | step: 303/422 | loss: 2.581441640853882\n",
      "Epoch: 27/100 | step: 304/422 | loss: 3.4795010089874268\n",
      "Epoch: 27/100 | step: 305/422 | loss: 2.936964273452759\n",
      "Epoch: 27/100 | step: 306/422 | loss: 2.4938926696777344\n",
      "Epoch: 27/100 | step: 307/422 | loss: 2.774561643600464\n",
      "Epoch: 27/100 | step: 308/422 | loss: 2.694282293319702\n",
      "Epoch: 27/100 | step: 309/422 | loss: 2.706010103225708\n",
      "Epoch: 27/100 | step: 310/422 | loss: 2.73447585105896\n",
      "Epoch: 27/100 | step: 311/422 | loss: 3.0340147018432617\n",
      "Epoch: 27/100 | step: 312/422 | loss: 3.5484578609466553\n",
      "Epoch: 27/100 | step: 313/422 | loss: 2.8926541805267334\n",
      "Epoch: 27/100 | step: 314/422 | loss: 2.826948404312134\n",
      "Epoch: 27/100 | step: 315/422 | loss: 3.066714286804199\n",
      "Epoch: 27/100 | step: 316/422 | loss: 3.1231260299682617\n",
      "Epoch: 27/100 | step: 317/422 | loss: 2.276409864425659\n",
      "Epoch: 27/100 | step: 318/422 | loss: 2.7548537254333496\n",
      "Epoch: 27/100 | step: 319/422 | loss: 2.9872777462005615\n",
      "Epoch: 27/100 | step: 320/422 | loss: 2.5879290103912354\n",
      "Epoch: 27/100 | step: 321/422 | loss: 2.548630714416504\n",
      "Epoch: 27/100 | step: 322/422 | loss: 2.702556848526001\n",
      "Epoch: 27/100 | step: 323/422 | loss: 2.6237738132476807\n",
      "Epoch: 27/100 | step: 324/422 | loss: 2.725114345550537\n",
      "Epoch: 27/100 | step: 325/422 | loss: 3.1893739700317383\n",
      "Epoch: 27/100 | step: 326/422 | loss: 2.910888433456421\n",
      "Epoch: 27/100 | step: 327/422 | loss: 2.765377998352051\n",
      "Epoch: 27/100 | step: 328/422 | loss: 2.413331985473633\n",
      "Epoch: 27/100 | step: 329/422 | loss: 2.4332449436187744\n",
      "Epoch: 27/100 | step: 330/422 | loss: 3.081230401992798\n",
      "Epoch: 27/100 | step: 331/422 | loss: 3.3284926414489746\n",
      "Epoch: 27/100 | step: 332/422 | loss: 3.116206407546997\n",
      "Epoch: 27/100 | step: 333/422 | loss: 2.721038579940796\n",
      "Epoch: 27/100 | step: 334/422 | loss: 2.951409339904785\n",
      "Epoch: 27/100 | step: 335/422 | loss: 2.962327241897583\n",
      "Epoch: 27/100 | step: 336/422 | loss: 2.95318603515625\n",
      "Epoch: 27/100 | step: 337/422 | loss: 2.9913644790649414\n",
      "Epoch: 27/100 | step: 338/422 | loss: 2.7569165229797363\n",
      "Epoch: 27/100 | step: 339/422 | loss: 2.775221824645996\n",
      "Epoch: 27/100 | step: 340/422 | loss: 2.6550450325012207\n",
      "Epoch: 27/100 | step: 341/422 | loss: 2.535886526107788\n",
      "Epoch: 27/100 | step: 342/422 | loss: 2.7015461921691895\n",
      "Epoch: 27/100 | step: 343/422 | loss: 3.021894693374634\n",
      "Epoch: 27/100 | step: 344/422 | loss: 2.7482752799987793\n",
      "Epoch: 27/100 | step: 345/422 | loss: 2.579969644546509\n",
      "Epoch: 27/100 | step: 346/422 | loss: 2.6143765449523926\n",
      "Epoch: 27/100 | step: 347/422 | loss: 2.612208843231201\n",
      "Epoch: 27/100 | step: 348/422 | loss: 3.0551133155822754\n",
      "Epoch: 27/100 | step: 349/422 | loss: 2.946337938308716\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 28/100 | step: 1/422 | loss: 3.1862103939056396\n",
      "Epoch: 28/100 | step: 2/422 | loss: 2.3275234699249268\n",
      "Epoch: 28/100 | step: 3/422 | loss: 2.2093894481658936\n",
      "Epoch: 28/100 | step: 4/422 | loss: 2.522662401199341\n",
      "Epoch: 28/100 | step: 5/422 | loss: 2.952063798904419\n",
      "Epoch: 28/100 | step: 6/422 | loss: 2.5076804161071777\n",
      "Epoch: 28/100 | step: 7/422 | loss: 2.4253289699554443\n",
      "Epoch: 28/100 | step: 8/422 | loss: 2.426116704940796\n",
      "Epoch: 28/100 | step: 9/422 | loss: 2.9646244049072266\n",
      "Epoch: 28/100 | step: 10/422 | loss: 2.717154026031494\n",
      "Epoch: 28/100 | step: 11/422 | loss: 2.437089443206787\n",
      "Epoch: 28/100 | step: 12/422 | loss: 2.486074447631836\n",
      "Epoch: 28/100 | step: 13/422 | loss: 2.884814977645874\n",
      "Epoch: 28/100 | step: 14/422 | loss: 2.4626383781433105\n",
      "Epoch: 28/100 | step: 15/422 | loss: 2.913846254348755\n",
      "Epoch: 28/100 | step: 16/422 | loss: 2.4734978675842285\n",
      "Epoch: 28/100 | step: 17/422 | loss: 2.402960777282715\n",
      "Epoch: 28/100 | step: 18/422 | loss: 2.6396515369415283\n",
      "Epoch: 28/100 | step: 19/422 | loss: 2.5323946475982666\n",
      "Epoch: 28/100 | step: 20/422 | loss: 2.6248891353607178\n",
      "Epoch: 28/100 | step: 21/422 | loss: 2.899059772491455\n",
      "Epoch: 28/100 | step: 22/422 | loss: 2.607523202896118\n",
      "Epoch: 28/100 | step: 23/422 | loss: 2.9156031608581543\n",
      "Epoch: 28/100 | step: 24/422 | loss: 2.5475592613220215\n",
      "Epoch: 28/100 | step: 25/422 | loss: 2.2536628246307373\n",
      "Epoch: 28/100 | step: 26/422 | loss: 2.538520574569702\n",
      "Epoch: 28/100 | step: 27/422 | loss: 2.79137921333313\n",
      "Epoch: 28/100 | step: 28/422 | loss: 2.565406322479248\n",
      "Epoch: 28/100 | step: 29/422 | loss: 2.8328800201416016\n",
      "Epoch: 28/100 | step: 30/422 | loss: 2.6356594562530518\n",
      "Epoch: 28/100 | step: 31/422 | loss: 2.755359649658203\n",
      "Epoch: 28/100 | step: 32/422 | loss: 2.766209363937378\n",
      "Epoch: 28/100 | step: 33/422 | loss: 1.989227294921875\n",
      "Epoch: 28/100 | step: 34/422 | loss: 2.547867774963379\n",
      "Epoch: 28/100 | step: 35/422 | loss: 2.8342835903167725\n",
      "Epoch: 28/100 | step: 36/422 | loss: 2.4111289978027344\n",
      "Epoch: 28/100 | step: 37/422 | loss: 3.2657570838928223\n",
      "Epoch: 28/100 | step: 38/422 | loss: 2.430968999862671\n",
      "Epoch: 28/100 | step: 39/422 | loss: 2.4444241523742676\n",
      "Epoch: 28/100 | step: 40/422 | loss: 2.3711020946502686\n",
      "Epoch: 28/100 | step: 41/422 | loss: 2.7046866416931152\n",
      "Epoch: 28/100 | step: 42/422 | loss: 2.677856683731079\n",
      "Epoch: 28/100 | step: 43/422 | loss: 2.7790467739105225\n",
      "Epoch: 28/100 | step: 44/422 | loss: 3.077291965484619\n",
      "Epoch: 28/100 | step: 45/422 | loss: 2.8999998569488525\n",
      "Epoch: 28/100 | step: 46/422 | loss: 2.434852123260498\n",
      "Epoch: 28/100 | step: 47/422 | loss: 2.8537473678588867\n",
      "Epoch: 28/100 | step: 48/422 | loss: 2.803475856781006\n",
      "Epoch: 28/100 | step: 49/422 | loss: 3.1912708282470703\n",
      "Epoch: 28/100 | step: 50/422 | loss: 2.531203508377075\n",
      "Epoch: 28/100 | step: 51/422 | loss: 2.450425863265991\n",
      "Epoch: 28/100 | step: 52/422 | loss: 2.9674723148345947\n",
      "Epoch: 28/100 | step: 53/422 | loss: 2.6625330448150635\n",
      "Epoch: 28/100 | step: 54/422 | loss: 1.93936026096344\n",
      "Epoch: 28/100 | step: 55/422 | loss: 2.9639604091644287\n",
      "Epoch: 28/100 | step: 56/422 | loss: 2.4824554920196533\n",
      "Epoch: 28/100 | step: 57/422 | loss: 2.6088204383850098\n",
      "Epoch: 28/100 | step: 58/422 | loss: 2.507415533065796\n",
      "Epoch: 28/100 | step: 59/422 | loss: 2.9475443363189697\n",
      "Epoch: 28/100 | step: 60/422 | loss: 3.105710506439209\n",
      "Epoch: 28/100 | step: 61/422 | loss: 2.3382017612457275\n",
      "Epoch: 28/100 | step: 62/422 | loss: 2.635261297225952\n",
      "Epoch: 28/100 | step: 63/422 | loss: 2.8392417430877686\n",
      "Epoch: 28/100 | step: 64/422 | loss: 2.7253541946411133\n",
      "Epoch: 28/100 | step: 65/422 | loss: 2.5664660930633545\n",
      "Epoch: 28/100 | step: 66/422 | loss: 2.6906375885009766\n",
      "Epoch: 28/100 | step: 67/422 | loss: 2.2337634563446045\n",
      "Epoch: 28/100 | step: 68/422 | loss: 3.178830862045288\n",
      "Epoch: 28/100 | step: 69/422 | loss: 2.366070032119751\n",
      "Epoch: 28/100 | step: 70/422 | loss: 2.273214101791382\n",
      "Epoch: 28/100 | step: 71/422 | loss: 2.473559856414795\n",
      "Epoch: 28/100 | step: 72/422 | loss: 2.8355979919433594\n",
      "Epoch: 28/100 | step: 73/422 | loss: 2.5130345821380615\n",
      "Epoch: 28/100 | step: 74/422 | loss: 2.5247833728790283\n",
      "Epoch: 28/100 | step: 75/422 | loss: 2.2609446048736572\n",
      "Epoch: 28/100 | step: 76/422 | loss: 2.663914442062378\n",
      "Epoch: 28/100 | step: 77/422 | loss: 2.579566240310669\n",
      "Epoch: 28/100 | step: 78/422 | loss: 2.5544769763946533\n",
      "Epoch: 28/100 | step: 79/422 | loss: 2.722891330718994\n",
      "Epoch: 28/100 | step: 80/422 | loss: 3.134218692779541\n",
      "Epoch: 28/100 | step: 81/422 | loss: 2.2272439002990723\n",
      "Epoch: 28/100 | step: 82/422 | loss: 2.5421154499053955\n",
      "Epoch: 28/100 | step: 83/422 | loss: 2.612325429916382\n",
      "Epoch: 28/100 | step: 84/422 | loss: 2.4302356243133545\n",
      "Epoch: 28/100 | step: 85/422 | loss: 2.9425578117370605\n",
      "Epoch: 28/100 | step: 86/422 | loss: 2.943941116333008\n",
      "Epoch: 28/100 | step: 87/422 | loss: 2.1127188205718994\n",
      "Epoch: 28/100 | step: 88/422 | loss: 2.926974296569824\n",
      "Epoch: 28/100 | step: 89/422 | loss: 2.5367696285247803\n",
      "Epoch: 28/100 | step: 90/422 | loss: 2.54801869392395\n",
      "Epoch: 28/100 | step: 91/422 | loss: 2.4126904010772705\n",
      "Epoch: 28/100 | step: 92/422 | loss: 2.58803391456604\n",
      "Epoch: 28/100 | step: 93/422 | loss: 3.1543283462524414\n",
      "Epoch: 28/100 | step: 94/422 | loss: 2.524393320083618\n",
      "Epoch: 28/100 | step: 95/422 | loss: 2.462123394012451\n",
      "Epoch: 28/100 | step: 96/422 | loss: 2.6274712085723877\n",
      "Epoch: 28/100 | step: 97/422 | loss: 2.7743077278137207\n",
      "Epoch: 28/100 | step: 98/422 | loss: 3.0757150650024414\n",
      "Epoch: 28/100 | step: 99/422 | loss: 3.0412986278533936\n",
      "Epoch: 28/100 | step: 100/422 | loss: 2.4175915718078613\n",
      "Epoch: 28/100 | step: 101/422 | loss: 2.608111619949341\n",
      "Epoch: 28/100 | step: 102/422 | loss: 2.950101375579834\n",
      "Epoch: 28/100 | step: 103/422 | loss: 2.9061224460601807\n",
      "Epoch: 28/100 | step: 104/422 | loss: 2.703317880630493\n",
      "Epoch: 28/100 | step: 105/422 | loss: 3.069873094558716\n",
      "Epoch: 28/100 | step: 106/422 | loss: 2.5013887882232666\n",
      "Epoch: 28/100 | step: 107/422 | loss: 2.691149950027466\n",
      "Epoch: 28/100 | step: 108/422 | loss: 3.2348275184631348\n",
      "Epoch: 28/100 | step: 109/422 | loss: 2.556161880493164\n",
      "Epoch: 28/100 | step: 110/422 | loss: 2.5051114559173584\n",
      "Epoch: 28/100 | step: 111/422 | loss: 2.5212137699127197\n",
      "Epoch: 28/100 | step: 112/422 | loss: 3.1022469997406006\n",
      "Epoch: 28/100 | step: 113/422 | loss: 2.6452016830444336\n",
      "Epoch: 28/100 | step: 114/422 | loss: 2.220363140106201\n",
      "Epoch: 28/100 | step: 115/422 | loss: 2.7515127658843994\n",
      "Epoch: 28/100 | step: 116/422 | loss: 3.4551520347595215\n",
      "Epoch: 28/100 | step: 117/422 | loss: 2.6545817852020264\n",
      "Epoch: 28/100 | step: 118/422 | loss: 2.4410359859466553\n",
      "Epoch: 28/100 | step: 119/422 | loss: 2.507483720779419\n",
      "Epoch: 28/100 | step: 120/422 | loss: 2.7144997119903564\n",
      "Epoch: 28/100 | step: 121/422 | loss: 2.425267457962036\n",
      "Epoch: 28/100 | step: 122/422 | loss: 2.6189441680908203\n",
      "Epoch: 28/100 | step: 123/422 | loss: 2.8225080966949463\n",
      "Epoch: 28/100 | step: 124/422 | loss: 2.9405508041381836\n",
      "Epoch: 28/100 | step: 125/422 | loss: 2.9137744903564453\n",
      "Epoch: 28/100 | step: 126/422 | loss: 2.6460793018341064\n",
      "Epoch: 28/100 | step: 127/422 | loss: 2.2960000038146973\n",
      "Epoch: 28/100 | step: 128/422 | loss: 2.6703593730926514\n",
      "Epoch: 28/100 | step: 129/422 | loss: 2.7721455097198486\n",
      "Epoch: 28/100 | step: 130/422 | loss: 3.1672747135162354\n",
      "Epoch: 28/100 | step: 131/422 | loss: 2.6795129776000977\n",
      "Epoch: 28/100 | step: 132/422 | loss: 2.6229944229125977\n",
      "Epoch: 28/100 | step: 133/422 | loss: 2.685213565826416\n",
      "Epoch: 28/100 | step: 134/422 | loss: 3.083953380584717\n",
      "Epoch: 28/100 | step: 135/422 | loss: 3.1491355895996094\n",
      "Epoch: 28/100 | step: 136/422 | loss: 2.293346881866455\n",
      "Epoch: 28/100 | step: 137/422 | loss: 2.770812511444092\n",
      "Epoch: 28/100 | step: 138/422 | loss: 2.749969244003296\n",
      "Epoch: 28/100 | step: 139/422 | loss: 2.785053014755249\n",
      "Epoch: 28/100 | step: 140/422 | loss: 2.5685982704162598\n",
      "Epoch: 28/100 | step: 141/422 | loss: 2.641155242919922\n",
      "Epoch: 28/100 | step: 142/422 | loss: 2.898900270462036\n",
      "Epoch: 28/100 | step: 143/422 | loss: 2.6990749835968018\n",
      "Epoch: 28/100 | step: 144/422 | loss: 2.721752166748047\n",
      "Epoch: 28/100 | step: 145/422 | loss: 3.077367067337036\n",
      "Epoch: 28/100 | step: 146/422 | loss: 3.1869471073150635\n",
      "Epoch: 28/100 | step: 147/422 | loss: 2.1791090965270996\n",
      "Epoch: 28/100 | step: 148/422 | loss: 3.2018816471099854\n",
      "Epoch: 28/100 | step: 149/422 | loss: 2.650496244430542\n",
      "Epoch: 28/100 | step: 150/422 | loss: 2.726112127304077\n",
      "Epoch: 28/100 | step: 151/422 | loss: 2.6645259857177734\n",
      "Epoch: 28/100 | step: 152/422 | loss: 2.668632984161377\n",
      "Epoch: 28/100 | step: 153/422 | loss: 2.8927619457244873\n",
      "Epoch: 28/100 | step: 154/422 | loss: 2.7085728645324707\n",
      "Epoch: 28/100 | step: 155/422 | loss: 3.056621789932251\n",
      "Epoch: 28/100 | step: 156/422 | loss: 2.6730363368988037\n",
      "Epoch: 28/100 | step: 157/422 | loss: 2.8153491020202637\n",
      "Epoch: 28/100 | step: 158/422 | loss: 2.7285351753234863\n",
      "Epoch: 28/100 | step: 159/422 | loss: 2.6245229244232178\n",
      "Epoch: 28/100 | step: 160/422 | loss: 3.1181559562683105\n",
      "Epoch: 28/100 | step: 161/422 | loss: 2.787445545196533\n",
      "Epoch: 28/100 | step: 162/422 | loss: 3.047306537628174\n",
      "Epoch: 28/100 | step: 163/422 | loss: 2.99491548538208\n",
      "Epoch: 28/100 | step: 164/422 | loss: 3.078700065612793\n",
      "Epoch: 28/100 | step: 165/422 | loss: 2.3258848190307617\n",
      "Epoch: 28/100 | step: 166/422 | loss: 2.903836965560913\n",
      "Epoch: 28/100 | step: 167/422 | loss: 2.690704107284546\n",
      "Epoch: 28/100 | step: 168/422 | loss: 2.763441801071167\n",
      "Epoch: 28/100 | step: 169/422 | loss: 2.616755247116089\n",
      "Epoch: 28/100 | step: 170/422 | loss: 2.2848029136657715\n",
      "Epoch: 28/100 | step: 171/422 | loss: 3.1238954067230225\n",
      "Epoch: 28/100 | step: 172/422 | loss: 3.210268020629883\n",
      "Epoch: 28/100 | step: 173/422 | loss: 2.725311279296875\n",
      "Epoch: 28/100 | step: 174/422 | loss: 2.8466341495513916\n",
      "Epoch: 28/100 | step: 175/422 | loss: 2.3267364501953125\n",
      "Epoch: 28/100 | step: 176/422 | loss: 2.824204921722412\n",
      "Epoch: 28/100 | step: 177/422 | loss: 2.601799488067627\n",
      "Epoch: 28/100 | step: 178/422 | loss: 2.5980849266052246\n",
      "Epoch: 28/100 | step: 179/422 | loss: 2.947299003601074\n",
      "Epoch: 28/100 | step: 180/422 | loss: 2.7608370780944824\n",
      "Epoch: 28/100 | step: 181/422 | loss: 2.992229700088501\n",
      "Epoch: 28/100 | step: 182/422 | loss: 2.204172372817993\n",
      "Epoch: 28/100 | step: 183/422 | loss: 3.343592405319214\n",
      "Epoch: 28/100 | step: 184/422 | loss: 3.213872194290161\n",
      "Epoch: 28/100 | step: 185/422 | loss: 2.8124873638153076\n",
      "Epoch: 28/100 | step: 186/422 | loss: 2.6695101261138916\n",
      "Epoch: 28/100 | step: 187/422 | loss: 2.8135721683502197\n",
      "Epoch: 28/100 | step: 188/422 | loss: 2.2887911796569824\n",
      "Epoch: 28/100 | step: 189/422 | loss: 2.386245012283325\n",
      "Epoch: 28/100 | step: 190/422 | loss: 2.1458497047424316\n",
      "Epoch: 28/100 | step: 191/422 | loss: 2.539968252182007\n",
      "Epoch: 28/100 | step: 192/422 | loss: 2.7466256618499756\n",
      "Epoch: 28/100 | step: 193/422 | loss: 3.0043890476226807\n",
      "Epoch: 28/100 | step: 194/422 | loss: 2.5596020221710205\n",
      "Epoch: 28/100 | step: 195/422 | loss: 2.559522867202759\n",
      "Epoch: 28/100 | step: 196/422 | loss: 3.017385244369507\n",
      "Epoch: 28/100 | step: 197/422 | loss: 2.8928709030151367\n",
      "Epoch: 28/100 | step: 198/422 | loss: 2.7178988456726074\n",
      "Epoch: 28/100 | step: 199/422 | loss: 3.294861316680908\n",
      "Epoch: 28/100 | step: 200/422 | loss: 2.226248025894165\n",
      "Epoch: 28/100 | step: 201/422 | loss: 2.6460208892822266\n",
      "Epoch: 28/100 | step: 202/422 | loss: 2.856741428375244\n",
      "Epoch: 28/100 | step: 203/422 | loss: 2.7535502910614014\n",
      "Epoch: 28/100 | step: 204/422 | loss: 2.6363072395324707\n",
      "Epoch: 28/100 | step: 205/422 | loss: 3.0178792476654053\n",
      "Epoch: 28/100 | step: 206/422 | loss: 3.065967082977295\n",
      "Epoch: 28/100 | step: 207/422 | loss: 2.7411367893218994\n",
      "Epoch: 28/100 | step: 208/422 | loss: 2.834894895553589\n",
      "Epoch: 28/100 | step: 209/422 | loss: 2.417898178100586\n",
      "Epoch: 28/100 | step: 210/422 | loss: 2.736043691635132\n",
      "Epoch: 28/100 | step: 211/422 | loss: 3.4692203998565674\n",
      "Epoch: 28/100 | step: 212/422 | loss: 2.2081758975982666\n",
      "Epoch: 28/100 | step: 213/422 | loss: 2.865347146987915\n",
      "Epoch: 28/100 | step: 214/422 | loss: 2.660959482192993\n",
      "Epoch: 28/100 | step: 215/422 | loss: 2.6852173805236816\n",
      "Epoch: 28/100 | step: 216/422 | loss: 2.6654276847839355\n",
      "Epoch: 28/100 | step: 217/422 | loss: 3.169304847717285\n",
      "Epoch: 28/100 | step: 218/422 | loss: 2.719215154647827\n",
      "Epoch: 28/100 | step: 219/422 | loss: 3.002260446548462\n",
      "Epoch: 28/100 | step: 220/422 | loss: 2.7992100715637207\n",
      "Epoch: 28/100 | step: 221/422 | loss: 2.609722137451172\n",
      "Epoch: 28/100 | step: 222/422 | loss: 2.6423287391662598\n",
      "Epoch: 28/100 | step: 223/422 | loss: 2.6632919311523438\n",
      "Epoch: 28/100 | step: 224/422 | loss: 2.629672050476074\n",
      "Epoch: 28/100 | step: 225/422 | loss: 2.779874086380005\n",
      "Epoch: 28/100 | step: 226/422 | loss: 2.360553503036499\n",
      "Epoch: 28/100 | step: 227/422 | loss: 3.0020925998687744\n",
      "Epoch: 28/100 | step: 228/422 | loss: 2.7034499645233154\n",
      "Epoch: 28/100 | step: 229/422 | loss: 3.2239341735839844\n",
      "Epoch: 28/100 | step: 230/422 | loss: 2.729477882385254\n",
      "Epoch: 28/100 | step: 231/422 | loss: 2.5275087356567383\n",
      "Epoch: 28/100 | step: 232/422 | loss: 2.552159309387207\n",
      "Epoch: 28/100 | step: 233/422 | loss: 2.578073024749756\n",
      "Epoch: 28/100 | step: 234/422 | loss: 2.711125135421753\n",
      "Epoch: 28/100 | step: 235/422 | loss: 2.93874192237854\n",
      "Epoch: 28/100 | step: 236/422 | loss: 3.0687437057495117\n",
      "Epoch: 28/100 | step: 237/422 | loss: 2.7893333435058594\n",
      "Epoch: 28/100 | step: 238/422 | loss: 2.6306722164154053\n",
      "Epoch: 28/100 | step: 239/422 | loss: 3.1709702014923096\n",
      "Epoch: 28/100 | step: 240/422 | loss: 2.6844284534454346\n",
      "Epoch: 28/100 | step: 241/422 | loss: 2.80119252204895\n",
      "Epoch: 28/100 | step: 242/422 | loss: 2.847421884536743\n",
      "Epoch: 28/100 | step: 243/422 | loss: 3.083495616912842\n",
      "Epoch: 28/100 | step: 244/422 | loss: 2.802103042602539\n",
      "Epoch: 28/100 | step: 245/422 | loss: 2.5816667079925537\n",
      "Epoch: 28/100 | step: 246/422 | loss: 2.964570999145508\n",
      "Epoch: 28/100 | step: 247/422 | loss: 2.520744562149048\n",
      "Epoch: 28/100 | step: 248/422 | loss: 2.5908870697021484\n",
      "Epoch: 28/100 | step: 249/422 | loss: 2.1719532012939453\n",
      "Epoch: 28/100 | step: 250/422 | loss: 2.966871500015259\n",
      "Epoch: 28/100 | step: 251/422 | loss: 2.6030218601226807\n",
      "Epoch: 28/100 | step: 252/422 | loss: 2.9904563426971436\n",
      "Epoch: 28/100 | step: 253/422 | loss: 3.0288262367248535\n",
      "Epoch: 28/100 | step: 254/422 | loss: 2.780648708343506\n",
      "Epoch: 28/100 | step: 255/422 | loss: 3.511446475982666\n",
      "Epoch: 28/100 | step: 256/422 | loss: 2.7168190479278564\n",
      "Epoch: 28/100 | step: 257/422 | loss: 2.736541271209717\n",
      "Epoch: 28/100 | step: 258/422 | loss: 2.853755235671997\n",
      "Epoch: 28/100 | step: 259/422 | loss: 2.6572060585021973\n",
      "Epoch: 28/100 | step: 260/422 | loss: 2.3175699710845947\n",
      "Epoch: 28/100 | step: 261/422 | loss: 2.6141574382781982\n",
      "Epoch: 28/100 | step: 262/422 | loss: 2.940617322921753\n",
      "Epoch: 28/100 | step: 263/422 | loss: 2.7816407680511475\n",
      "Epoch: 28/100 | step: 264/422 | loss: 3.2650146484375\n",
      "Epoch: 28/100 | step: 265/422 | loss: 2.481499433517456\n",
      "Epoch: 28/100 | step: 266/422 | loss: 2.871368169784546\n",
      "Epoch: 28/100 | step: 267/422 | loss: 2.6519722938537598\n",
      "Epoch: 28/100 | step: 268/422 | loss: 2.991950750350952\n",
      "Epoch: 28/100 | step: 269/422 | loss: 2.6823675632476807\n",
      "Epoch: 28/100 | step: 270/422 | loss: 2.406787633895874\n",
      "Epoch: 28/100 | step: 271/422 | loss: 2.952859878540039\n",
      "Epoch: 28/100 | step: 272/422 | loss: 2.1955983638763428\n",
      "Epoch: 28/100 | step: 273/422 | loss: 2.417583703994751\n",
      "Epoch: 28/100 | step: 274/422 | loss: 2.5434932708740234\n",
      "Epoch: 28/100 | step: 275/422 | loss: 2.3619613647460938\n",
      "Epoch: 28/100 | step: 276/422 | loss: 2.3708372116088867\n",
      "Epoch: 28/100 | step: 277/422 | loss: 3.082627534866333\n",
      "Epoch: 28/100 | step: 278/422 | loss: 2.646063804626465\n",
      "Epoch: 28/100 | step: 279/422 | loss: 2.7559654712677\n",
      "Epoch: 28/100 | step: 280/422 | loss: 3.1596169471740723\n",
      "Epoch: 28/100 | step: 281/422 | loss: 2.6347732543945312\n",
      "Epoch: 28/100 | step: 282/422 | loss: 2.4826738834381104\n",
      "Epoch: 28/100 | step: 283/422 | loss: 3.081993341445923\n",
      "Epoch: 28/100 | step: 284/422 | loss: 3.3273732662200928\n",
      "Epoch: 28/100 | step: 285/422 | loss: 3.2179372310638428\n",
      "Epoch: 28/100 | step: 286/422 | loss: 2.4438915252685547\n",
      "Epoch: 28/100 | step: 287/422 | loss: 2.71500563621521\n",
      "Epoch: 28/100 | step: 288/422 | loss: 2.5338823795318604\n",
      "Epoch: 28/100 | step: 289/422 | loss: 2.2239162921905518\n",
      "Epoch: 28/100 | step: 290/422 | loss: 3.013850688934326\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 29/100 | step: 1/422 | loss: 2.642122268676758\n",
      "Epoch: 29/100 | step: 2/422 | loss: 2.9805517196655273\n",
      "Epoch: 29/100 | step: 3/422 | loss: 2.6165921688079834\n",
      "Epoch: 29/100 | step: 4/422 | loss: 2.953216075897217\n",
      "Epoch: 29/100 | step: 5/422 | loss: 2.707037925720215\n",
      "Epoch: 29/100 | step: 6/422 | loss: 2.2592177391052246\n",
      "Epoch: 29/100 | step: 7/422 | loss: 2.538707971572876\n",
      "Epoch: 29/100 | step: 8/422 | loss: 2.579749822616577\n",
      "Epoch: 29/100 | step: 9/422 | loss: 2.3481602668762207\n",
      "Epoch: 29/100 | step: 10/422 | loss: 2.4159317016601562\n",
      "Epoch: 29/100 | step: 11/422 | loss: 2.696464776992798\n",
      "Epoch: 29/100 | step: 12/422 | loss: 2.0800602436065674\n",
      "Epoch: 29/100 | step: 13/422 | loss: 2.185387372970581\n",
      "Epoch: 29/100 | step: 14/422 | loss: 2.7620279788970947\n",
      "Epoch: 29/100 | step: 15/422 | loss: 2.7334067821502686\n",
      "Epoch: 29/100 | step: 16/422 | loss: 2.6511783599853516\n",
      "Epoch: 29/100 | step: 17/422 | loss: 2.5120978355407715\n",
      "Epoch: 29/100 | step: 18/422 | loss: 2.723994493484497\n",
      "Epoch: 29/100 | step: 19/422 | loss: 1.9774882793426514\n",
      "Epoch: 29/100 | step: 20/422 | loss: 2.7325775623321533\n",
      "Epoch: 29/100 | step: 21/422 | loss: 2.5902116298675537\n",
      "Epoch: 29/100 | step: 22/422 | loss: 2.3510031700134277\n",
      "Epoch: 29/100 | step: 23/422 | loss: 2.5249862670898438\n",
      "Epoch: 29/100 | step: 24/422 | loss: 2.2942583560943604\n",
      "Epoch: 29/100 | step: 25/422 | loss: 2.663792133331299\n",
      "Epoch: 29/100 | step: 26/422 | loss: 2.5500142574310303\n",
      "Epoch: 29/100 | step: 27/422 | loss: 2.972533702850342\n",
      "Epoch: 29/100 | step: 28/422 | loss: 2.7495157718658447\n",
      "Epoch: 29/100 | step: 29/422 | loss: 2.558990240097046\n",
      "Epoch: 29/100 | step: 30/422 | loss: 2.734701156616211\n",
      "Epoch: 29/100 | step: 31/422 | loss: 2.5095670223236084\n",
      "Epoch: 29/100 | step: 32/422 | loss: 2.8498618602752686\n",
      "Epoch: 29/100 | step: 33/422 | loss: 2.6762259006500244\n",
      "Epoch: 29/100 | step: 34/422 | loss: 2.544985294342041\n",
      "Epoch: 29/100 | step: 35/422 | loss: 1.8829487562179565\n",
      "Epoch: 29/100 | step: 36/422 | loss: 2.7923247814178467\n",
      "Epoch: 29/100 | step: 37/422 | loss: 3.0516269207000732\n",
      "Epoch: 29/100 | step: 38/422 | loss: 2.5897626876831055\n",
      "Epoch: 29/100 | step: 39/422 | loss: 2.1253511905670166\n",
      "Epoch: 29/100 | step: 40/422 | loss: 1.7237523794174194\n",
      "Epoch: 29/100 | step: 41/422 | loss: 2.1377224922180176\n",
      "Epoch: 29/100 | step: 42/422 | loss: 2.8302769660949707\n",
      "Epoch: 29/100 | step: 43/422 | loss: 2.851040840148926\n",
      "Epoch: 29/100 | step: 44/422 | loss: 2.4309656620025635\n",
      "Epoch: 29/100 | step: 45/422 | loss: 2.6075758934020996\n",
      "Epoch: 29/100 | step: 46/422 | loss: 2.0154943466186523\n",
      "Epoch: 29/100 | step: 47/422 | loss: 2.215639591217041\n",
      "Epoch: 29/100 | step: 48/422 | loss: 2.392366409301758\n",
      "Epoch: 29/100 | step: 49/422 | loss: 2.1809937953948975\n",
      "Epoch: 29/100 | step: 50/422 | loss: 2.548002243041992\n",
      "Epoch: 29/100 | step: 51/422 | loss: 2.637033462524414\n",
      "Epoch: 29/100 | step: 52/422 | loss: 2.466151714324951\n",
      "Epoch: 29/100 | step: 53/422 | loss: 3.234001398086548\n",
      "Epoch: 29/100 | step: 54/422 | loss: 2.5731024742126465\n",
      "Epoch: 29/100 | step: 55/422 | loss: 2.4433746337890625\n",
      "Epoch: 29/100 | step: 56/422 | loss: 2.679582118988037\n",
      "Epoch: 29/100 | step: 57/422 | loss: 2.440889358520508\n",
      "Epoch: 29/100 | step: 58/422 | loss: 2.8165040016174316\n",
      "Epoch: 29/100 | step: 59/422 | loss: 2.580594301223755\n",
      "Epoch: 29/100 | step: 60/422 | loss: 2.5659914016723633\n",
      "Epoch: 29/100 | step: 61/422 | loss: 3.089906930923462\n",
      "Epoch: 29/100 | step: 62/422 | loss: 2.5971169471740723\n",
      "Epoch: 29/100 | step: 63/422 | loss: 2.9062623977661133\n",
      "Epoch: 29/100 | step: 64/422 | loss: 3.3113644123077393\n",
      "Epoch: 29/100 | step: 65/422 | loss: 2.468583583831787\n",
      "Epoch: 29/100 | step: 66/422 | loss: 2.504220485687256\n",
      "Epoch: 29/100 | step: 67/422 | loss: 2.292724370956421\n",
      "Epoch: 29/100 | step: 68/422 | loss: 3.085784673690796\n",
      "Epoch: 29/100 | step: 69/422 | loss: 2.846198797225952\n",
      "Epoch: 29/100 | step: 70/422 | loss: 2.3541276454925537\n",
      "Epoch: 29/100 | step: 71/422 | loss: 3.5359537601470947\n",
      "Epoch: 29/100 | step: 72/422 | loss: 2.344618082046509\n",
      "Epoch: 29/100 | step: 73/422 | loss: 2.5628154277801514\n",
      "Epoch: 29/100 | step: 74/422 | loss: 2.5743343830108643\n",
      "Epoch: 29/100 | step: 75/422 | loss: 2.8810040950775146\n",
      "Epoch: 29/100 | step: 76/422 | loss: 2.6677229404449463\n",
      "Epoch: 29/100 | step: 77/422 | loss: 2.9435548782348633\n",
      "Epoch: 29/100 | step: 78/422 | loss: 3.1180269718170166\n",
      "Epoch: 29/100 | step: 79/422 | loss: 2.5255210399627686\n",
      "Epoch: 29/100 | step: 80/422 | loss: 2.9323055744171143\n",
      "Epoch: 29/100 | step: 81/422 | loss: 2.9074149131774902\n",
      "Epoch: 29/100 | step: 82/422 | loss: 2.4097604751586914\n",
      "Epoch: 29/100 | step: 83/422 | loss: 2.5430655479431152\n",
      "Epoch: 29/100 | step: 84/422 | loss: 2.3315577507019043\n",
      "Epoch: 29/100 | step: 85/422 | loss: 1.7456704378128052\n",
      "Epoch: 29/100 | step: 86/422 | loss: 2.5866007804870605\n",
      "Epoch: 29/100 | step: 87/422 | loss: 2.329495429992676\n",
      "Epoch: 29/100 | step: 88/422 | loss: 2.1921803951263428\n",
      "Epoch: 29/100 | step: 89/422 | loss: 2.662630558013916\n",
      "Epoch: 29/100 | step: 90/422 | loss: 2.7283132076263428\n",
      "Epoch: 29/100 | step: 91/422 | loss: 2.7977943420410156\n",
      "Epoch: 29/100 | step: 92/422 | loss: 2.5277621746063232\n",
      "Epoch: 29/100 | step: 93/422 | loss: 2.951101303100586\n",
      "Epoch: 29/100 | step: 94/422 | loss: 2.3382604122161865\n",
      "Epoch: 29/100 | step: 95/422 | loss: 2.528887987136841\n",
      "Epoch: 29/100 | step: 96/422 | loss: 2.465128183364868\n",
      "Epoch: 29/100 | step: 97/422 | loss: 2.5527591705322266\n",
      "Epoch: 29/100 | step: 98/422 | loss: 2.6125566959381104\n",
      "Epoch: 29/100 | step: 99/422 | loss: 2.349505662918091\n",
      "Epoch: 29/100 | step: 100/422 | loss: 2.513584852218628\n",
      "Epoch: 29/100 | step: 101/422 | loss: 2.398021936416626\n",
      "Epoch: 29/100 | step: 102/422 | loss: 2.711622953414917\n",
      "Epoch: 29/100 | step: 103/422 | loss: 2.2561047077178955\n",
      "Epoch: 29/100 | step: 104/422 | loss: 2.764727830886841\n",
      "Epoch: 29/100 | step: 105/422 | loss: 2.8510401248931885\n",
      "Epoch: 29/100 | step: 106/422 | loss: 2.3358359336853027\n",
      "Epoch: 29/100 | step: 107/422 | loss: 2.533132791519165\n",
      "Epoch: 29/100 | step: 108/422 | loss: 2.746572256088257\n",
      "Epoch: 29/100 | step: 109/422 | loss: 2.3275558948516846\n",
      "Epoch: 29/100 | step: 110/422 | loss: 2.2971184253692627\n",
      "Epoch: 29/100 | step: 111/422 | loss: 2.0659477710723877\n",
      "Epoch: 29/100 | step: 112/422 | loss: 2.4818270206451416\n",
      "Epoch: 29/100 | step: 113/422 | loss: 2.0800485610961914\n",
      "Epoch: 29/100 | step: 114/422 | loss: 2.4031288623809814\n",
      "Epoch: 29/100 | step: 115/422 | loss: 2.359257698059082\n",
      "Epoch: 29/100 | step: 116/422 | loss: 3.1279988288879395\n",
      "Epoch: 29/100 | step: 117/422 | loss: 2.902094841003418\n",
      "Epoch: 29/100 | step: 118/422 | loss: 2.1080141067504883\n",
      "Epoch: 29/100 | step: 119/422 | loss: 2.910430431365967\n",
      "Epoch: 29/100 | step: 120/422 | loss: 2.473069667816162\n",
      "Epoch: 29/100 | step: 121/422 | loss: 2.5862698554992676\n",
      "Epoch: 29/100 | step: 122/422 | loss: 2.722524881362915\n",
      "Epoch: 29/100 | step: 123/422 | loss: 2.8721518516540527\n",
      "Epoch: 29/100 | step: 124/422 | loss: 3.071108341217041\n",
      "Epoch: 29/100 | step: 125/422 | loss: 2.5992822647094727\n",
      "Epoch: 29/100 | step: 126/422 | loss: 2.136716365814209\n",
      "Epoch: 29/100 | step: 127/422 | loss: 2.7423653602600098\n",
      "Epoch: 29/100 | step: 128/422 | loss: 2.45560359954834\n",
      "Epoch: 29/100 | step: 129/422 | loss: 2.904735803604126\n",
      "Epoch: 29/100 | step: 130/422 | loss: 2.300459861755371\n",
      "Epoch: 29/100 | step: 131/422 | loss: 3.087336540222168\n",
      "Epoch: 29/100 | step: 132/422 | loss: 2.9002444744110107\n",
      "Epoch: 29/100 | step: 133/422 | loss: 2.5810978412628174\n",
      "Epoch: 29/100 | step: 134/422 | loss: 2.6096839904785156\n",
      "Epoch: 29/100 | step: 135/422 | loss: 2.661644697189331\n",
      "Epoch: 29/100 | step: 136/422 | loss: 2.311664342880249\n",
      "Epoch: 29/100 | step: 137/422 | loss: 2.9816553592681885\n",
      "Epoch: 29/100 | step: 138/422 | loss: 2.424450397491455\n",
      "Epoch: 29/100 | step: 139/422 | loss: 2.415627956390381\n",
      "Epoch: 29/100 | step: 140/422 | loss: 2.2937357425689697\n",
      "Epoch: 29/100 | step: 141/422 | loss: 3.2322449684143066\n",
      "Epoch: 29/100 | step: 142/422 | loss: 2.8695826530456543\n",
      "Epoch: 29/100 | step: 143/422 | loss: 2.7874770164489746\n",
      "Epoch: 29/100 | step: 144/422 | loss: 2.871034860610962\n",
      "Epoch: 29/100 | step: 145/422 | loss: 2.5699496269226074\n",
      "Epoch: 29/100 | step: 146/422 | loss: 2.634479522705078\n",
      "Epoch: 29/100 | step: 147/422 | loss: 2.498462438583374\n",
      "Epoch: 29/100 | step: 148/422 | loss: 2.9507694244384766\n",
      "Epoch: 29/100 | step: 149/422 | loss: 2.2227063179016113\n",
      "Epoch: 29/100 | step: 150/422 | loss: 1.87678861618042\n",
      "Epoch: 29/100 | step: 151/422 | loss: 2.644050121307373\n",
      "Epoch: 29/100 | step: 152/422 | loss: 2.17856502532959\n",
      "Epoch: 29/100 | step: 153/422 | loss: 2.6886050701141357\n",
      "Epoch: 29/100 | step: 154/422 | loss: 3.0635874271392822\n",
      "Epoch: 29/100 | step: 155/422 | loss: 2.5870866775512695\n",
      "Epoch: 29/100 | step: 156/422 | loss: 3.190159797668457\n",
      "Epoch: 29/100 | step: 157/422 | loss: 2.9681341648101807\n",
      "Epoch: 29/100 | step: 158/422 | loss: 2.7055776119232178\n",
      "Epoch: 29/100 | step: 159/422 | loss: 2.1340434551239014\n",
      "Epoch: 29/100 | step: 160/422 | loss: 2.5794575214385986\n",
      "Epoch: 29/100 | step: 161/422 | loss: 2.639939069747925\n",
      "Epoch: 29/100 | step: 162/422 | loss: 2.4275574684143066\n",
      "Epoch: 29/100 | step: 163/422 | loss: 3.0038270950317383\n",
      "Epoch: 29/100 | step: 164/422 | loss: 2.8403513431549072\n",
      "Epoch: 29/100 | step: 165/422 | loss: 3.0624639987945557\n",
      "Epoch: 29/100 | step: 166/422 | loss: 2.5964596271514893\n",
      "Epoch: 29/100 | step: 167/422 | loss: 2.6784157752990723\n",
      "Epoch: 29/100 | step: 168/422 | loss: 2.369453191757202\n",
      "Epoch: 29/100 | step: 169/422 | loss: 2.788555145263672\n",
      "Epoch: 29/100 | step: 170/422 | loss: 2.3885338306427\n",
      "Epoch: 29/100 | step: 171/422 | loss: 2.4602468013763428\n",
      "Epoch: 29/100 | step: 172/422 | loss: 3.33232045173645\n",
      "Epoch: 29/100 | step: 173/422 | loss: 2.7537734508514404\n",
      "Epoch: 29/100 | step: 174/422 | loss: 3.0346055030822754\n",
      "Epoch: 29/100 | step: 175/422 | loss: 2.3611629009246826\n",
      "Epoch: 29/100 | step: 176/422 | loss: 2.455611228942871\n",
      "Epoch: 29/100 | step: 177/422 | loss: 2.8323307037353516\n",
      "Epoch: 29/100 | step: 178/422 | loss: 2.7481255531311035\n",
      "Epoch: 29/100 | step: 179/422 | loss: 2.5810935497283936\n",
      "Epoch: 29/100 | step: 180/422 | loss: 2.222233772277832\n",
      "Epoch: 29/100 | step: 181/422 | loss: 2.8447089195251465\n",
      "Epoch: 29/100 | step: 182/422 | loss: 2.5448598861694336\n",
      "Epoch: 29/100 | step: 183/422 | loss: 2.5795674324035645\n",
      "Epoch: 29/100 | step: 184/422 | loss: 2.1680290699005127\n",
      "Epoch: 29/100 | step: 185/422 | loss: 2.7137651443481445\n",
      "Epoch: 29/100 | step: 186/422 | loss: 2.4892196655273438\n",
      "Epoch: 29/100 | step: 187/422 | loss: 2.8518850803375244\n",
      "Epoch: 29/100 | step: 188/422 | loss: 2.2827157974243164\n",
      "Epoch: 29/100 | step: 189/422 | loss: 2.4352049827575684\n",
      "Epoch: 29/100 | step: 190/422 | loss: 3.146368980407715\n",
      "Epoch: 29/100 | step: 191/422 | loss: 2.573247194290161\n",
      "Epoch: 29/100 | step: 192/422 | loss: 2.2785391807556152\n",
      "Epoch: 29/100 | step: 193/422 | loss: 2.2580204010009766\n",
      "Epoch: 29/100 | step: 194/422 | loss: 2.8616859912872314\n",
      "Epoch: 29/100 | step: 195/422 | loss: 2.3730764389038086\n",
      "Epoch: 29/100 | step: 196/422 | loss: 2.9383957386016846\n",
      "Epoch: 29/100 | step: 197/422 | loss: 3.314636707305908\n",
      "Epoch: 29/100 | step: 198/422 | loss: 2.721397638320923\n",
      "Epoch: 29/100 | step: 199/422 | loss: 2.8660757541656494\n",
      "Epoch: 29/100 | step: 200/422 | loss: 3.031200885772705\n",
      "Epoch: 29/100 | step: 201/422 | loss: 2.6533894538879395\n",
      "Epoch: 29/100 | step: 202/422 | loss: 2.9214258193969727\n",
      "Epoch: 29/100 | step: 203/422 | loss: 2.489529609680176\n",
      "Epoch: 29/100 | step: 204/422 | loss: 2.8352742195129395\n",
      "Epoch: 29/100 | step: 205/422 | loss: 2.894554376602173\n",
      "Epoch: 29/100 | step: 206/422 | loss: 2.876544952392578\n",
      "Epoch: 29/100 | step: 207/422 | loss: 2.683638334274292\n",
      "Epoch: 29/100 | step: 208/422 | loss: 2.6367106437683105\n",
      "Epoch: 29/100 | step: 209/422 | loss: 2.5650763511657715\n",
      "Epoch: 29/100 | step: 210/422 | loss: 2.974177122116089\n",
      "Epoch: 29/100 | step: 211/422 | loss: 2.504364013671875\n",
      "Epoch: 29/100 | step: 212/422 | loss: 2.632105827331543\n",
      "Epoch: 29/100 | step: 213/422 | loss: 2.890385150909424\n",
      "Epoch: 29/100 | step: 214/422 | loss: 2.3990800380706787\n",
      "Epoch: 29/100 | step: 215/422 | loss: 2.6712229251861572\n",
      "Epoch: 29/100 | step: 216/422 | loss: 2.7790863513946533\n",
      "Epoch: 29/100 | step: 217/422 | loss: 2.4424498081207275\n",
      "Epoch: 29/100 | step: 218/422 | loss: 2.353363037109375\n",
      "Epoch: 29/100 | step: 219/422 | loss: 2.5645196437835693\n",
      "Epoch: 29/100 | step: 220/422 | loss: 2.5096752643585205\n",
      "Epoch: 29/100 | step: 221/422 | loss: 2.4982268810272217\n",
      "Epoch: 29/100 | step: 222/422 | loss: 2.251940965652466\n",
      "Epoch: 29/100 | step: 223/422 | loss: 2.382391929626465\n",
      "Epoch: 29/100 | step: 224/422 | loss: 2.5841970443725586\n",
      "Epoch: 29/100 | step: 225/422 | loss: 2.8400251865386963\n",
      "Epoch: 29/100 | step: 226/422 | loss: 3.0950465202331543\n",
      "Epoch: 29/100 | step: 227/422 | loss: 2.6392643451690674\n",
      "Epoch: 29/100 | step: 228/422 | loss: 2.92875337600708\n",
      "Epoch: 29/100 | step: 229/422 | loss: 2.9881300926208496\n",
      "Epoch: 29/100 | step: 230/422 | loss: 2.6815273761749268\n",
      "Epoch: 29/100 | step: 231/422 | loss: 3.0487353801727295\n",
      "Epoch: 29/100 | step: 232/422 | loss: 2.86673903465271\n",
      "Epoch: 29/100 | step: 233/422 | loss: 3.3321781158447266\n",
      "Epoch: 29/100 | step: 234/422 | loss: 2.4938952922821045\n",
      "Epoch: 29/100 | step: 235/422 | loss: 2.244959592819214\n",
      "Epoch: 29/100 | step: 236/422 | loss: 2.9272351264953613\n",
      "Epoch: 29/100 | step: 237/422 | loss: 2.725327491760254\n",
      "Epoch: 29/100 | step: 238/422 | loss: 2.348560333251953\n",
      "Epoch: 29/100 | step: 239/422 | loss: 3.0215797424316406\n",
      "Epoch: 29/100 | step: 240/422 | loss: 2.6489241123199463\n",
      "Epoch: 29/100 | step: 241/422 | loss: 2.5891175270080566\n",
      "Epoch: 29/100 | step: 242/422 | loss: 2.719989538192749\n",
      "Epoch: 29/100 | step: 243/422 | loss: 2.983854055404663\n",
      "Epoch: 29/100 | step: 244/422 | loss: 2.6791462898254395\n",
      "Epoch: 29/100 | step: 245/422 | loss: 2.4844536781311035\n",
      "Epoch: 29/100 | step: 246/422 | loss: 2.93497371673584\n",
      "Epoch: 29/100 | step: 247/422 | loss: 2.565600872039795\n",
      "Epoch: 29/100 | step: 248/422 | loss: 2.7715141773223877\n",
      "Epoch: 29/100 | step: 249/422 | loss: 2.577160120010376\n",
      "Epoch: 29/100 | step: 250/422 | loss: 2.759005546569824\n",
      "Epoch: 29/100 | step: 251/422 | loss: 2.542811870574951\n",
      "Epoch: 29/100 | step: 252/422 | loss: 2.367079973220825\n",
      "Epoch: 29/100 | step: 253/422 | loss: 3.177604913711548\n",
      "Epoch: 29/100 | step: 254/422 | loss: 2.7202181816101074\n",
      "Epoch: 29/100 | step: 255/422 | loss: 2.6921026706695557\n",
      "Epoch: 29/100 | step: 256/422 | loss: 2.172003984451294\n",
      "Epoch: 29/100 | step: 257/422 | loss: 2.5981462001800537\n",
      "Epoch: 29/100 | step: 258/422 | loss: 2.8465986251831055\n",
      "Epoch: 29/100 | step: 259/422 | loss: 2.978165864944458\n",
      "Epoch: 29/100 | step: 260/422 | loss: 2.5112719535827637\n",
      "Epoch: 29/100 | step: 261/422 | loss: 2.2398593425750732\n",
      "Epoch: 29/100 | step: 262/422 | loss: 2.9935476779937744\n",
      "Epoch: 29/100 | step: 263/422 | loss: 2.9130334854125977\n",
      "Epoch: 29/100 | step: 264/422 | loss: 2.4189414978027344\n",
      "Epoch: 29/100 | step: 265/422 | loss: 2.354754686355591\n",
      "Epoch: 29/100 | step: 266/422 | loss: 3.0469725131988525\n",
      "Epoch: 29/100 | step: 267/422 | loss: 3.051283836364746\n",
      "Epoch: 29/100 | step: 268/422 | loss: 2.6551969051361084\n",
      "Epoch: 29/100 | step: 269/422 | loss: 2.522932529449463\n",
      "Epoch: 29/100 | step: 270/422 | loss: 2.7244415283203125\n",
      "Epoch: 29/100 | step: 271/422 | loss: 2.40057635307312\n",
      "Epoch: 29/100 | step: 272/422 | loss: 2.4912936687469482\n",
      "Epoch: 29/100 | step: 273/422 | loss: 2.4097509384155273\n",
      "Epoch: 29/100 | step: 274/422 | loss: 2.721924304962158\n",
      "Epoch: 29/100 | step: 275/422 | loss: 2.1531596183776855\n",
      "Epoch: 29/100 | step: 276/422 | loss: 3.1141631603240967\n",
      "Epoch: 29/100 | step: 277/422 | loss: 2.325321674346924\n",
      "Epoch: 29/100 | step: 278/422 | loss: 2.7732973098754883\n",
      "Epoch: 29/100 | step: 279/422 | loss: 3.3159501552581787\n",
      "Epoch: 29/100 | step: 280/422 | loss: 2.713892936706543\n",
      "Epoch: 29/100 | step: 281/422 | loss: 2.4983108043670654\n",
      "Epoch: 29/100 | step: 282/422 | loss: 2.9022927284240723\n",
      "Epoch: 29/100 | step: 283/422 | loss: 2.1110382080078125\n",
      "Epoch: 29/100 | step: 284/422 | loss: 2.933791160583496\n",
      "Epoch: 29/100 | step: 285/422 | loss: 2.6918540000915527\n",
      "Epoch: 29/100 | step: 286/422 | loss: 2.2318923473358154\n",
      "Epoch: 29/100 | step: 287/422 | loss: 2.4530739784240723\n",
      "Epoch: 29/100 | step: 288/422 | loss: 2.132288932800293\n",
      "Epoch: 29/100 | step: 289/422 | loss: 2.5714609622955322\n",
      "Epoch: 29/100 | step: 290/422 | loss: 2.56441330909729\n",
      "Epoch: 29/100 | step: 291/422 | loss: 2.4599008560180664\n",
      "Epoch: 29/100 | step: 292/422 | loss: 3.077554941177368\n",
      "Epoch: 29/100 | step: 293/422 | loss: 2.3221118450164795\n",
      "Epoch: 29/100 | step: 294/422 | loss: 3.2610225677490234\n",
      "Epoch: 29/100 | step: 295/422 | loss: 2.8366947174072266\n",
      "Epoch: 29/100 | step: 296/422 | loss: 2.7549712657928467\n",
      "Epoch: 29/100 | step: 297/422 | loss: 2.846367597579956\n",
      "Epoch: 29/100 | step: 298/422 | loss: 2.184798240661621\n",
      "Epoch: 29/100 | step: 299/422 | loss: 2.463757276535034\n",
      "Epoch: 29/100 | step: 300/422 | loss: 2.5609922409057617\n",
      "Epoch: 29/100 | step: 301/422 | loss: 2.300133466720581\n",
      "Epoch: 29/100 | step: 302/422 | loss: 2.14412784576416\n",
      "Epoch: 29/100 | step: 303/422 | loss: 2.280977487564087\n",
      "Epoch: 29/100 | step: 304/422 | loss: 3.39224910736084\n",
      "Epoch: 29/100 | step: 305/422 | loss: 2.561465263366699\n",
      "Epoch: 29/100 | step: 306/422 | loss: 2.8322365283966064\n",
      "Epoch: 29/100 | step: 307/422 | loss: 3.472397804260254\n",
      "Epoch: 29/100 | step: 308/422 | loss: 2.61911678314209\n",
      "Epoch: 29/100 | step: 309/422 | loss: 2.2689764499664307\n",
      "Epoch: 29/100 | step: 310/422 | loss: 2.974167585372925\n",
      "Epoch: 29/100 | step: 311/422 | loss: 2.504850387573242\n",
      "Epoch: 29/100 | step: 312/422 | loss: 2.501796245574951\n",
      "Epoch: 29/100 | step: 313/422 | loss: 3.0386178493499756\n",
      "Epoch: 29/100 | step: 314/422 | loss: 2.386725902557373\n",
      "Epoch: 29/100 | step: 315/422 | loss: 2.305635929107666\n",
      "Epoch: 29/100 | step: 316/422 | loss: 3.089059352874756\n",
      "Epoch: 29/100 | step: 317/422 | loss: 2.864549398422241\n",
      "Epoch: 29/100 | step: 318/422 | loss: 2.7033114433288574\n",
      "Epoch: 29/100 | step: 319/422 | loss: 2.5507583618164062\n",
      "Epoch: 29/100 | step: 320/422 | loss: 3.1337332725524902\n",
      "Epoch: 29/100 | step: 321/422 | loss: 2.6008808612823486\n",
      "Epoch: 29/100 | step: 322/422 | loss: 2.4038844108581543\n",
      "Epoch: 29/100 | step: 323/422 | loss: 2.229393720626831\n",
      "Epoch: 29/100 | step: 324/422 | loss: 2.5642757415771484\n",
      "Epoch: 29/100 | step: 325/422 | loss: 2.856877326965332\n",
      "Epoch: 29/100 | step: 326/422 | loss: 2.845749616622925\n",
      "Epoch: 29/100 | step: 327/422 | loss: 2.506530284881592\n",
      "Epoch: 29/100 | step: 328/422 | loss: 2.650373935699463\n",
      "Epoch: 29/100 | step: 329/422 | loss: 2.7256176471710205\n",
      "Epoch: 29/100 | step: 330/422 | loss: 2.916546583175659\n",
      "Epoch: 29/100 | step: 331/422 | loss: 2.776782512664795\n",
      "Epoch: 29/100 | step: 332/422 | loss: 2.8466122150421143\n",
      "Epoch: 29/100 | step: 333/422 | loss: 2.390331745147705\n",
      "Epoch: 29/100 | step: 334/422 | loss: 2.9297642707824707\n",
      "Epoch: 29/100 | step: 335/422 | loss: 2.266482353210449\n",
      "Epoch: 29/100 | step: 336/422 | loss: 2.1313014030456543\n",
      "Epoch: 29/100 | step: 337/422 | loss: 2.502786636352539\n",
      "Epoch: 29/100 | step: 338/422 | loss: 2.2571282386779785\n",
      "Epoch: 29/100 | step: 339/422 | loss: 2.7675940990448\n",
      "Epoch: 29/100 | step: 340/422 | loss: 2.8151497840881348\n",
      "Epoch: 29/100 | step: 341/422 | loss: 2.7612764835357666\n",
      "Epoch: 29/100 | step: 342/422 | loss: 2.564713478088379\n",
      "Epoch: 29/100 | step: 343/422 | loss: 2.938659429550171\n",
      "Epoch: 29/100 | step: 344/422 | loss: 2.044679641723633\n",
      "Epoch: 29/100 | step: 345/422 | loss: 2.649120807647705\n",
      "Epoch: 29/100 | step: 346/422 | loss: 2.7538790702819824\n",
      "Epoch: 29/100 | step: 347/422 | loss: 2.291921615600586\n",
      "Epoch: 29/100 | step: 348/422 | loss: 3.071206569671631\n",
      "Epoch: 29/100 | step: 349/422 | loss: 2.318063497543335\n",
      "Epoch: 29/100 | step: 350/422 | loss: 3.0566234588623047\n",
      "Epoch: 29/100 | step: 351/422 | loss: 2.6505579948425293\n",
      "Epoch: 29/100 | step: 352/422 | loss: 2.7265305519104004\n",
      "Epoch: 29/100 | step: 353/422 | loss: 2.511993408203125\n",
      "Epoch: 29/100 | step: 354/422 | loss: 2.4614014625549316\n",
      "Epoch: 29/100 | step: 355/422 | loss: 2.798948287963867\n",
      "Epoch: 29/100 | step: 356/422 | loss: 2.3955888748168945\n",
      "Epoch: 29/100 | step: 357/422 | loss: 2.600955009460449\n",
      "Epoch: 29/100 | step: 358/422 | loss: 3.0249295234680176\n",
      "Epoch: 29/100 | step: 359/422 | loss: 2.268531084060669\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 30/100 | step: 1/422 | loss: 2.426638603210449\n",
      "Epoch: 30/100 | step: 2/422 | loss: 2.03716778755188\n",
      "Epoch: 30/100 | step: 3/422 | loss: 2.8135218620300293\n",
      "Epoch: 30/100 | step: 4/422 | loss: 3.1077961921691895\n",
      "Epoch: 30/100 | step: 5/422 | loss: 2.4338326454162598\n",
      "Epoch: 30/100 | step: 6/422 | loss: 2.7899160385131836\n",
      "Epoch: 30/100 | step: 7/422 | loss: 2.569032669067383\n",
      "Epoch: 30/100 | step: 8/422 | loss: 2.407127618789673\n",
      "Epoch: 30/100 | step: 9/422 | loss: 2.06685209274292\n",
      "Epoch: 30/100 | step: 10/422 | loss: 2.531857967376709\n",
      "Epoch: 30/100 | step: 11/422 | loss: 2.2842819690704346\n",
      "Epoch: 30/100 | step: 12/422 | loss: 2.337785005569458\n",
      "Epoch: 30/100 | step: 13/422 | loss: 2.1531872749328613\n",
      "Epoch: 30/100 | step: 14/422 | loss: 2.35461163520813\n",
      "Epoch: 30/100 | step: 15/422 | loss: 2.6668593883514404\n",
      "Epoch: 30/100 | step: 16/422 | loss: 2.4889731407165527\n",
      "Epoch: 30/100 | step: 17/422 | loss: 2.4520859718322754\n",
      "Epoch: 30/100 | step: 18/422 | loss: 2.829319953918457\n",
      "Epoch: 30/100 | step: 19/422 | loss: 2.5402281284332275\n",
      "Epoch: 30/100 | step: 20/422 | loss: 2.598160743713379\n",
      "Epoch: 30/100 | step: 21/422 | loss: 2.673893690109253\n",
      "Epoch: 30/100 | step: 22/422 | loss: 1.9851595163345337\n",
      "Epoch: 30/100 | step: 23/422 | loss: 2.705012083053589\n",
      "Epoch: 30/100 | step: 24/422 | loss: 2.1738576889038086\n",
      "Epoch: 30/100 | step: 25/422 | loss: 2.74393367767334\n",
      "Epoch: 30/100 | step: 26/422 | loss: 2.127804756164551\n",
      "Epoch: 30/100 | step: 27/422 | loss: 2.333859920501709\n",
      "Epoch: 30/100 | step: 28/422 | loss: 2.491724967956543\n",
      "Epoch: 30/100 | step: 29/422 | loss: 2.9106600284576416\n",
      "Epoch: 30/100 | step: 30/422 | loss: 2.1737914085388184\n",
      "Epoch: 30/100 | step: 31/422 | loss: 2.3168857097625732\n",
      "Epoch: 30/100 | step: 32/422 | loss: 2.156863212585449\n",
      "Epoch: 30/100 | step: 33/422 | loss: 2.1626570224761963\n",
      "Epoch: 30/100 | step: 34/422 | loss: 2.155031204223633\n",
      "Epoch: 30/100 | step: 35/422 | loss: 2.609621047973633\n",
      "Epoch: 30/100 | step: 36/422 | loss: 2.548367738723755\n",
      "Epoch: 30/100 | step: 37/422 | loss: 2.367725133895874\n",
      "Epoch: 30/100 | step: 38/422 | loss: 2.7660999298095703\n",
      "Epoch: 30/100 | step: 39/422 | loss: 1.9782326221466064\n",
      "Epoch: 30/100 | step: 40/422 | loss: 2.776031970977783\n",
      "Epoch: 30/100 | step: 41/422 | loss: 2.35168194770813\n",
      "Epoch: 30/100 | step: 42/422 | loss: 2.4829587936401367\n",
      "Epoch: 30/100 | step: 43/422 | loss: 2.5684900283813477\n",
      "Epoch: 30/100 | step: 44/422 | loss: 2.3623645305633545\n",
      "Epoch: 30/100 | step: 45/422 | loss: 2.7883100509643555\n",
      "Epoch: 30/100 | step: 46/422 | loss: 2.411961793899536\n",
      "Epoch: 30/100 | step: 47/422 | loss: 2.3092076778411865\n",
      "Epoch: 30/100 | step: 48/422 | loss: 2.264240264892578\n",
      "Epoch: 30/100 | step: 49/422 | loss: 2.264969825744629\n",
      "Epoch: 30/100 | step: 50/422 | loss: 2.786860942840576\n",
      "Epoch: 30/100 | step: 51/422 | loss: 2.271249532699585\n",
      "Epoch: 30/100 | step: 52/422 | loss: 2.4310319423675537\n",
      "Epoch: 30/100 | step: 53/422 | loss: 2.549769401550293\n",
      "Epoch: 30/100 | step: 54/422 | loss: 2.378044605255127\n",
      "Epoch: 30/100 | step: 55/422 | loss: 2.2638356685638428\n",
      "Epoch: 30/100 | step: 56/422 | loss: 2.7032358646392822\n",
      "Epoch: 30/100 | step: 57/422 | loss: 2.097618579864502\n",
      "Epoch: 30/100 | step: 58/422 | loss: 2.2936367988586426\n",
      "Epoch: 30/100 | step: 59/422 | loss: 1.8022911548614502\n",
      "Epoch: 30/100 | step: 60/422 | loss: 2.726579189300537\n",
      "Epoch: 30/100 | step: 61/422 | loss: 3.10337233543396\n",
      "Epoch: 30/100 | step: 62/422 | loss: 2.8085384368896484\n",
      "Epoch: 30/100 | step: 63/422 | loss: 2.185121774673462\n",
      "Epoch: 30/100 | step: 64/422 | loss: 1.9732439517974854\n",
      "Epoch: 30/100 | step: 65/422 | loss: 2.353639841079712\n",
      "Epoch: 30/100 | step: 66/422 | loss: 3.2017970085144043\n",
      "Epoch: 30/100 | step: 67/422 | loss: 2.273340940475464\n",
      "Epoch: 30/100 | step: 68/422 | loss: 2.4444212913513184\n",
      "Epoch: 30/100 | step: 69/422 | loss: 2.115691900253296\n",
      "Epoch: 30/100 | step: 70/422 | loss: 2.6545016765594482\n",
      "Epoch: 30/100 | step: 71/422 | loss: 2.4995596408843994\n",
      "Epoch: 30/100 | step: 72/422 | loss: 2.8877084255218506\n",
      "Epoch: 30/100 | step: 73/422 | loss: 2.637098789215088\n",
      "Epoch: 30/100 | step: 74/422 | loss: 2.572847604751587\n",
      "Epoch: 30/100 | step: 75/422 | loss: 2.5304813385009766\n",
      "Epoch: 30/100 | step: 76/422 | loss: 2.2114202976226807\n",
      "Epoch: 30/100 | step: 77/422 | loss: 2.4540703296661377\n",
      "Epoch: 30/100 | step: 78/422 | loss: 2.2568531036376953\n",
      "Epoch: 30/100 | step: 79/422 | loss: 2.9710845947265625\n",
      "Epoch: 30/100 | step: 80/422 | loss: 2.6129302978515625\n",
      "Epoch: 30/100 | step: 81/422 | loss: 2.7851524353027344\n",
      "Epoch: 30/100 | step: 82/422 | loss: 2.3728537559509277\n",
      "Epoch: 30/100 | step: 83/422 | loss: 2.200563669204712\n",
      "Epoch: 30/100 | step: 84/422 | loss: 2.537518262863159\n",
      "Epoch: 30/100 | step: 85/422 | loss: 3.068558692932129\n",
      "Epoch: 30/100 | step: 86/422 | loss: 2.2166473865509033\n",
      "Epoch: 30/100 | step: 87/422 | loss: 2.839839458465576\n",
      "Epoch: 30/100 | step: 88/422 | loss: 2.3160603046417236\n",
      "Epoch: 30/100 | step: 89/422 | loss: 2.7516748905181885\n",
      "Epoch: 30/100 | step: 90/422 | loss: 2.997676372528076\n",
      "Epoch: 30/100 | step: 91/422 | loss: 2.345494270324707\n",
      "Epoch: 30/100 | step: 92/422 | loss: 3.0432956218719482\n",
      "Epoch: 30/100 | step: 93/422 | loss: 1.7860543727874756\n",
      "Epoch: 30/100 | step: 94/422 | loss: 2.3727521896362305\n",
      "Epoch: 30/100 | step: 95/422 | loss: 2.6456668376922607\n",
      "Epoch: 30/100 | step: 96/422 | loss: 2.1534860134124756\n",
      "Epoch: 30/100 | step: 97/422 | loss: 2.835137128829956\n",
      "Epoch: 30/100 | step: 98/422 | loss: 2.207937240600586\n",
      "Epoch: 30/100 | step: 99/422 | loss: 2.7448551654815674\n",
      "Epoch: 30/100 | step: 100/422 | loss: 2.179277181625366\n",
      "Epoch: 30/100 | step: 101/422 | loss: 3.1372246742248535\n",
      "Epoch: 30/100 | step: 102/422 | loss: 2.5468294620513916\n",
      "Epoch: 30/100 | step: 103/422 | loss: 2.322369337081909\n",
      "Epoch: 30/100 | step: 104/422 | loss: 2.240994930267334\n",
      "Epoch: 30/100 | step: 105/422 | loss: 2.0455310344696045\n",
      "Epoch: 30/100 | step: 106/422 | loss: 2.548516035079956\n",
      "Epoch: 30/100 | step: 107/422 | loss: 2.639420509338379\n",
      "Epoch: 30/100 | step: 108/422 | loss: 2.8293230533599854\n",
      "Epoch: 30/100 | step: 109/422 | loss: 2.335878610610962\n",
      "Epoch: 30/100 | step: 110/422 | loss: 2.425750970840454\n",
      "Epoch: 30/100 | step: 111/422 | loss: 2.6485347747802734\n",
      "Epoch: 30/100 | step: 112/422 | loss: 2.872443199157715\n",
      "Epoch: 30/100 | step: 113/422 | loss: 2.489872455596924\n",
      "Epoch: 30/100 | step: 114/422 | loss: 2.238152265548706\n",
      "Epoch: 30/100 | step: 115/422 | loss: 2.467512845993042\n",
      "Epoch: 30/100 | step: 116/422 | loss: 2.469160556793213\n",
      "Epoch: 30/100 | step: 117/422 | loss: 2.3923745155334473\n",
      "Epoch: 30/100 | step: 118/422 | loss: 2.513040781021118\n",
      "Epoch: 30/100 | step: 119/422 | loss: 2.532287836074829\n",
      "Epoch: 30/100 | step: 120/422 | loss: 2.6048777103424072\n",
      "Epoch: 30/100 | step: 121/422 | loss: 2.6564273834228516\n",
      "Epoch: 30/100 | step: 122/422 | loss: 2.1672768592834473\n",
      "Epoch: 30/100 | step: 123/422 | loss: 2.3829758167266846\n",
      "Epoch: 30/100 | step: 124/422 | loss: 2.7891173362731934\n",
      "Epoch: 30/100 | step: 125/422 | loss: 2.337739944458008\n",
      "Epoch: 30/100 | step: 126/422 | loss: 2.172656536102295\n",
      "Epoch: 30/100 | step: 127/422 | loss: 2.435319662094116\n",
      "Epoch: 30/100 | step: 128/422 | loss: 2.280172109603882\n",
      "Epoch: 30/100 | step: 129/422 | loss: 2.383906841278076\n",
      "Epoch: 30/100 | step: 130/422 | loss: 2.653855800628662\n",
      "Epoch: 30/100 | step: 131/422 | loss: 2.160663604736328\n",
      "Epoch: 30/100 | step: 132/422 | loss: 2.6537418365478516\n",
      "Epoch: 30/100 | step: 133/422 | loss: 2.8799984455108643\n",
      "Epoch: 30/100 | step: 134/422 | loss: 2.2060625553131104\n",
      "Epoch: 30/100 | step: 135/422 | loss: 2.973430871963501\n",
      "Epoch: 30/100 | step: 136/422 | loss: 2.708892345428467\n",
      "Epoch: 30/100 | step: 137/422 | loss: 2.6149258613586426\n",
      "Epoch: 30/100 | step: 138/422 | loss: 2.930246591567993\n",
      "Epoch: 30/100 | step: 139/422 | loss: 2.5203075408935547\n",
      "Epoch: 30/100 | step: 140/422 | loss: 2.866097927093506\n",
      "Epoch: 30/100 | step: 141/422 | loss: 2.8690474033355713\n",
      "Epoch: 30/100 | step: 142/422 | loss: 2.363375663757324\n",
      "Epoch: 30/100 | step: 143/422 | loss: 2.5884158611297607\n",
      "Epoch: 30/100 | step: 144/422 | loss: 1.9115477800369263\n",
      "Epoch: 30/100 | step: 145/422 | loss: 2.9729557037353516\n",
      "Epoch: 30/100 | step: 146/422 | loss: 2.276121139526367\n",
      "Epoch: 30/100 | step: 147/422 | loss: 2.6042299270629883\n",
      "Epoch: 30/100 | step: 148/422 | loss: 3.194338321685791\n",
      "Epoch: 30/100 | step: 149/422 | loss: 2.116643190383911\n",
      "Epoch: 30/100 | step: 150/422 | loss: 2.3285515308380127\n",
      "Epoch: 30/100 | step: 151/422 | loss: 2.5223512649536133\n",
      "Epoch: 30/100 | step: 152/422 | loss: 2.5223686695098877\n",
      "Epoch: 30/100 | step: 153/422 | loss: 2.522677421569824\n",
      "Epoch: 30/100 | step: 154/422 | loss: 3.0362915992736816\n",
      "Epoch: 30/100 | step: 155/422 | loss: 2.5603232383728027\n",
      "Epoch: 30/100 | step: 156/422 | loss: 2.670910120010376\n",
      "Epoch: 30/100 | step: 157/422 | loss: 2.7441093921661377\n",
      "Epoch: 30/100 | step: 158/422 | loss: 2.7177560329437256\n",
      "Epoch: 30/100 | step: 159/422 | loss: 2.47019100189209\n",
      "Epoch: 30/100 | step: 160/422 | loss: 2.9288480281829834\n",
      "Epoch: 30/100 | step: 161/422 | loss: 3.1044211387634277\n",
      "Epoch: 30/100 | step: 162/422 | loss: 2.3289644718170166\n",
      "Epoch: 30/100 | step: 163/422 | loss: 2.3175032138824463\n",
      "Epoch: 30/100 | step: 164/422 | loss: 2.3883397579193115\n",
      "Epoch: 30/100 | step: 165/422 | loss: 2.4199719429016113\n",
      "Epoch: 30/100 | step: 166/422 | loss: 2.865574598312378\n",
      "Epoch: 30/100 | step: 167/422 | loss: 2.746190071105957\n",
      "Epoch: 30/100 | step: 168/422 | loss: 2.7477428913116455\n",
      "Epoch: 30/100 | step: 169/422 | loss: 2.4563441276550293\n",
      "Epoch: 30/100 | step: 170/422 | loss: 3.101343870162964\n",
      "Epoch: 30/100 | step: 171/422 | loss: 2.6967833042144775\n",
      "Epoch: 30/100 | step: 172/422 | loss: 2.4210140705108643\n",
      "Epoch: 30/100 | step: 173/422 | loss: 2.7715632915496826\n",
      "Epoch: 30/100 | step: 174/422 | loss: 2.7553348541259766\n",
      "Epoch: 30/100 | step: 175/422 | loss: 2.721759796142578\n",
      "Epoch: 30/100 | step: 176/422 | loss: 2.5142180919647217\n",
      "Epoch: 30/100 | step: 177/422 | loss: 3.346388816833496\n",
      "Epoch: 30/100 | step: 178/422 | loss: 2.680842399597168\n",
      "Epoch: 30/100 | step: 179/422 | loss: 2.5306520462036133\n",
      "Epoch: 30/100 | step: 180/422 | loss: 2.284748077392578\n",
      "Epoch: 30/100 | step: 181/422 | loss: 2.9323418140411377\n",
      "Epoch: 30/100 | step: 182/422 | loss: 2.243380308151245\n",
      "Epoch: 30/100 | step: 183/422 | loss: 2.9432265758514404\n",
      "Epoch: 30/100 | step: 184/422 | loss: 2.648905038833618\n",
      "Epoch: 30/100 | step: 185/422 | loss: 2.893990993499756\n",
      "Epoch: 30/100 | step: 186/422 | loss: 2.627990245819092\n",
      "Epoch: 30/100 | step: 187/422 | loss: 2.5408170223236084\n",
      "Epoch: 30/100 | step: 188/422 | loss: 2.5031278133392334\n",
      "Epoch: 30/100 | step: 189/422 | loss: 2.7542033195495605\n",
      "Epoch: 30/100 | step: 190/422 | loss: 2.8596622943878174\n",
      "Epoch: 30/100 | step: 191/422 | loss: 2.198385715484619\n",
      "Epoch: 30/100 | step: 192/422 | loss: 2.831287145614624\n",
      "Epoch: 30/100 | step: 193/422 | loss: 2.917327642440796\n",
      "Epoch: 30/100 | step: 194/422 | loss: 2.2496864795684814\n",
      "Epoch: 30/100 | step: 195/422 | loss: 2.7306275367736816\n",
      "Epoch: 30/100 | step: 196/422 | loss: 2.672484874725342\n",
      "Epoch: 30/100 | step: 197/422 | loss: 2.1976466178894043\n",
      "Epoch: 30/100 | step: 198/422 | loss: 2.4384567737579346\n",
      "Epoch: 30/100 | step: 199/422 | loss: 2.9536426067352295\n",
      "Epoch: 30/100 | step: 200/422 | loss: 2.7832741737365723\n",
      "Epoch: 30/100 | step: 201/422 | loss: 2.844449281692505\n",
      "Epoch: 30/100 | step: 202/422 | loss: 2.471526861190796\n",
      "Epoch: 30/100 | step: 203/422 | loss: 2.837353229522705\n",
      "Epoch: 30/100 | step: 204/422 | loss: 2.0477895736694336\n",
      "Epoch: 30/100 | step: 205/422 | loss: 2.872602939605713\n",
      "Epoch: 30/100 | step: 206/422 | loss: 2.598947048187256\n",
      "Epoch: 30/100 | step: 207/422 | loss: 2.435446262359619\n",
      "Epoch: 30/100 | step: 208/422 | loss: 2.4900131225585938\n",
      "Epoch: 30/100 | step: 209/422 | loss: 2.1161069869995117\n",
      "Epoch: 30/100 | step: 210/422 | loss: 2.5845518112182617\n",
      "Epoch: 30/100 | step: 211/422 | loss: 1.8692312240600586\n",
      "Epoch: 30/100 | step: 212/422 | loss: 2.4838199615478516\n",
      "Epoch: 30/100 | step: 213/422 | loss: 2.024806022644043\n",
      "Epoch: 30/100 | step: 214/422 | loss: 3.0192174911499023\n",
      "Epoch: 30/100 | step: 215/422 | loss: 2.661111831665039\n",
      "Epoch: 30/100 | step: 216/422 | loss: 2.421797513961792\n",
      "Epoch: 30/100 | step: 217/422 | loss: 1.9813499450683594\n",
      "Epoch: 30/100 | step: 218/422 | loss: 1.8640803098678589\n",
      "Epoch: 30/100 | step: 219/422 | loss: 2.4621071815490723\n",
      "Epoch: 30/100 | step: 220/422 | loss: 2.572737455368042\n",
      "Epoch: 30/100 | step: 221/422 | loss: 2.6921868324279785\n",
      "Epoch: 30/100 | step: 222/422 | loss: 2.619878053665161\n",
      "Epoch: 30/100 | step: 223/422 | loss: 2.9609005451202393\n",
      "Epoch: 30/100 | step: 224/422 | loss: 2.573676586151123\n",
      "Epoch: 30/100 | step: 225/422 | loss: 3.1458632946014404\n",
      "Epoch: 30/100 | step: 226/422 | loss: 2.7838027477264404\n",
      "Epoch: 30/100 | step: 227/422 | loss: 2.0269269943237305\n",
      "Epoch: 30/100 | step: 228/422 | loss: 2.660390615463257\n",
      "Epoch: 30/100 | step: 229/422 | loss: 2.3697245121002197\n",
      "Epoch: 30/100 | step: 230/422 | loss: 2.5308027267456055\n",
      "Epoch: 30/100 | step: 231/422 | loss: 2.6049342155456543\n",
      "Epoch: 30/100 | step: 232/422 | loss: 2.62235164642334\n",
      "Epoch: 30/100 | step: 233/422 | loss: 2.7451093196868896\n",
      "Epoch: 30/100 | step: 234/422 | loss: 2.093964099884033\n",
      "Epoch: 30/100 | step: 235/422 | loss: 2.7406957149505615\n",
      "Epoch: 30/100 | step: 236/422 | loss: 2.9086008071899414\n",
      "Epoch: 30/100 | step: 237/422 | loss: 2.9234230518341064\n",
      "Epoch: 30/100 | step: 238/422 | loss: 2.3504185676574707\n",
      "Epoch: 30/100 | step: 239/422 | loss: 2.835608720779419\n",
      "Epoch: 30/100 | step: 240/422 | loss: 2.4452457427978516\n",
      "Epoch: 30/100 | step: 241/422 | loss: 2.2564070224761963\n",
      "Epoch: 30/100 | step: 242/422 | loss: 2.545900344848633\n",
      "Epoch: 30/100 | step: 243/422 | loss: 2.73734450340271\n",
      "Epoch: 30/100 | step: 244/422 | loss: 2.749605655670166\n",
      "Epoch: 30/100 | step: 245/422 | loss: 2.349010944366455\n",
      "Epoch: 30/100 | step: 246/422 | loss: 2.6039247512817383\n",
      "Epoch: 30/100 | step: 247/422 | loss: 2.5231194496154785\n",
      "Epoch: 30/100 | step: 248/422 | loss: 2.0537161827087402\n",
      "Epoch: 30/100 | step: 249/422 | loss: 2.7037577629089355\n",
      "Epoch: 30/100 | step: 250/422 | loss: 2.084878444671631\n",
      "Epoch: 30/100 | step: 251/422 | loss: 2.662397861480713\n",
      "Epoch: 30/100 | step: 252/422 | loss: 2.63489031791687\n",
      "Epoch: 30/100 | step: 253/422 | loss: 2.0612964630126953\n",
      "Epoch: 30/100 | step: 254/422 | loss: 2.263657331466675\n",
      "Epoch: 30/100 | step: 255/422 | loss: 2.4809112548828125\n",
      "Epoch: 30/100 | step: 256/422 | loss: 2.8234188556671143\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 31/100 | step: 1/422 | loss: 2.6012089252471924\n",
      "Epoch: 31/100 | step: 2/422 | loss: 2.318493127822876\n",
      "Epoch: 31/100 | step: 3/422 | loss: 2.572279691696167\n",
      "Epoch: 31/100 | step: 4/422 | loss: 2.5900111198425293\n",
      "Epoch: 31/100 | step: 5/422 | loss: 1.9991393089294434\n",
      "Epoch: 31/100 | step: 6/422 | loss: 2.1805875301361084\n",
      "Epoch: 31/100 | step: 7/422 | loss: 2.3321311473846436\n",
      "Epoch: 31/100 | step: 8/422 | loss: 1.9712419509887695\n",
      "Epoch: 31/100 | step: 9/422 | loss: 2.2545604705810547\n",
      "Epoch: 31/100 | step: 10/422 | loss: 2.2639870643615723\n",
      "Epoch: 31/100 | step: 11/422 | loss: 2.5848546028137207\n",
      "Epoch: 31/100 | step: 12/422 | loss: 2.2373781204223633\n",
      "Epoch: 31/100 | step: 13/422 | loss: 2.4033234119415283\n",
      "Epoch: 31/100 | step: 14/422 | loss: 2.8194997310638428\n",
      "Epoch: 31/100 | step: 15/422 | loss: 2.2078216075897217\n",
      "Epoch: 31/100 | step: 16/422 | loss: 2.7009711265563965\n",
      "Epoch: 31/100 | step: 17/422 | loss: 2.106013059616089\n",
      "Epoch: 31/100 | step: 18/422 | loss: 2.365403413772583\n",
      "Epoch: 31/100 | step: 19/422 | loss: 2.622955083847046\n",
      "Epoch: 31/100 | step: 20/422 | loss: 1.9717001914978027\n",
      "Epoch: 31/100 | step: 21/422 | loss: 2.6378488540649414\n",
      "Epoch: 31/100 | step: 22/422 | loss: 2.1514663696289062\n",
      "Epoch: 31/100 | step: 23/422 | loss: 2.185377597808838\n",
      "Epoch: 31/100 | step: 24/422 | loss: 2.808715581893921\n",
      "Epoch: 31/100 | step: 25/422 | loss: 2.3100638389587402\n",
      "Epoch: 31/100 | step: 26/422 | loss: 2.361692428588867\n",
      "Epoch: 31/100 | step: 27/422 | loss: 2.7026727199554443\n",
      "Epoch: 31/100 | step: 28/422 | loss: 2.324150800704956\n",
      "Epoch: 31/100 | step: 29/422 | loss: 2.6685683727264404\n",
      "Epoch: 31/100 | step: 30/422 | loss: 2.56658673286438\n",
      "Epoch: 31/100 | step: 31/422 | loss: 3.0495846271514893\n",
      "Epoch: 31/100 | step: 32/422 | loss: 2.9291067123413086\n",
      "Epoch: 31/100 | step: 33/422 | loss: 2.3724608421325684\n",
      "Epoch: 31/100 | step: 34/422 | loss: 1.941859483718872\n",
      "Epoch: 31/100 | step: 35/422 | loss: 2.2868778705596924\n",
      "Epoch: 31/100 | step: 36/422 | loss: 2.768280267715454\n",
      "Epoch: 31/100 | step: 37/422 | loss: 2.3015682697296143\n",
      "Epoch: 31/100 | step: 38/422 | loss: 2.7789199352264404\n",
      "Epoch: 31/100 | step: 39/422 | loss: 1.9783700704574585\n",
      "Epoch: 31/100 | step: 40/422 | loss: 2.1963841915130615\n",
      "Epoch: 31/100 | step: 41/422 | loss: 1.999131679534912\n",
      "Epoch: 31/100 | step: 42/422 | loss: 2.8013978004455566\n",
      "Epoch: 31/100 | step: 43/422 | loss: 2.4337852001190186\n",
      "Epoch: 31/100 | step: 44/422 | loss: 2.1301989555358887\n",
      "Epoch: 31/100 | step: 45/422 | loss: 2.3658623695373535\n",
      "Epoch: 31/100 | step: 46/422 | loss: 2.2900664806365967\n",
      "Epoch: 31/100 | step: 47/422 | loss: 2.7360668182373047\n",
      "Epoch: 31/100 | step: 48/422 | loss: 2.780277967453003\n",
      "Epoch: 31/100 | step: 49/422 | loss: 2.403571844100952\n",
      "Epoch: 31/100 | step: 50/422 | loss: 2.482774496078491\n",
      "Epoch: 31/100 | step: 51/422 | loss: 2.5281264781951904\n",
      "Error occurred during training: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple\n",
      "Epoch: 32/100 | step: 1/422 | loss: 2.361469030380249\n",
      "Epoch: 32/100 | step: 2/422 | loss: 1.9750920534133911\n",
      "Epoch: 32/100 | step: 3/422 | loss: 2.1566195487976074\n",
      "Epoch: 32/100 | step: 4/422 | loss: 2.1255578994750977\n",
      "Epoch: 32/100 | step: 5/422 | loss: 2.2313520908355713\n",
      "Epoch: 32/100 | step: 6/422 | loss: 2.323871612548828\n",
      "Epoch: 32/100 | step: 7/422 | loss: 2.6482956409454346\n",
      "Epoch: 32/100 | step: 8/422 | loss: 2.1108837127685547\n",
      "Epoch: 32/100 | step: 9/422 | loss: 2.8157639503479004\n",
      "Epoch: 32/100 | step: 10/422 | loss: 2.408853054046631\n",
      "Epoch: 32/100 | step: 11/422 | loss: 2.466264009475708\n",
      "Epoch: 32/100 | step: 12/422 | loss: 2.728210210800171\n",
      "Epoch: 32/100 | step: 13/422 | loss: 2.2185938358306885\n",
      "Epoch: 32/100 | step: 14/422 | loss: 2.856820821762085\n",
      "Epoch: 32/100 | step: 15/422 | loss: 1.963082194328308\n",
      "Epoch: 32/100 | step: 16/422 | loss: 2.1094510555267334\n",
      "Epoch: 32/100 | step: 17/422 | loss: 2.3747918605804443\n",
      "Epoch: 32/100 | step: 18/422 | loss: 2.1345534324645996\n",
      "Epoch: 32/100 | step: 19/422 | loss: 2.5567171573638916\n",
      "Epoch: 32/100 | step: 20/422 | loss: 2.2406857013702393\n",
      "Epoch: 32/100 | step: 21/422 | loss: 2.417815685272217\n",
      "Epoch: 32/100 | step: 22/422 | loss: 1.9581457376480103\n",
      "Epoch: 32/100 | step: 23/422 | loss: 1.9267468452453613\n",
      "Epoch: 32/100 | step: 24/422 | loss: 2.2498347759246826\n",
      "Epoch: 32/100 | step: 25/422 | loss: 2.6212971210479736\n",
      "Epoch: 32/100 | step: 26/422 | loss: 1.9943938255310059\n",
      "Epoch: 32/100 | step: 27/422 | loss: 2.2231600284576416\n",
      "Epoch: 32/100 | step: 28/422 | loss: 2.539410352706909\n",
      "Epoch: 32/100 | step: 29/422 | loss: 2.427906036376953\n",
      "Epoch: 32/100 | step: 30/422 | loss: 2.737602949142456\n",
      "Epoch: 32/100 | step: 31/422 | loss: 2.8246922492980957\n",
      "Epoch: 32/100 | step: 32/422 | loss: 2.4200937747955322\n",
      "Epoch: 32/100 | step: 33/422 | loss: 2.1270911693573\n",
      "Epoch: 32/100 | step: 34/422 | loss: 2.192826986312866\n",
      "Epoch: 32/100 | step: 35/422 | loss: 2.5323965549468994\n",
      "Epoch: 32/100 | step: 36/422 | loss: 2.4786264896392822\n",
      "Epoch: 32/100 | step: 37/422 | loss: 2.3159501552581787\n",
      "Epoch: 32/100 | step: 38/422 | loss: 2.304671049118042\n",
      "Epoch: 32/100 | step: 39/422 | loss: 2.5330779552459717\n",
      "Epoch: 32/100 | step: 40/422 | loss: 2.5377349853515625\n",
      "Epoch: 32/100 | step: 41/422 | loss: 2.7806506156921387\n",
      "Epoch: 32/100 | step: 42/422 | loss: 2.2486419677734375\n",
      "Epoch: 32/100 | step: 43/422 | loss: 2.351947069168091\n",
      "Epoch: 32/100 | step: 44/422 | loss: 2.736589193344116\n",
      "Epoch: 32/100 | step: 45/422 | loss: 2.5938215255737305\n",
      "Epoch: 32/100 | step: 46/422 | loss: 2.1998069286346436\n",
      "Epoch: 32/100 | step: 47/422 | loss: 2.2019026279449463\n",
      "Epoch: 32/100 | step: 48/422 | loss: 2.4072089195251465\n",
      "Epoch: 32/100 | step: 49/422 | loss: 2.228863000869751\n",
      "Epoch: 32/100 | step: 50/422 | loss: 2.3755462169647217\n",
      "Epoch: 32/100 | step: 51/422 | loss: 2.318734645843506\n",
      "Epoch: 32/100 | step: 52/422 | loss: 2.228792190551758\n",
      "Epoch: 32/100 | step: 53/422 | loss: 2.0866153240203857\n",
      "Epoch: 32/100 | step: 54/422 | loss: 2.1939542293548584\n",
      "Epoch: 32/100 | step: 55/422 | loss: 2.7487828731536865\n",
      "Epoch: 32/100 | step: 56/422 | loss: 2.5329835414886475\n",
      "Epoch: 32/100 | step: 57/422 | loss: 2.478496551513672\n",
      "Epoch: 32/100 | step: 58/422 | loss: 2.422376871109009\n",
      "Epoch: 32/100 | step: 59/422 | loss: 2.6959354877471924\n",
      "Epoch: 32/100 | step: 60/422 | loss: 2.654395580291748\n",
      "Epoch: 32/100 | step: 61/422 | loss: 1.7268788814544678\n",
      "Epoch: 32/100 | step: 62/422 | loss: 2.4172260761260986\n",
      "Epoch: 32/100 | step: 63/422 | loss: 2.5981578826904297\n",
      "Epoch: 32/100 | step: 64/422 | loss: 2.2227628231048584\n",
      "Epoch: 32/100 | step: 65/422 | loss: 2.0249242782592773\n",
      "Epoch: 32/100 | step: 66/422 | loss: 2.551912784576416\n",
      "Epoch: 32/100 | step: 67/422 | loss: 2.439879894256592\n",
      "Epoch: 32/100 | step: 68/422 | loss: 1.707884430885315\n",
      "Epoch: 32/100 | step: 69/422 | loss: 2.320261001586914\n",
      "Epoch: 32/100 | step: 70/422 | loss: 2.404629945755005\n",
      "Epoch: 32/100 | step: 71/422 | loss: 2.770582914352417\n",
      "Epoch: 32/100 | step: 72/422 | loss: 2.2134897708892822\n",
      "Epoch: 32/100 | step: 73/422 | loss: 2.189404010772705\n",
      "Epoch: 32/100 | step: 74/422 | loss: 2.360281467437744\n",
      "Epoch: 32/100 | step: 75/422 | loss: 2.617286443710327\n",
      "Epoch: 32/100 | step: 76/422 | loss: 2.351167678833008\n",
      "Epoch: 32/100 | step: 77/422 | loss: 2.818324565887451\n",
      "Epoch: 32/100 | step: 78/422 | loss: 1.9341989755630493\n",
      "Epoch: 32/100 | step: 79/422 | loss: 2.41674542427063\n",
      "Epoch: 32/100 | step: 80/422 | loss: 2.244008779525757\n",
      "Epoch: 32/100 | step: 81/422 | loss: 2.556027889251709\n",
      "Epoch: 32/100 | step: 82/422 | loss: 2.2007970809936523\n",
      "Epoch: 32/100 | step: 83/422 | loss: 2.5822696685791016\n",
      "Epoch: 32/100 | step: 84/422 | loss: 2.6241557598114014\n",
      "Epoch: 32/100 | step: 85/422 | loss: 2.1178479194641113\n",
      "Epoch: 32/100 | step: 86/422 | loss: 2.256944179534912\n",
      "Epoch: 32/100 | step: 87/422 | loss: 2.5766749382019043\n",
      "Epoch: 32/100 | step: 88/422 | loss: 2.905888557434082\n",
      "Epoch: 32/100 | step: 89/422 | loss: 2.3918237686157227\n",
      "Epoch: 32/100 | step: 90/422 | loss: 2.6013190746307373\n",
      "Epoch: 32/100 | step: 91/422 | loss: 2.535097122192383\n",
      "Epoch: 32/100 | step: 92/422 | loss: 2.837968111038208\n",
      "Epoch: 32/100 | step: 93/422 | loss: 2.998408794403076\n",
      "Epoch: 32/100 | step: 94/422 | loss: 1.90169095993042\n",
      "Epoch: 32/100 | step: 95/422 | loss: 2.24137544631958\n",
      "Epoch: 32/100 | step: 96/422 | loss: 2.6116161346435547\n",
      "Epoch: 32/100 | step: 97/422 | loss: 2.221283197402954\n",
      "Epoch: 32/100 | step: 98/422 | loss: 2.5520148277282715\n",
      "Epoch: 32/100 | step: 99/422 | loss: 2.5454697608947754\n",
      "Epoch: 32/100 | step: 100/422 | loss: 2.1327273845672607\n",
      "Epoch: 32/100 | step: 101/422 | loss: 2.201894760131836\n",
      "Epoch: 32/100 | step: 102/422 | loss: 2.54024076461792\n",
      "Epoch: 32/100 | step: 103/422 | loss: 1.9995064735412598\n",
      "Epoch: 32/100 | step: 104/422 | loss: 2.397156238555908\n",
      "Epoch: 32/100 | step: 105/422 | loss: 1.9979112148284912\n",
      "Epoch: 32/100 | step: 106/422 | loss: 2.590888500213623\n",
      "Epoch: 32/100 | step: 107/422 | loss: 2.56669545173645\n",
      "Epoch: 32/100 | step: 108/422 | loss: 2.588144302368164\n",
      "Epoch: 32/100 | step: 109/422 | loss: 2.2517006397247314\n",
      "Epoch: 32/100 | step: 110/422 | loss: 3.0937764644622803\n",
      "Epoch: 32/100 | step: 111/422 | loss: 2.3213295936584473\n",
      "Epoch: 32/100 | step: 112/422 | loss: 2.396785259246826\n",
      "Epoch: 32/100 | step: 113/422 | loss: 2.387139081954956\n",
      "Epoch: 32/100 | step: 114/422 | loss: 2.2879083156585693\n",
      "Epoch: 32/100 | step: 115/422 | loss: 2.5389301776885986\n",
      "Epoch: 32/100 | step: 116/422 | loss: 2.5674667358398438\n",
      "Epoch: 32/100 | step: 117/422 | loss: 2.9709372520446777\n",
      "Epoch: 32/100 | step: 118/422 | loss: 2.5267646312713623\n",
      "Epoch: 32/100 | step: 119/422 | loss: 2.5596396923065186\n",
      "Epoch: 32/100 | step: 120/422 | loss: 2.3112246990203857\n",
      "Epoch: 32/100 | step: 121/422 | loss: 1.9744588136672974\n",
      "Epoch: 32/100 | step: 122/422 | loss: 2.787175416946411\n",
      "Epoch: 32/100 | step: 123/422 | loss: 2.9262983798980713\n",
      "Epoch: 32/100 | step: 124/422 | loss: 2.763061046600342\n",
      "Epoch: 32/100 | step: 125/422 | loss: 2.7744715213775635\n",
      "Epoch: 32/100 | step: 126/422 | loss: 2.301241397857666\n",
      "Epoch: 32/100 | step: 127/422 | loss: 2.466719150543213\n",
      "Epoch: 32/100 | step: 128/422 | loss: 2.854499578475952\n",
      "Epoch: 32/100 | step: 129/422 | loss: 2.3318474292755127\n",
      "Epoch: 32/100 | step: 130/422 | loss: 2.3785901069641113\n",
      "Epoch: 32/100 | step: 131/422 | loss: 2.773547887802124\n",
      "Epoch: 32/100 | step: 132/422 | loss: 2.266326427459717\n",
      "Epoch: 32/100 | step: 133/422 | loss: 2.375258684158325\n",
      "Epoch: 32/100 | step: 134/422 | loss: 2.0013113021850586\n",
      "Epoch: 32/100 | step: 135/422 | loss: 2.2885522842407227\n",
      "Epoch: 32/100 | step: 136/422 | loss: 2.4867823123931885\n",
      "Epoch: 32/100 | step: 137/422 | loss: 2.4438140392303467\n",
      "Epoch: 32/100 | step: 138/422 | loss: 2.4983582496643066\n",
      "Epoch: 32/100 | step: 139/422 | loss: 2.579073905944824\n",
      "Epoch: 32/100 | step: 140/422 | loss: 2.9928085803985596\n",
      "Epoch: 32/100 | step: 141/422 | loss: 2.4663078784942627\n",
      "Epoch: 32/100 | step: 142/422 | loss: 2.2547736167907715\n",
      "Epoch: 32/100 | step: 143/422 | loss: 2.8809447288513184\n",
      "Epoch: 32/100 | step: 144/422 | loss: 2.1427228450775146\n",
      "Epoch: 32/100 | step: 145/422 | loss: 2.2937536239624023\n",
      "Epoch: 32/100 | step: 146/422 | loss: 2.7070789337158203\n",
      "Epoch: 32/100 | step: 147/422 | loss: 2.1799378395080566\n",
      "Epoch: 32/100 | step: 148/422 | loss: 2.3159091472625732\n",
      "Epoch: 32/100 | step: 149/422 | loss: 2.5906496047973633\n",
      "Epoch: 32/100 | step: 150/422 | loss: 2.692701578140259\n",
      "Epoch: 32/100 | step: 151/422 | loss: 2.5426886081695557\n",
      "Epoch: 32/100 | step: 152/422 | loss: 3.0024728775024414\n",
      "Epoch: 32/100 | step: 153/422 | loss: 2.6832008361816406\n",
      "Epoch: 32/100 | step: 154/422 | loss: 2.4867162704467773\n",
      "Epoch: 32/100 | step: 155/422 | loss: 3.1644914150238037\n",
      "Epoch: 32/100 | step: 156/422 | loss: 3.0129311084747314\n",
      "Epoch: 32/100 | step: 157/422 | loss: 2.106924295425415\n",
      "Epoch: 32/100 | step: 158/422 | loss: 2.4709980487823486\n",
      "Epoch: 32/100 | step: 159/422 | loss: 2.2296013832092285\n",
      "Epoch: 32/100 | step: 160/422 | loss: 2.650958299636841\n",
      "Epoch: 32/100 | step: 161/422 | loss: 2.3697972297668457\n",
      "Epoch: 32/100 | step: 162/422 | loss: 2.0956993103027344\n",
      "Epoch: 32/100 | step: 163/422 | loss: 1.9236758947372437\n",
      "Epoch: 32/100 | step: 164/422 | loss: 2.204495429992676\n",
      "Epoch: 32/100 | step: 165/422 | loss: 2.512554883956909\n",
      "Epoch: 32/100 | step: 166/422 | loss: 2.1127681732177734\n",
      "Epoch: 32/100 | step: 167/422 | loss: 3.2198987007141113\n",
      "Epoch: 32/100 | step: 168/422 | loss: 2.4418184757232666\n",
      "Epoch: 32/100 | step: 169/422 | loss: 3.064915657043457\n",
      "Epoch: 32/100 | step: 170/422 | loss: 2.8973348140716553\n",
      "Epoch: 32/100 | step: 171/422 | loss: 3.306886672973633\n",
      "Epoch: 32/100 | step: 172/422 | loss: 2.606410026550293\n",
      "Epoch: 32/100 | step: 173/422 | loss: 1.9988957643508911\n",
      "Epoch: 32/100 | step: 174/422 | loss: 2.7025375366210938\n",
      "Epoch: 32/100 | step: 175/422 | loss: 2.225039005279541\n",
      "Epoch: 32/100 | step: 176/422 | loss: 2.276641845703125\n",
      "Epoch: 32/100 | step: 177/422 | loss: 2.1402981281280518\n",
      "Epoch: 32/100 | step: 178/422 | loss: 2.0304360389709473\n",
      "Epoch: 32/100 | step: 179/422 | loss: 2.235203981399536\n",
      "Epoch: 32/100 | step: 180/422 | loss: 2.3846042156219482\n",
      "Epoch: 32/100 | step: 181/422 | loss: 2.5168495178222656\n",
      "Epoch: 32/100 | step: 182/422 | loss: 1.9183858633041382\n",
      "Epoch: 32/100 | step: 183/422 | loss: 2.303591251373291\n",
      "Epoch: 32/100 | step: 184/422 | loss: 2.3651843070983887\n",
      "Epoch: 32/100 | step: 185/422 | loss: 2.796691656112671\n",
      "Epoch: 32/100 | step: 186/422 | loss: 2.260155439376831\n",
      "Epoch: 32/100 | step: 187/422 | loss: 2.559654474258423\n",
      "Epoch: 32/100 | step: 188/422 | loss: 2.4210357666015625\n",
      "Epoch: 32/100 | step: 189/422 | loss: 2.1410164833068848\n",
      "Epoch: 32/100 | step: 190/422 | loss: 2.7434988021850586\n",
      "Epoch: 32/100 | step: 191/422 | loss: 2.8757481575012207\n",
      "Epoch: 32/100 | step: 192/422 | loss: 2.761808395385742\n",
      "Epoch: 32/100 | step: 193/422 | loss: 1.9056289196014404\n",
      "Epoch: 32/100 | step: 194/422 | loss: 2.3923115730285645\n",
      "Epoch: 32/100 | step: 195/422 | loss: 2.779956102371216\n",
      "Epoch: 32/100 | step: 196/422 | loss: 2.620771884918213\n",
      "Epoch: 32/100 | step: 197/422 | loss: 2.683842420578003\n",
      "Epoch: 32/100 | step: 198/422 | loss: 2.189723491668701\n",
      "Epoch: 32/100 | step: 199/422 | loss: 2.4141693115234375\n",
      "Epoch: 32/100 | step: 200/422 | loss: 2.64274263381958\n",
      "Epoch: 32/100 | step: 201/422 | loss: 2.4453272819519043\n",
      "Epoch: 32/100 | step: 202/422 | loss: 2.5802855491638184\n",
      "Epoch: 32/100 | step: 203/422 | loss: 2.5850536823272705\n",
      "Epoch: 32/100 | step: 204/422 | loss: 2.342008113861084\n",
      "Epoch: 32/100 | step: 205/422 | loss: 2.127424955368042\n",
      "Epoch: 32/100 | step: 206/422 | loss: 2.2239620685577393\n",
      "Epoch: 32/100 | step: 207/422 | loss: 2.559840679168701\n",
      "Epoch: 32/100 | step: 208/422 | loss: 2.842355251312256\n",
      "Epoch: 32/100 | step: 209/422 | loss: 2.6028685569763184\n",
      "Epoch: 32/100 | step: 210/422 | loss: 2.577796459197998\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 33/100 | step: 1/422 | loss: 2.1751301288604736\n",
      "Epoch: 33/100 | step: 2/422 | loss: 2.2535061836242676\n",
      "Epoch: 33/100 | step: 3/422 | loss: 1.9836671352386475\n",
      "Epoch: 33/100 | step: 4/422 | loss: 1.7817046642303467\n",
      "Epoch: 33/100 | step: 5/422 | loss: 2.7593042850494385\n",
      "Epoch: 33/100 | step: 6/422 | loss: 2.115358352661133\n",
      "Epoch: 33/100 | step: 7/422 | loss: 2.432742118835449\n",
      "Epoch: 33/100 | step: 8/422 | loss: 2.701882839202881\n",
      "Epoch: 33/100 | step: 9/422 | loss: 2.350565195083618\n",
      "Epoch: 33/100 | step: 10/422 | loss: 1.7644017934799194\n",
      "Epoch: 33/100 | step: 11/422 | loss: 1.7684763669967651\n",
      "Epoch: 33/100 | step: 12/422 | loss: 2.18560791015625\n",
      "Epoch: 33/100 | step: 13/422 | loss: 2.1162478923797607\n",
      "Epoch: 33/100 | step: 14/422 | loss: 2.3792524337768555\n",
      "Epoch: 33/100 | step: 15/422 | loss: 1.9584627151489258\n",
      "Epoch: 33/100 | step: 16/422 | loss: 2.095858097076416\n",
      "Epoch: 33/100 | step: 17/422 | loss: 2.3543317317962646\n",
      "Epoch: 33/100 | step: 18/422 | loss: 2.3597209453582764\n",
      "Epoch: 33/100 | step: 19/422 | loss: 2.1585986614227295\n",
      "Epoch: 33/100 | step: 20/422 | loss: 2.2192766666412354\n",
      "Epoch: 33/100 | step: 21/422 | loss: 2.0468218326568604\n",
      "Epoch: 33/100 | step: 22/422 | loss: 2.543374538421631\n",
      "Epoch: 33/100 | step: 23/422 | loss: 2.533966064453125\n",
      "Epoch: 33/100 | step: 24/422 | loss: 2.3778955936431885\n",
      "Epoch: 33/100 | step: 25/422 | loss: 2.7254793643951416\n",
      "Epoch: 33/100 | step: 26/422 | loss: 2.344398260116577\n",
      "Epoch: 33/100 | step: 27/422 | loss: 2.0338737964630127\n",
      "Epoch: 33/100 | step: 28/422 | loss: 2.415349245071411\n",
      "Epoch: 33/100 | step: 29/422 | loss: 2.2150912284851074\n",
      "Epoch: 33/100 | step: 30/422 | loss: 2.12713885307312\n",
      "Epoch: 33/100 | step: 31/422 | loss: 2.0098648071289062\n",
      "Epoch: 33/100 | step: 32/422 | loss: 1.9564781188964844\n",
      "Epoch: 33/100 | step: 33/422 | loss: 1.712485432624817\n",
      "Epoch: 33/100 | step: 34/422 | loss: 1.9259804487228394\n",
      "Epoch: 33/100 | step: 35/422 | loss: 2.4319918155670166\n",
      "Epoch: 33/100 | step: 36/422 | loss: 2.3112375736236572\n",
      "Epoch: 33/100 | step: 37/422 | loss: 3.2315919399261475\n",
      "Epoch: 33/100 | step: 38/422 | loss: 2.095445156097412\n",
      "Epoch: 33/100 | step: 39/422 | loss: 2.037019729614258\n",
      "Epoch: 33/100 | step: 40/422 | loss: 2.6359500885009766\n",
      "Epoch: 33/100 | step: 41/422 | loss: 2.879450798034668\n",
      "Epoch: 33/100 | step: 42/422 | loss: 2.499772787094116\n",
      "Epoch: 33/100 | step: 43/422 | loss: 2.1624035835266113\n",
      "Epoch: 33/100 | step: 44/422 | loss: 2.1963043212890625\n",
      "Epoch: 33/100 | step: 45/422 | loss: 2.356999397277832\n",
      "Epoch: 33/100 | step: 46/422 | loss: 2.4125044345855713\n",
      "Epoch: 33/100 | step: 47/422 | loss: 2.2396490573883057\n",
      "Epoch: 33/100 | step: 48/422 | loss: 2.2378382682800293\n",
      "Epoch: 33/100 | step: 49/422 | loss: 2.3144636154174805\n",
      "Epoch: 33/100 | step: 50/422 | loss: 2.5953333377838135\n",
      "Epoch: 33/100 | step: 51/422 | loss: 2.4902524948120117\n",
      "Epoch: 33/100 | step: 52/422 | loss: 2.937220811843872\n",
      "Epoch: 33/100 | step: 53/422 | loss: 2.903895139694214\n",
      "Epoch: 33/100 | step: 54/422 | loss: 2.350484609603882\n",
      "Epoch: 33/100 | step: 55/422 | loss: 2.1559324264526367\n",
      "Epoch: 33/100 | step: 56/422 | loss: 2.1225154399871826\n",
      "Epoch: 33/100 | step: 57/422 | loss: 2.1088101863861084\n",
      "Epoch: 33/100 | step: 58/422 | loss: 1.796990156173706\n",
      "Epoch: 33/100 | step: 59/422 | loss: 2.432068109512329\n",
      "Epoch: 33/100 | step: 60/422 | loss: 2.443073034286499\n",
      "Epoch: 33/100 | step: 61/422 | loss: 1.9101498126983643\n",
      "Epoch: 33/100 | step: 62/422 | loss: 1.8892067670822144\n",
      "Epoch: 33/100 | step: 63/422 | loss: 1.8972052335739136\n",
      "Epoch: 33/100 | step: 64/422 | loss: 2.1254796981811523\n",
      "Epoch: 33/100 | step: 65/422 | loss: 2.2143774032592773\n",
      "Epoch: 33/100 | step: 66/422 | loss: 2.9070253372192383\n",
      "Epoch: 33/100 | step: 67/422 | loss: 2.220137357711792\n",
      "Epoch: 33/100 | step: 68/422 | loss: 2.843531847000122\n",
      "Epoch: 33/100 | step: 69/422 | loss: 2.257298707962036\n",
      "Epoch: 33/100 | step: 70/422 | loss: 2.5987048149108887\n",
      "Epoch: 33/100 | step: 71/422 | loss: 2.0957536697387695\n",
      "Epoch: 33/100 | step: 72/422 | loss: 2.6721699237823486\n",
      "Epoch: 33/100 | step: 73/422 | loss: 2.097959041595459\n",
      "Epoch: 33/100 | step: 74/422 | loss: 2.519867420196533\n",
      "Epoch: 33/100 | step: 75/422 | loss: 1.9574692249298096\n",
      "Epoch: 33/100 | step: 76/422 | loss: 2.5118637084960938\n",
      "Epoch: 33/100 | step: 77/422 | loss: 2.466273784637451\n",
      "Epoch: 33/100 | step: 78/422 | loss: 2.298532485961914\n",
      "Epoch: 33/100 | step: 79/422 | loss: 2.3187813758850098\n",
      "Epoch: 33/100 | step: 80/422 | loss: 1.9474382400512695\n",
      "Epoch: 33/100 | step: 81/422 | loss: 1.9194444417953491\n",
      "Epoch: 33/100 | step: 82/422 | loss: 2.8832027912139893\n",
      "Epoch: 33/100 | step: 83/422 | loss: 2.357448101043701\n",
      "Epoch: 33/100 | step: 84/422 | loss: 2.6022372245788574\n",
      "Epoch: 33/100 | step: 85/422 | loss: 2.3610546588897705\n",
      "Epoch: 33/100 | step: 86/422 | loss: 2.4220354557037354\n",
      "Epoch: 33/100 | step: 87/422 | loss: 2.1377556324005127\n",
      "Epoch: 33/100 | step: 88/422 | loss: 2.228698492050171\n",
      "Epoch: 33/100 | step: 89/422 | loss: 1.9646921157836914\n",
      "Epoch: 33/100 | step: 90/422 | loss: 2.026588201522827\n",
      "Epoch: 33/100 | step: 91/422 | loss: 2.3435254096984863\n",
      "Epoch: 33/100 | step: 92/422 | loss: 2.2462806701660156\n",
      "Epoch: 33/100 | step: 93/422 | loss: 2.552664041519165\n",
      "Epoch: 33/100 | step: 94/422 | loss: 1.9176234006881714\n",
      "Epoch: 33/100 | step: 95/422 | loss: 2.497943878173828\n",
      "Epoch: 33/100 | step: 96/422 | loss: 2.7039601802825928\n",
      "Epoch: 33/100 | step: 97/422 | loss: 2.2032806873321533\n",
      "Epoch: 33/100 | step: 98/422 | loss: 2.3473398685455322\n",
      "Epoch: 33/100 | step: 99/422 | loss: 2.502945899963379\n",
      "Epoch: 33/100 | step: 100/422 | loss: 2.3387439250946045\n",
      "Epoch: 33/100 | step: 101/422 | loss: 2.4424185752868652\n",
      "Epoch: 33/100 | step: 102/422 | loss: 2.423893690109253\n",
      "Epoch: 33/100 | step: 103/422 | loss: 2.1892006397247314\n",
      "Epoch: 33/100 | step: 104/422 | loss: 2.4610323905944824\n",
      "Epoch: 33/100 | step: 105/422 | loss: 2.9874486923217773\n",
      "Epoch: 33/100 | step: 106/422 | loss: 1.8991034030914307\n",
      "Epoch: 33/100 | step: 107/422 | loss: 2.3306725025177\n",
      "Epoch: 33/100 | step: 108/422 | loss: 2.0683951377868652\n",
      "Epoch: 33/100 | step: 109/422 | loss: 2.6230175495147705\n",
      "Epoch: 33/100 | step: 110/422 | loss: 2.7676682472229004\n",
      "Epoch: 33/100 | step: 111/422 | loss: 2.503138542175293\n",
      "Epoch: 33/100 | step: 112/422 | loss: 1.7881898880004883\n",
      "Epoch: 33/100 | step: 113/422 | loss: 1.8464851379394531\n",
      "Epoch: 33/100 | step: 114/422 | loss: 2.2215397357940674\n",
      "Epoch: 33/100 | step: 115/422 | loss: 2.7695107460021973\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 34/100 | step: 1/422 | loss: 2.110175132751465\n",
      "Epoch: 34/100 | step: 2/422 | loss: 1.962088704109192\n",
      "Epoch: 34/100 | step: 3/422 | loss: 2.127845287322998\n",
      "Epoch: 34/100 | step: 4/422 | loss: 2.5376806259155273\n",
      "Epoch: 34/100 | step: 5/422 | loss: 1.9049992561340332\n",
      "Epoch: 34/100 | step: 6/422 | loss: 2.3167662620544434\n",
      "Epoch: 34/100 | step: 7/422 | loss: 2.5994832515716553\n",
      "Epoch: 34/100 | step: 8/422 | loss: 3.0723395347595215\n",
      "Epoch: 34/100 | step: 9/422 | loss: 2.573184013366699\n",
      "Epoch: 34/100 | step: 10/422 | loss: 2.31179141998291\n",
      "Epoch: 34/100 | step: 11/422 | loss: 2.1166083812713623\n",
      "Epoch: 34/100 | step: 12/422 | loss: 2.285616397857666\n",
      "Epoch: 34/100 | step: 13/422 | loss: 1.9446207284927368\n",
      "Epoch: 34/100 | step: 14/422 | loss: 2.47334885597229\n",
      "Epoch: 34/100 | step: 15/422 | loss: 2.1220829486846924\n",
      "Epoch: 34/100 | step: 16/422 | loss: 2.350313901901245\n",
      "Epoch: 34/100 | step: 17/422 | loss: 2.4205405712127686\n",
      "Epoch: 34/100 | step: 18/422 | loss: 2.8331172466278076\n",
      "Epoch: 34/100 | step: 19/422 | loss: 2.9085395336151123\n",
      "Epoch: 34/100 | step: 20/422 | loss: 2.3015594482421875\n",
      "Epoch: 34/100 | step: 21/422 | loss: 2.371879816055298\n",
      "Epoch: 34/100 | step: 22/422 | loss: 2.6257636547088623\n",
      "Epoch: 34/100 | step: 23/422 | loss: 2.1564559936523438\n",
      "Epoch: 34/100 | step: 24/422 | loss: 1.9421024322509766\n",
      "Epoch: 34/100 | step: 25/422 | loss: 2.341468572616577\n",
      "Epoch: 34/100 | step: 26/422 | loss: 2.077962636947632\n",
      "Epoch: 34/100 | step: 27/422 | loss: 2.039172887802124\n",
      "Epoch: 34/100 | step: 28/422 | loss: 2.2320432662963867\n",
      "Epoch: 34/100 | step: 29/422 | loss: 2.829826593399048\n",
      "Epoch: 34/100 | step: 30/422 | loss: 2.445476531982422\n",
      "Epoch: 34/100 | step: 31/422 | loss: 2.7342045307159424\n",
      "Epoch: 34/100 | step: 32/422 | loss: 2.2457659244537354\n",
      "Epoch: 34/100 | step: 33/422 | loss: 2.476688861846924\n",
      "Epoch: 34/100 | step: 34/422 | loss: 2.346957206726074\n",
      "Epoch: 34/100 | step: 35/422 | loss: 2.623741626739502\n",
      "Epoch: 34/100 | step: 36/422 | loss: 2.5794692039489746\n",
      "Epoch: 34/100 | step: 37/422 | loss: 2.2618942260742188\n",
      "Epoch: 34/100 | step: 38/422 | loss: 1.971203327178955\n",
      "Epoch: 34/100 | step: 39/422 | loss: 2.007697820663452\n",
      "Epoch: 34/100 | step: 40/422 | loss: 1.508770227432251\n",
      "Epoch: 34/100 | step: 41/422 | loss: 2.5034258365631104\n",
      "Epoch: 34/100 | step: 42/422 | loss: 2.1275007724761963\n",
      "Epoch: 34/100 | step: 43/422 | loss: 2.2107858657836914\n",
      "Epoch: 34/100 | step: 44/422 | loss: 2.2905375957489014\n",
      "Epoch: 34/100 | step: 45/422 | loss: 2.0133743286132812\n",
      "Epoch: 34/100 | step: 46/422 | loss: 2.5348501205444336\n",
      "Epoch: 34/100 | step: 47/422 | loss: 2.2610394954681396\n",
      "Epoch: 34/100 | step: 48/422 | loss: 1.8323713541030884\n",
      "Epoch: 34/100 | step: 49/422 | loss: 2.208620309829712\n",
      "Epoch: 34/100 | step: 50/422 | loss: 2.196942090988159\n",
      "Epoch: 34/100 | step: 51/422 | loss: 2.3283867835998535\n",
      "Epoch: 34/100 | step: 52/422 | loss: 1.7686285972595215\n",
      "Epoch: 34/100 | step: 53/422 | loss: 2.2882447242736816\n",
      "Epoch: 34/100 | step: 54/422 | loss: 2.5842020511627197\n",
      "Epoch: 34/100 | step: 55/422 | loss: 2.0964534282684326\n",
      "Epoch: 34/100 | step: 56/422 | loss: 2.5494587421417236\n",
      "Epoch: 34/100 | step: 57/422 | loss: 2.394191026687622\n",
      "Epoch: 34/100 | step: 58/422 | loss: 2.5354182720184326\n",
      "Epoch: 34/100 | step: 59/422 | loss: 2.1121633052825928\n",
      "Epoch: 34/100 | step: 60/422 | loss: 2.176370859146118\n",
      "Epoch: 34/100 | step: 61/422 | loss: 2.2754032611846924\n",
      "Epoch: 34/100 | step: 62/422 | loss: 2.158151626586914\n",
      "Epoch: 34/100 | step: 63/422 | loss: 2.1671416759490967\n",
      "Epoch: 34/100 | step: 64/422 | loss: 2.334656000137329\n",
      "Epoch: 34/100 | step: 65/422 | loss: 2.3354289531707764\n",
      "Epoch: 34/100 | step: 66/422 | loss: 2.484680414199829\n",
      "Epoch: 34/100 | step: 67/422 | loss: 1.907848596572876\n",
      "Epoch: 34/100 | step: 68/422 | loss: 1.9475950002670288\n",
      "Epoch: 34/100 | step: 69/422 | loss: 2.3174307346343994\n",
      "Epoch: 34/100 | step: 70/422 | loss: 2.2091639041900635\n",
      "Epoch: 34/100 | step: 71/422 | loss: 2.401577949523926\n",
      "Epoch: 34/100 | step: 72/422 | loss: 2.2753384113311768\n",
      "Epoch: 34/100 | step: 73/422 | loss: 2.2064547538757324\n",
      "Epoch: 34/100 | step: 74/422 | loss: 2.3815839290618896\n",
      "Epoch: 34/100 | step: 75/422 | loss: 3.1313905715942383\n",
      "Epoch: 34/100 | step: 76/422 | loss: 1.7635055780410767\n",
      "Epoch: 34/100 | step: 77/422 | loss: 2.489914655685425\n",
      "Epoch: 34/100 | step: 78/422 | loss: 2.6193532943725586\n",
      "Epoch: 34/100 | step: 79/422 | loss: 2.2959678173065186\n",
      "Epoch: 34/100 | step: 80/422 | loss: 2.120908260345459\n",
      "Epoch: 34/100 | step: 81/422 | loss: 2.1094985008239746\n",
      "Epoch: 34/100 | step: 82/422 | loss: 2.0187151432037354\n",
      "Epoch: 34/100 | step: 83/422 | loss: 2.415823459625244\n",
      "Epoch: 34/100 | step: 84/422 | loss: 2.3714728355407715\n",
      "Epoch: 34/100 | step: 85/422 | loss: 2.6905345916748047\n",
      "Epoch: 34/100 | step: 86/422 | loss: 2.0085771083831787\n",
      "Epoch: 34/100 | step: 87/422 | loss: 2.4880459308624268\n",
      "Epoch: 34/100 | step: 88/422 | loss: 2.4657256603240967\n",
      "Epoch: 34/100 | step: 89/422 | loss: 2.430521011352539\n",
      "Epoch: 34/100 | step: 90/422 | loss: 1.683595061302185\n",
      "Epoch: 34/100 | step: 91/422 | loss: 2.1669652462005615\n",
      "Epoch: 34/100 | step: 92/422 | loss: 1.8942699432373047\n",
      "Epoch: 34/100 | step: 93/422 | loss: 2.2332770824432373\n",
      "Epoch: 34/100 | step: 94/422 | loss: 2.023991584777832\n",
      "Epoch: 34/100 | step: 95/422 | loss: 3.0007448196411133\n",
      "Epoch: 34/100 | step: 96/422 | loss: 2.003089189529419\n",
      "Epoch: 34/100 | step: 97/422 | loss: 2.072970390319824\n",
      "Epoch: 34/100 | step: 98/422 | loss: 2.177506446838379\n",
      "Epoch: 34/100 | step: 99/422 | loss: 2.6471059322357178\n",
      "Epoch: 34/100 | step: 100/422 | loss: 2.6828725337982178\n",
      "Epoch: 34/100 | step: 101/422 | loss: 2.31132173538208\n",
      "Epoch: 34/100 | step: 102/422 | loss: 2.1220195293426514\n",
      "Epoch: 34/100 | step: 103/422 | loss: 2.4056358337402344\n",
      "Epoch: 34/100 | step: 104/422 | loss: 2.775953769683838\n",
      "Epoch: 34/100 | step: 105/422 | loss: 2.5206637382507324\n",
      "Epoch: 34/100 | step: 106/422 | loss: 2.5904765129089355\n",
      "Epoch: 34/100 | step: 107/422 | loss: 2.019233226776123\n",
      "Epoch: 34/100 | step: 108/422 | loss: 1.807469367980957\n",
      "Epoch: 34/100 | step: 109/422 | loss: 2.0255744457244873\n",
      "Epoch: 34/100 | step: 110/422 | loss: 1.7966499328613281\n",
      "Epoch: 34/100 | step: 111/422 | loss: 2.519420862197876\n",
      "Epoch: 34/100 | step: 112/422 | loss: 2.6509859561920166\n",
      "Epoch: 34/100 | step: 113/422 | loss: 2.5293822288513184\n",
      "Epoch: 34/100 | step: 114/422 | loss: 2.509910821914673\n",
      "Epoch: 34/100 | step: 115/422 | loss: 2.288649320602417\n",
      "Epoch: 34/100 | step: 116/422 | loss: 2.5576884746551514\n",
      "Epoch: 34/100 | step: 117/422 | loss: 2.85949444770813\n",
      "Epoch: 34/100 | step: 118/422 | loss: 2.1138081550598145\n",
      "Epoch: 34/100 | step: 119/422 | loss: 1.8864874839782715\n",
      "Epoch: 34/100 | step: 120/422 | loss: 2.195164918899536\n",
      "Epoch: 34/100 | step: 121/422 | loss: 2.4712674617767334\n",
      "Epoch: 34/100 | step: 122/422 | loss: 2.1900954246520996\n",
      "Epoch: 34/100 | step: 123/422 | loss: 2.2568745613098145\n",
      "Epoch: 34/100 | step: 124/422 | loss: 2.0698845386505127\n",
      "Epoch: 34/100 | step: 125/422 | loss: 2.6143813133239746\n",
      "Epoch: 34/100 | step: 126/422 | loss: 2.1682896614074707\n",
      "Epoch: 34/100 | step: 127/422 | loss: 1.9214601516723633\n",
      "Epoch: 34/100 | step: 128/422 | loss: 2.860480546951294\n",
      "Epoch: 34/100 | step: 129/422 | loss: 2.5307300090789795\n",
      "Epoch: 34/100 | step: 130/422 | loss: 2.596317768096924\n",
      "Epoch: 34/100 | step: 131/422 | loss: 2.485543727874756\n",
      "Epoch: 34/100 | step: 132/422 | loss: 2.4746389389038086\n",
      "Epoch: 34/100 | step: 133/422 | loss: 1.4718680381774902\n",
      "Epoch: 34/100 | step: 134/422 | loss: 2.018129825592041\n",
      "Epoch: 34/100 | step: 135/422 | loss: 2.4786128997802734\n",
      "Epoch: 34/100 | step: 136/422 | loss: 2.059229612350464\n",
      "Epoch: 34/100 | step: 137/422 | loss: 2.5939724445343018\n",
      "Epoch: 34/100 | step: 138/422 | loss: 2.1856274604797363\n",
      "Epoch: 34/100 | step: 139/422 | loss: 2.086653470993042\n",
      "Epoch: 34/100 | step: 140/422 | loss: 2.658250331878662\n",
      "Epoch: 34/100 | step: 141/422 | loss: 2.7034130096435547\n",
      "Epoch: 34/100 | step: 142/422 | loss: 2.0899384021759033\n",
      "Epoch: 34/100 | step: 143/422 | loss: 2.1355059146881104\n",
      "Epoch: 34/100 | step: 144/422 | loss: 2.2245616912841797\n",
      "Epoch: 34/100 | step: 145/422 | loss: 1.7520555257797241\n",
      "Epoch: 34/100 | step: 146/422 | loss: 2.389921188354492\n",
      "Epoch: 34/100 | step: 147/422 | loss: 2.285280466079712\n",
      "Epoch: 34/100 | step: 148/422 | loss: 2.2451858520507812\n",
      "Epoch: 34/100 | step: 149/422 | loss: 2.3063900470733643\n",
      "Epoch: 34/100 | step: 150/422 | loss: 2.3166706562042236\n",
      "Epoch: 34/100 | step: 151/422 | loss: 2.272655963897705\n",
      "Epoch: 34/100 | step: 152/422 | loss: 2.181234836578369\n",
      "Epoch: 34/100 | step: 153/422 | loss: 2.169356346130371\n",
      "Epoch: 34/100 | step: 154/422 | loss: 2.2242844104766846\n",
      "Epoch: 34/100 | step: 155/422 | loss: 2.422851800918579\n",
      "Epoch: 34/100 | step: 156/422 | loss: 1.8887262344360352\n",
      "Epoch: 34/100 | step: 157/422 | loss: 2.034006357192993\n",
      "Epoch: 34/100 | step: 158/422 | loss: 1.6984878778457642\n",
      "Epoch: 34/100 | step: 159/422 | loss: 2.437927007675171\n",
      "Epoch: 34/100 | step: 160/422 | loss: 2.5215821266174316\n",
      "Epoch: 34/100 | step: 161/422 | loss: 2.4532687664031982\n",
      "Epoch: 34/100 | step: 162/422 | loss: 2.800158977508545\n",
      "Epoch: 34/100 | step: 163/422 | loss: 2.5427026748657227\n",
      "Epoch: 34/100 | step: 164/422 | loss: 2.562990188598633\n",
      "Epoch: 34/100 | step: 165/422 | loss: 2.166205883026123\n",
      "Epoch: 34/100 | step: 166/422 | loss: 2.139477252960205\n",
      "Epoch: 34/100 | step: 167/422 | loss: 2.468412399291992\n",
      "Epoch: 34/100 | step: 168/422 | loss: 2.5521116256713867\n",
      "Epoch: 34/100 | step: 169/422 | loss: 2.3732528686523438\n",
      "Epoch: 34/100 | step: 170/422 | loss: 2.768734931945801\n",
      "Epoch: 34/100 | step: 171/422 | loss: 1.593746304512024\n",
      "Epoch: 34/100 | step: 172/422 | loss: 1.878401517868042\n",
      "Epoch: 34/100 | step: 173/422 | loss: 2.658816337585449\n",
      "Epoch: 34/100 | step: 174/422 | loss: 2.0310699939727783\n",
      "Epoch: 34/100 | step: 175/422 | loss: 2.0162296295166016\n",
      "Epoch: 34/100 | step: 176/422 | loss: 2.792447566986084\n",
      "Epoch: 34/100 | step: 177/422 | loss: 1.9670006036758423\n",
      "Epoch: 34/100 | step: 178/422 | loss: 2.5871713161468506\n",
      "Epoch: 34/100 | step: 179/422 | loss: 2.031520128250122\n",
      "Epoch: 34/100 | step: 180/422 | loss: 2.553133726119995\n",
      "Epoch: 34/100 | step: 181/422 | loss: 2.1502633094787598\n",
      "Epoch: 34/100 | step: 182/422 | loss: 2.601862668991089\n",
      "Epoch: 34/100 | step: 183/422 | loss: 2.309087038040161\n",
      "Epoch: 34/100 | step: 184/422 | loss: 2.3896002769470215\n",
      "Epoch: 34/100 | step: 185/422 | loss: 2.241316318511963\n",
      "Epoch: 34/100 | step: 186/422 | loss: 2.2203783988952637\n",
      "Epoch: 34/100 | step: 187/422 | loss: 2.6834990978240967\n",
      "Epoch: 34/100 | step: 188/422 | loss: 2.7583584785461426\n",
      "Epoch: 34/100 | step: 189/422 | loss: 2.6974966526031494\n",
      "Epoch: 34/100 | step: 190/422 | loss: 2.5792477130889893\n",
      "Epoch: 34/100 | step: 191/422 | loss: 2.0064923763275146\n",
      "Epoch: 34/100 | step: 192/422 | loss: 2.0870018005371094\n",
      "Epoch: 34/100 | step: 193/422 | loss: 3.0820093154907227\n",
      "Epoch: 34/100 | step: 194/422 | loss: 2.6982553005218506\n",
      "Epoch: 34/100 | step: 195/422 | loss: 2.6258411407470703\n",
      "Epoch: 34/100 | step: 196/422 | loss: 1.7664064168930054\n",
      "Epoch: 34/100 | step: 197/422 | loss: 2.6812891960144043\n",
      "Epoch: 34/100 | step: 198/422 | loss: 2.1830475330352783\n",
      "Epoch: 34/100 | step: 199/422 | loss: 3.098203659057617\n",
      "Epoch: 34/100 | step: 200/422 | loss: 2.7304039001464844\n",
      "Epoch: 34/100 | step: 201/422 | loss: 2.5087203979492188\n",
      "Epoch: 34/100 | step: 202/422 | loss: 2.9406955242156982\n",
      "Epoch: 34/100 | step: 203/422 | loss: 2.5664687156677246\n",
      "Epoch: 34/100 | step: 204/422 | loss: 2.3900249004364014\n",
      "Epoch: 34/100 | step: 205/422 | loss: 1.8012851476669312\n",
      "Epoch: 34/100 | step: 206/422 | loss: 2.1541531085968018\n",
      "Epoch: 34/100 | step: 207/422 | loss: 2.487917423248291\n",
      "Epoch: 34/100 | step: 208/422 | loss: 2.0864460468292236\n",
      "Epoch: 34/100 | step: 209/422 | loss: 3.06343936920166\n",
      "Epoch: 34/100 | step: 210/422 | loss: 2.744190216064453\n",
      "Epoch: 34/100 | step: 211/422 | loss: 2.4691686630249023\n",
      "Epoch: 34/100 | step: 212/422 | loss: 2.966423273086548\n",
      "Epoch: 34/100 | step: 213/422 | loss: 2.123328685760498\n",
      "Epoch: 34/100 | step: 214/422 | loss: 2.3517303466796875\n",
      "Epoch: 34/100 | step: 215/422 | loss: 2.615769624710083\n",
      "Epoch: 34/100 | step: 216/422 | loss: 2.194648504257202\n",
      "Epoch: 34/100 | step: 217/422 | loss: 2.5118870735168457\n",
      "Epoch: 34/100 | step: 218/422 | loss: 2.8284289836883545\n",
      "Epoch: 34/100 | step: 219/422 | loss: 2.54891037940979\n",
      "Epoch: 34/100 | step: 220/422 | loss: 2.1953470706939697\n",
      "Epoch: 34/100 | step: 221/422 | loss: 2.4265313148498535\n",
      "Epoch: 34/100 | step: 222/422 | loss: 2.233307361602783\n",
      "Epoch: 34/100 | step: 223/422 | loss: 2.492382287979126\n",
      "Epoch: 34/100 | step: 224/422 | loss: 2.7453453540802\n",
      "Epoch: 34/100 | step: 225/422 | loss: 1.9397996664047241\n",
      "Epoch: 34/100 | step: 226/422 | loss: 2.0545473098754883\n",
      "Epoch: 34/100 | step: 227/422 | loss: 2.150846004486084\n",
      "Epoch: 34/100 | step: 228/422 | loss: 2.0092856884002686\n",
      "Epoch: 34/100 | step: 229/422 | loss: 2.616114616394043\n",
      "Epoch: 34/100 | step: 230/422 | loss: 2.2281148433685303\n",
      "Epoch: 34/100 | step: 231/422 | loss: 2.1663010120391846\n",
      "Epoch: 34/100 | step: 232/422 | loss: 1.8676024675369263\n",
      "Epoch: 34/100 | step: 233/422 | loss: 2.609557628631592\n",
      "Epoch: 34/100 | step: 234/422 | loss: 2.3467516899108887\n",
      "Epoch: 34/100 | step: 235/422 | loss: 1.9319374561309814\n",
      "Epoch: 34/100 | step: 236/422 | loss: 2.4591870307922363\n",
      "Epoch: 34/100 | step: 237/422 | loss: 2.1755735874176025\n",
      "Epoch: 34/100 | step: 238/422 | loss: 2.1590139865875244\n",
      "Epoch: 34/100 | step: 239/422 | loss: 2.456981897354126\n",
      "Epoch: 34/100 | step: 240/422 | loss: 2.2995870113372803\n",
      "Epoch: 34/100 | step: 241/422 | loss: 2.63071346282959\n",
      "Epoch: 34/100 | step: 242/422 | loss: 2.5020482540130615\n",
      "Epoch: 34/100 | step: 243/422 | loss: 2.0793747901916504\n",
      "Epoch: 34/100 | step: 244/422 | loss: 2.072624444961548\n",
      "Epoch: 34/100 | step: 245/422 | loss: 2.664350986480713\n",
      "Epoch: 34/100 | step: 246/422 | loss: 2.221820592880249\n",
      "Epoch: 34/100 | step: 247/422 | loss: 2.313089370727539\n",
      "Epoch: 34/100 | step: 248/422 | loss: 2.848837375640869\n",
      "Epoch: 34/100 | step: 249/422 | loss: 2.2994697093963623\n",
      "Epoch: 34/100 | step: 250/422 | loss: 2.4983110427856445\n",
      "Epoch: 34/100 | step: 251/422 | loss: 2.003499984741211\n",
      "Epoch: 34/100 | step: 252/422 | loss: 1.975237488746643\n",
      "Epoch: 34/100 | step: 253/422 | loss: 2.3039820194244385\n",
      "Epoch: 34/100 | step: 254/422 | loss: 1.9761269092559814\n",
      "Epoch: 34/100 | step: 255/422 | loss: 2.583662509918213\n",
      "Epoch: 34/100 | step: 256/422 | loss: 2.9416751861572266\n",
      "Epoch: 34/100 | step: 257/422 | loss: 2.3977200984954834\n",
      "Epoch: 34/100 | step: 258/422 | loss: 2.3162682056427\n",
      "Epoch: 34/100 | step: 259/422 | loss: 2.139647960662842\n",
      "Epoch: 34/100 | step: 260/422 | loss: 1.8748810291290283\n",
      "Epoch: 34/100 | step: 261/422 | loss: 2.835850477218628\n",
      "Epoch: 34/100 | step: 262/422 | loss: 2.601569175720215\n",
      "Epoch: 34/100 | step: 263/422 | loss: 2.3777387142181396\n",
      "Epoch: 34/100 | step: 264/422 | loss: 2.4051120281219482\n",
      "Epoch: 34/100 | step: 265/422 | loss: 1.7465498447418213\n",
      "Epoch: 34/100 | step: 266/422 | loss: 2.4444751739501953\n",
      "Epoch: 34/100 | step: 267/422 | loss: 2.1950910091400146\n",
      "Epoch: 34/100 | step: 268/422 | loss: 2.690091848373413\n",
      "Epoch: 34/100 | step: 269/422 | loss: 2.431285858154297\n",
      "Epoch: 34/100 | step: 270/422 | loss: 1.974534034729004\n",
      "Epoch: 34/100 | step: 271/422 | loss: 2.095989465713501\n",
      "Epoch: 34/100 | step: 272/422 | loss: 2.448401927947998\n",
      "Epoch: 34/100 | step: 273/422 | loss: 2.5888030529022217\n",
      "Epoch: 34/100 | step: 274/422 | loss: 2.390779495239258\n",
      "Epoch: 34/100 | step: 275/422 | loss: 2.664234161376953\n",
      "Epoch: 34/100 | step: 276/422 | loss: 2.4131109714508057\n",
      "Epoch: 34/100 | step: 277/422 | loss: 2.1987266540527344\n",
      "Epoch: 34/100 | step: 278/422 | loss: 2.143216848373413\n",
      "Epoch: 34/100 | step: 279/422 | loss: 2.31023907661438\n",
      "Epoch: 34/100 | step: 280/422 | loss: 2.7051663398742676\n",
      "Epoch: 34/100 | step: 281/422 | loss: 1.9873759746551514\n",
      "Epoch: 34/100 | step: 282/422 | loss: 3.2001357078552246\n",
      "Epoch: 34/100 | step: 283/422 | loss: 2.5259885787963867\n",
      "Epoch: 34/100 | step: 284/422 | loss: 2.067765235900879\n",
      "Epoch: 34/100 | step: 285/422 | loss: 2.4707510471343994\n",
      "Epoch: 34/100 | step: 286/422 | loss: 2.2494115829467773\n",
      "Epoch: 34/100 | step: 287/422 | loss: 2.4431509971618652\n",
      "Epoch: 34/100 | step: 288/422 | loss: 2.1957592964172363\n",
      "Epoch: 34/100 | step: 289/422 | loss: 2.8649353981018066\n",
      "Epoch: 34/100 | step: 290/422 | loss: 2.5028765201568604\n",
      "Epoch: 34/100 | step: 291/422 | loss: 2.8819773197174072\n",
      "Epoch: 34/100 | step: 292/422 | loss: 2.4146907329559326\n",
      "Epoch: 34/100 | step: 293/422 | loss: 1.8578554391860962\n",
      "Epoch: 34/100 | step: 294/422 | loss: 2.5307271480560303\n",
      "Epoch: 34/100 | step: 295/422 | loss: 2.532505512237549\n",
      "Epoch: 34/100 | step: 296/422 | loss: 2.490400552749634\n",
      "Epoch: 34/100 | step: 297/422 | loss: 3.098262071609497\n",
      "Epoch: 34/100 | step: 298/422 | loss: 2.4881536960601807\n",
      "Epoch: 34/100 | step: 299/422 | loss: 2.5167245864868164\n",
      "Epoch: 34/100 | step: 300/422 | loss: 2.3006808757781982\n",
      "Epoch: 34/100 | step: 301/422 | loss: 2.1592702865600586\n",
      "Epoch: 34/100 | step: 302/422 | loss: 2.3071792125701904\n",
      "Epoch: 34/100 | step: 303/422 | loss: 1.7385425567626953\n",
      "Epoch: 34/100 | step: 304/422 | loss: 2.035872459411621\n",
      "Epoch: 34/100 | step: 305/422 | loss: 2.3613834381103516\n",
      "Epoch: 34/100 | step: 306/422 | loss: 2.7534847259521484\n",
      "Epoch: 34/100 | step: 307/422 | loss: 1.9818156957626343\n",
      "Epoch: 34/100 | step: 308/422 | loss: 2.710636615753174\n",
      "Epoch: 34/100 | step: 309/422 | loss: 2.704511880874634\n",
      "Epoch: 34/100 | step: 310/422 | loss: 2.141672372817993\n",
      "Epoch: 34/100 | step: 311/422 | loss: 2.7938785552978516\n",
      "Epoch: 34/100 | step: 312/422 | loss: 2.240229845046997\n",
      "Epoch: 34/100 | step: 313/422 | loss: 2.071711540222168\n",
      "Epoch: 34/100 | step: 314/422 | loss: 2.280270576477051\n",
      "Epoch: 34/100 | step: 315/422 | loss: 2.4147772789001465\n",
      "Epoch: 34/100 | step: 316/422 | loss: 2.3877768516540527\n",
      "Epoch: 34/100 | step: 317/422 | loss: 2.3262224197387695\n",
      "Epoch: 34/100 | step: 318/422 | loss: 2.205714702606201\n",
      "Epoch: 34/100 | step: 319/422 | loss: 2.8128600120544434\n",
      "Epoch: 34/100 | step: 320/422 | loss: 2.812697649002075\n",
      "Epoch: 34/100 | step: 321/422 | loss: 1.9746193885803223\n",
      "Epoch: 34/100 | step: 322/422 | loss: 2.187415361404419\n",
      "Epoch: 34/100 | step: 323/422 | loss: 2.2103354930877686\n",
      "Epoch: 34/100 | step: 324/422 | loss: 2.109675884246826\n",
      "Epoch: 34/100 | step: 325/422 | loss: 2.399357318878174\n",
      "Epoch: 34/100 | step: 326/422 | loss: 2.0470757484436035\n",
      "Epoch: 34/100 | step: 327/422 | loss: 2.574841260910034\n",
      "Epoch: 34/100 | step: 328/422 | loss: 2.2752294540405273\n",
      "Epoch: 34/100 | step: 329/422 | loss: 2.7474424839019775\n",
      "Epoch: 34/100 | step: 330/422 | loss: 2.4639875888824463\n",
      "Epoch: 34/100 | step: 331/422 | loss: 2.8799026012420654\n",
      "Epoch: 34/100 | step: 332/422 | loss: 2.1872305870056152\n",
      "Epoch: 34/100 | step: 333/422 | loss: 2.4766685962677\n",
      "Epoch: 34/100 | step: 334/422 | loss: 2.2768685817718506\n",
      "Epoch: 34/100 | step: 335/422 | loss: 2.210253953933716\n",
      "Epoch: 34/100 | step: 336/422 | loss: 3.064272403717041\n",
      "Epoch: 34/100 | step: 337/422 | loss: 2.1187121868133545\n",
      "Epoch: 34/100 | step: 338/422 | loss: 2.3016085624694824\n",
      "Epoch: 34/100 | step: 339/422 | loss: 2.3694777488708496\n",
      "Epoch: 34/100 | step: 340/422 | loss: 2.5925354957580566\n",
      "Epoch: 34/100 | step: 341/422 | loss: 3.3860442638397217\n",
      "Epoch: 34/100 | step: 342/422 | loss: 2.4888126850128174\n",
      "Epoch: 34/100 | step: 343/422 | loss: 2.8501718044281006\n",
      "Epoch: 34/100 | step: 344/422 | loss: 2.5171260833740234\n",
      "Epoch: 34/100 | step: 345/422 | loss: 2.129716396331787\n",
      "Epoch: 34/100 | step: 346/422 | loss: 2.1939053535461426\n",
      "Epoch: 34/100 | step: 347/422 | loss: 2.5091490745544434\n",
      "Epoch: 34/100 | step: 348/422 | loss: 2.0624642372131348\n",
      "Epoch: 34/100 | step: 349/422 | loss: 2.3412044048309326\n",
      "Epoch: 34/100 | step: 350/422 | loss: 1.3632447719573975\n",
      "Epoch: 34/100 | step: 351/422 | loss: 2.8811914920806885\n",
      "Epoch: 34/100 | step: 352/422 | loss: 2.150330066680908\n",
      "Epoch: 34/100 | step: 353/422 | loss: 2.501603364944458\n",
      "Epoch: 34/100 | step: 354/422 | loss: 2.316014289855957\n",
      "Epoch: 34/100 | step: 355/422 | loss: 2.150641679763794\n",
      "Epoch: 34/100 | step: 356/422 | loss: 1.9986021518707275\n",
      "Epoch: 34/100 | step: 357/422 | loss: 2.3684005737304688\n",
      "Epoch: 34/100 | step: 358/422 | loss: 2.744906425476074\n",
      "Epoch: 34/100 | step: 359/422 | loss: 2.724108934402466\n",
      "Epoch: 34/100 | step: 360/422 | loss: 2.7634453773498535\n",
      "Epoch: 34/100 | step: 361/422 | loss: 2.4680583477020264\n",
      "Epoch: 34/100 | step: 362/422 | loss: 1.9687856435775757\n",
      "Epoch: 34/100 | step: 363/422 | loss: 2.8729424476623535\n",
      "Epoch: 34/100 | step: 364/422 | loss: 2.074012041091919\n",
      "Epoch: 34/100 | step: 365/422 | loss: 2.2661964893341064\n",
      "Epoch: 34/100 | step: 366/422 | loss: 2.067026138305664\n",
      "Epoch: 34/100 | step: 367/422 | loss: 2.12290096282959\n",
      "Epoch: 34/100 | step: 368/422 | loss: 2.498305559158325\n",
      "Epoch: 34/100 | step: 369/422 | loss: 2.541135549545288\n",
      "Epoch: 34/100 | step: 370/422 | loss: 2.812906265258789\n",
      "Epoch: 34/100 | step: 371/422 | loss: 2.2910995483398438\n",
      "Epoch: 34/100 | step: 372/422 | loss: 1.8867014646530151\n",
      "Epoch: 34/100 | step: 373/422 | loss: 2.571320056915283\n",
      "Epoch: 34/100 | step: 374/422 | loss: 3.042153835296631\n",
      "Epoch: 34/100 | step: 375/422 | loss: 2.6338553428649902\n",
      "Epoch: 34/100 | step: 376/422 | loss: 2.494328498840332\n",
      "Epoch: 34/100 | step: 377/422 | loss: 2.841810464859009\n",
      "Epoch: 34/100 | step: 378/422 | loss: 2.909996509552002\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 35/100 | step: 1/422 | loss: 2.267665147781372\n",
      "Epoch: 35/100 | step: 2/422 | loss: 2.344266176223755\n",
      "Epoch: 35/100 | step: 3/422 | loss: 2.173720121383667\n",
      "Epoch: 35/100 | step: 4/422 | loss: 2.0108368396759033\n",
      "Epoch: 35/100 | step: 5/422 | loss: 1.6089870929718018\n",
      "Epoch: 35/100 | step: 6/422 | loss: 2.0162928104400635\n",
      "Epoch: 35/100 | step: 7/422 | loss: 2.3005518913269043\n",
      "Epoch: 35/100 | step: 8/422 | loss: 1.4175245761871338\n",
      "Epoch: 35/100 | step: 9/422 | loss: 2.213669776916504\n",
      "Epoch: 35/100 | step: 10/422 | loss: 1.7223385572433472\n",
      "Epoch: 35/100 | step: 11/422 | loss: 1.5164047479629517\n",
      "Epoch: 35/100 | step: 12/422 | loss: 2.1863441467285156\n",
      "Epoch: 35/100 | step: 13/422 | loss: 2.1062498092651367\n",
      "Epoch: 35/100 | step: 14/422 | loss: 2.5400664806365967\n",
      "Epoch: 35/100 | step: 15/422 | loss: 2.3119747638702393\n",
      "Epoch: 35/100 | step: 16/422 | loss: 1.7696020603179932\n",
      "Epoch: 35/100 | step: 17/422 | loss: 2.3634939193725586\n",
      "Epoch: 35/100 | step: 18/422 | loss: 1.8957500457763672\n",
      "Epoch: 35/100 | step: 19/422 | loss: 2.3504085540771484\n",
      "Epoch: 35/100 | step: 20/422 | loss: 2.3983685970306396\n",
      "Epoch: 35/100 | step: 21/422 | loss: 1.6996030807495117\n",
      "Epoch: 35/100 | step: 22/422 | loss: 2.3010919094085693\n",
      "Epoch: 35/100 | step: 23/422 | loss: 2.484140157699585\n",
      "Epoch: 35/100 | step: 24/422 | loss: 2.6697919368743896\n",
      "Epoch: 35/100 | step: 25/422 | loss: 2.376422166824341\n",
      "Epoch: 35/100 | step: 26/422 | loss: 2.117499828338623\n",
      "Epoch: 35/100 | step: 27/422 | loss: 2.205476760864258\n",
      "Epoch: 35/100 | step: 28/422 | loss: 2.1197760105133057\n",
      "Epoch: 35/100 | step: 29/422 | loss: 2.424861431121826\n",
      "Epoch: 35/100 | step: 30/422 | loss: 2.4507992267608643\n",
      "Epoch: 35/100 | step: 31/422 | loss: 2.2362136840820312\n",
      "Epoch: 35/100 | step: 32/422 | loss: 2.6833856105804443\n",
      "Epoch: 35/100 | step: 33/422 | loss: 1.7783551216125488\n",
      "Epoch: 35/100 | step: 34/422 | loss: 2.443861484527588\n",
      "Epoch: 35/100 | step: 35/422 | loss: 2.4022488594055176\n",
      "Epoch: 35/100 | step: 36/422 | loss: 2.094130277633667\n",
      "Epoch: 35/100 | step: 37/422 | loss: 1.9622130393981934\n",
      "Epoch: 35/100 | step: 38/422 | loss: 2.4570438861846924\n",
      "Epoch: 35/100 | step: 39/422 | loss: 2.1652750968933105\n",
      "Epoch: 35/100 | step: 40/422 | loss: 2.060621976852417\n",
      "Epoch: 35/100 | step: 41/422 | loss: 2.6255106925964355\n",
      "Epoch: 35/100 | step: 42/422 | loss: 2.179107666015625\n",
      "Epoch: 35/100 | step: 43/422 | loss: 2.2416017055511475\n",
      "Epoch: 35/100 | step: 44/422 | loss: 2.254901170730591\n",
      "Epoch: 35/100 | step: 45/422 | loss: 2.1227917671203613\n",
      "Epoch: 35/100 | step: 46/422 | loss: 2.500833034515381\n",
      "Epoch: 35/100 | step: 47/422 | loss: 1.9871166944503784\n",
      "Epoch: 35/100 | step: 48/422 | loss: 2.1634836196899414\n",
      "Epoch: 35/100 | step: 49/422 | loss: 1.9256761074066162\n",
      "Epoch: 35/100 | step: 50/422 | loss: 1.83329176902771\n",
      "Epoch: 35/100 | step: 51/422 | loss: 1.6160566806793213\n",
      "Epoch: 35/100 | step: 52/422 | loss: 2.098896026611328\n",
      "Epoch: 35/100 | step: 53/422 | loss: 2.120399236679077\n",
      "Epoch: 35/100 | step: 54/422 | loss: 2.2367374897003174\n",
      "Epoch: 35/100 | step: 55/422 | loss: 1.8290891647338867\n",
      "Epoch: 35/100 | step: 56/422 | loss: 1.939638376235962\n",
      "Epoch: 35/100 | step: 57/422 | loss: 2.5341219902038574\n",
      "Epoch: 35/100 | step: 58/422 | loss: 2.5758488178253174\n",
      "Epoch: 35/100 | step: 59/422 | loss: 2.446974039077759\n",
      "Epoch: 35/100 | step: 60/422 | loss: 2.514939785003662\n",
      "Epoch: 35/100 | step: 61/422 | loss: 2.2465670108795166\n",
      "Epoch: 35/100 | step: 62/422 | loss: 1.9781452417373657\n",
      "Epoch: 35/100 | step: 63/422 | loss: 2.1309213638305664\n",
      "Epoch: 35/100 | step: 64/422 | loss: 1.9818307161331177\n",
      "Epoch: 35/100 | step: 65/422 | loss: 2.079132080078125\n",
      "Epoch: 35/100 | step: 66/422 | loss: 2.374591588973999\n",
      "Epoch: 35/100 | step: 67/422 | loss: 2.1432247161865234\n",
      "Epoch: 35/100 | step: 68/422 | loss: 2.082946300506592\n",
      "Epoch: 35/100 | step: 69/422 | loss: 1.902391791343689\n",
      "Epoch: 35/100 | step: 70/422 | loss: 2.143122673034668\n",
      "Epoch: 35/100 | step: 71/422 | loss: 2.417710781097412\n",
      "Epoch: 35/100 | step: 72/422 | loss: 2.4848008155822754\n",
      "Epoch: 35/100 | step: 73/422 | loss: 2.1180343627929688\n",
      "Epoch: 35/100 | step: 74/422 | loss: 2.1594936847686768\n",
      "Epoch: 35/100 | step: 75/422 | loss: 2.063969135284424\n",
      "Epoch: 35/100 | step: 76/422 | loss: 2.438471794128418\n",
      "Epoch: 35/100 | step: 77/422 | loss: 1.9438583850860596\n",
      "Epoch: 35/100 | step: 78/422 | loss: 1.7060372829437256\n",
      "Epoch: 35/100 | step: 79/422 | loss: 1.696120023727417\n",
      "Epoch: 35/100 | step: 80/422 | loss: 2.176151752471924\n",
      "Epoch: 35/100 | step: 81/422 | loss: 1.7222245931625366\n",
      "Epoch: 35/100 | step: 82/422 | loss: 1.9657691717147827\n",
      "Epoch: 35/100 | step: 83/422 | loss: 2.0259575843811035\n",
      "Epoch: 35/100 | step: 84/422 | loss: 2.655993700027466\n",
      "Epoch: 35/100 | step: 85/422 | loss: 2.0143070220947266\n",
      "Epoch: 35/100 | step: 86/422 | loss: 2.7373764514923096\n",
      "Epoch: 35/100 | step: 87/422 | loss: 2.4208171367645264\n",
      "Epoch: 35/100 | step: 88/422 | loss: 2.0022501945495605\n",
      "Epoch: 35/100 | step: 89/422 | loss: 2.3726463317871094\n",
      "Epoch: 35/100 | step: 90/422 | loss: 1.9834750890731812\n",
      "Epoch: 35/100 | step: 91/422 | loss: 2.2429234981536865\n",
      "Epoch: 35/100 | step: 92/422 | loss: 2.029042959213257\n",
      "Epoch: 35/100 | step: 93/422 | loss: 1.9303297996520996\n",
      "Epoch: 35/100 | step: 94/422 | loss: 2.155801773071289\n",
      "Epoch: 35/100 | step: 95/422 | loss: 2.17260479927063\n",
      "Epoch: 35/100 | step: 96/422 | loss: 2.821166753768921\n",
      "Epoch: 35/100 | step: 97/422 | loss: 2.5304832458496094\n",
      "Epoch: 35/100 | step: 98/422 | loss: 2.4169089794158936\n",
      "Epoch: 35/100 | step: 99/422 | loss: 2.272345542907715\n",
      "Epoch: 35/100 | step: 100/422 | loss: 2.362354040145874\n",
      "Epoch: 35/100 | step: 101/422 | loss: 2.3202264308929443\n",
      "Epoch: 35/100 | step: 102/422 | loss: 2.4668455123901367\n",
      "Epoch: 35/100 | step: 103/422 | loss: 1.9900013208389282\n",
      "Epoch: 35/100 | step: 104/422 | loss: 2.081831216812134\n",
      "Epoch: 35/100 | step: 105/422 | loss: 2.1507222652435303\n",
      "Epoch: 35/100 | step: 106/422 | loss: 2.783259630203247\n",
      "Epoch: 35/100 | step: 107/422 | loss: 1.9842846393585205\n",
      "Epoch: 35/100 | step: 108/422 | loss: 2.038137912750244\n",
      "Epoch: 35/100 | step: 109/422 | loss: 2.4096810817718506\n",
      "Epoch: 35/100 | step: 110/422 | loss: 2.1303763389587402\n",
      "Epoch: 35/100 | step: 111/422 | loss: 2.5869877338409424\n",
      "Epoch: 35/100 | step: 112/422 | loss: 2.543203115463257\n",
      "Epoch: 35/100 | step: 113/422 | loss: 1.6768066883087158\n",
      "Epoch: 35/100 | step: 114/422 | loss: 2.340043067932129\n",
      "Epoch: 35/100 | step: 115/422 | loss: 1.866263747215271\n",
      "Epoch: 35/100 | step: 116/422 | loss: 2.1121227741241455\n",
      "Epoch: 35/100 | step: 117/422 | loss: 2.1876401901245117\n",
      "Epoch: 35/100 | step: 118/422 | loss: 2.7834243774414062\n",
      "Epoch: 35/100 | step: 119/422 | loss: 2.2214603424072266\n",
      "Epoch: 35/100 | step: 120/422 | loss: 2.332042932510376\n",
      "Epoch: 35/100 | step: 121/422 | loss: 1.3344285488128662\n",
      "Epoch: 35/100 | step: 122/422 | loss: 2.0082576274871826\n",
      "Epoch: 35/100 | step: 123/422 | loss: 1.9650181531906128\n",
      "Epoch: 35/100 | step: 124/422 | loss: 1.552275538444519\n",
      "Epoch: 35/100 | step: 125/422 | loss: 2.9141037464141846\n",
      "Epoch: 35/100 | step: 126/422 | loss: 2.234790563583374\n",
      "Epoch: 35/100 | step: 127/422 | loss: 1.805166244506836\n",
      "Epoch: 35/100 | step: 128/422 | loss: 2.0363667011260986\n",
      "Epoch: 35/100 | step: 129/422 | loss: 1.8294470310211182\n",
      "Epoch: 35/100 | step: 130/422 | loss: 2.1684670448303223\n",
      "Epoch: 35/100 | step: 131/422 | loss: 1.9431613683700562\n",
      "Epoch: 35/100 | step: 132/422 | loss: 1.9414788484573364\n",
      "Epoch: 35/100 | step: 133/422 | loss: 2.1661927700042725\n",
      "Epoch: 35/100 | step: 134/422 | loss: 2.4563865661621094\n",
      "Epoch: 35/100 | step: 135/422 | loss: 2.0136592388153076\n",
      "Epoch: 35/100 | step: 136/422 | loss: 2.299100875854492\n",
      "Epoch: 35/100 | step: 137/422 | loss: 2.920562267303467\n",
      "Epoch: 35/100 | step: 138/422 | loss: 2.148588180541992\n",
      "Epoch: 35/100 | step: 139/422 | loss: 2.6136248111724854\n",
      "Epoch: 35/100 | step: 140/422 | loss: 2.4778122901916504\n",
      "Epoch: 35/100 | step: 141/422 | loss: 2.1126415729522705\n",
      "Epoch: 35/100 | step: 142/422 | loss: 2.303462266921997\n",
      "Epoch: 35/100 | step: 143/422 | loss: 1.9364383220672607\n",
      "Epoch: 35/100 | step: 144/422 | loss: 2.51735782623291\n",
      "Epoch: 35/100 | step: 145/422 | loss: 2.4531102180480957\n",
      "Epoch: 35/100 | step: 146/422 | loss: 2.2081801891326904\n",
      "Epoch: 35/100 | step: 147/422 | loss: 2.1618306636810303\n",
      "Epoch: 35/100 | step: 148/422 | loss: 1.7859212160110474\n",
      "Epoch: 35/100 | step: 149/422 | loss: 2.5888776779174805\n",
      "Epoch: 35/100 | step: 150/422 | loss: 2.395728349685669\n",
      "Epoch: 35/100 | step: 151/422 | loss: 2.512066602706909\n",
      "Epoch: 35/100 | step: 152/422 | loss: 2.022141933441162\n",
      "Epoch: 35/100 | step: 153/422 | loss: 2.169344425201416\n",
      "Epoch: 35/100 | step: 154/422 | loss: 2.181471824645996\n",
      "Epoch: 35/100 | step: 155/422 | loss: 2.7998509407043457\n",
      "Epoch: 35/100 | step: 156/422 | loss: 2.3419148921966553\n",
      "Epoch: 35/100 | step: 157/422 | loss: 2.263594627380371\n",
      "Epoch: 35/100 | step: 158/422 | loss: 2.3534984588623047\n",
      "Epoch: 35/100 | step: 159/422 | loss: 2.0235748291015625\n",
      "Epoch: 35/100 | step: 160/422 | loss: 2.411785364151001\n",
      "Epoch: 35/100 | step: 161/422 | loss: 2.364018678665161\n",
      "Epoch: 35/100 | step: 162/422 | loss: 2.1225314140319824\n",
      "Epoch: 35/100 | step: 163/422 | loss: 1.8806486129760742\n",
      "Epoch: 35/100 | step: 164/422 | loss: 1.9387471675872803\n",
      "Epoch: 35/100 | step: 165/422 | loss: 2.2067408561706543\n",
      "Epoch: 35/100 | step: 166/422 | loss: 1.9706943035125732\n",
      "Epoch: 35/100 | step: 167/422 | loss: 2.042952537536621\n",
      "Epoch: 35/100 | step: 168/422 | loss: 2.738044500350952\n",
      "Epoch: 35/100 | step: 169/422 | loss: 2.1418299674987793\n",
      "Epoch: 35/100 | step: 170/422 | loss: 2.7990474700927734\n",
      "Epoch: 35/100 | step: 171/422 | loss: 2.771873712539673\n",
      "Epoch: 35/100 | step: 172/422 | loss: 2.243295907974243\n",
      "Epoch: 35/100 | step: 173/422 | loss: 2.3335142135620117\n",
      "Epoch: 35/100 | step: 174/422 | loss: 2.363410234451294\n",
      "Epoch: 35/100 | step: 175/422 | loss: 2.1919262409210205\n",
      "Epoch: 35/100 | step: 176/422 | loss: 2.326880693435669\n",
      "Epoch: 35/100 | step: 177/422 | loss: 2.5861270427703857\n",
      "Epoch: 35/100 | step: 178/422 | loss: 2.103419065475464\n",
      "Epoch: 35/100 | step: 179/422 | loss: 2.019920587539673\n",
      "Epoch: 35/100 | step: 180/422 | loss: 2.004775047302246\n",
      "Epoch: 35/100 | step: 181/422 | loss: 1.8663372993469238\n",
      "Epoch: 35/100 | step: 182/422 | loss: 2.5377092361450195\n",
      "Epoch: 35/100 | step: 183/422 | loss: 2.0168609619140625\n",
      "Epoch: 35/100 | step: 184/422 | loss: 2.3944649696350098\n",
      "Epoch: 35/100 | step: 185/422 | loss: 2.5843372344970703\n",
      "Epoch: 35/100 | step: 186/422 | loss: 2.982757091522217\n",
      "Epoch: 35/100 | step: 187/422 | loss: 2.240219831466675\n",
      "Epoch: 35/100 | step: 188/422 | loss: 2.116206407546997\n",
      "Epoch: 35/100 | step: 189/422 | loss: 2.1708850860595703\n",
      "Epoch: 35/100 | step: 190/422 | loss: 2.1311230659484863\n",
      "Epoch: 35/100 | step: 191/422 | loss: 2.0730040073394775\n",
      "Epoch: 35/100 | step: 192/422 | loss: 2.3615236282348633\n",
      "Epoch: 35/100 | step: 193/422 | loss: 2.1465277671813965\n",
      "Epoch: 35/100 | step: 194/422 | loss: 2.495246648788452\n",
      "Epoch: 35/100 | step: 195/422 | loss: 1.7370530366897583\n",
      "Epoch: 35/100 | step: 196/422 | loss: 2.6136488914489746\n",
      "Epoch: 35/100 | step: 197/422 | loss: 1.9964091777801514\n",
      "Epoch: 35/100 | step: 198/422 | loss: 1.9578254222869873\n",
      "Epoch: 35/100 | step: 199/422 | loss: 1.7917401790618896\n",
      "Epoch: 35/100 | step: 200/422 | loss: 2.4226367473602295\n",
      "Epoch: 35/100 | step: 201/422 | loss: 2.5063321590423584\n",
      "Epoch: 35/100 | step: 202/422 | loss: 2.103175401687622\n",
      "Epoch: 35/100 | step: 203/422 | loss: 2.0063881874084473\n",
      "Epoch: 35/100 | step: 204/422 | loss: 2.1920671463012695\n",
      "Epoch: 35/100 | step: 205/422 | loss: 2.782823085784912\n",
      "Epoch: 35/100 | step: 206/422 | loss: 2.1225855350494385\n",
      "Epoch: 35/100 | step: 207/422 | loss: 2.5431289672851562\n",
      "Epoch: 35/100 | step: 208/422 | loss: 1.8743886947631836\n",
      "Epoch: 35/100 | step: 209/422 | loss: 2.467822551727295\n",
      "Epoch: 35/100 | step: 210/422 | loss: 2.3103318214416504\n",
      "Epoch: 35/100 | step: 211/422 | loss: 2.0285329818725586\n",
      "Epoch: 35/100 | step: 212/422 | loss: 2.2783730030059814\n",
      "Epoch: 35/100 | step: 213/422 | loss: 2.1835219860076904\n",
      "Epoch: 35/100 | step: 214/422 | loss: 1.8456274271011353\n",
      "Epoch: 35/100 | step: 215/422 | loss: 2.2015767097473145\n",
      "Epoch: 35/100 | step: 216/422 | loss: 3.1197257041931152\n",
      "Epoch: 35/100 | step: 217/422 | loss: 2.7688772678375244\n",
      "Epoch: 35/100 | step: 218/422 | loss: 1.9933186769485474\n",
      "Epoch: 35/100 | step: 219/422 | loss: 2.6294500827789307\n",
      "Epoch: 35/100 | step: 220/422 | loss: 2.1575894355773926\n",
      "Epoch: 35/100 | step: 221/422 | loss: 2.426240921020508\n",
      "Epoch: 35/100 | step: 222/422 | loss: 2.1065030097961426\n",
      "Epoch: 35/100 | step: 223/422 | loss: 2.124218702316284\n",
      "Epoch: 35/100 | step: 224/422 | loss: 1.9258683919906616\n",
      "Epoch: 35/100 | step: 225/422 | loss: 2.2894845008850098\n",
      "Epoch: 35/100 | step: 226/422 | loss: 2.5894649028778076\n",
      "Epoch: 35/100 | step: 227/422 | loss: 2.081821918487549\n",
      "Epoch: 35/100 | step: 228/422 | loss: 2.539628505706787\n",
      "Epoch: 35/100 | step: 229/422 | loss: 2.5894625186920166\n",
      "Epoch: 35/100 | step: 230/422 | loss: 2.132049798965454\n",
      "Epoch: 35/100 | step: 231/422 | loss: 2.1258459091186523\n",
      "Epoch: 35/100 | step: 232/422 | loss: 1.8180878162384033\n",
      "Epoch: 35/100 | step: 233/422 | loss: 2.3565564155578613\n",
      "Epoch: 35/100 | step: 234/422 | loss: 2.2492899894714355\n",
      "Epoch: 35/100 | step: 235/422 | loss: 2.3848447799682617\n",
      "Epoch: 35/100 | step: 236/422 | loss: 2.494335412979126\n",
      "Epoch: 35/100 | step: 237/422 | loss: 2.098926067352295\n",
      "Epoch: 35/100 | step: 238/422 | loss: 2.2502951622009277\n",
      "Epoch: 35/100 | step: 239/422 | loss: 1.8910998106002808\n",
      "Epoch: 35/100 | step: 240/422 | loss: 2.3659510612487793\n",
      "Epoch: 35/100 | step: 241/422 | loss: 2.365473508834839\n",
      "Epoch: 35/100 | step: 242/422 | loss: 2.1236369609832764\n",
      "Epoch: 35/100 | step: 243/422 | loss: 2.2959697246551514\n",
      "Epoch: 35/100 | step: 244/422 | loss: 1.6265151500701904\n",
      "Epoch: 35/100 | step: 245/422 | loss: 1.991575837135315\n",
      "Epoch: 35/100 | step: 246/422 | loss: 2.3158042430877686\n",
      "Epoch: 35/100 | step: 247/422 | loss: 2.281798839569092\n",
      "Epoch: 35/100 | step: 248/422 | loss: 2.336000680923462\n",
      "Epoch: 35/100 | step: 249/422 | loss: 2.6034345626831055\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 36/100 | step: 1/422 | loss: 1.9117194414138794\n",
      "Epoch: 36/100 | step: 2/422 | loss: 2.6751787662506104\n",
      "Epoch: 36/100 | step: 3/422 | loss: 2.019047498703003\n",
      "Epoch: 36/100 | step: 4/422 | loss: 1.8093159198760986\n",
      "Epoch: 36/100 | step: 5/422 | loss: 2.215177059173584\n",
      "Epoch: 36/100 | step: 6/422 | loss: 2.0515921115875244\n",
      "Epoch: 36/100 | step: 7/422 | loss: 2.0609941482543945\n",
      "Epoch: 36/100 | step: 8/422 | loss: 2.0700697898864746\n",
      "Epoch: 36/100 | step: 9/422 | loss: 2.2249040603637695\n",
      "Epoch: 36/100 | step: 10/422 | loss: 2.0891737937927246\n",
      "Epoch: 36/100 | step: 11/422 | loss: 1.9586780071258545\n",
      "Epoch: 36/100 | step: 12/422 | loss: 1.9021321535110474\n",
      "Epoch: 36/100 | step: 13/422 | loss: 2.5502896308898926\n",
      "Epoch: 36/100 | step: 14/422 | loss: 2.1796767711639404\n",
      "Epoch: 36/100 | step: 15/422 | loss: 1.754948616027832\n",
      "Epoch: 36/100 | step: 16/422 | loss: 2.123391628265381\n",
      "Epoch: 36/100 | step: 17/422 | loss: 1.9166886806488037\n",
      "Epoch: 36/100 | step: 18/422 | loss: 2.0959866046905518\n",
      "Epoch: 36/100 | step: 19/422 | loss: 2.0529274940490723\n",
      "Epoch: 36/100 | step: 20/422 | loss: 2.169710874557495\n",
      "Epoch: 36/100 | step: 21/422 | loss: 2.133410930633545\n",
      "Epoch: 36/100 | step: 22/422 | loss: 2.0871996879577637\n",
      "Epoch: 36/100 | step: 23/422 | loss: 2.2367732524871826\n",
      "Epoch: 36/100 | step: 24/422 | loss: 1.7950823307037354\n",
      "Epoch: 36/100 | step: 25/422 | loss: 1.637168526649475\n",
      "Epoch: 36/100 | step: 26/422 | loss: 1.8024537563323975\n",
      "Epoch: 36/100 | step: 27/422 | loss: 1.790930986404419\n",
      "Epoch: 36/100 | step: 28/422 | loss: 1.5902734994888306\n",
      "Epoch: 36/100 | step: 29/422 | loss: 1.915252923965454\n",
      "Epoch: 36/100 | step: 30/422 | loss: 2.284804344177246\n",
      "Epoch: 36/100 | step: 31/422 | loss: 2.0598645210266113\n",
      "Epoch: 36/100 | step: 32/422 | loss: 2.2877118587493896\n",
      "Epoch: 36/100 | step: 33/422 | loss: 2.3345861434936523\n",
      "Epoch: 36/100 | step: 34/422 | loss: 2.1916632652282715\n",
      "Epoch: 36/100 | step: 35/422 | loss: 2.7223260402679443\n",
      "Epoch: 36/100 | step: 36/422 | loss: 2.121224880218506\n",
      "Epoch: 36/100 | step: 37/422 | loss: 2.1636133193969727\n",
      "Epoch: 36/100 | step: 38/422 | loss: 2.3035199642181396\n",
      "Epoch: 36/100 | step: 39/422 | loss: 2.3948473930358887\n",
      "Epoch: 36/100 | step: 40/422 | loss: 2.2463552951812744\n",
      "Epoch: 36/100 | step: 41/422 | loss: 1.8587366342544556\n",
      "Epoch: 36/100 | step: 42/422 | loss: 2.110196113586426\n",
      "Epoch: 36/100 | step: 43/422 | loss: 2.0988175868988037\n",
      "Epoch: 36/100 | step: 44/422 | loss: 1.9092953205108643\n",
      "Epoch: 36/100 | step: 45/422 | loss: 2.1594345569610596\n",
      "Epoch: 36/100 | step: 46/422 | loss: 2.23940372467041\n",
      "Epoch: 36/100 | step: 47/422 | loss: 2.3067502975463867\n",
      "Epoch: 36/100 | step: 48/422 | loss: 2.086575746536255\n",
      "Epoch: 36/100 | step: 49/422 | loss: 2.570054292678833\n",
      "Epoch: 36/100 | step: 50/422 | loss: 2.2108240127563477\n",
      "Epoch: 36/100 | step: 51/422 | loss: 2.272690773010254\n",
      "Epoch: 36/100 | step: 52/422 | loss: 1.853353500366211\n",
      "Epoch: 36/100 | step: 53/422 | loss: 1.9258614778518677\n",
      "Epoch: 36/100 | step: 54/422 | loss: 1.690526008605957\n",
      "Epoch: 36/100 | step: 55/422 | loss: 2.3186874389648438\n",
      "Epoch: 36/100 | step: 56/422 | loss: 2.2410311698913574\n",
      "Epoch: 36/100 | step: 57/422 | loss: 2.190229892730713\n",
      "Epoch: 36/100 | step: 58/422 | loss: 2.1798434257507324\n",
      "Epoch: 36/100 | step: 59/422 | loss: 2.1679575443267822\n",
      "Epoch: 36/100 | step: 60/422 | loss: 2.5278220176696777\n",
      "Epoch: 36/100 | step: 61/422 | loss: 2.083296775817871\n",
      "Epoch: 36/100 | step: 62/422 | loss: 2.0645859241485596\n",
      "Epoch: 36/100 | step: 63/422 | loss: 2.2887959480285645\n",
      "Epoch: 36/100 | step: 64/422 | loss: 1.8963241577148438\n",
      "Epoch: 36/100 | step: 65/422 | loss: 2.2172908782958984\n",
      "Epoch: 36/100 | step: 66/422 | loss: 2.2274420261383057\n",
      "Epoch: 36/100 | step: 67/422 | loss: 2.1981422901153564\n",
      "Epoch: 36/100 | step: 68/422 | loss: 2.496389627456665\n",
      "Epoch: 36/100 | step: 69/422 | loss: 2.171733856201172\n",
      "Epoch: 36/100 | step: 70/422 | loss: 2.0677528381347656\n",
      "Epoch: 36/100 | step: 71/422 | loss: 1.9615755081176758\n",
      "Epoch: 36/100 | step: 72/422 | loss: 1.7242382764816284\n",
      "Epoch: 36/100 | step: 73/422 | loss: 2.4396467208862305\n",
      "Epoch: 36/100 | step: 74/422 | loss: 2.498101234436035\n",
      "Epoch: 36/100 | step: 75/422 | loss: 2.4046823978424072\n",
      "Epoch: 36/100 | step: 76/422 | loss: 2.485368490219116\n",
      "Epoch: 36/100 | step: 77/422 | loss: 1.9886106252670288\n",
      "Epoch: 36/100 | step: 78/422 | loss: 2.1957485675811768\n",
      "Epoch: 36/100 | step: 79/422 | loss: 1.793803095817566\n",
      "Epoch: 36/100 | step: 80/422 | loss: 2.5814967155456543\n",
      "Epoch: 36/100 | step: 81/422 | loss: 1.759129524230957\n",
      "Epoch: 36/100 | step: 82/422 | loss: 2.5115950107574463\n",
      "Epoch: 36/100 | step: 83/422 | loss: 1.7333096265792847\n",
      "Epoch: 36/100 | step: 84/422 | loss: 2.58793568611145\n",
      "Epoch: 36/100 | step: 85/422 | loss: 1.6013058423995972\n",
      "Epoch: 36/100 | step: 86/422 | loss: 2.278856039047241\n",
      "Epoch: 36/100 | step: 87/422 | loss: 2.382794141769409\n",
      "Epoch: 36/100 | step: 88/422 | loss: 2.2364487648010254\n",
      "Epoch: 36/100 | step: 89/422 | loss: 2.1423141956329346\n",
      "Epoch: 36/100 | step: 90/422 | loss: 2.4751083850860596\n",
      "Epoch: 36/100 | step: 91/422 | loss: 1.8015021085739136\n",
      "Epoch: 36/100 | step: 92/422 | loss: 1.8245863914489746\n",
      "Epoch: 36/100 | step: 93/422 | loss: 2.174349069595337\n",
      "Epoch: 36/100 | step: 94/422 | loss: 2.2133426666259766\n",
      "Epoch: 36/100 | step: 95/422 | loss: 2.2438130378723145\n",
      "Epoch: 36/100 | step: 96/422 | loss: 1.7837804555892944\n",
      "Epoch: 36/100 | step: 97/422 | loss: 2.574678421020508\n",
      "Epoch: 36/100 | step: 98/422 | loss: 2.1141531467437744\n",
      "Epoch: 36/100 | step: 99/422 | loss: 1.9167473316192627\n",
      "Epoch: 36/100 | step: 100/422 | loss: 2.1116080284118652\n",
      "Epoch: 36/100 | step: 101/422 | loss: 2.0630531311035156\n",
      "Epoch: 36/100 | step: 102/422 | loss: 2.2818639278411865\n",
      "Epoch: 36/100 | step: 103/422 | loss: 2.4905242919921875\n",
      "Epoch: 36/100 | step: 104/422 | loss: 2.281977653503418\n",
      "Epoch: 36/100 | step: 105/422 | loss: 1.7829967737197876\n",
      "Epoch: 36/100 | step: 106/422 | loss: 2.2497401237487793\n",
      "Epoch: 36/100 | step: 107/422 | loss: 2.2238118648529053\n",
      "Epoch: 36/100 | step: 108/422 | loss: 1.7097578048706055\n",
      "Epoch: 36/100 | step: 109/422 | loss: 2.102356433868408\n",
      "Epoch: 36/100 | step: 110/422 | loss: 2.131439685821533\n",
      "Epoch: 36/100 | step: 111/422 | loss: 1.5536620616912842\n",
      "Epoch: 36/100 | step: 112/422 | loss: 1.7541990280151367\n",
      "Epoch: 36/100 | step: 113/422 | loss: 2.1403486728668213\n",
      "Epoch: 36/100 | step: 114/422 | loss: 2.731895685195923\n",
      "Epoch: 36/100 | step: 115/422 | loss: 1.8891351222991943\n",
      "Epoch: 36/100 | step: 116/422 | loss: 2.429287910461426\n",
      "Epoch: 36/100 | step: 117/422 | loss: 2.3777215480804443\n",
      "Epoch: 36/100 | step: 118/422 | loss: 1.8167598247528076\n",
      "Epoch: 36/100 | step: 119/422 | loss: 1.988640546798706\n",
      "Epoch: 36/100 | step: 120/422 | loss: 2.1332311630249023\n",
      "Epoch: 36/100 | step: 121/422 | loss: 2.6830503940582275\n",
      "Epoch: 36/100 | step: 122/422 | loss: 2.0938267707824707\n",
      "Epoch: 36/100 | step: 123/422 | loss: 1.8898594379425049\n",
      "Epoch: 36/100 | step: 124/422 | loss: 2.278996706008911\n",
      "Epoch: 36/100 | step: 125/422 | loss: 1.8078902959823608\n",
      "Epoch: 36/100 | step: 126/422 | loss: 2.161600112915039\n",
      "Epoch: 36/100 | step: 127/422 | loss: 1.8577170372009277\n",
      "Epoch: 36/100 | step: 128/422 | loss: 2.067230701446533\n",
      "Epoch: 36/100 | step: 129/422 | loss: 2.3422648906707764\n",
      "Epoch: 36/100 | step: 130/422 | loss: 1.7635358572006226\n",
      "Epoch: 36/100 | step: 131/422 | loss: 2.0391018390655518\n",
      "Epoch: 36/100 | step: 132/422 | loss: 2.2463972568511963\n",
      "Epoch: 36/100 | step: 133/422 | loss: 2.755530595779419\n",
      "Epoch: 36/100 | step: 134/422 | loss: 2.242767810821533\n",
      "Epoch: 36/100 | step: 135/422 | loss: 1.9894309043884277\n",
      "Epoch: 36/100 | step: 136/422 | loss: 2.0198771953582764\n",
      "Epoch: 36/100 | step: 137/422 | loss: 1.9270262718200684\n",
      "Epoch: 36/100 | step: 138/422 | loss: 1.7908267974853516\n",
      "Epoch: 36/100 | step: 139/422 | loss: 2.499762773513794\n",
      "Epoch: 36/100 | step: 140/422 | loss: 1.9821226596832275\n",
      "Epoch: 36/100 | step: 141/422 | loss: 1.9253194332122803\n",
      "Epoch: 36/100 | step: 142/422 | loss: 1.9926937818527222\n",
      "Epoch: 36/100 | step: 143/422 | loss: 1.969988226890564\n",
      "Epoch: 36/100 | step: 144/422 | loss: 2.1453399658203125\n",
      "Epoch: 36/100 | step: 145/422 | loss: 1.849489688873291\n",
      "Epoch: 36/100 | step: 146/422 | loss: 2.2974915504455566\n",
      "Epoch: 36/100 | step: 147/422 | loss: 2.470529317855835\n",
      "Epoch: 36/100 | step: 148/422 | loss: 2.3951468467712402\n",
      "Epoch: 36/100 | step: 149/422 | loss: 2.000509738922119\n",
      "Epoch: 36/100 | step: 150/422 | loss: 1.9665213823318481\n",
      "Epoch: 36/100 | step: 151/422 | loss: 1.5054881572723389\n",
      "Epoch: 36/100 | step: 152/422 | loss: 2.3586089611053467\n",
      "Epoch: 36/100 | step: 153/422 | loss: 2.3952059745788574\n",
      "Epoch: 36/100 | step: 154/422 | loss: 2.040865421295166\n",
      "Epoch: 36/100 | step: 155/422 | loss: 1.8137233257293701\n",
      "Epoch: 36/100 | step: 156/422 | loss: 2.5194058418273926\n",
      "Epoch: 36/100 | step: 157/422 | loss: 2.123544931411743\n",
      "Epoch: 36/100 | step: 158/422 | loss: 2.374145746231079\n",
      "Epoch: 36/100 | step: 159/422 | loss: 2.006099224090576\n",
      "Epoch: 36/100 | step: 160/422 | loss: 2.466177225112915\n",
      "Epoch: 36/100 | step: 161/422 | loss: 1.9575084447860718\n",
      "Epoch: 36/100 | step: 162/422 | loss: 2.3949685096740723\n",
      "Epoch: 36/100 | step: 163/422 | loss: 2.4572629928588867\n",
      "Epoch: 36/100 | step: 164/422 | loss: 2.761162519454956\n",
      "Epoch: 36/100 | step: 165/422 | loss: 2.197580575942993\n",
      "Epoch: 36/100 | step: 166/422 | loss: 2.6840837001800537\n",
      "Epoch: 36/100 | step: 167/422 | loss: 2.224745750427246\n",
      "Epoch: 36/100 | step: 168/422 | loss: 2.202169418334961\n",
      "Epoch: 36/100 | step: 169/422 | loss: 2.234095811843872\n",
      "Epoch: 36/100 | step: 170/422 | loss: 2.2967689037323\n",
      "Epoch: 36/100 | step: 171/422 | loss: 1.7562158107757568\n",
      "Epoch: 36/100 | step: 172/422 | loss: 1.9687944650650024\n",
      "Epoch: 36/100 | step: 173/422 | loss: 2.0333359241485596\n",
      "Epoch: 36/100 | step: 174/422 | loss: 2.1212244033813477\n",
      "Epoch: 36/100 | step: 175/422 | loss: 2.1336753368377686\n",
      "Epoch: 36/100 | step: 176/422 | loss: 1.9945690631866455\n",
      "Epoch: 36/100 | step: 177/422 | loss: 2.135478973388672\n",
      "Epoch: 36/100 | step: 178/422 | loss: 2.801994562149048\n",
      "Epoch: 36/100 | step: 179/422 | loss: 2.0132253170013428\n",
      "Epoch: 36/100 | step: 180/422 | loss: 2.6318962574005127\n",
      "Epoch: 36/100 | step: 181/422 | loss: 1.7694286108016968\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 37/100 | step: 1/422 | loss: 2.0720183849334717\n",
      "Epoch: 37/100 | step: 2/422 | loss: 2.0677547454833984\n",
      "Epoch: 37/100 | step: 3/422 | loss: 1.9458749294281006\n",
      "Epoch: 37/100 | step: 4/422 | loss: 2.233322858810425\n",
      "Epoch: 37/100 | step: 5/422 | loss: 1.766981840133667\n",
      "Epoch: 37/100 | step: 6/422 | loss: 2.6260597705841064\n",
      "Epoch: 37/100 | step: 7/422 | loss: 1.9385015964508057\n",
      "Epoch: 37/100 | step: 8/422 | loss: 2.0030465126037598\n",
      "Epoch: 37/100 | step: 9/422 | loss: 2.1500563621520996\n",
      "Epoch: 37/100 | step: 10/422 | loss: 1.7681893110275269\n",
      "Epoch: 37/100 | step: 11/422 | loss: 2.170637845993042\n",
      "Epoch: 37/100 | step: 12/422 | loss: 1.9244168996810913\n",
      "Epoch: 37/100 | step: 13/422 | loss: 1.972288727760315\n",
      "Epoch: 37/100 | step: 14/422 | loss: 2.1665730476379395\n",
      "Epoch: 37/100 | step: 15/422 | loss: 2.0648813247680664\n",
      "Epoch: 37/100 | step: 16/422 | loss: 1.3050609827041626\n",
      "Epoch: 37/100 | step: 17/422 | loss: 1.9293652772903442\n",
      "Epoch: 37/100 | step: 18/422 | loss: 2.2583727836608887\n",
      "Epoch: 37/100 | step: 19/422 | loss: 2.3417673110961914\n",
      "Epoch: 37/100 | step: 20/422 | loss: 1.8769152164459229\n",
      "Epoch: 37/100 | step: 21/422 | loss: 2.063217878341675\n",
      "Epoch: 37/100 | step: 22/422 | loss: 1.7811548709869385\n",
      "Epoch: 37/100 | step: 23/422 | loss: 1.9224692583084106\n",
      "Epoch: 37/100 | step: 24/422 | loss: 2.3604671955108643\n",
      "Epoch: 37/100 | step: 25/422 | loss: 2.3368425369262695\n",
      "Epoch: 37/100 | step: 26/422 | loss: 2.571624517440796\n",
      "Epoch: 37/100 | step: 27/422 | loss: 2.4107465744018555\n",
      "Epoch: 37/100 | step: 28/422 | loss: 2.3093621730804443\n",
      "Epoch: 37/100 | step: 29/422 | loss: 1.9692493677139282\n",
      "Epoch: 37/100 | step: 30/422 | loss: 1.8125932216644287\n",
      "Epoch: 37/100 | step: 31/422 | loss: 1.9130922555923462\n",
      "Epoch: 37/100 | step: 32/422 | loss: 1.8820335865020752\n",
      "Epoch: 37/100 | step: 33/422 | loss: 1.99232816696167\n",
      "Epoch: 37/100 | step: 34/422 | loss: 2.44785737991333\n",
      "Epoch: 37/100 | step: 35/422 | loss: 1.8141025304794312\n",
      "Epoch: 37/100 | step: 36/422 | loss: 1.9518415927886963\n",
      "Epoch: 37/100 | step: 37/422 | loss: 1.7621746063232422\n",
      "Epoch: 37/100 | step: 38/422 | loss: 2.420440673828125\n",
      "Epoch: 37/100 | step: 39/422 | loss: 1.5443958044052124\n",
      "Epoch: 37/100 | step: 40/422 | loss: 1.8125423192977905\n",
      "Epoch: 37/100 | step: 41/422 | loss: 1.853991985321045\n",
      "Epoch: 37/100 | step: 42/422 | loss: 3.1939942836761475\n",
      "Epoch: 37/100 | step: 43/422 | loss: 2.1535613536834717\n",
      "Epoch: 37/100 | step: 44/422 | loss: 2.0197291374206543\n",
      "Epoch: 37/100 | step: 45/422 | loss: 1.7338621616363525\n",
      "Epoch: 37/100 | step: 46/422 | loss: 2.278237819671631\n",
      "Epoch: 37/100 | step: 47/422 | loss: 2.0246975421905518\n",
      "Epoch: 37/100 | step: 48/422 | loss: 2.144117832183838\n",
      "Epoch: 37/100 | step: 49/422 | loss: 2.2019166946411133\n",
      "Epoch: 37/100 | step: 50/422 | loss: 1.5942734479904175\n",
      "Epoch: 37/100 | step: 51/422 | loss: 2.0638949871063232\n",
      "Epoch: 37/100 | step: 52/422 | loss: 1.7669190168380737\n",
      "Epoch: 37/100 | step: 53/422 | loss: 1.5382699966430664\n",
      "Epoch: 37/100 | step: 54/422 | loss: 1.5112398862838745\n",
      "Epoch: 37/100 | step: 55/422 | loss: 1.950158953666687\n",
      "Epoch: 37/100 | step: 56/422 | loss: 3.332911491394043\n",
      "Epoch: 37/100 | step: 57/422 | loss: 2.245378255844116\n",
      "Epoch: 37/100 | step: 58/422 | loss: 1.8189826011657715\n",
      "Epoch: 37/100 | step: 59/422 | loss: 2.133969306945801\n",
      "Epoch: 37/100 | step: 60/422 | loss: 1.7832874059677124\n",
      "Epoch: 37/100 | step: 61/422 | loss: 1.6904433965682983\n",
      "Epoch: 37/100 | step: 62/422 | loss: 1.6696536540985107\n",
      "Epoch: 37/100 | step: 63/422 | loss: 2.351604700088501\n",
      "Epoch: 37/100 | step: 64/422 | loss: 1.996026635169983\n",
      "Epoch: 37/100 | step: 65/422 | loss: 2.2119624614715576\n",
      "Epoch: 37/100 | step: 66/422 | loss: 2.1462955474853516\n",
      "Epoch: 37/100 | step: 67/422 | loss: 2.1337130069732666\n",
      "Epoch: 37/100 | step: 68/422 | loss: 2.4817538261413574\n",
      "Epoch: 37/100 | step: 69/422 | loss: 1.614357352256775\n",
      "Epoch: 37/100 | step: 70/422 | loss: 2.4201555252075195\n",
      "Epoch: 37/100 | step: 71/422 | loss: 1.6749274730682373\n",
      "Epoch: 37/100 | step: 72/422 | loss: 1.9264329671859741\n",
      "Epoch: 37/100 | step: 73/422 | loss: 2.600046396255493\n",
      "Epoch: 37/100 | step: 74/422 | loss: 2.663686513900757\n",
      "Epoch: 37/100 | step: 75/422 | loss: 2.166268825531006\n",
      "Epoch: 37/100 | step: 76/422 | loss: 2.389364004135132\n",
      "Epoch: 37/100 | step: 77/422 | loss: 2.737499475479126\n",
      "Epoch: 37/100 | step: 78/422 | loss: 2.1208879947662354\n",
      "Epoch: 37/100 | step: 79/422 | loss: 2.354717493057251\n",
      "Epoch: 37/100 | step: 80/422 | loss: 1.4622483253479004\n",
      "Epoch: 37/100 | step: 81/422 | loss: 2.50192928314209\n",
      "Epoch: 37/100 | step: 82/422 | loss: 2.248457670211792\n",
      "Epoch: 37/100 | step: 83/422 | loss: 2.0690653324127197\n",
      "Epoch: 37/100 | step: 84/422 | loss: 1.9032844305038452\n",
      "Epoch: 37/100 | step: 85/422 | loss: 2.2170445919036865\n",
      "Epoch: 37/100 | step: 86/422 | loss: 1.6735994815826416\n",
      "Epoch: 37/100 | step: 87/422 | loss: 1.950609564781189\n",
      "Epoch: 37/100 | step: 88/422 | loss: 1.7642639875411987\n",
      "Epoch: 37/100 | step: 89/422 | loss: 2.455909490585327\n",
      "Epoch: 37/100 | step: 90/422 | loss: 2.159651517868042\n",
      "Epoch: 37/100 | step: 91/422 | loss: 2.0866708755493164\n",
      "Epoch: 37/100 | step: 92/422 | loss: 2.401143789291382\n",
      "Epoch: 37/100 | step: 93/422 | loss: 2.40830397605896\n",
      "Epoch: 37/100 | step: 94/422 | loss: 2.29813814163208\n",
      "Epoch: 37/100 | step: 95/422 | loss: 2.1772091388702393\n",
      "Epoch: 37/100 | step: 96/422 | loss: 1.8581713438034058\n",
      "Epoch: 37/100 | step: 97/422 | loss: 1.7121464014053345\n",
      "Epoch: 37/100 | step: 98/422 | loss: 2.014657974243164\n",
      "Epoch: 37/100 | step: 99/422 | loss: 2.205859661102295\n",
      "Epoch: 37/100 | step: 100/422 | loss: 1.9914182424545288\n",
      "Epoch: 37/100 | step: 101/422 | loss: 2.1748218536376953\n",
      "Epoch: 37/100 | step: 102/422 | loss: 1.6148499250411987\n",
      "Epoch: 37/100 | step: 103/422 | loss: 2.080144166946411\n",
      "Epoch: 37/100 | step: 104/422 | loss: 1.5504366159439087\n",
      "Epoch: 37/100 | step: 105/422 | loss: 1.875563621520996\n",
      "Epoch: 37/100 | step: 106/422 | loss: 1.8717217445373535\n",
      "Epoch: 37/100 | step: 107/422 | loss: 1.7895137071609497\n",
      "Epoch: 37/100 | step: 108/422 | loss: 1.9246432781219482\n",
      "Epoch: 37/100 | step: 109/422 | loss: 1.8636492490768433\n",
      "Epoch: 37/100 | step: 110/422 | loss: 2.0384325981140137\n",
      "Epoch: 37/100 | step: 111/422 | loss: 2.020033597946167\n",
      "Epoch: 37/100 | step: 112/422 | loss: 1.8928544521331787\n",
      "Epoch: 37/100 | step: 113/422 | loss: 1.8710664510726929\n",
      "Epoch: 37/100 | step: 114/422 | loss: 2.215582847595215\n",
      "Epoch: 37/100 | step: 115/422 | loss: 2.1939682960510254\n",
      "Epoch: 37/100 | step: 116/422 | loss: 1.8378143310546875\n",
      "Epoch: 37/100 | step: 117/422 | loss: 2.0080156326293945\n",
      "Epoch: 37/100 | step: 118/422 | loss: 2.0162150859832764\n",
      "Epoch: 37/100 | step: 119/422 | loss: 1.326909065246582\n",
      "Epoch: 37/100 | step: 120/422 | loss: 2.070827007293701\n",
      "Epoch: 37/100 | step: 121/422 | loss: 2.8952324390411377\n",
      "Epoch: 37/100 | step: 122/422 | loss: 1.6348564624786377\n",
      "Epoch: 37/100 | step: 123/422 | loss: 1.5741609334945679\n",
      "Epoch: 37/100 | step: 124/422 | loss: 2.169650077819824\n",
      "Epoch: 37/100 | step: 125/422 | loss: 2.3553717136383057\n",
      "Epoch: 37/100 | step: 126/422 | loss: 2.447857141494751\n",
      "Epoch: 37/100 | step: 127/422 | loss: 2.306669235229492\n",
      "Epoch: 37/100 | step: 128/422 | loss: 2.0143039226531982\n",
      "Epoch: 37/100 | step: 129/422 | loss: 2.0237979888916016\n",
      "Epoch: 37/100 | step: 130/422 | loss: 2.169981002807617\n",
      "Epoch: 37/100 | step: 131/422 | loss: 1.9790489673614502\n",
      "Epoch: 37/100 | step: 132/422 | loss: 2.6150317192077637\n",
      "Epoch: 37/100 | step: 133/422 | loss: 1.800520420074463\n",
      "Epoch: 37/100 | step: 134/422 | loss: 2.048004627227783\n",
      "Epoch: 37/100 | step: 135/422 | loss: 2.7545435428619385\n",
      "Epoch: 37/100 | step: 136/422 | loss: 1.8802902698516846\n",
      "Epoch: 37/100 | step: 137/422 | loss: 1.7933710813522339\n",
      "Epoch: 37/100 | step: 138/422 | loss: 2.3088226318359375\n",
      "Epoch: 37/100 | step: 139/422 | loss: 1.5721774101257324\n",
      "Epoch: 37/100 | step: 140/422 | loss: 1.967408537864685\n",
      "Epoch: 37/100 | step: 141/422 | loss: 1.8485983610153198\n",
      "Epoch: 37/100 | step: 142/422 | loss: 2.616896867752075\n",
      "Epoch: 37/100 | step: 143/422 | loss: 2.4611685276031494\n",
      "Epoch: 37/100 | step: 144/422 | loss: 2.200716257095337\n",
      "Epoch: 37/100 | step: 145/422 | loss: 1.9248915910720825\n",
      "Epoch: 37/100 | step: 146/422 | loss: 1.8896182775497437\n",
      "Epoch: 37/100 | step: 147/422 | loss: 2.082239866256714\n",
      "Epoch: 37/100 | step: 148/422 | loss: 2.3900017738342285\n",
      "Epoch: 37/100 | step: 149/422 | loss: 2.1673548221588135\n",
      "Epoch: 37/100 | step: 150/422 | loss: 1.8910900354385376\n",
      "Epoch: 37/100 | step: 151/422 | loss: 1.5677452087402344\n",
      "Epoch: 37/100 | step: 152/422 | loss: 2.168248414993286\n",
      "Epoch: 37/100 | step: 153/422 | loss: 1.9144046306610107\n",
      "Epoch: 37/100 | step: 154/422 | loss: 1.5794973373413086\n",
      "Epoch: 37/100 | step: 155/422 | loss: 1.8421800136566162\n",
      "Epoch: 37/100 | step: 156/422 | loss: 1.903343677520752\n",
      "Epoch: 37/100 | step: 157/422 | loss: 2.473177194595337\n",
      "Epoch: 37/100 | step: 158/422 | loss: 1.7422280311584473\n",
      "Epoch: 37/100 | step: 159/422 | loss: 2.359356641769409\n",
      "Epoch: 37/100 | step: 160/422 | loss: 2.4910683631896973\n",
      "Epoch: 37/100 | step: 161/422 | loss: 2.6143479347229004\n",
      "Epoch: 37/100 | step: 162/422 | loss: 1.8434617519378662\n",
      "Epoch: 37/100 | step: 163/422 | loss: 1.9797968864440918\n",
      "Epoch: 37/100 | step: 164/422 | loss: 2.5631885528564453\n",
      "Epoch: 37/100 | step: 165/422 | loss: 1.980955958366394\n",
      "Epoch: 37/100 | step: 166/422 | loss: 1.7034715414047241\n",
      "Epoch: 37/100 | step: 167/422 | loss: 2.055791139602661\n",
      "Epoch: 37/100 | step: 168/422 | loss: 2.3802871704101562\n",
      "Epoch: 37/100 | step: 169/422 | loss: 2.289860248565674\n",
      "Epoch: 37/100 | step: 170/422 | loss: 2.079446315765381\n",
      "Epoch: 37/100 | step: 171/422 | loss: 2.5645456314086914\n",
      "Epoch: 37/100 | step: 172/422 | loss: 1.7088700532913208\n",
      "Epoch: 37/100 | step: 173/422 | loss: 2.156665802001953\n",
      "Epoch: 37/100 | step: 174/422 | loss: 1.9887927770614624\n",
      "Epoch: 37/100 | step: 175/422 | loss: 2.754121780395508\n",
      "Epoch: 37/100 | step: 176/422 | loss: 2.589611053466797\n",
      "Epoch: 37/100 | step: 177/422 | loss: 1.7961342334747314\n",
      "Epoch: 37/100 | step: 178/422 | loss: 1.8922433853149414\n",
      "Epoch: 37/100 | step: 179/422 | loss: 2.0562567710876465\n",
      "Epoch: 37/100 | step: 180/422 | loss: 1.9154444932937622\n",
      "Epoch: 37/100 | step: 181/422 | loss: 1.91188645362854\n",
      "Epoch: 37/100 | step: 182/422 | loss: 1.6073662042617798\n",
      "Epoch: 37/100 | step: 183/422 | loss: 1.5048160552978516\n",
      "Epoch: 37/100 | step: 184/422 | loss: 2.052382230758667\n",
      "Epoch: 37/100 | step: 185/422 | loss: 1.9654735326766968\n",
      "Epoch: 37/100 | step: 186/422 | loss: 1.76862633228302\n",
      "Epoch: 37/100 | step: 187/422 | loss: 2.1202268600463867\n",
      "Epoch: 37/100 | step: 188/422 | loss: 2.3717784881591797\n",
      "Epoch: 37/100 | step: 189/422 | loss: 2.091076374053955\n",
      "Epoch: 37/100 | step: 190/422 | loss: 1.5763757228851318\n",
      "Epoch: 37/100 | step: 191/422 | loss: 2.4587225914001465\n",
      "Epoch: 37/100 | step: 192/422 | loss: 2.387681245803833\n",
      "Epoch: 37/100 | step: 193/422 | loss: 2.1083221435546875\n",
      "Epoch: 37/100 | step: 194/422 | loss: 2.810696601867676\n",
      "Epoch: 37/100 | step: 195/422 | loss: 2.2200191020965576\n",
      "Epoch: 37/100 | step: 196/422 | loss: 1.5257645845413208\n",
      "Epoch: 37/100 | step: 197/422 | loss: 1.99330735206604\n",
      "Epoch: 37/100 | step: 198/422 | loss: 2.3886704444885254\n",
      "Epoch: 37/100 | step: 199/422 | loss: 2.30242657661438\n",
      "Epoch: 37/100 | step: 200/422 | loss: 2.061009407043457\n",
      "Epoch: 37/100 | step: 201/422 | loss: 2.294466495513916\n",
      "Epoch: 37/100 | step: 202/422 | loss: 1.7463878393173218\n",
      "Epoch: 37/100 | step: 203/422 | loss: 2.5873680114746094\n",
      "Epoch: 37/100 | step: 204/422 | loss: 1.848191738128662\n",
      "Epoch: 37/100 | step: 205/422 | loss: 1.9302679300308228\n",
      "Epoch: 37/100 | step: 206/422 | loss: 1.9965202808380127\n",
      "Epoch: 37/100 | step: 207/422 | loss: 2.346881866455078\n",
      "Epoch: 37/100 | step: 208/422 | loss: 2.089838743209839\n",
      "Epoch: 37/100 | step: 209/422 | loss: 1.8721561431884766\n",
      "Epoch: 37/100 | step: 210/422 | loss: 2.221231460571289\n",
      "Epoch: 37/100 | step: 211/422 | loss: 2.4970951080322266\n",
      "Epoch: 37/100 | step: 212/422 | loss: 1.9230319261550903\n",
      "Epoch: 37/100 | step: 213/422 | loss: 2.0410194396972656\n",
      "Epoch: 37/100 | step: 214/422 | loss: 1.9136416912078857\n",
      "Epoch: 37/100 | step: 215/422 | loss: 1.8945152759552002\n",
      "Epoch: 37/100 | step: 216/422 | loss: 1.764297366142273\n",
      "Epoch: 37/100 | step: 217/422 | loss: 2.0153095722198486\n",
      "Epoch: 37/100 | step: 218/422 | loss: 2.0673913955688477\n",
      "Epoch: 37/100 | step: 219/422 | loss: 2.5425307750701904\n",
      "Epoch: 37/100 | step: 220/422 | loss: 2.952671766281128\n",
      "Epoch: 37/100 | step: 221/422 | loss: 2.054098606109619\n",
      "Epoch: 37/100 | step: 222/422 | loss: 1.7048243284225464\n",
      "Epoch: 37/100 | step: 223/422 | loss: 1.8553261756896973\n",
      "Epoch: 37/100 | step: 224/422 | loss: 2.4230828285217285\n",
      "Epoch: 37/100 | step: 225/422 | loss: 2.023249626159668\n",
      "Epoch: 37/100 | step: 226/422 | loss: 2.512468099594116\n",
      "Epoch: 37/100 | step: 227/422 | loss: 2.0549018383026123\n",
      "Epoch: 37/100 | step: 228/422 | loss: 2.038867235183716\n",
      "Epoch: 37/100 | step: 229/422 | loss: 1.7727864980697632\n",
      "Epoch: 37/100 | step: 230/422 | loss: 1.776959776878357\n",
      "Epoch: 37/100 | step: 231/422 | loss: 2.1270453929901123\n",
      "Epoch: 37/100 | step: 232/422 | loss: 1.846165418624878\n",
      "Epoch: 37/100 | step: 233/422 | loss: 2.2833521366119385\n",
      "Epoch: 37/100 | step: 234/422 | loss: 1.7224889993667603\n",
      "Epoch: 37/100 | step: 235/422 | loss: 2.0476644039154053\n",
      "Epoch: 37/100 | step: 236/422 | loss: 2.1938517093658447\n",
      "Epoch: 37/100 | step: 237/422 | loss: 1.8812544345855713\n",
      "Epoch: 37/100 | step: 238/422 | loss: 2.005143642425537\n",
      "Epoch: 37/100 | step: 239/422 | loss: 2.0789616107940674\n",
      "Epoch: 37/100 | step: 240/422 | loss: 1.802451729774475\n",
      "Epoch: 37/100 | step: 241/422 | loss: 2.2187557220458984\n",
      "Epoch: 37/100 | step: 242/422 | loss: 2.238393545150757\n",
      "Epoch: 37/100 | step: 243/422 | loss: 2.5553057193756104\n",
      "Epoch: 37/100 | step: 244/422 | loss: 2.0981523990631104\n",
      "Epoch: 37/100 | step: 245/422 | loss: 2.6235389709472656\n",
      "Epoch: 37/100 | step: 246/422 | loss: 2.512073278427124\n",
      "Epoch: 37/100 | step: 247/422 | loss: 1.7828949689865112\n",
      "Epoch: 37/100 | step: 248/422 | loss: 1.7666481733322144\n",
      "Epoch: 37/100 | step: 249/422 | loss: 1.7539234161376953\n",
      "Epoch: 37/100 | step: 250/422 | loss: 1.9327903985977173\n",
      "Epoch: 37/100 | step: 251/422 | loss: 1.9486886262893677\n",
      "Epoch: 37/100 | step: 252/422 | loss: 2.259192943572998\n",
      "Epoch: 37/100 | step: 253/422 | loss: 2.2598066329956055\n",
      "Epoch: 37/100 | step: 254/422 | loss: 2.388901948928833\n",
      "Epoch: 37/100 | step: 255/422 | loss: 1.9741020202636719\n",
      "Epoch: 37/100 | step: 256/422 | loss: 2.044165849685669\n",
      "Epoch: 37/100 | step: 257/422 | loss: 2.7146151065826416\n",
      "Epoch: 37/100 | step: 258/422 | loss: 2.320483922958374\n",
      "Epoch: 37/100 | step: 259/422 | loss: 2.199516773223877\n",
      "Epoch: 37/100 | step: 260/422 | loss: 1.8345946073532104\n",
      "Epoch: 37/100 | step: 261/422 | loss: 2.3354601860046387\n",
      "Epoch: 37/100 | step: 262/422 | loss: 2.334977865219116\n",
      "Epoch: 37/100 | step: 263/422 | loss: 2.441551446914673\n",
      "Epoch: 37/100 | step: 264/422 | loss: 2.3651866912841797\n",
      "Epoch: 37/100 | step: 265/422 | loss: 2.0552141666412354\n",
      "Epoch: 37/100 | step: 266/422 | loss: 1.9758763313293457\n",
      "Epoch: 37/100 | step: 267/422 | loss: 1.8734413385391235\n",
      "Epoch: 37/100 | step: 268/422 | loss: 2.2344837188720703\n",
      "Epoch: 37/100 | step: 269/422 | loss: 2.192931652069092\n",
      "Epoch: 37/100 | step: 270/422 | loss: 2.1993489265441895\n",
      "Epoch: 37/100 | step: 271/422 | loss: 2.334613084793091\n",
      "Epoch: 37/100 | step: 272/422 | loss: 2.1174306869506836\n",
      "Epoch: 37/100 | step: 273/422 | loss: 2.183183193206787\n",
      "Epoch: 37/100 | step: 274/422 | loss: 1.8954906463623047\n",
      "Epoch: 37/100 | step: 275/422 | loss: 1.6324567794799805\n",
      "Epoch: 37/100 | step: 276/422 | loss: 1.9061286449432373\n",
      "Epoch: 37/100 | step: 277/422 | loss: 2.4336860179901123\n",
      "Epoch: 37/100 | step: 278/422 | loss: 2.1530778408050537\n",
      "Epoch: 37/100 | step: 279/422 | loss: 2.238584518432617\n",
      "Epoch: 37/100 | step: 280/422 | loss: 1.79641854763031\n",
      "Epoch: 37/100 | step: 281/422 | loss: 2.295844554901123\n",
      "Epoch: 37/100 | step: 282/422 | loss: 2.369612455368042\n",
      "Epoch: 37/100 | step: 283/422 | loss: 2.1046435832977295\n",
      "Epoch: 37/100 | step: 284/422 | loss: 2.040631055831909\n",
      "Epoch: 37/100 | step: 285/422 | loss: 1.6272828578948975\n",
      "Epoch: 37/100 | step: 286/422 | loss: 2.423201560974121\n",
      "Epoch: 37/100 | step: 287/422 | loss: 2.361741781234741\n",
      "Epoch: 37/100 | step: 288/422 | loss: 2.6782052516937256\n",
      "Epoch: 37/100 | step: 289/422 | loss: 1.2827767133712769\n",
      "Epoch: 37/100 | step: 290/422 | loss: 2.1645030975341797\n",
      "Epoch: 37/100 | step: 291/422 | loss: 1.805720567703247\n",
      "Epoch: 37/100 | step: 292/422 | loss: 1.7484039068222046\n",
      "Epoch: 37/100 | step: 293/422 | loss: 1.7631006240844727\n",
      "Epoch: 37/100 | step: 294/422 | loss: 1.71531343460083\n",
      "Epoch: 37/100 | step: 295/422 | loss: 2.5051939487457275\n",
      "Epoch: 37/100 | step: 296/422 | loss: 1.7429383993148804\n",
      "Epoch: 37/100 | step: 297/422 | loss: 2.449276924133301\n",
      "Epoch: 37/100 | step: 298/422 | loss: 1.8200044631958008\n",
      "Epoch: 37/100 | step: 299/422 | loss: 1.7514753341674805\n",
      "Epoch: 37/100 | step: 300/422 | loss: 1.88883376121521\n",
      "Epoch: 37/100 | step: 301/422 | loss: 1.9170266389846802\n",
      "Epoch: 37/100 | step: 302/422 | loss: 2.414170503616333\n",
      "Epoch: 37/100 | step: 303/422 | loss: 2.4138600826263428\n",
      "Epoch: 37/100 | step: 304/422 | loss: 2.1996452808380127\n",
      "Epoch: 37/100 | step: 305/422 | loss: 2.5175864696502686\n",
      "Epoch: 37/100 | step: 306/422 | loss: 2.2766833305358887\n",
      "Epoch: 37/100 | step: 307/422 | loss: 2.052938461303711\n",
      "Epoch: 37/100 | step: 308/422 | loss: 1.779506802558899\n",
      "Epoch: 37/100 | step: 309/422 | loss: 1.9641871452331543\n",
      "Epoch: 37/100 | step: 310/422 | loss: 2.3041462898254395\n",
      "Epoch: 37/100 | step: 311/422 | loss: 2.3914082050323486\n",
      "Epoch: 37/100 | step: 312/422 | loss: 2.4402244091033936\n",
      "Epoch: 37/100 | step: 313/422 | loss: 2.1256697177886963\n",
      "Epoch: 37/100 | step: 314/422 | loss: 2.4074270725250244\n",
      "Epoch: 37/100 | step: 315/422 | loss: 2.380923271179199\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 38/100 | step: 1/422 | loss: 2.366156578063965\n",
      "Epoch: 38/100 | step: 2/422 | loss: 1.9031388759613037\n",
      "Epoch: 38/100 | step: 3/422 | loss: 2.219007968902588\n",
      "Epoch: 38/100 | step: 4/422 | loss: 2.0945355892181396\n",
      "Epoch: 38/100 | step: 5/422 | loss: 1.447304129600525\n",
      "Epoch: 38/100 | step: 6/422 | loss: 1.785259485244751\n",
      "Epoch: 38/100 | step: 7/422 | loss: 2.223930835723877\n",
      "Epoch: 38/100 | step: 8/422 | loss: 2.501631021499634\n",
      "Epoch: 38/100 | step: 9/422 | loss: 2.3059303760528564\n",
      "Epoch: 38/100 | step: 10/422 | loss: 1.8433537483215332\n",
      "Epoch: 38/100 | step: 11/422 | loss: 2.1364545822143555\n",
      "Epoch: 38/100 | step: 12/422 | loss: 1.6946933269500732\n",
      "Epoch: 38/100 | step: 13/422 | loss: 2.146881341934204\n",
      "Epoch: 38/100 | step: 14/422 | loss: 2.050309181213379\n",
      "Epoch: 38/100 | step: 15/422 | loss: 1.8619472980499268\n",
      "Epoch: 38/100 | step: 16/422 | loss: 2.3447916507720947\n",
      "Epoch: 38/100 | step: 17/422 | loss: 2.087575674057007\n",
      "Epoch: 38/100 | step: 18/422 | loss: 1.966698408126831\n",
      "Epoch: 38/100 | step: 19/422 | loss: 1.581877589225769\n",
      "Epoch: 38/100 | step: 20/422 | loss: 1.8199543952941895\n",
      "Epoch: 38/100 | step: 21/422 | loss: 2.5455870628356934\n",
      "Epoch: 38/100 | step: 22/422 | loss: 1.509303092956543\n",
      "Epoch: 38/100 | step: 23/422 | loss: 2.418402910232544\n",
      "Epoch: 38/100 | step: 24/422 | loss: 2.067586660385132\n",
      "Epoch: 38/100 | step: 25/422 | loss: 1.7744364738464355\n",
      "Epoch: 38/100 | step: 26/422 | loss: 1.6115983724594116\n",
      "Epoch: 38/100 | step: 27/422 | loss: 1.8287129402160645\n",
      "Epoch: 38/100 | step: 28/422 | loss: 1.7834222316741943\n",
      "Epoch: 38/100 | step: 29/422 | loss: 1.4033530950546265\n",
      "Epoch: 38/100 | step: 30/422 | loss: 2.5238332748413086\n",
      "Epoch: 38/100 | step: 31/422 | loss: 1.900290846824646\n",
      "Epoch: 38/100 | step: 32/422 | loss: 2.4674384593963623\n",
      "Epoch: 38/100 | step: 33/422 | loss: 1.852913737297058\n",
      "Epoch: 38/100 | step: 34/422 | loss: 2.199241876602173\n",
      "Epoch: 38/100 | step: 35/422 | loss: 1.5921428203582764\n",
      "Epoch: 38/100 | step: 36/422 | loss: 1.7308329343795776\n",
      "Epoch: 38/100 | step: 37/422 | loss: 2.2477428913116455\n",
      "Epoch: 38/100 | step: 38/422 | loss: 2.3602488040924072\n",
      "Epoch: 38/100 | step: 39/422 | loss: 2.1186487674713135\n",
      "Epoch: 38/100 | step: 40/422 | loss: 1.7982537746429443\n",
      "Epoch: 38/100 | step: 41/422 | loss: 1.7466548681259155\n",
      "Epoch: 38/100 | step: 42/422 | loss: 2.2580771446228027\n",
      "Epoch: 38/100 | step: 43/422 | loss: 2.0671844482421875\n",
      "Epoch: 38/100 | step: 44/422 | loss: 1.6817922592163086\n",
      "Epoch: 38/100 | step: 45/422 | loss: 2.2183055877685547\n",
      "Epoch: 38/100 | step: 46/422 | loss: 1.9718968868255615\n",
      "Epoch: 38/100 | step: 47/422 | loss: 1.5987493991851807\n",
      "Epoch: 38/100 | step: 48/422 | loss: 2.241647243499756\n",
      "Epoch: 38/100 | step: 49/422 | loss: 1.974884271621704\n",
      "Epoch: 38/100 | step: 50/422 | loss: 2.0486764907836914\n",
      "Epoch: 38/100 | step: 51/422 | loss: 1.5664018392562866\n",
      "Epoch: 38/100 | step: 52/422 | loss: 1.8506734371185303\n",
      "Epoch: 38/100 | step: 53/422 | loss: 1.6681612730026245\n",
      "Epoch: 38/100 | step: 54/422 | loss: 2.5640344619750977\n",
      "Epoch: 38/100 | step: 55/422 | loss: 2.7601025104522705\n",
      "Epoch: 38/100 | step: 56/422 | loss: 2.651750326156616\n",
      "Epoch: 38/100 | step: 57/422 | loss: 2.2194464206695557\n",
      "Epoch: 38/100 | step: 58/422 | loss: 1.9516465663909912\n",
      "Epoch: 38/100 | step: 59/422 | loss: 2.0790348052978516\n",
      "Epoch: 38/100 | step: 60/422 | loss: 1.961740493774414\n",
      "Epoch: 38/100 | step: 61/422 | loss: 2.8356826305389404\n",
      "Epoch: 38/100 | step: 62/422 | loss: 1.8338329792022705\n",
      "Epoch: 38/100 | step: 63/422 | loss: 2.2808518409729004\n",
      "Epoch: 38/100 | step: 64/422 | loss: 1.8695405721664429\n",
      "Epoch: 38/100 | step: 65/422 | loss: 2.1120736598968506\n",
      "Epoch: 38/100 | step: 66/422 | loss: 2.408731460571289\n",
      "Epoch: 38/100 | step: 67/422 | loss: 2.0307281017303467\n",
      "Epoch: 38/100 | step: 68/422 | loss: 2.1291608810424805\n",
      "Epoch: 38/100 | step: 69/422 | loss: 1.9956696033477783\n",
      "Epoch: 38/100 | step: 70/422 | loss: 2.4268546104431152\n",
      "Epoch: 38/100 | step: 71/422 | loss: 1.9188824892044067\n",
      "Epoch: 38/100 | step: 72/422 | loss: 2.0871007442474365\n",
      "Epoch: 38/100 | step: 73/422 | loss: 1.977057695388794\n",
      "Epoch: 38/100 | step: 74/422 | loss: 1.9979604482650757\n",
      "Epoch: 38/100 | step: 75/422 | loss: 2.201533794403076\n",
      "Epoch: 38/100 | step: 76/422 | loss: 1.769423246383667\n",
      "Epoch: 38/100 | step: 77/422 | loss: 1.8172096014022827\n",
      "Epoch: 38/100 | step: 78/422 | loss: 1.664140224456787\n",
      "Epoch: 38/100 | step: 79/422 | loss: 1.8714516162872314\n",
      "Epoch: 38/100 | step: 80/422 | loss: 2.352583408355713\n",
      "Epoch: 38/100 | step: 81/422 | loss: 2.2941224575042725\n",
      "Epoch: 38/100 | step: 82/422 | loss: 1.7841404676437378\n",
      "Epoch: 38/100 | step: 83/422 | loss: 2.026904582977295\n",
      "Epoch: 38/100 | step: 84/422 | loss: 1.7132456302642822\n",
      "Epoch: 38/100 | step: 85/422 | loss: 1.8301161527633667\n",
      "Epoch: 38/100 | step: 86/422 | loss: 2.017730951309204\n",
      "Epoch: 38/100 | step: 87/422 | loss: 1.9641615152359009\n",
      "Epoch: 38/100 | step: 88/422 | loss: 1.8237333297729492\n",
      "Epoch: 38/100 | step: 89/422 | loss: 1.8079622983932495\n",
      "Epoch: 38/100 | step: 90/422 | loss: 2.1489312648773193\n",
      "Epoch: 38/100 | step: 91/422 | loss: 1.4523159265518188\n",
      "Epoch: 38/100 | step: 92/422 | loss: 2.0702927112579346\n",
      "Epoch: 38/100 | step: 93/422 | loss: 2.032514810562134\n",
      "Epoch: 38/100 | step: 94/422 | loss: 2.1613075733184814\n",
      "Epoch: 38/100 | step: 95/422 | loss: 2.1493780612945557\n",
      "Epoch: 38/100 | step: 96/422 | loss: 2.220613718032837\n",
      "Epoch: 38/100 | step: 97/422 | loss: 2.1431171894073486\n",
      "Epoch: 38/100 | step: 98/422 | loss: 2.396912097930908\n",
      "Epoch: 38/100 | step: 99/422 | loss: 1.7952957153320312\n",
      "Epoch: 38/100 | step: 100/422 | loss: 2.512087821960449\n",
      "Epoch: 38/100 | step: 101/422 | loss: 1.9540845155715942\n",
      "Epoch: 38/100 | step: 102/422 | loss: 1.6616944074630737\n",
      "Epoch: 38/100 | step: 103/422 | loss: 1.400498390197754\n",
      "Epoch: 38/100 | step: 104/422 | loss: 2.107987880706787\n",
      "Epoch: 38/100 | step: 105/422 | loss: 1.7099884748458862\n",
      "Epoch: 38/100 | step: 106/422 | loss: 1.8693749904632568\n",
      "Epoch: 38/100 | step: 107/422 | loss: 1.9634554386138916\n",
      "Epoch: 38/100 | step: 108/422 | loss: 1.4350638389587402\n",
      "Epoch: 38/100 | step: 109/422 | loss: 2.2576887607574463\n",
      "Epoch: 38/100 | step: 110/422 | loss: 1.990288496017456\n",
      "Epoch: 38/100 | step: 111/422 | loss: 1.7201733589172363\n",
      "Epoch: 38/100 | step: 112/422 | loss: 1.9655224084854126\n",
      "Epoch: 38/100 | step: 113/422 | loss: 1.804378628730774\n",
      "Epoch: 38/100 | step: 114/422 | loss: 1.6193656921386719\n",
      "Epoch: 38/100 | step: 115/422 | loss: 1.6382780075073242\n",
      "Epoch: 38/100 | step: 116/422 | loss: 1.9173297882080078\n",
      "Epoch: 38/100 | step: 117/422 | loss: 2.094184637069702\n",
      "Epoch: 38/100 | step: 118/422 | loss: 1.7508102655410767\n",
      "Epoch: 38/100 | step: 119/422 | loss: 1.860073208808899\n",
      "Epoch: 38/100 | step: 120/422 | loss: 1.8612847328186035\n",
      "Epoch: 38/100 | step: 121/422 | loss: 1.496179461479187\n",
      "Epoch: 38/100 | step: 122/422 | loss: 1.977950096130371\n",
      "Epoch: 38/100 | step: 123/422 | loss: 2.284823179244995\n",
      "Epoch: 38/100 | step: 124/422 | loss: 1.803840160369873\n",
      "Epoch: 38/100 | step: 125/422 | loss: 1.655825138092041\n",
      "Epoch: 38/100 | step: 126/422 | loss: 1.8114135265350342\n",
      "Epoch: 38/100 | step: 127/422 | loss: 1.9504616260528564\n",
      "Epoch: 38/100 | step: 128/422 | loss: 2.2191736698150635\n",
      "Epoch: 38/100 | step: 129/422 | loss: 1.791106104850769\n",
      "Epoch: 38/100 | step: 130/422 | loss: 2.0341570377349854\n",
      "Epoch: 38/100 | step: 131/422 | loss: 1.8498998880386353\n",
      "Epoch: 38/100 | step: 132/422 | loss: 2.0042057037353516\n",
      "Epoch: 38/100 | step: 133/422 | loss: 2.297992706298828\n",
      "Epoch: 38/100 | step: 134/422 | loss: 1.4016169309616089\n",
      "Epoch: 38/100 | step: 135/422 | loss: 1.9867321252822876\n",
      "Epoch: 38/100 | step: 136/422 | loss: 2.024050712585449\n",
      "Epoch: 38/100 | step: 137/422 | loss: 1.8496122360229492\n",
      "Epoch: 38/100 | step: 138/422 | loss: 2.1656110286712646\n",
      "Epoch: 38/100 | step: 139/422 | loss: 1.6396615505218506\n",
      "Epoch: 38/100 | step: 140/422 | loss: 2.0656540393829346\n",
      "Epoch: 38/100 | step: 141/422 | loss: 2.121005058288574\n",
      "Epoch: 38/100 | step: 142/422 | loss: 2.438076972961426\n",
      "Epoch: 38/100 | step: 143/422 | loss: 2.3221640586853027\n",
      "Epoch: 38/100 | step: 144/422 | loss: 2.0400197505950928\n",
      "Epoch: 38/100 | step: 145/422 | loss: 1.8907002210617065\n",
      "Epoch: 38/100 | step: 146/422 | loss: 2.210538625717163\n",
      "Epoch: 38/100 | step: 147/422 | loss: 2.2747368812561035\n",
      "Epoch: 38/100 | step: 148/422 | loss: 1.9556798934936523\n",
      "Epoch: 38/100 | step: 149/422 | loss: 1.6809266805648804\n",
      "Epoch: 38/100 | step: 150/422 | loss: 1.837774395942688\n",
      "Epoch: 38/100 | step: 151/422 | loss: 1.6703550815582275\n",
      "Epoch: 38/100 | step: 152/422 | loss: 2.72849440574646\n",
      "Epoch: 38/100 | step: 153/422 | loss: 2.5460031032562256\n",
      "Epoch: 38/100 | step: 154/422 | loss: 2.246577739715576\n",
      "Epoch: 38/100 | step: 155/422 | loss: 2.50038743019104\n",
      "Epoch: 38/100 | step: 156/422 | loss: 2.1207385063171387\n",
      "Epoch: 38/100 | step: 157/422 | loss: 1.568110704421997\n",
      "Epoch: 38/100 | step: 158/422 | loss: 2.0861241817474365\n",
      "Epoch: 38/100 | step: 159/422 | loss: 2.325181007385254\n",
      "Epoch: 38/100 | step: 160/422 | loss: 2.07462739944458\n",
      "Epoch: 38/100 | step: 161/422 | loss: 1.8141913414001465\n",
      "Epoch: 38/100 | step: 162/422 | loss: 1.5822863578796387\n",
      "Epoch: 38/100 | step: 163/422 | loss: 2.4697556495666504\n",
      "Epoch: 38/100 | step: 164/422 | loss: 2.5940587520599365\n",
      "Epoch: 38/100 | step: 165/422 | loss: 2.2560760974884033\n",
      "Epoch: 38/100 | step: 166/422 | loss: 2.236396312713623\n",
      "Epoch: 38/100 | step: 167/422 | loss: 1.7882964611053467\n",
      "Epoch: 38/100 | step: 168/422 | loss: 1.774686574935913\n",
      "Epoch: 38/100 | step: 169/422 | loss: 2.5752806663513184\n",
      "Epoch: 38/100 | step: 170/422 | loss: 1.9078675508499146\n",
      "Epoch: 38/100 | step: 171/422 | loss: 2.4316792488098145\n",
      "Epoch: 38/100 | step: 172/422 | loss: 1.9285461902618408\n",
      "Epoch: 38/100 | step: 173/422 | loss: 2.017390012741089\n",
      "Epoch: 38/100 | step: 174/422 | loss: 2.299502372741699\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 39/100 | step: 1/422 | loss: 1.741163730621338\n",
      "Epoch: 39/100 | step: 2/422 | loss: 1.8032306432724\n",
      "Epoch: 39/100 | step: 3/422 | loss: 2.7303802967071533\n",
      "Epoch: 39/100 | step: 4/422 | loss: 2.0172836780548096\n",
      "Epoch: 39/100 | step: 5/422 | loss: 1.215816855430603\n",
      "Epoch: 39/100 | step: 6/422 | loss: 1.9552960395812988\n",
      "Epoch: 39/100 | step: 7/422 | loss: 2.368415594100952\n",
      "Epoch: 39/100 | step: 8/422 | loss: 1.6864150762557983\n",
      "Epoch: 39/100 | step: 9/422 | loss: 2.47444486618042\n",
      "Epoch: 39/100 | step: 10/422 | loss: 1.9684735536575317\n",
      "Epoch: 39/100 | step: 11/422 | loss: 1.876785397529602\n",
      "Epoch: 39/100 | step: 12/422 | loss: 1.9397380352020264\n",
      "Epoch: 39/100 | step: 13/422 | loss: 1.8001526594161987\n",
      "Epoch: 39/100 | step: 14/422 | loss: 1.5469293594360352\n",
      "Epoch: 39/100 | step: 15/422 | loss: 1.8276853561401367\n",
      "Epoch: 39/100 | step: 16/422 | loss: 1.6753323078155518\n",
      "Epoch: 39/100 | step: 17/422 | loss: 1.9445959329605103\n",
      "Epoch: 39/100 | step: 18/422 | loss: 1.9724276065826416\n",
      "Epoch: 39/100 | step: 19/422 | loss: 1.6266388893127441\n",
      "Epoch: 39/100 | step: 20/422 | loss: 2.199124574661255\n",
      "Epoch: 39/100 | step: 21/422 | loss: 2.594409465789795\n",
      "Epoch: 39/100 | step: 22/422 | loss: 1.5868687629699707\n",
      "Epoch: 39/100 | step: 23/422 | loss: 1.8389806747436523\n",
      "Epoch: 39/100 | step: 24/422 | loss: 2.921309471130371\n",
      "Epoch: 39/100 | step: 25/422 | loss: 2.0388059616088867\n",
      "Epoch: 39/100 | step: 26/422 | loss: 1.6165658235549927\n",
      "Epoch: 39/100 | step: 27/422 | loss: 1.9806345701217651\n",
      "Epoch: 39/100 | step: 28/422 | loss: 1.577717900276184\n",
      "Epoch: 39/100 | step: 29/422 | loss: 1.8057606220245361\n",
      "Epoch: 39/100 | step: 30/422 | loss: 1.5261493921279907\n",
      "Epoch: 39/100 | step: 31/422 | loss: 2.1559627056121826\n",
      "Epoch: 39/100 | step: 32/422 | loss: 1.9296104907989502\n",
      "Epoch: 39/100 | step: 33/422 | loss: 2.1129982471466064\n",
      "Epoch: 39/100 | step: 34/422 | loss: 2.1945550441741943\n",
      "Epoch: 39/100 | step: 35/422 | loss: 2.132850408554077\n",
      "Epoch: 39/100 | step: 36/422 | loss: 2.257441520690918\n",
      "Epoch: 39/100 | step: 37/422 | loss: 2.0402309894561768\n",
      "Epoch: 39/100 | step: 38/422 | loss: 1.8463643789291382\n",
      "Epoch: 39/100 | step: 39/422 | loss: 1.520101547241211\n",
      "Epoch: 39/100 | step: 40/422 | loss: 2.09322190284729\n",
      "Epoch: 39/100 | step: 41/422 | loss: 1.4684791564941406\n",
      "Epoch: 39/100 | step: 42/422 | loss: 1.640599012374878\n",
      "Epoch: 39/100 | step: 43/422 | loss: 2.450479745864868\n",
      "Epoch: 39/100 | step: 44/422 | loss: 1.7336117029190063\n",
      "Epoch: 39/100 | step: 45/422 | loss: 1.9882787466049194\n",
      "Epoch: 39/100 | step: 46/422 | loss: 1.9302654266357422\n",
      "Epoch: 39/100 | step: 47/422 | loss: 1.9616636037826538\n",
      "Epoch: 39/100 | step: 48/422 | loss: 2.4561407566070557\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 40/100 | step: 1/422 | loss: 1.640136957168579\n",
      "Epoch: 40/100 | step: 2/422 | loss: 1.9289677143096924\n",
      "Epoch: 40/100 | step: 3/422 | loss: 1.2051200866699219\n",
      "Epoch: 40/100 | step: 4/422 | loss: 2.1360256671905518\n",
      "Epoch: 40/100 | step: 5/422 | loss: 1.7550605535507202\n",
      "Epoch: 40/100 | step: 6/422 | loss: 1.56068754196167\n",
      "Epoch: 40/100 | step: 7/422 | loss: 2.0243465900421143\n",
      "Epoch: 40/100 | step: 8/422 | loss: 1.9113013744354248\n",
      "Epoch: 40/100 | step: 9/422 | loss: 1.9107879400253296\n",
      "Epoch: 40/100 | step: 10/422 | loss: 1.704804539680481\n",
      "Epoch: 40/100 | step: 11/422 | loss: 1.8813633918762207\n",
      "Epoch: 40/100 | step: 12/422 | loss: 1.6329280138015747\n",
      "Epoch: 40/100 | step: 13/422 | loss: 1.7213059663772583\n",
      "Epoch: 40/100 | step: 14/422 | loss: 1.487462043762207\n",
      "Epoch: 40/100 | step: 15/422 | loss: 2.2413253784179688\n",
      "Epoch: 40/100 | step: 16/422 | loss: 1.6843069791793823\n",
      "Epoch: 40/100 | step: 17/422 | loss: 2.0314066410064697\n",
      "Epoch: 40/100 | step: 18/422 | loss: 1.7766690254211426\n",
      "Epoch: 40/100 | step: 19/422 | loss: 2.0493578910827637\n",
      "Epoch: 40/100 | step: 20/422 | loss: 2.2237465381622314\n",
      "Epoch: 40/100 | step: 21/422 | loss: 2.028508186340332\n",
      "Epoch: 40/100 | step: 22/422 | loss: 2.223546266555786\n",
      "Epoch: 40/100 | step: 23/422 | loss: 1.6798710823059082\n",
      "Epoch: 40/100 | step: 24/422 | loss: 1.9337812662124634\n",
      "Epoch: 40/100 | step: 25/422 | loss: 2.0976762771606445\n",
      "Epoch: 40/100 | step: 26/422 | loss: 1.5626479387283325\n",
      "Epoch: 40/100 | step: 27/422 | loss: 2.0254101753234863\n",
      "Epoch: 40/100 | step: 28/422 | loss: 2.1064181327819824\n",
      "Epoch: 40/100 | step: 29/422 | loss: 1.9630383253097534\n",
      "Epoch: 40/100 | step: 30/422 | loss: 1.6745115518569946\n",
      "Epoch: 40/100 | step: 31/422 | loss: 2.236436605453491\n",
      "Epoch: 40/100 | step: 32/422 | loss: 1.7397750616073608\n",
      "Epoch: 40/100 | step: 33/422 | loss: 1.6162010431289673\n",
      "Epoch: 40/100 | step: 34/422 | loss: 1.7787973880767822\n",
      "Epoch: 40/100 | step: 35/422 | loss: 1.9566866159439087\n",
      "Epoch: 40/100 | step: 36/422 | loss: 2.1826064586639404\n",
      "Epoch: 40/100 | step: 37/422 | loss: 1.9542642831802368\n",
      "Epoch: 40/100 | step: 38/422 | loss: 1.959295630455017\n",
      "Epoch: 40/100 | step: 39/422 | loss: 1.3671776056289673\n",
      "Epoch: 40/100 | step: 40/422 | loss: 1.79329514503479\n",
      "Epoch: 40/100 | step: 41/422 | loss: 1.3634697198867798\n",
      "Epoch: 40/100 | step: 42/422 | loss: 2.0132076740264893\n",
      "Epoch: 40/100 | step: 43/422 | loss: 2.1924166679382324\n",
      "Epoch: 40/100 | step: 44/422 | loss: 1.8031442165374756\n",
      "Epoch: 40/100 | step: 45/422 | loss: 1.7992268800735474\n",
      "Epoch: 40/100 | step: 46/422 | loss: 1.8116350173950195\n",
      "Epoch: 40/100 | step: 47/422 | loss: 1.6442701816558838\n",
      "Epoch: 40/100 | step: 48/422 | loss: 2.0179708003997803\n",
      "Epoch: 40/100 | step: 49/422 | loss: 2.2209713459014893\n",
      "Epoch: 40/100 | step: 50/422 | loss: 1.6988240480422974\n",
      "Epoch: 40/100 | step: 51/422 | loss: 2.4533283710479736\n",
      "Epoch: 40/100 | step: 52/422 | loss: 1.7668044567108154\n",
      "Epoch: 40/100 | step: 53/422 | loss: 1.5566970109939575\n",
      "Epoch: 40/100 | step: 54/422 | loss: 1.7765700817108154\n",
      "Epoch: 40/100 | step: 55/422 | loss: 1.9739055633544922\n",
      "Epoch: 40/100 | step: 56/422 | loss: 1.9140205383300781\n",
      "Epoch: 40/100 | step: 57/422 | loss: 1.6335971355438232\n",
      "Epoch: 40/100 | step: 58/422 | loss: 1.6518018245697021\n",
      "Epoch: 40/100 | step: 59/422 | loss: 1.5975500345230103\n",
      "Epoch: 40/100 | step: 60/422 | loss: 1.7658406496047974\n",
      "Epoch: 40/100 | step: 61/422 | loss: 1.6788363456726074\n",
      "Epoch: 40/100 | step: 62/422 | loss: 1.8434324264526367\n",
      "Epoch: 40/100 | step: 63/422 | loss: 1.9561322927474976\n",
      "Epoch: 40/100 | step: 64/422 | loss: 2.173597812652588\n",
      "Epoch: 40/100 | step: 65/422 | loss: 2.1065304279327393\n",
      "Epoch: 40/100 | step: 66/422 | loss: 2.2786102294921875\n",
      "Epoch: 40/100 | step: 67/422 | loss: 1.764586091041565\n",
      "Epoch: 40/100 | step: 68/422 | loss: 1.6358569860458374\n",
      "Epoch: 40/100 | step: 69/422 | loss: 1.963167428970337\n",
      "Epoch: 40/100 | step: 70/422 | loss: 2.2505478858947754\n",
      "Epoch: 40/100 | step: 71/422 | loss: 1.3325189352035522\n",
      "Epoch: 40/100 | step: 72/422 | loss: 1.672652006149292\n",
      "Epoch: 40/100 | step: 73/422 | loss: 1.408235788345337\n",
      "Epoch: 40/100 | step: 74/422 | loss: 2.1610467433929443\n",
      "Epoch: 40/100 | step: 75/422 | loss: 1.5725300312042236\n",
      "Epoch: 40/100 | step: 76/422 | loss: 2.3875834941864014\n",
      "Epoch: 40/100 | step: 77/422 | loss: 1.7945458889007568\n",
      "Epoch: 40/100 | step: 78/422 | loss: 2.1598598957061768\n",
      "Epoch: 40/100 | step: 79/422 | loss: 2.1410980224609375\n",
      "Epoch: 40/100 | step: 80/422 | loss: 2.108102798461914\n",
      "Epoch: 40/100 | step: 81/422 | loss: 1.7313390970230103\n",
      "Epoch: 40/100 | step: 82/422 | loss: 1.7939767837524414\n",
      "Epoch: 40/100 | step: 83/422 | loss: 1.6502994298934937\n",
      "Epoch: 40/100 | step: 84/422 | loss: 1.8475127220153809\n",
      "Epoch: 40/100 | step: 85/422 | loss: 1.7984387874603271\n",
      "Epoch: 40/100 | step: 86/422 | loss: 1.7216306924819946\n",
      "Epoch: 40/100 | step: 87/422 | loss: 1.8041789531707764\n",
      "Epoch: 40/100 | step: 88/422 | loss: 2.0090858936309814\n",
      "Epoch: 40/100 | step: 89/422 | loss: 2.0296144485473633\n",
      "Epoch: 40/100 | step: 90/422 | loss: 1.6469568014144897\n",
      "Epoch: 40/100 | step: 91/422 | loss: 1.908319115638733\n",
      "Epoch: 40/100 | step: 92/422 | loss: 1.9077914953231812\n",
      "Epoch: 40/100 | step: 93/422 | loss: 1.9401177167892456\n",
      "Epoch: 40/100 | step: 94/422 | loss: 2.068676710128784\n",
      "Epoch: 40/100 | step: 95/422 | loss: 2.41336989402771\n",
      "Epoch: 40/100 | step: 96/422 | loss: 1.1473019123077393\n",
      "Epoch: 40/100 | step: 97/422 | loss: 2.178490161895752\n",
      "Epoch: 40/100 | step: 98/422 | loss: 1.939902424812317\n",
      "Epoch: 40/100 | step: 99/422 | loss: 1.974845290184021\n",
      "Epoch: 40/100 | step: 100/422 | loss: 1.5613102912902832\n",
      "Epoch: 40/100 | step: 101/422 | loss: 1.5988953113555908\n",
      "Epoch: 40/100 | step: 102/422 | loss: 1.8595579862594604\n",
      "Epoch: 40/100 | step: 103/422 | loss: 1.0932457447052002\n",
      "Epoch: 40/100 | step: 104/422 | loss: 1.7904378175735474\n",
      "Epoch: 40/100 | step: 105/422 | loss: 2.045063018798828\n",
      "Epoch: 40/100 | step: 106/422 | loss: 2.3430397510528564\n",
      "Epoch: 40/100 | step: 107/422 | loss: 1.8699201345443726\n",
      "Epoch: 40/100 | step: 108/422 | loss: 1.8862648010253906\n",
      "Epoch: 40/100 | step: 109/422 | loss: 2.287679672241211\n",
      "Epoch: 40/100 | step: 110/422 | loss: 2.3676223754882812\n",
      "Epoch: 40/100 | step: 111/422 | loss: 1.9861581325531006\n",
      "Epoch: 40/100 | step: 112/422 | loss: 2.0952911376953125\n",
      "Epoch: 40/100 | step: 113/422 | loss: 1.8850046396255493\n",
      "Epoch: 40/100 | step: 114/422 | loss: 2.0404791831970215\n",
      "Epoch: 40/100 | step: 115/422 | loss: 2.1075620651245117\n",
      "Epoch: 40/100 | step: 116/422 | loss: 2.0412559509277344\n",
      "Epoch: 40/100 | step: 117/422 | loss: 1.8378798961639404\n",
      "Epoch: 40/100 | step: 118/422 | loss: 1.5607595443725586\n",
      "Epoch: 40/100 | step: 119/422 | loss: 2.0524063110351562\n",
      "Epoch: 40/100 | step: 120/422 | loss: 2.053574800491333\n",
      "Epoch: 40/100 | step: 121/422 | loss: 1.6177735328674316\n",
      "Epoch: 40/100 | step: 122/422 | loss: 1.848747730255127\n",
      "Epoch: 40/100 | step: 123/422 | loss: 1.72415292263031\n",
      "Epoch: 40/100 | step: 124/422 | loss: 3.034067392349243\n",
      "Epoch: 40/100 | step: 125/422 | loss: 1.8443496227264404\n",
      "Epoch: 40/100 | step: 126/422 | loss: 1.6696468591690063\n",
      "Epoch: 40/100 | step: 127/422 | loss: 1.9345612525939941\n",
      "Epoch: 40/100 | step: 128/422 | loss: 1.6876826286315918\n",
      "Epoch: 40/100 | step: 129/422 | loss: 2.304316997528076\n",
      "Epoch: 40/100 | step: 130/422 | loss: 1.3996851444244385\n",
      "Epoch: 40/100 | step: 131/422 | loss: 2.063474655151367\n",
      "Epoch: 40/100 | step: 132/422 | loss: 1.8476547002792358\n",
      "Epoch: 40/100 | step: 133/422 | loss: 2.1275768280029297\n",
      "Epoch: 40/100 | step: 134/422 | loss: 1.492497205734253\n",
      "Epoch: 40/100 | step: 135/422 | loss: 1.6903986930847168\n",
      "Epoch: 40/100 | step: 136/422 | loss: 2.182088613510132\n",
      "Epoch: 40/100 | step: 137/422 | loss: 1.7873080968856812\n",
      "Epoch: 40/100 | step: 138/422 | loss: 2.4757328033447266\n",
      "Epoch: 40/100 | step: 139/422 | loss: 1.7304821014404297\n",
      "Epoch: 40/100 | step: 140/422 | loss: 1.726250410079956\n",
      "Epoch: 40/100 | step: 141/422 | loss: 2.167757034301758\n",
      "Epoch: 40/100 | step: 142/422 | loss: 2.4847981929779053\n",
      "Epoch: 40/100 | step: 143/422 | loss: 1.827357530593872\n",
      "Epoch: 40/100 | step: 144/422 | loss: 1.9371598958969116\n",
      "Epoch: 40/100 | step: 145/422 | loss: 1.8563706874847412\n",
      "Epoch: 40/100 | step: 146/422 | loss: 2.102776527404785\n",
      "Epoch: 40/100 | step: 147/422 | loss: 2.0644731521606445\n",
      "Epoch: 40/100 | step: 148/422 | loss: 2.2990784645080566\n",
      "Epoch: 40/100 | step: 149/422 | loss: 1.5253835916519165\n",
      "Epoch: 40/100 | step: 150/422 | loss: 1.8730111122131348\n",
      "Epoch: 40/100 | step: 151/422 | loss: 1.4775750637054443\n",
      "Epoch: 40/100 | step: 152/422 | loss: 2.5182676315307617\n",
      "Epoch: 40/100 | step: 153/422 | loss: 1.5268551111221313\n",
      "Epoch: 40/100 | step: 154/422 | loss: 2.0283782482147217\n",
      "Epoch: 40/100 | step: 155/422 | loss: 1.9451638460159302\n",
      "Epoch: 40/100 | step: 156/422 | loss: 1.7954144477844238\n",
      "Epoch: 40/100 | step: 157/422 | loss: 1.9142532348632812\n",
      "Epoch: 40/100 | step: 158/422 | loss: 2.0326061248779297\n",
      "Epoch: 40/100 | step: 159/422 | loss: 2.4735310077667236\n",
      "Epoch: 40/100 | step: 160/422 | loss: 2.0872156620025635\n",
      "Epoch: 40/100 | step: 161/422 | loss: 2.0776262283325195\n",
      "Epoch: 40/100 | step: 162/422 | loss: 2.079505205154419\n",
      "Epoch: 40/100 | step: 163/422 | loss: 1.636643409729004\n",
      "Epoch: 40/100 | step: 164/422 | loss: 2.0569748878479004\n",
      "Epoch: 40/100 | step: 165/422 | loss: 2.1677708625793457\n",
      "Epoch: 40/100 | step: 166/422 | loss: 1.136852741241455\n",
      "Epoch: 40/100 | step: 167/422 | loss: 2.3973770141601562\n",
      "Epoch: 40/100 | step: 168/422 | loss: 2.2519285678863525\n",
      "Epoch: 40/100 | step: 169/422 | loss: 1.9912633895874023\n",
      "Epoch: 40/100 | step: 170/422 | loss: 2.1840426921844482\n",
      "Epoch: 40/100 | step: 171/422 | loss: 2.380891799926758\n",
      "Epoch: 40/100 | step: 172/422 | loss: 2.573570966720581\n",
      "Epoch: 40/100 | step: 173/422 | loss: 1.1856098175048828\n",
      "Epoch: 40/100 | step: 174/422 | loss: 2.119690418243408\n",
      "Epoch: 40/100 | step: 175/422 | loss: 2.0577030181884766\n",
      "Epoch: 40/100 | step: 176/422 | loss: 1.5991525650024414\n",
      "Epoch: 40/100 | step: 177/422 | loss: 1.9090924263000488\n",
      "Epoch: 40/100 | step: 178/422 | loss: 1.8008923530578613\n",
      "Epoch: 40/100 | step: 179/422 | loss: 1.986962080001831\n",
      "Epoch: 40/100 | step: 180/422 | loss: 1.8065544366836548\n",
      "Epoch: 40/100 | step: 181/422 | loss: 1.488977313041687\n",
      "Epoch: 40/100 | step: 182/422 | loss: 1.2739307880401611\n",
      "Epoch: 40/100 | step: 183/422 | loss: 2.0105857849121094\n",
      "Epoch: 40/100 | step: 184/422 | loss: 1.5873180627822876\n",
      "Epoch: 40/100 | step: 185/422 | loss: 1.7848953008651733\n",
      "Epoch: 40/100 | step: 186/422 | loss: 1.8664153814315796\n",
      "Epoch: 40/100 | step: 187/422 | loss: 1.6651151180267334\n",
      "Epoch: 40/100 | step: 188/422 | loss: 2.2865960597991943\n",
      "Epoch: 40/100 | step: 189/422 | loss: 2.224520206451416\n",
      "Epoch: 40/100 | step: 190/422 | loss: 1.7134737968444824\n",
      "Epoch: 40/100 | step: 191/422 | loss: 1.336949348449707\n",
      "Epoch: 40/100 | step: 192/422 | loss: 1.791986107826233\n",
      "Epoch: 40/100 | step: 193/422 | loss: 1.6631841659545898\n",
      "Epoch: 40/100 | step: 194/422 | loss: 2.021354913711548\n",
      "Epoch: 40/100 | step: 195/422 | loss: 2.415475845336914\n",
      "Epoch: 40/100 | step: 196/422 | loss: 2.231748342514038\n",
      "Epoch: 40/100 | step: 197/422 | loss: 1.5034323930740356\n",
      "Epoch: 40/100 | step: 198/422 | loss: 1.908311367034912\n",
      "Epoch: 40/100 | step: 199/422 | loss: 1.5907387733459473\n",
      "Epoch: 40/100 | step: 200/422 | loss: 1.7822291851043701\n",
      "Epoch: 40/100 | step: 201/422 | loss: 1.5583311319351196\n",
      "Epoch: 40/100 | step: 202/422 | loss: 2.4168105125427246\n",
      "Epoch: 40/100 | step: 203/422 | loss: 2.366363048553467\n",
      "Epoch: 40/100 | step: 204/422 | loss: 1.5978381633758545\n",
      "Epoch: 40/100 | step: 205/422 | loss: 2.0122177600860596\n",
      "Epoch: 40/100 | step: 206/422 | loss: 2.2889323234558105\n",
      "Epoch: 40/100 | step: 207/422 | loss: 1.352156162261963\n",
      "Epoch: 40/100 | step: 208/422 | loss: 1.6383917331695557\n",
      "Epoch: 40/100 | step: 209/422 | loss: 2.2192540168762207\n",
      "Epoch: 40/100 | step: 210/422 | loss: 2.0674033164978027\n",
      "Epoch: 40/100 | step: 211/422 | loss: 1.6848301887512207\n",
      "Epoch: 40/100 | step: 212/422 | loss: 2.1381754875183105\n",
      "Epoch: 40/100 | step: 213/422 | loss: 1.8557593822479248\n",
      "Epoch: 40/100 | step: 214/422 | loss: 2.06258487701416\n",
      "Epoch: 40/100 | step: 215/422 | loss: 1.9469388723373413\n",
      "Epoch: 40/100 | step: 216/422 | loss: 2.2588367462158203\n",
      "Epoch: 40/100 | step: 217/422 | loss: 2.3249855041503906\n",
      "Epoch: 40/100 | step: 218/422 | loss: 2.6381008625030518\n",
      "Epoch: 40/100 | step: 219/422 | loss: 2.4738264083862305\n",
      "Epoch: 40/100 | step: 220/422 | loss: 1.8633440732955933\n",
      "Epoch: 40/100 | step: 221/422 | loss: 2.6322500705718994\n",
      "Epoch: 40/100 | step: 222/422 | loss: 1.8467222452163696\n",
      "Epoch: 40/100 | step: 223/422 | loss: 2.024676561355591\n",
      "Epoch: 40/100 | step: 224/422 | loss: 1.5284115076065063\n",
      "Epoch: 40/100 | step: 225/422 | loss: 1.9174787998199463\n",
      "Epoch: 40/100 | step: 226/422 | loss: 2.006990909576416\n",
      "Epoch: 40/100 | step: 227/422 | loss: 2.354759454727173\n",
      "Epoch: 40/100 | step: 228/422 | loss: 2.5723180770874023\n",
      "Epoch: 40/100 | step: 229/422 | loss: 1.9341373443603516\n",
      "Epoch: 40/100 | step: 230/422 | loss: 2.2123513221740723\n",
      "Epoch: 40/100 | step: 231/422 | loss: 1.794695496559143\n",
      "Epoch: 40/100 | step: 232/422 | loss: 2.1301183700561523\n",
      "Epoch: 40/100 | step: 233/422 | loss: 1.6486512422561646\n",
      "Epoch: 40/100 | step: 234/422 | loss: 2.1740074157714844\n",
      "Epoch: 40/100 | step: 235/422 | loss: 2.010153293609619\n",
      "Epoch: 40/100 | step: 236/422 | loss: 1.9952151775360107\n",
      "Epoch: 40/100 | step: 237/422 | loss: 1.9239251613616943\n",
      "Epoch: 40/100 | step: 238/422 | loss: 1.831130027770996\n",
      "Epoch: 40/100 | step: 239/422 | loss: 1.8708117008209229\n",
      "Epoch: 40/100 | step: 240/422 | loss: 2.10011625289917\n",
      "Epoch: 40/100 | step: 241/422 | loss: 2.4369964599609375\n",
      "Epoch: 40/100 | step: 242/422 | loss: 1.783621072769165\n",
      "Epoch: 40/100 | step: 243/422 | loss: 2.078791618347168\n",
      "Epoch: 40/100 | step: 244/422 | loss: 2.179138660430908\n",
      "Epoch: 40/100 | step: 245/422 | loss: 1.3407297134399414\n",
      "Epoch: 40/100 | step: 246/422 | loss: 1.9814530611038208\n",
      "Epoch: 40/100 | step: 247/422 | loss: 1.786787748336792\n",
      "Epoch: 40/100 | step: 248/422 | loss: 2.0876381397247314\n",
      "Epoch: 40/100 | step: 249/422 | loss: 1.7391722202301025\n",
      "Epoch: 40/100 | step: 250/422 | loss: 1.3868992328643799\n",
      "Epoch: 40/100 | step: 251/422 | loss: 2.073122024536133\n",
      "Epoch: 40/100 | step: 252/422 | loss: 1.7932701110839844\n",
      "Epoch: 40/100 | step: 253/422 | loss: 2.057586431503296\n",
      "Epoch: 40/100 | step: 254/422 | loss: 2.252741813659668\n",
      "Epoch: 40/100 | step: 255/422 | loss: 2.1576130390167236\n",
      "Epoch: 40/100 | step: 256/422 | loss: 2.008411169052124\n",
      "Epoch: 40/100 | step: 257/422 | loss: 1.6811431646347046\n",
      "Epoch: 40/100 | step: 258/422 | loss: 2.498110771179199\n",
      "Epoch: 40/100 | step: 259/422 | loss: 2.4162847995758057\n",
      "Epoch: 40/100 | step: 260/422 | loss: 1.7770329713821411\n",
      "Epoch: 40/100 | step: 261/422 | loss: 2.4937069416046143\n",
      "Epoch: 40/100 | step: 262/422 | loss: 2.4319372177124023\n",
      "Epoch: 40/100 | step: 263/422 | loss: 2.2605950832366943\n",
      "Epoch: 40/100 | step: 264/422 | loss: 2.2487480640411377\n",
      "Epoch: 40/100 | step: 265/422 | loss: 1.5993801355361938\n",
      "Epoch: 40/100 | step: 266/422 | loss: 1.9944239854812622\n",
      "Epoch: 40/100 | step: 267/422 | loss: 1.6013778448104858\n",
      "Epoch: 40/100 | step: 268/422 | loss: 1.8819807767868042\n",
      "Epoch: 40/100 | step: 269/422 | loss: 1.6021618843078613\n",
      "Epoch: 40/100 | step: 270/422 | loss: 2.39188289642334\n",
      "Epoch: 40/100 | step: 271/422 | loss: 1.8867101669311523\n",
      "Epoch: 40/100 | step: 272/422 | loss: 1.9136767387390137\n",
      "Epoch: 40/100 | step: 273/422 | loss: 2.431729316711426\n",
      "Epoch: 40/100 | step: 274/422 | loss: 1.936730146408081\n",
      "Epoch: 40/100 | step: 275/422 | loss: 1.7323423624038696\n",
      "Epoch: 40/100 | step: 276/422 | loss: 1.3860853910446167\n",
      "Epoch: 40/100 | step: 277/422 | loss: 1.8127185106277466\n",
      "Epoch: 40/100 | step: 278/422 | loss: 1.9925051927566528\n",
      "Epoch: 40/100 | step: 279/422 | loss: 1.7571734189987183\n",
      "Epoch: 40/100 | step: 280/422 | loss: 1.4915729761123657\n",
      "Epoch: 40/100 | step: 281/422 | loss: 1.7797869443893433\n",
      "Epoch: 40/100 | step: 282/422 | loss: 2.1580283641815186\n",
      "Epoch: 40/100 | step: 283/422 | loss: 2.319544553756714\n",
      "Epoch: 40/100 | step: 284/422 | loss: 2.9077770709991455\n",
      "Epoch: 40/100 | step: 285/422 | loss: 1.5882320404052734\n",
      "Epoch: 40/100 | step: 286/422 | loss: 2.164665460586548\n",
      "Epoch: 40/100 | step: 287/422 | loss: 1.8854973316192627\n",
      "Epoch: 40/100 | step: 288/422 | loss: 2.2594330310821533\n",
      "Epoch: 40/100 | step: 289/422 | loss: 1.909356713294983\n",
      "Epoch: 40/100 | step: 290/422 | loss: 2.1763298511505127\n",
      "Epoch: 40/100 | step: 291/422 | loss: 1.539245843887329\n",
      "Epoch: 40/100 | step: 292/422 | loss: 2.622096538543701\n",
      "Epoch: 40/100 | step: 293/422 | loss: 1.8911266326904297\n",
      "Epoch: 40/100 | step: 294/422 | loss: 2.0088672637939453\n",
      "Epoch: 40/100 | step: 295/422 | loss: 1.6064345836639404\n",
      "Epoch: 40/100 | step: 296/422 | loss: 1.574182152748108\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 41/100 | step: 1/422 | loss: 2.330463171005249\n",
      "Epoch: 41/100 | step: 2/422 | loss: 1.728711724281311\n",
      "Epoch: 41/100 | step: 3/422 | loss: 1.9020724296569824\n",
      "Epoch: 41/100 | step: 4/422 | loss: 1.599758505821228\n",
      "Epoch: 41/100 | step: 5/422 | loss: 2.1289560794830322\n",
      "Epoch: 41/100 | step: 6/422 | loss: 1.5768792629241943\n",
      "Epoch: 41/100 | step: 7/422 | loss: 1.3746953010559082\n",
      "Epoch: 41/100 | step: 8/422 | loss: 2.340991497039795\n",
      "Epoch: 41/100 | step: 9/422 | loss: 1.718909502029419\n",
      "Epoch: 41/100 | step: 10/422 | loss: 1.7054316997528076\n",
      "Epoch: 41/100 | step: 11/422 | loss: 1.7550677061080933\n",
      "Epoch: 41/100 | step: 12/422 | loss: 1.581903100013733\n",
      "Epoch: 41/100 | step: 13/422 | loss: 1.674647569656372\n",
      "Epoch: 41/100 | step: 14/422 | loss: 1.938034176826477\n",
      "Epoch: 41/100 | step: 15/422 | loss: 2.173764705657959\n",
      "Epoch: 41/100 | step: 16/422 | loss: 1.7136504650115967\n",
      "Epoch: 41/100 | step: 17/422 | loss: 1.745280385017395\n",
      "Epoch: 41/100 | step: 18/422 | loss: 1.4570484161376953\n",
      "Epoch: 41/100 | step: 19/422 | loss: 2.2009689807891846\n",
      "Epoch: 41/100 | step: 20/422 | loss: 1.9935758113861084\n",
      "Epoch: 41/100 | step: 21/422 | loss: 1.6406993865966797\n",
      "Epoch: 41/100 | step: 22/422 | loss: 1.4953669309616089\n",
      "Epoch: 41/100 | step: 23/422 | loss: 1.7090846300125122\n",
      "Epoch: 41/100 | step: 24/422 | loss: 1.8643046617507935\n",
      "Epoch: 41/100 | step: 25/422 | loss: 1.836197018623352\n",
      "Epoch: 41/100 | step: 26/422 | loss: 1.4604671001434326\n",
      "Epoch: 41/100 | step: 27/422 | loss: 2.229672908782959\n",
      "Epoch: 41/100 | step: 28/422 | loss: 1.7191779613494873\n",
      "Epoch: 41/100 | step: 29/422 | loss: 1.3392277956008911\n",
      "Epoch: 41/100 | step: 30/422 | loss: 1.8478116989135742\n",
      "Epoch: 41/100 | step: 31/422 | loss: 1.4518413543701172\n",
      "Epoch: 41/100 | step: 32/422 | loss: 1.5550603866577148\n",
      "Epoch: 41/100 | step: 33/422 | loss: 2.1334621906280518\n",
      "Epoch: 41/100 | step: 34/422 | loss: 1.6307778358459473\n",
      "Epoch: 41/100 | step: 35/422 | loss: 1.3654015064239502\n",
      "Epoch: 41/100 | step: 36/422 | loss: 2.219590663909912\n",
      "Epoch: 41/100 | step: 37/422 | loss: 1.3577336072921753\n",
      "Epoch: 41/100 | step: 38/422 | loss: 1.8383368253707886\n",
      "Epoch: 41/100 | step: 39/422 | loss: 1.9018880128860474\n",
      "Epoch: 41/100 | step: 40/422 | loss: 1.277040719985962\n",
      "Epoch: 41/100 | step: 41/422 | loss: 1.786503791809082\n",
      "Epoch: 41/100 | step: 42/422 | loss: 1.8545656204223633\n",
      "Epoch: 41/100 | step: 43/422 | loss: 2.120237350463867\n",
      "Epoch: 41/100 | step: 44/422 | loss: 1.5947071313858032\n",
      "Epoch: 41/100 | step: 45/422 | loss: 1.9524492025375366\n",
      "Epoch: 41/100 | step: 46/422 | loss: 1.8412202596664429\n",
      "Epoch: 41/100 | step: 47/422 | loss: 1.812179446220398\n",
      "Epoch: 41/100 | step: 48/422 | loss: 2.2659523487091064\n",
      "Epoch: 41/100 | step: 49/422 | loss: 1.5912253856658936\n",
      "Epoch: 41/100 | step: 50/422 | loss: 2.0003669261932373\n",
      "Epoch: 41/100 | step: 51/422 | loss: 2.3818273544311523\n",
      "Epoch: 41/100 | step: 52/422 | loss: 1.890549659729004\n",
      "Epoch: 41/100 | step: 53/422 | loss: 1.8210726976394653\n",
      "Epoch: 41/100 | step: 54/422 | loss: 1.4591212272644043\n",
      "Epoch: 41/100 | step: 55/422 | loss: 1.1138482093811035\n",
      "Epoch: 41/100 | step: 56/422 | loss: 1.9815524816513062\n",
      "Epoch: 41/100 | step: 57/422 | loss: 1.4045517444610596\n",
      "Epoch: 41/100 | step: 58/422 | loss: 2.1854734420776367\n",
      "Epoch: 41/100 | step: 59/422 | loss: 1.6407867670059204\n",
      "Epoch: 41/100 | step: 60/422 | loss: 2.188924789428711\n",
      "Epoch: 41/100 | step: 61/422 | loss: 1.6092031002044678\n",
      "Epoch: 41/100 | step: 62/422 | loss: 1.8821338415145874\n",
      "Epoch: 41/100 | step: 63/422 | loss: 1.5443158149719238\n",
      "Epoch: 41/100 | step: 64/422 | loss: 1.5793801546096802\n",
      "Epoch: 41/100 | step: 65/422 | loss: 1.5815492868423462\n",
      "Epoch: 41/100 | step: 66/422 | loss: 2.5022313594818115\n",
      "Epoch: 41/100 | step: 67/422 | loss: 1.7015581130981445\n",
      "Epoch: 41/100 | step: 68/422 | loss: 1.8466955423355103\n",
      "Epoch: 41/100 | step: 69/422 | loss: 1.5469683408737183\n",
      "Epoch: 41/100 | step: 70/422 | loss: 1.8779304027557373\n",
      "Epoch: 41/100 | step: 71/422 | loss: 1.5172792673110962\n",
      "Epoch: 41/100 | step: 72/422 | loss: 2.3053765296936035\n",
      "Epoch: 41/100 | step: 73/422 | loss: 2.0006065368652344\n",
      "Epoch: 41/100 | step: 74/422 | loss: 1.9131052494049072\n",
      "Epoch: 41/100 | step: 75/422 | loss: 1.6867072582244873\n",
      "Epoch: 41/100 | step: 76/422 | loss: 2.3988966941833496\n",
      "Epoch: 41/100 | step: 77/422 | loss: 2.342926025390625\n",
      "Epoch: 41/100 | step: 78/422 | loss: 1.6147817373275757\n",
      "Epoch: 41/100 | step: 79/422 | loss: 2.0742173194885254\n",
      "Epoch: 41/100 | step: 80/422 | loss: 2.204244375228882\n",
      "Epoch: 41/100 | step: 81/422 | loss: 1.8493541479110718\n",
      "Epoch: 41/100 | step: 82/422 | loss: 1.7835925817489624\n",
      "Epoch: 41/100 | step: 83/422 | loss: 1.8438442945480347\n",
      "Epoch: 41/100 | step: 84/422 | loss: 2.0524067878723145\n",
      "Epoch: 41/100 | step: 85/422 | loss: 1.8658732175827026\n",
      "Epoch: 41/100 | step: 86/422 | loss: 1.5873498916625977\n",
      "Epoch: 41/100 | step: 87/422 | loss: 1.6185855865478516\n",
      "Epoch: 41/100 | step: 88/422 | loss: 2.031425714492798\n",
      "Epoch: 41/100 | step: 89/422 | loss: 1.9671024084091187\n",
      "Epoch: 41/100 | step: 90/422 | loss: 1.9316939115524292\n",
      "Epoch: 41/100 | step: 91/422 | loss: 1.5503746271133423\n",
      "Epoch: 41/100 | step: 92/422 | loss: 1.6021524667739868\n",
      "Epoch: 41/100 | step: 93/422 | loss: 2.3112645149230957\n",
      "Epoch: 41/100 | step: 94/422 | loss: 1.8666433095932007\n",
      "Epoch: 41/100 | step: 95/422 | loss: 1.8837673664093018\n",
      "Epoch: 41/100 | step: 96/422 | loss: 1.5593959093093872\n",
      "Epoch: 41/100 | step: 97/422 | loss: 2.094119071960449\n",
      "Epoch: 41/100 | step: 98/422 | loss: 2.1007182598114014\n",
      "Epoch: 41/100 | step: 99/422 | loss: 2.2127859592437744\n",
      "Epoch: 41/100 | step: 100/422 | loss: 1.7931342124938965\n",
      "Epoch: 41/100 | step: 101/422 | loss: 2.0951671600341797\n",
      "Epoch: 41/100 | step: 102/422 | loss: 1.535354733467102\n",
      "Epoch: 41/100 | step: 103/422 | loss: 1.8953191041946411\n",
      "Epoch: 41/100 | step: 104/422 | loss: 2.0560426712036133\n",
      "Epoch: 41/100 | step: 105/422 | loss: 1.5400416851043701\n",
      "Epoch: 41/100 | step: 106/422 | loss: 1.9884670972824097\n",
      "Epoch: 41/100 | step: 107/422 | loss: 1.9265896081924438\n",
      "Epoch: 41/100 | step: 108/422 | loss: 1.9607486724853516\n",
      "Epoch: 41/100 | step: 109/422 | loss: 1.6601744890213013\n",
      "Epoch: 41/100 | step: 110/422 | loss: 1.9697662591934204\n",
      "Epoch: 41/100 | step: 111/422 | loss: 1.6087431907653809\n",
      "Epoch: 41/100 | step: 112/422 | loss: 1.6740920543670654\n",
      "Epoch: 41/100 | step: 113/422 | loss: 1.3056910037994385\n",
      "Epoch: 41/100 | step: 114/422 | loss: 1.996120810508728\n",
      "Epoch: 41/100 | step: 115/422 | loss: 2.0097219944000244\n",
      "Epoch: 41/100 | step: 116/422 | loss: 2.818784475326538\n",
      "Epoch: 41/100 | step: 117/422 | loss: 1.7014185190200806\n",
      "Epoch: 41/100 | step: 118/422 | loss: 1.6547470092773438\n",
      "Epoch: 41/100 | step: 119/422 | loss: 1.1936894655227661\n",
      "Epoch: 41/100 | step: 120/422 | loss: 1.7453022003173828\n",
      "Epoch: 41/100 | step: 121/422 | loss: 2.0313682556152344\n",
      "Epoch: 41/100 | step: 122/422 | loss: 1.7127562761306763\n",
      "Epoch: 41/100 | step: 123/422 | loss: 1.7773064374923706\n",
      "Epoch: 41/100 | step: 124/422 | loss: 2.026106595993042\n",
      "Epoch: 41/100 | step: 125/422 | loss: 2.353445291519165\n",
      "Epoch: 41/100 | step: 126/422 | loss: 1.9190216064453125\n",
      "Epoch: 41/100 | step: 127/422 | loss: 2.3184542655944824\n",
      "Epoch: 41/100 | step: 128/422 | loss: 2.1991474628448486\n",
      "Epoch: 41/100 | step: 129/422 | loss: 1.3402082920074463\n",
      "Epoch: 41/100 | step: 130/422 | loss: 2.187619686126709\n",
      "Epoch: 41/100 | step: 131/422 | loss: 2.3014416694641113\n",
      "Epoch: 41/100 | step: 132/422 | loss: 1.300901174545288\n",
      "Epoch: 41/100 | step: 133/422 | loss: 1.8654811382293701\n",
      "Epoch: 41/100 | step: 134/422 | loss: 1.4411745071411133\n",
      "Epoch: 41/100 | step: 135/422 | loss: 1.2967946529388428\n",
      "Epoch: 41/100 | step: 136/422 | loss: 1.8828787803649902\n",
      "Epoch: 41/100 | step: 137/422 | loss: 2.1006879806518555\n",
      "Epoch: 41/100 | step: 138/422 | loss: 1.7352954149246216\n",
      "Epoch: 41/100 | step: 139/422 | loss: 1.6317347288131714\n",
      "Epoch: 41/100 | step: 140/422 | loss: 2.0387468338012695\n",
      "Epoch: 41/100 | step: 141/422 | loss: 1.7292176485061646\n",
      "Epoch: 41/100 | step: 142/422 | loss: 1.4536432027816772\n",
      "Epoch: 41/100 | step: 143/422 | loss: 1.9617202281951904\n",
      "Epoch: 41/100 | step: 144/422 | loss: 1.9592891931533813\n",
      "Epoch: 41/100 | step: 145/422 | loss: 1.6793274879455566\n",
      "Epoch: 41/100 | step: 146/422 | loss: 1.6403244733810425\n",
      "Epoch: 41/100 | step: 147/422 | loss: 1.885043978691101\n",
      "Epoch: 41/100 | step: 148/422 | loss: 1.8368576765060425\n",
      "Epoch: 41/100 | step: 149/422 | loss: 1.7472941875457764\n",
      "Epoch: 41/100 | step: 150/422 | loss: 2.437886953353882\n",
      "Epoch: 41/100 | step: 151/422 | loss: 2.081268310546875\n",
      "Epoch: 41/100 | step: 152/422 | loss: 2.29934024810791\n",
      "Epoch: 41/100 | step: 153/422 | loss: 1.6527918577194214\n",
      "Epoch: 41/100 | step: 154/422 | loss: 1.8372522592544556\n",
      "Epoch: 41/100 | step: 155/422 | loss: 1.5353307723999023\n",
      "Epoch: 41/100 | step: 156/422 | loss: 1.5393335819244385\n",
      "Epoch: 41/100 | step: 157/422 | loss: 1.3901232481002808\n",
      "Epoch: 41/100 | step: 158/422 | loss: 1.3856440782546997\n",
      "Epoch: 41/100 | step: 159/422 | loss: 2.0743274688720703\n",
      "Epoch: 41/100 | step: 160/422 | loss: 1.625257968902588\n",
      "Epoch: 41/100 | step: 161/422 | loss: 2.1614186763763428\n",
      "Epoch: 41/100 | step: 162/422 | loss: 1.677147626876831\n",
      "Epoch: 41/100 | step: 163/422 | loss: 1.6840076446533203\n",
      "Epoch: 41/100 | step: 164/422 | loss: 1.364527702331543\n",
      "Epoch: 41/100 | step: 165/422 | loss: 1.7999145984649658\n",
      "Epoch: 41/100 | step: 166/422 | loss: 1.9127302169799805\n",
      "Epoch: 41/100 | step: 167/422 | loss: 1.7202739715576172\n",
      "Epoch: 41/100 | step: 168/422 | loss: 2.4077813625335693\n",
      "Epoch: 41/100 | step: 169/422 | loss: 2.0766472816467285\n",
      "Epoch: 41/100 | step: 170/422 | loss: 1.6148136854171753\n",
      "Epoch: 41/100 | step: 171/422 | loss: 2.475315809249878\n",
      "Epoch: 41/100 | step: 172/422 | loss: 1.3914002180099487\n",
      "Epoch: 41/100 | step: 173/422 | loss: 1.5219749212265015\n",
      "Epoch: 41/100 | step: 174/422 | loss: 2.1614491939544678\n",
      "Epoch: 41/100 | step: 175/422 | loss: 1.7291231155395508\n",
      "Epoch: 41/100 | step: 176/422 | loss: 2.119910955429077\n",
      "Epoch: 41/100 | step: 177/422 | loss: 1.866100549697876\n",
      "Epoch: 41/100 | step: 178/422 | loss: 1.8483563661575317\n",
      "Epoch: 41/100 | step: 179/422 | loss: 2.2256064414978027\n",
      "Epoch: 41/100 | step: 180/422 | loss: 1.7531970739364624\n",
      "Epoch: 41/100 | step: 181/422 | loss: 2.021775245666504\n",
      "Epoch: 41/100 | step: 182/422 | loss: 1.4241009950637817\n",
      "Epoch: 41/100 | step: 183/422 | loss: 1.7328122854232788\n",
      "Epoch: 41/100 | step: 184/422 | loss: 1.6136404275894165\n",
      "Epoch: 41/100 | step: 185/422 | loss: 1.3092405796051025\n",
      "Epoch: 41/100 | step: 186/422 | loss: 1.7105151414871216\n",
      "Epoch: 41/100 | step: 187/422 | loss: 2.1094963550567627\n",
      "Epoch: 41/100 | step: 188/422 | loss: 1.7620360851287842\n",
      "Epoch: 41/100 | step: 189/422 | loss: 1.4037282466888428\n",
      "Epoch: 41/100 | step: 190/422 | loss: 2.250580310821533\n",
      "Epoch: 41/100 | step: 191/422 | loss: 1.526297688484192\n",
      "Epoch: 41/100 | step: 192/422 | loss: 2.1991751194000244\n",
      "Epoch: 41/100 | step: 193/422 | loss: 1.884060263633728\n",
      "Epoch: 41/100 | step: 194/422 | loss: 2.3796637058258057\n",
      "Epoch: 41/100 | step: 195/422 | loss: 1.8822243213653564\n",
      "Epoch: 41/100 | step: 196/422 | loss: 1.7936503887176514\n",
      "Epoch: 41/100 | step: 197/422 | loss: 1.7602314949035645\n",
      "Epoch: 41/100 | step: 198/422 | loss: 2.2060556411743164\n",
      "Epoch: 41/100 | step: 199/422 | loss: 1.7374516725540161\n",
      "Epoch: 41/100 | step: 200/422 | loss: 1.9867993593215942\n",
      "Epoch: 41/100 | step: 201/422 | loss: 1.536561369895935\n",
      "Epoch: 41/100 | step: 202/422 | loss: 2.3131723403930664\n",
      "Epoch: 41/100 | step: 203/422 | loss: 1.8355294466018677\n",
      "Epoch: 41/100 | step: 204/422 | loss: 1.5928865671157837\n",
      "Epoch: 41/100 | step: 205/422 | loss: 1.7478097677230835\n",
      "Epoch: 41/100 | step: 206/422 | loss: 1.8168833255767822\n",
      "Epoch: 41/100 | step: 207/422 | loss: 1.7822237014770508\n",
      "Epoch: 41/100 | step: 208/422 | loss: 2.0974907875061035\n",
      "Epoch: 41/100 | step: 209/422 | loss: 1.9320471286773682\n",
      "Epoch: 41/100 | step: 210/422 | loss: 1.3900572061538696\n",
      "Epoch: 41/100 | step: 211/422 | loss: 1.2940787076950073\n",
      "Epoch: 41/100 | step: 212/422 | loss: 1.9679187536239624\n",
      "Epoch: 41/100 | step: 213/422 | loss: 1.8753578662872314\n",
      "Epoch: 41/100 | step: 214/422 | loss: 2.278629779815674\n",
      "Epoch: 41/100 | step: 215/422 | loss: 1.190548300743103\n",
      "Epoch: 41/100 | step: 216/422 | loss: 2.1087777614593506\n",
      "Epoch: 41/100 | step: 217/422 | loss: 2.0470001697540283\n",
      "Epoch: 41/100 | step: 218/422 | loss: 1.7702138423919678\n",
      "Epoch: 41/100 | step: 219/422 | loss: 1.9759371280670166\n",
      "Epoch: 41/100 | step: 220/422 | loss: 2.3267159461975098\n",
      "Epoch: 41/100 | step: 221/422 | loss: 1.9499636888504028\n",
      "Epoch: 41/100 | step: 222/422 | loss: 2.3888256549835205\n",
      "Epoch: 41/100 | step: 223/422 | loss: 2.2921488285064697\n",
      "Epoch: 41/100 | step: 224/422 | loss: 1.5156751871109009\n",
      "Epoch: 41/100 | step: 225/422 | loss: 1.6473382711410522\n",
      "Epoch: 41/100 | step: 226/422 | loss: 1.2514349222183228\n",
      "Epoch: 41/100 | step: 227/422 | loss: 1.658422827720642\n",
      "Epoch: 41/100 | step: 228/422 | loss: 1.9599857330322266\n",
      "Epoch: 41/100 | step: 229/422 | loss: 2.071587085723877\n",
      "Epoch: 41/100 | step: 230/422 | loss: 1.8771421909332275\n",
      "Epoch: 41/100 | step: 231/422 | loss: 1.9414489269256592\n",
      "Epoch: 41/100 | step: 232/422 | loss: 1.5647058486938477\n",
      "Epoch: 41/100 | step: 233/422 | loss: 1.4841171503067017\n",
      "Epoch: 41/100 | step: 234/422 | loss: 1.9147167205810547\n",
      "Epoch: 41/100 | step: 235/422 | loss: 2.0510194301605225\n",
      "Epoch: 41/100 | step: 236/422 | loss: 2.1058506965637207\n",
      "Epoch: 41/100 | step: 237/422 | loss: 1.5959833860397339\n",
      "Epoch: 41/100 | step: 238/422 | loss: 1.3389513492584229\n",
      "Epoch: 41/100 | step: 239/422 | loss: 2.8686625957489014\n",
      "Epoch: 41/100 | step: 240/422 | loss: 1.753823161125183\n",
      "Epoch: 41/100 | step: 241/422 | loss: 2.4909284114837646\n",
      "Epoch: 41/100 | step: 242/422 | loss: 1.5531011819839478\n",
      "Epoch: 41/100 | step: 243/422 | loss: 2.0417697429656982\n",
      "Epoch: 41/100 | step: 244/422 | loss: 1.6848558187484741\n",
      "Epoch: 41/100 | step: 245/422 | loss: 1.3284212350845337\n",
      "Epoch: 41/100 | step: 246/422 | loss: 1.9744384288787842\n",
      "Epoch: 41/100 | step: 247/422 | loss: 2.050668239593506\n",
      "Epoch: 41/100 | step: 248/422 | loss: 2.1179115772247314\n",
      "Epoch: 41/100 | step: 249/422 | loss: 2.2466540336608887\n",
      "Epoch: 41/100 | step: 250/422 | loss: 1.8113154172897339\n",
      "Epoch: 41/100 | step: 251/422 | loss: 1.6716159582138062\n",
      "Epoch: 41/100 | step: 252/422 | loss: 1.3725123405456543\n",
      "Epoch: 41/100 | step: 253/422 | loss: 2.0742316246032715\n",
      "Epoch: 41/100 | step: 254/422 | loss: 1.779680609703064\n",
      "Epoch: 41/100 | step: 255/422 | loss: 2.504208564758301\n",
      "Epoch: 41/100 | step: 256/422 | loss: 2.0190398693084717\n",
      "Epoch: 41/100 | step: 257/422 | loss: 2.371192216873169\n",
      "Epoch: 41/100 | step: 258/422 | loss: 1.6757655143737793\n",
      "Epoch: 41/100 | step: 259/422 | loss: 1.8310273885726929\n",
      "Epoch: 41/100 | step: 260/422 | loss: 1.8994324207305908\n",
      "Epoch: 41/100 | step: 261/422 | loss: 2.214613437652588\n",
      "Epoch: 41/100 | step: 262/422 | loss: 1.262018084526062\n",
      "Epoch: 41/100 | step: 263/422 | loss: 2.3023834228515625\n",
      "Epoch: 41/100 | step: 264/422 | loss: 2.0597586631774902\n",
      "Epoch: 41/100 | step: 265/422 | loss: 1.5984035730361938\n",
      "Epoch: 41/100 | step: 266/422 | loss: 1.8594717979431152\n",
      "Epoch: 41/100 | step: 267/422 | loss: 2.036829710006714\n",
      "Epoch: 41/100 | step: 268/422 | loss: 2.0321829319000244\n",
      "Epoch: 41/100 | step: 269/422 | loss: 2.316671848297119\n",
      "Epoch: 41/100 | step: 270/422 | loss: 1.6622010469436646\n",
      "Epoch: 41/100 | step: 271/422 | loss: 1.9743659496307373\n",
      "Epoch: 41/100 | step: 272/422 | loss: 2.021929979324341\n",
      "Epoch: 41/100 | step: 273/422 | loss: 2.096508264541626\n",
      "Epoch: 41/100 | step: 274/422 | loss: 1.7097365856170654\n",
      "Epoch: 41/100 | step: 275/422 | loss: 2.3058736324310303\n",
      "Epoch: 41/100 | step: 276/422 | loss: 1.9063013792037964\n",
      "Epoch: 41/100 | step: 277/422 | loss: 1.9246459007263184\n",
      "Epoch: 41/100 | step: 278/422 | loss: 1.6133592128753662\n",
      "Epoch: 41/100 | step: 279/422 | loss: 1.7281391620635986\n",
      "Epoch: 41/100 | step: 280/422 | loss: 2.213629722595215\n",
      "Epoch: 41/100 | step: 281/422 | loss: 1.6870728731155396\n",
      "Epoch: 41/100 | step: 282/422 | loss: 1.2026610374450684\n",
      "Epoch: 41/100 | step: 283/422 | loss: 1.3502556085586548\n",
      "Epoch: 41/100 | step: 284/422 | loss: 1.68031907081604\n",
      "Epoch: 41/100 | step: 285/422 | loss: 1.965582251548767\n",
      "Epoch: 41/100 | step: 286/422 | loss: 1.8077174425125122\n",
      "Epoch: 41/100 | step: 287/422 | loss: 2.325662612915039\n",
      "Epoch: 41/100 | step: 288/422 | loss: 1.620459794998169\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 42/100 | step: 1/422 | loss: 1.4911354780197144\n",
      "Epoch: 42/100 | step: 2/422 | loss: 1.4845554828643799\n",
      "Epoch: 42/100 | step: 3/422 | loss: 2.1563527584075928\n",
      "Epoch: 42/100 | step: 4/422 | loss: 1.9086859226226807\n",
      "Epoch: 42/100 | step: 5/422 | loss: 1.2677232027053833\n",
      "Epoch: 42/100 | step: 6/422 | loss: 1.9133539199829102\n",
      "Epoch: 42/100 | step: 7/422 | loss: 1.6648627519607544\n",
      "Epoch: 42/100 | step: 8/422 | loss: 1.2140653133392334\n",
      "Epoch: 42/100 | step: 9/422 | loss: 1.7574234008789062\n",
      "Epoch: 42/100 | step: 10/422 | loss: 1.7252529859542847\n",
      "Epoch: 42/100 | step: 11/422 | loss: 1.5359485149383545\n",
      "Epoch: 42/100 | step: 12/422 | loss: 1.6791621446609497\n",
      "Epoch: 42/100 | step: 13/422 | loss: 1.825129508972168\n",
      "Epoch: 42/100 | step: 14/422 | loss: 1.6079012155532837\n",
      "Epoch: 42/100 | step: 15/422 | loss: 1.5692954063415527\n",
      "Epoch: 42/100 | step: 16/422 | loss: 1.9757996797561646\n",
      "Epoch: 42/100 | step: 17/422 | loss: 1.370522379875183\n",
      "Epoch: 42/100 | step: 18/422 | loss: 1.4052674770355225\n",
      "Epoch: 42/100 | step: 19/422 | loss: 1.8493883609771729\n",
      "Epoch: 42/100 | step: 20/422 | loss: 1.3550785779953003\n",
      "Epoch: 42/100 | step: 21/422 | loss: 1.9158049821853638\n",
      "Epoch: 42/100 | step: 22/422 | loss: 1.6260141134262085\n",
      "Epoch: 42/100 | step: 23/422 | loss: 1.5271353721618652\n",
      "Epoch: 42/100 | step: 24/422 | loss: 2.0618715286254883\n",
      "Epoch: 42/100 | step: 25/422 | loss: 1.568738341331482\n",
      "Epoch: 42/100 | step: 26/422 | loss: 1.6310269832611084\n",
      "Epoch: 42/100 | step: 27/422 | loss: 2.207791805267334\n",
      "Epoch: 42/100 | step: 28/422 | loss: 1.6192337274551392\n",
      "Epoch: 42/100 | step: 29/422 | loss: 1.4397703409194946\n",
      "Epoch: 42/100 | step: 30/422 | loss: 2.3326849937438965\n",
      "Epoch: 42/100 | step: 31/422 | loss: 1.7484683990478516\n",
      "Epoch: 42/100 | step: 32/422 | loss: 1.5900301933288574\n",
      "Epoch: 42/100 | step: 33/422 | loss: 2.0814356803894043\n",
      "Epoch: 42/100 | step: 34/422 | loss: 1.6724716424942017\n",
      "Epoch: 42/100 | step: 35/422 | loss: 1.435075283050537\n",
      "Epoch: 42/100 | step: 36/422 | loss: 1.387123465538025\n",
      "Epoch: 42/100 | step: 37/422 | loss: 1.935889482498169\n",
      "Epoch: 42/100 | step: 38/422 | loss: 1.4702279567718506\n",
      "Epoch: 42/100 | step: 39/422 | loss: 1.5494136810302734\n",
      "Epoch: 42/100 | step: 40/422 | loss: 2.073315382003784\n",
      "Epoch: 42/100 | step: 41/422 | loss: 1.8406329154968262\n",
      "Epoch: 42/100 | step: 42/422 | loss: 1.4910858869552612\n",
      "Epoch: 42/100 | step: 43/422 | loss: 1.5980290174484253\n",
      "Epoch: 42/100 | step: 44/422 | loss: 1.677740216255188\n",
      "Epoch: 42/100 | step: 45/422 | loss: 1.5626921653747559\n",
      "Epoch: 42/100 | step: 46/422 | loss: 1.5236213207244873\n",
      "Epoch: 42/100 | step: 47/422 | loss: 1.4631898403167725\n",
      "Epoch: 42/100 | step: 48/422 | loss: 1.8909718990325928\n",
      "Epoch: 42/100 | step: 49/422 | loss: 2.416013717651367\n",
      "Epoch: 42/100 | step: 50/422 | loss: 1.7392586469650269\n",
      "Epoch: 42/100 | step: 51/422 | loss: 1.62838876247406\n",
      "Epoch: 42/100 | step: 52/422 | loss: 2.182466745376587\n",
      "Epoch: 42/100 | step: 53/422 | loss: 1.2393583059310913\n",
      "Epoch: 42/100 | step: 54/422 | loss: 1.721542239189148\n",
      "Epoch: 42/100 | step: 55/422 | loss: 1.7366455793380737\n",
      "Epoch: 42/100 | step: 56/422 | loss: 2.2018144130706787\n",
      "Epoch: 42/100 | step: 57/422 | loss: 1.578275203704834\n",
      "Epoch: 42/100 | step: 58/422 | loss: 1.5735706090927124\n",
      "Epoch: 42/100 | step: 59/422 | loss: 1.484883427619934\n",
      "Epoch: 42/100 | step: 60/422 | loss: 1.4398517608642578\n",
      "Epoch: 42/100 | step: 61/422 | loss: 1.424329400062561\n",
      "Epoch: 42/100 | step: 62/422 | loss: 1.787126064300537\n",
      "Epoch: 42/100 | step: 63/422 | loss: 2.301494836807251\n",
      "Epoch: 42/100 | step: 64/422 | loss: 1.9390299320220947\n",
      "Epoch: 42/100 | step: 65/422 | loss: 1.5377358198165894\n",
      "Epoch: 42/100 | step: 66/422 | loss: 1.2949304580688477\n",
      "Epoch: 42/100 | step: 67/422 | loss: 1.8616294860839844\n",
      "Epoch: 42/100 | step: 68/422 | loss: 1.3995494842529297\n",
      "Epoch: 42/100 | step: 69/422 | loss: 1.584291934967041\n",
      "Epoch: 42/100 | step: 70/422 | loss: 1.7769224643707275\n",
      "Epoch: 42/100 | step: 71/422 | loss: 0.9666891098022461\n",
      "Epoch: 42/100 | step: 72/422 | loss: 1.4764353036880493\n",
      "Epoch: 42/100 | step: 73/422 | loss: 1.720423936843872\n",
      "Epoch: 42/100 | step: 74/422 | loss: 1.398389220237732\n",
      "Epoch: 42/100 | step: 75/422 | loss: 1.3637372255325317\n",
      "Epoch: 42/100 | step: 76/422 | loss: 1.4765241146087646\n",
      "Epoch: 42/100 | step: 77/422 | loss: 1.2213081121444702\n",
      "Epoch: 42/100 | step: 78/422 | loss: 1.7204344272613525\n",
      "Epoch: 42/100 | step: 79/422 | loss: 1.1503287553787231\n",
      "Epoch: 42/100 | step: 80/422 | loss: 2.2077298164367676\n",
      "Epoch: 42/100 | step: 81/422 | loss: 1.706965446472168\n",
      "Epoch: 42/100 | step: 82/422 | loss: 1.625815749168396\n",
      "Epoch: 42/100 | step: 83/422 | loss: 1.62505304813385\n",
      "Epoch: 42/100 | step: 84/422 | loss: 1.502274751663208\n",
      "Epoch: 42/100 | step: 85/422 | loss: 1.914961814880371\n",
      "Epoch: 42/100 | step: 86/422 | loss: 1.4285979270935059\n",
      "Epoch: 42/100 | step: 87/422 | loss: 1.3392807245254517\n",
      "Epoch: 42/100 | step: 88/422 | loss: 1.3796018362045288\n",
      "Epoch: 42/100 | step: 89/422 | loss: 1.4615715742111206\n",
      "Epoch: 42/100 | step: 90/422 | loss: 2.252542495727539\n",
      "Epoch: 42/100 | step: 91/422 | loss: 1.7903978824615479\n",
      "Epoch: 42/100 | step: 92/422 | loss: 1.7249796390533447\n",
      "Epoch: 42/100 | step: 93/422 | loss: 1.9215989112854004\n",
      "Epoch: 42/100 | step: 94/422 | loss: 1.9901213645935059\n",
      "Epoch: 42/100 | step: 95/422 | loss: 1.9436471462249756\n",
      "Epoch: 42/100 | step: 96/422 | loss: 1.8862426280975342\n",
      "Epoch: 42/100 | step: 97/422 | loss: 1.3860673904418945\n",
      "Epoch: 42/100 | step: 98/422 | loss: 1.4807385206222534\n",
      "Epoch: 42/100 | step: 99/422 | loss: 1.8699098825454712\n",
      "Epoch: 42/100 | step: 100/422 | loss: 1.7563918828964233\n",
      "Epoch: 42/100 | step: 101/422 | loss: 1.9676947593688965\n",
      "Epoch: 42/100 | step: 102/422 | loss: 1.889511227607727\n",
      "Epoch: 42/100 | step: 103/422 | loss: 1.2454808950424194\n",
      "Epoch: 42/100 | step: 104/422 | loss: 1.7710087299346924\n",
      "Epoch: 42/100 | step: 105/422 | loss: 1.6261237859725952\n",
      "Epoch: 42/100 | step: 106/422 | loss: 1.728729486465454\n",
      "Epoch: 42/100 | step: 107/422 | loss: 1.713828206062317\n",
      "Epoch: 42/100 | step: 108/422 | loss: 2.391618490219116\n",
      "Epoch: 42/100 | step: 109/422 | loss: 1.6794947385787964\n",
      "Epoch: 42/100 | step: 110/422 | loss: 1.5917384624481201\n",
      "Epoch: 42/100 | step: 111/422 | loss: 2.3164267539978027\n",
      "Epoch: 42/100 | step: 112/422 | loss: 2.076580047607422\n",
      "Epoch: 42/100 | step: 113/422 | loss: 2.240738868713379\n",
      "Epoch: 42/100 | step: 114/422 | loss: 2.0771806240081787\n",
      "Epoch: 42/100 | step: 115/422 | loss: 1.7739858627319336\n",
      "Epoch: 42/100 | step: 116/422 | loss: 1.7223843336105347\n",
      "Epoch: 42/100 | step: 117/422 | loss: 1.3187532424926758\n",
      "Epoch: 42/100 | step: 118/422 | loss: 1.350980281829834\n",
      "Epoch: 42/100 | step: 119/422 | loss: 1.9951298236846924\n",
      "Epoch: 42/100 | step: 120/422 | loss: 1.9797790050506592\n",
      "Epoch: 42/100 | step: 121/422 | loss: 1.6919324398040771\n",
      "Epoch: 42/100 | step: 122/422 | loss: 1.8987138271331787\n",
      "Epoch: 42/100 | step: 123/422 | loss: 1.4691238403320312\n",
      "Epoch: 42/100 | step: 124/422 | loss: 2.1153345108032227\n",
      "Epoch: 42/100 | step: 125/422 | loss: 2.2267167568206787\n",
      "Epoch: 42/100 | step: 126/422 | loss: 2.3696537017822266\n",
      "Epoch: 42/100 | step: 127/422 | loss: 1.620579719543457\n",
      "Epoch: 42/100 | step: 128/422 | loss: 1.45537269115448\n",
      "Epoch: 42/100 | step: 129/422 | loss: 1.7736058235168457\n",
      "Epoch: 42/100 | step: 130/422 | loss: 1.6872004270553589\n",
      "Epoch: 42/100 | step: 131/422 | loss: 1.7276389598846436\n",
      "Epoch: 42/100 | step: 132/422 | loss: 1.6552441120147705\n",
      "Epoch: 42/100 | step: 133/422 | loss: 1.5069586038589478\n",
      "Epoch: 42/100 | step: 134/422 | loss: 1.7893697023391724\n",
      "Epoch: 42/100 | step: 135/422 | loss: 2.1501238346099854\n",
      "Epoch: 42/100 | step: 136/422 | loss: 1.8424516916275024\n",
      "Epoch: 42/100 | step: 137/422 | loss: 2.18198561668396\n",
      "Epoch: 42/100 | step: 138/422 | loss: 1.2951353788375854\n",
      "Epoch: 42/100 | step: 139/422 | loss: 1.4000403881072998\n",
      "Epoch: 42/100 | step: 140/422 | loss: 1.5476709604263306\n",
      "Epoch: 42/100 | step: 141/422 | loss: 1.8807907104492188\n",
      "Epoch: 42/100 | step: 142/422 | loss: 2.035879611968994\n",
      "Epoch: 42/100 | step: 143/422 | loss: 1.116442084312439\n",
      "Epoch: 42/100 | step: 144/422 | loss: 1.872668981552124\n",
      "Epoch: 42/100 | step: 145/422 | loss: 1.698364019393921\n",
      "Epoch: 42/100 | step: 146/422 | loss: 1.5174039602279663\n",
      "Epoch: 42/100 | step: 147/422 | loss: 1.116836667060852\n",
      "Epoch: 42/100 | step: 148/422 | loss: 1.051926612854004\n",
      "Epoch: 42/100 | step: 149/422 | loss: 1.7586485147476196\n",
      "Epoch: 42/100 | step: 150/422 | loss: 1.9801748991012573\n",
      "Epoch: 42/100 | step: 151/422 | loss: 1.4403594732284546\n",
      "Epoch: 42/100 | step: 152/422 | loss: 2.0090644359588623\n",
      "Epoch: 42/100 | step: 153/422 | loss: 1.4376224279403687\n",
      "Epoch: 42/100 | step: 154/422 | loss: 1.7535680532455444\n",
      "Epoch: 42/100 | step: 155/422 | loss: 1.8294148445129395\n",
      "Epoch: 42/100 | step: 156/422 | loss: 1.5611335039138794\n",
      "Epoch: 42/100 | step: 157/422 | loss: 2.1313321590423584\n",
      "Epoch: 42/100 | step: 158/422 | loss: 1.8522263765335083\n",
      "Epoch: 42/100 | step: 159/422 | loss: 1.9663845300674438\n",
      "Epoch: 42/100 | step: 160/422 | loss: 1.8498574495315552\n",
      "Epoch: 42/100 | step: 161/422 | loss: 1.5868650674819946\n",
      "Epoch: 42/100 | step: 162/422 | loss: 1.5298824310302734\n",
      "Epoch: 42/100 | step: 163/422 | loss: 1.7012304067611694\n",
      "Epoch: 42/100 | step: 164/422 | loss: 1.6733304262161255\n",
      "Epoch: 42/100 | step: 165/422 | loss: 1.6808998584747314\n",
      "Epoch: 42/100 | step: 166/422 | loss: 1.3163326978683472\n",
      "Epoch: 42/100 | step: 167/422 | loss: 1.3734982013702393\n",
      "Epoch: 42/100 | step: 168/422 | loss: 1.4294319152832031\n",
      "Epoch: 42/100 | step: 169/422 | loss: 1.769690990447998\n",
      "Epoch: 42/100 | step: 170/422 | loss: 1.868769884109497\n",
      "Epoch: 42/100 | step: 171/422 | loss: 1.434696912765503\n",
      "Epoch: 42/100 | step: 172/422 | loss: 1.7368839979171753\n",
      "Epoch: 42/100 | step: 173/422 | loss: 1.2127540111541748\n",
      "Epoch: 42/100 | step: 174/422 | loss: 1.3186017274856567\n",
      "Epoch: 42/100 | step: 175/422 | loss: 1.6213046312332153\n",
      "Epoch: 42/100 | step: 176/422 | loss: 1.8237004280090332\n",
      "Epoch: 42/100 | step: 177/422 | loss: 1.9903348684310913\n",
      "Epoch: 42/100 | step: 178/422 | loss: 1.5833663940429688\n",
      "Epoch: 42/100 | step: 179/422 | loss: 2.1595845222473145\n",
      "Epoch: 42/100 | step: 180/422 | loss: 2.121580123901367\n",
      "Epoch: 42/100 | step: 181/422 | loss: 1.7927544116973877\n",
      "Epoch: 42/100 | step: 182/422 | loss: 1.5831377506256104\n",
      "Epoch: 42/100 | step: 183/422 | loss: 2.2376868724823\n",
      "Epoch: 42/100 | step: 184/422 | loss: 1.4783427715301514\n",
      "Epoch: 42/100 | step: 185/422 | loss: 1.4276306629180908\n",
      "Epoch: 42/100 | step: 186/422 | loss: 1.7702264785766602\n",
      "Epoch: 42/100 | step: 187/422 | loss: 2.1439716815948486\n",
      "Epoch: 42/100 | step: 188/422 | loss: 1.614344835281372\n",
      "Epoch: 42/100 | step: 189/422 | loss: 1.594163417816162\n",
      "Epoch: 42/100 | step: 190/422 | loss: 2.0187795162200928\n",
      "Epoch: 42/100 | step: 191/422 | loss: 1.9136017560958862\n",
      "Epoch: 42/100 | step: 192/422 | loss: 1.412480354309082\n",
      "Epoch: 42/100 | step: 193/422 | loss: 1.5001215934753418\n",
      "Epoch: 42/100 | step: 194/422 | loss: 1.6299221515655518\n",
      "Epoch: 42/100 | step: 195/422 | loss: 1.647291660308838\n",
      "Epoch: 42/100 | step: 196/422 | loss: 1.864580750465393\n",
      "Epoch: 42/100 | step: 197/422 | loss: 1.5585088729858398\n",
      "Epoch: 42/100 | step: 198/422 | loss: 2.068424940109253\n",
      "Epoch: 42/100 | step: 199/422 | loss: 1.356553316116333\n",
      "Epoch: 42/100 | step: 200/422 | loss: 1.84287428855896\n",
      "Epoch: 42/100 | step: 201/422 | loss: 1.762216567993164\n",
      "Epoch: 42/100 | step: 202/422 | loss: 2.0703229904174805\n",
      "Epoch: 42/100 | step: 203/422 | loss: 1.6267802715301514\n",
      "Epoch: 42/100 | step: 204/422 | loss: 1.8787261247634888\n",
      "Epoch: 42/100 | step: 205/422 | loss: 2.0933268070220947\n",
      "Epoch: 42/100 | step: 206/422 | loss: 1.764716625213623\n",
      "Epoch: 42/100 | step: 207/422 | loss: 2.0520401000976562\n",
      "Epoch: 42/100 | step: 208/422 | loss: 1.7031145095825195\n",
      "Epoch: 42/100 | step: 209/422 | loss: 1.61415696144104\n",
      "Epoch: 42/100 | step: 210/422 | loss: 1.843104600906372\n",
      "Epoch: 42/100 | step: 211/422 | loss: 1.6583250761032104\n",
      "Epoch: 42/100 | step: 212/422 | loss: 1.625261902809143\n",
      "Epoch: 42/100 | step: 213/422 | loss: 1.4480841159820557\n",
      "Epoch: 42/100 | step: 214/422 | loss: 1.3308848142623901\n",
      "Epoch: 42/100 | step: 215/422 | loss: 1.5571576356887817\n",
      "Epoch: 42/100 | step: 216/422 | loss: 1.4793587923049927\n",
      "Epoch: 42/100 | step: 217/422 | loss: 2.002213716506958\n",
      "Epoch: 42/100 | step: 218/422 | loss: 1.8731385469436646\n",
      "Epoch: 42/100 | step: 219/422 | loss: 1.6384671926498413\n",
      "Epoch: 42/100 | step: 220/422 | loss: 1.344420313835144\n",
      "Epoch: 42/100 | step: 221/422 | loss: 1.6466714143753052\n",
      "Epoch: 42/100 | step: 222/422 | loss: 1.9007514715194702\n",
      "Epoch: 42/100 | step: 223/422 | loss: 1.1418800354003906\n",
      "Epoch: 42/100 | step: 224/422 | loss: 1.8825992345809937\n",
      "Epoch: 42/100 | step: 225/422 | loss: 1.9238665103912354\n",
      "Epoch: 42/100 | step: 226/422 | loss: 1.9578216075897217\n",
      "Epoch: 42/100 | step: 227/422 | loss: 2.018806219100952\n",
      "Epoch: 42/100 | step: 228/422 | loss: 2.3068978786468506\n",
      "Epoch: 42/100 | step: 229/422 | loss: 1.4126962423324585\n",
      "Epoch: 42/100 | step: 230/422 | loss: 1.8277097940444946\n",
      "Epoch: 42/100 | step: 231/422 | loss: 1.583693504333496\n",
      "Epoch: 42/100 | step: 232/422 | loss: 1.876829981803894\n",
      "Epoch: 42/100 | step: 233/422 | loss: 2.5229742527008057\n",
      "Epoch: 42/100 | step: 234/422 | loss: 1.7990317344665527\n",
      "Epoch: 42/100 | step: 235/422 | loss: 2.1242761611938477\n",
      "Epoch: 42/100 | step: 236/422 | loss: 1.3759723901748657\n",
      "Epoch: 42/100 | step: 237/422 | loss: 2.4395272731781006\n",
      "Epoch: 42/100 | step: 238/422 | loss: 2.084390878677368\n",
      "Epoch: 42/100 | step: 239/422 | loss: 1.7281068563461304\n",
      "Epoch: 42/100 | step: 240/422 | loss: 1.9588561058044434\n",
      "Epoch: 42/100 | step: 241/422 | loss: 1.753333330154419\n",
      "Epoch: 42/100 | step: 242/422 | loss: 1.8848907947540283\n",
      "Epoch: 42/100 | step: 243/422 | loss: 2.3465464115142822\n",
      "Epoch: 42/100 | step: 244/422 | loss: 1.7931450605392456\n",
      "Epoch: 42/100 | step: 245/422 | loss: 1.3369139432907104\n",
      "Epoch: 42/100 | step: 246/422 | loss: 1.8907185792922974\n",
      "Epoch: 42/100 | step: 247/422 | loss: 1.341810703277588\n",
      "Epoch: 42/100 | step: 248/422 | loss: 1.7206023931503296\n",
      "Epoch: 42/100 | step: 249/422 | loss: 1.5104577541351318\n",
      "Epoch: 42/100 | step: 250/422 | loss: 1.6073386669158936\n",
      "Epoch: 42/100 | step: 251/422 | loss: 2.4716837406158447\n",
      "Epoch: 42/100 | step: 252/422 | loss: 1.3591527938842773\n",
      "Epoch: 42/100 | step: 253/422 | loss: 2.220343828201294\n",
      "Epoch: 42/100 | step: 254/422 | loss: 1.5466525554656982\n",
      "Epoch: 42/100 | step: 255/422 | loss: 1.6967278718948364\n",
      "Epoch: 42/100 | step: 256/422 | loss: 1.5186896324157715\n",
      "Epoch: 42/100 | step: 257/422 | loss: 1.4930404424667358\n",
      "Epoch: 42/100 | step: 258/422 | loss: 1.889280915260315\n",
      "Epoch: 42/100 | step: 259/422 | loss: 1.8889068365097046\n",
      "Epoch: 42/100 | step: 260/422 | loss: 1.5538073778152466\n",
      "Epoch: 42/100 | step: 261/422 | loss: 1.8220837116241455\n",
      "Epoch: 42/100 | step: 262/422 | loss: 1.6020493507385254\n",
      "Epoch: 42/100 | step: 263/422 | loss: 1.9332005977630615\n",
      "Epoch: 42/100 | step: 264/422 | loss: 1.9519280195236206\n",
      "Epoch: 42/100 | step: 265/422 | loss: 1.8405804634094238\n",
      "Epoch: 42/100 | step: 266/422 | loss: 1.7734180688858032\n",
      "Epoch: 42/100 | step: 267/422 | loss: 1.698704481124878\n",
      "Epoch: 42/100 | step: 268/422 | loss: 1.8873459100723267\n",
      "Epoch: 42/100 | step: 269/422 | loss: 1.4435778856277466\n",
      "Epoch: 42/100 | step: 270/422 | loss: 2.0714612007141113\n",
      "Epoch: 42/100 | step: 271/422 | loss: 1.6027370691299438\n",
      "Epoch: 42/100 | step: 272/422 | loss: 1.9979947805404663\n",
      "Epoch: 42/100 | step: 273/422 | loss: 1.7718759775161743\n",
      "Epoch: 42/100 | step: 274/422 | loss: 2.1036832332611084\n",
      "Epoch: 42/100 | step: 275/422 | loss: 1.71934974193573\n",
      "Epoch: 42/100 | step: 276/422 | loss: 1.2847023010253906\n",
      "Epoch: 42/100 | step: 277/422 | loss: 2.059351682662964\n",
      "Epoch: 42/100 | step: 278/422 | loss: 2.462963104248047\n",
      "Epoch: 42/100 | step: 279/422 | loss: 2.04246187210083\n",
      "Epoch: 42/100 | step: 280/422 | loss: 1.4272183179855347\n",
      "Epoch: 42/100 | step: 281/422 | loss: 1.7354453802108765\n",
      "Epoch: 42/100 | step: 282/422 | loss: 1.581786036491394\n",
      "Epoch: 42/100 | step: 283/422 | loss: 1.7437716722488403\n",
      "Epoch: 42/100 | step: 284/422 | loss: 1.411069393157959\n",
      "Epoch: 42/100 | step: 285/422 | loss: 1.4703937768936157\n",
      "Epoch: 42/100 | step: 286/422 | loss: 2.005685329437256\n",
      "Epoch: 42/100 | step: 287/422 | loss: 2.323342800140381\n",
      "Epoch: 42/100 | step: 288/422 | loss: 1.7510128021240234\n",
      "Epoch: 42/100 | step: 289/422 | loss: 1.127967357635498\n",
      "Epoch: 42/100 | step: 290/422 | loss: 2.239079236984253\n",
      "Epoch: 42/100 | step: 291/422 | loss: 1.5068528652191162\n",
      "Epoch: 42/100 | step: 292/422 | loss: 1.2681736946105957\n",
      "Epoch: 42/100 | step: 293/422 | loss: 1.731787085533142\n",
      "Epoch: 42/100 | step: 294/422 | loss: 1.936602234840393\n",
      "Epoch: 42/100 | step: 295/422 | loss: 1.8402832746505737\n",
      "Epoch: 42/100 | step: 296/422 | loss: 1.6772645711898804\n",
      "Epoch: 42/100 | step: 297/422 | loss: 1.8896301984786987\n",
      "Epoch: 42/100 | step: 298/422 | loss: 2.126492977142334\n",
      "Epoch: 42/100 | step: 299/422 | loss: 2.1349432468414307\n",
      "Epoch: 42/100 | step: 300/422 | loss: 1.8266589641571045\n",
      "Epoch: 42/100 | step: 301/422 | loss: 1.5261940956115723\n",
      "Epoch: 42/100 | step: 302/422 | loss: 1.5047181844711304\n",
      "Epoch: 42/100 | step: 303/422 | loss: 1.990979552268982\n",
      "Epoch: 42/100 | step: 304/422 | loss: 1.823779821395874\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 43/100 | step: 1/422 | loss: 1.4978301525115967\n",
      "Epoch: 43/100 | step: 2/422 | loss: 1.9208934307098389\n",
      "Epoch: 43/100 | step: 3/422 | loss: 1.1209830045700073\n",
      "Epoch: 43/100 | step: 4/422 | loss: 1.4536807537078857\n",
      "Epoch: 43/100 | step: 5/422 | loss: 1.350620985031128\n",
      "Epoch: 43/100 | step: 6/422 | loss: 1.4738101959228516\n",
      "Epoch: 43/100 | step: 7/422 | loss: 1.5139954090118408\n",
      "Epoch: 43/100 | step: 8/422 | loss: 1.377175211906433\n",
      "Epoch: 43/100 | step: 9/422 | loss: 1.2664774656295776\n",
      "Epoch: 43/100 | step: 10/422 | loss: 2.0704755783081055\n",
      "Epoch: 43/100 | step: 11/422 | loss: 1.623382568359375\n",
      "Epoch: 43/100 | step: 12/422 | loss: 1.485971212387085\n",
      "Epoch: 43/100 | step: 13/422 | loss: 1.281738519668579\n",
      "Epoch: 43/100 | step: 14/422 | loss: 1.429715871810913\n",
      "Epoch: 43/100 | step: 15/422 | loss: 1.5665608644485474\n",
      "Epoch: 43/100 | step: 16/422 | loss: 1.583426833152771\n",
      "Epoch: 43/100 | step: 17/422 | loss: 1.3195003271102905\n",
      "Epoch: 43/100 | step: 18/422 | loss: 1.751596450805664\n",
      "Epoch: 43/100 | step: 19/422 | loss: 1.2730473279953003\n",
      "Epoch: 43/100 | step: 20/422 | loss: 1.6103920936584473\n",
      "Epoch: 43/100 | step: 21/422 | loss: 1.3286179304122925\n",
      "Epoch: 43/100 | step: 22/422 | loss: 1.646043300628662\n",
      "Epoch: 43/100 | step: 23/422 | loss: 1.862045168876648\n",
      "Epoch: 43/100 | step: 24/422 | loss: 1.5243979692459106\n",
      "Epoch: 43/100 | step: 25/422 | loss: 2.054640531539917\n",
      "Epoch: 43/100 | step: 26/422 | loss: 1.190287709236145\n",
      "Epoch: 43/100 | step: 27/422 | loss: 1.5637812614440918\n",
      "Epoch: 43/100 | step: 28/422 | loss: 1.323763370513916\n",
      "Epoch: 43/100 | step: 29/422 | loss: 1.9195815324783325\n",
      "Epoch: 43/100 | step: 30/422 | loss: 2.122840404510498\n",
      "Epoch: 43/100 | step: 31/422 | loss: 1.7351816892623901\n",
      "Epoch: 43/100 | step: 32/422 | loss: 1.4902148246765137\n",
      "Epoch: 43/100 | step: 33/422 | loss: 2.044424533843994\n",
      "Epoch: 43/100 | step: 34/422 | loss: 1.9096167087554932\n",
      "Epoch: 43/100 | step: 35/422 | loss: 1.7098616361618042\n",
      "Epoch: 43/100 | step: 36/422 | loss: 2.0426039695739746\n",
      "Epoch: 43/100 | step: 37/422 | loss: 1.6180166006088257\n",
      "Epoch: 43/100 | step: 38/422 | loss: 2.077172040939331\n",
      "Epoch: 43/100 | step: 39/422 | loss: 1.511448860168457\n",
      "Epoch: 43/100 | step: 40/422 | loss: 1.6453722715377808\n",
      "Epoch: 43/100 | step: 41/422 | loss: 1.4859446287155151\n",
      "Epoch: 43/100 | step: 42/422 | loss: 1.7718292474746704\n",
      "Epoch: 43/100 | step: 43/422 | loss: 2.3016116619110107\n",
      "Epoch: 43/100 | step: 44/422 | loss: 1.25152587890625\n",
      "Epoch: 43/100 | step: 45/422 | loss: 1.6187677383422852\n",
      "Epoch: 43/100 | step: 46/422 | loss: 1.6287450790405273\n",
      "Epoch: 43/100 | step: 47/422 | loss: 1.5011723041534424\n",
      "Epoch: 43/100 | step: 48/422 | loss: 1.6524571180343628\n",
      "Epoch: 43/100 | step: 49/422 | loss: 1.1504542827606201\n",
      "Epoch: 43/100 | step: 50/422 | loss: 1.58603835105896\n",
      "Epoch: 43/100 | step: 51/422 | loss: 1.6222039461135864\n",
      "Epoch: 43/100 | step: 52/422 | loss: 1.1732004880905151\n",
      "Epoch: 43/100 | step: 53/422 | loss: 1.8493773937225342\n",
      "Epoch: 43/100 | step: 54/422 | loss: 1.4074499607086182\n",
      "Epoch: 43/100 | step: 55/422 | loss: 2.2245678901672363\n",
      "Epoch: 43/100 | step: 56/422 | loss: 1.6595561504364014\n",
      "Epoch: 43/100 | step: 57/422 | loss: 1.891302466392517\n",
      "Epoch: 43/100 | step: 58/422 | loss: 1.3098496198654175\n",
      "Epoch: 43/100 | step: 59/422 | loss: 1.4268592596054077\n",
      "Epoch: 43/100 | step: 60/422 | loss: 1.5696375370025635\n",
      "Epoch: 43/100 | step: 61/422 | loss: 1.453550934791565\n",
      "Epoch: 43/100 | step: 62/422 | loss: 1.3034647703170776\n",
      "Epoch: 43/100 | step: 63/422 | loss: 1.5885508060455322\n",
      "Epoch: 43/100 | step: 64/422 | loss: 1.5443904399871826\n",
      "Epoch: 43/100 | step: 65/422 | loss: 1.465745449066162\n",
      "Epoch: 43/100 | step: 66/422 | loss: 1.588660717010498\n",
      "Epoch: 43/100 | step: 67/422 | loss: 1.3111751079559326\n",
      "Epoch: 43/100 | step: 68/422 | loss: 1.537390112876892\n",
      "Epoch: 43/100 | step: 69/422 | loss: 1.9777164459228516\n",
      "Epoch: 43/100 | step: 70/422 | loss: 1.10530686378479\n",
      "Epoch: 43/100 | step: 71/422 | loss: 1.5837695598602295\n",
      "Epoch: 43/100 | step: 72/422 | loss: 1.5333465337753296\n",
      "Epoch: 43/100 | step: 73/422 | loss: 1.2882647514343262\n",
      "Epoch: 43/100 | step: 74/422 | loss: 1.793066143989563\n",
      "Epoch: 43/100 | step: 75/422 | loss: 1.6440703868865967\n",
      "Epoch: 43/100 | step: 76/422 | loss: 1.3797048330307007\n",
      "Epoch: 43/100 | step: 77/422 | loss: 2.0424716472625732\n",
      "Epoch: 43/100 | step: 78/422 | loss: 1.5296965837478638\n",
      "Epoch: 43/100 | step: 79/422 | loss: 1.9190772771835327\n",
      "Epoch: 43/100 | step: 80/422 | loss: 1.537691354751587\n",
      "Epoch: 43/100 | step: 81/422 | loss: 1.3592575788497925\n",
      "Epoch: 43/100 | step: 82/422 | loss: 1.571326732635498\n",
      "Epoch: 43/100 | step: 83/422 | loss: 1.188144326210022\n",
      "Epoch: 43/100 | step: 84/422 | loss: 1.706982135772705\n",
      "Epoch: 43/100 | step: 85/422 | loss: 1.341241717338562\n",
      "Epoch: 43/100 | step: 86/422 | loss: 1.894255518913269\n",
      "Epoch: 43/100 | step: 87/422 | loss: 1.3016778230667114\n",
      "Epoch: 43/100 | step: 88/422 | loss: 1.718902587890625\n",
      "Epoch: 43/100 | step: 89/422 | loss: 1.8657270669937134\n",
      "Epoch: 43/100 | step: 90/422 | loss: 1.8741766214370728\n",
      "Epoch: 43/100 | step: 91/422 | loss: 1.9002383947372437\n",
      "Epoch: 43/100 | step: 92/422 | loss: 1.8321384191513062\n",
      "Epoch: 43/100 | step: 93/422 | loss: 1.820366382598877\n",
      "Epoch: 43/100 | step: 94/422 | loss: 1.8786152601242065\n",
      "Epoch: 43/100 | step: 95/422 | loss: 1.245540738105774\n",
      "Epoch: 43/100 | step: 96/422 | loss: 1.7164082527160645\n",
      "Epoch: 43/100 | step: 97/422 | loss: 1.4062165021896362\n",
      "Epoch: 43/100 | step: 98/422 | loss: 1.5404731035232544\n",
      "Epoch: 43/100 | step: 99/422 | loss: 2.2396843433380127\n",
      "Epoch: 43/100 | step: 100/422 | loss: 1.1727731227874756\n",
      "Epoch: 43/100 | step: 101/422 | loss: 1.6652759313583374\n",
      "Epoch: 43/100 | step: 102/422 | loss: 1.6341370344161987\n",
      "Epoch: 43/100 | step: 103/422 | loss: 1.5030068159103394\n",
      "Epoch: 43/100 | step: 104/422 | loss: 1.746130347251892\n",
      "Epoch: 43/100 | step: 105/422 | loss: 1.9683496952056885\n",
      "Epoch: 43/100 | step: 106/422 | loss: 2.0830159187316895\n",
      "Epoch: 43/100 | step: 107/422 | loss: 1.5493388175964355\n",
      "Epoch: 43/100 | step: 108/422 | loss: 1.2084580659866333\n",
      "Epoch: 43/100 | step: 109/422 | loss: 1.2980746030807495\n",
      "Epoch: 43/100 | step: 110/422 | loss: 1.3524876832962036\n",
      "Epoch: 43/100 | step: 111/422 | loss: 1.4298328161239624\n",
      "Epoch: 43/100 | step: 112/422 | loss: 1.513257622718811\n",
      "Epoch: 43/100 | step: 113/422 | loss: 1.5192410945892334\n",
      "Epoch: 43/100 | step: 114/422 | loss: 1.6795003414154053\n",
      "Epoch: 43/100 | step: 115/422 | loss: 1.9775832891464233\n",
      "Epoch: 43/100 | step: 116/422 | loss: 1.1592867374420166\n",
      "Epoch: 43/100 | step: 117/422 | loss: 1.8855295181274414\n",
      "Epoch: 43/100 | step: 118/422 | loss: 1.1938406229019165\n",
      "Epoch: 43/100 | step: 119/422 | loss: 1.830603837966919\n",
      "Epoch: 43/100 | step: 120/422 | loss: 1.9699790477752686\n",
      "Epoch: 43/100 | step: 121/422 | loss: 1.4867361783981323\n",
      "Epoch: 43/100 | step: 122/422 | loss: 1.6533759832382202\n",
      "Epoch: 43/100 | step: 123/422 | loss: 1.5657626390457153\n",
      "Epoch: 43/100 | step: 124/422 | loss: 1.8320239782333374\n",
      "Epoch: 43/100 | step: 125/422 | loss: 2.161710262298584\n",
      "Epoch: 43/100 | step: 126/422 | loss: 1.464475154876709\n",
      "Epoch: 43/100 | step: 127/422 | loss: 1.3749302625656128\n",
      "Epoch: 43/100 | step: 128/422 | loss: 1.1695570945739746\n",
      "Epoch: 43/100 | step: 129/422 | loss: 1.2796787023544312\n",
      "Epoch: 43/100 | step: 130/422 | loss: 1.3843461275100708\n",
      "Epoch: 43/100 | step: 131/422 | loss: 1.7478396892547607\n",
      "Epoch: 43/100 | step: 132/422 | loss: 1.8919105529785156\n",
      "Epoch: 43/100 | step: 133/422 | loss: 1.9140251874923706\n",
      "Epoch: 43/100 | step: 134/422 | loss: 1.1967016458511353\n",
      "Epoch: 43/100 | step: 135/422 | loss: 1.6416844129562378\n",
      "Epoch: 43/100 | step: 136/422 | loss: 1.529365062713623\n",
      "Epoch: 43/100 | step: 137/422 | loss: 1.5561397075653076\n",
      "Epoch: 43/100 | step: 138/422 | loss: 1.6355940103530884\n",
      "Epoch: 43/100 | step: 139/422 | loss: 2.005737543106079\n",
      "Epoch: 43/100 | step: 140/422 | loss: 1.900930643081665\n",
      "Epoch: 43/100 | step: 141/422 | loss: 1.7013697624206543\n",
      "Epoch: 43/100 | step: 142/422 | loss: 1.448430061340332\n",
      "Epoch: 43/100 | step: 143/422 | loss: 1.3235468864440918\n",
      "Epoch: 43/100 | step: 144/422 | loss: 2.2720088958740234\n",
      "Epoch: 43/100 | step: 145/422 | loss: 1.4711443185806274\n",
      "Epoch: 43/100 | step: 146/422 | loss: 1.7132940292358398\n",
      "Epoch: 43/100 | step: 147/422 | loss: 1.8981873989105225\n",
      "Epoch: 43/100 | step: 148/422 | loss: 1.4037805795669556\n",
      "Epoch: 43/100 | step: 149/422 | loss: 1.808258056640625\n",
      "Epoch: 43/100 | step: 150/422 | loss: 1.3938058614730835\n",
      "Epoch: 43/100 | step: 151/422 | loss: 1.7533526420593262\n",
      "Epoch: 43/100 | step: 152/422 | loss: 1.2692450284957886\n",
      "Epoch: 43/100 | step: 153/422 | loss: 1.7670828104019165\n",
      "Epoch: 43/100 | step: 154/422 | loss: 1.5955086946487427\n",
      "Epoch: 43/100 | step: 155/422 | loss: 1.3640098571777344\n",
      "Epoch: 43/100 | step: 156/422 | loss: 1.7667995691299438\n",
      "Epoch: 43/100 | step: 157/422 | loss: 1.4922752380371094\n",
      "Epoch: 43/100 | step: 158/422 | loss: 1.5640342235565186\n",
      "Epoch: 43/100 | step: 159/422 | loss: 1.7302595376968384\n",
      "Epoch: 43/100 | step: 160/422 | loss: 1.527594804763794\n",
      "Epoch: 43/100 | step: 161/422 | loss: 1.8509689569473267\n",
      "Epoch: 43/100 | step: 162/422 | loss: 2.3206260204315186\n",
      "Epoch: 43/100 | step: 163/422 | loss: 1.0680872201919556\n",
      "Epoch: 43/100 | step: 164/422 | loss: 1.8361910581588745\n",
      "Epoch: 43/100 | step: 165/422 | loss: 1.3824955224990845\n",
      "Epoch: 43/100 | step: 166/422 | loss: 1.795287013053894\n",
      "Epoch: 43/100 | step: 167/422 | loss: 1.8028552532196045\n",
      "Epoch: 43/100 | step: 168/422 | loss: 1.9771713018417358\n",
      "Epoch: 43/100 | step: 169/422 | loss: 1.4247820377349854\n",
      "Epoch: 43/100 | step: 170/422 | loss: 1.3938796520233154\n",
      "Epoch: 43/100 | step: 171/422 | loss: 1.793350100517273\n",
      "Epoch: 43/100 | step: 172/422 | loss: 2.4248743057250977\n",
      "Epoch: 43/100 | step: 173/422 | loss: 1.6787605285644531\n",
      "Epoch: 43/100 | step: 174/422 | loss: 1.756288766860962\n",
      "Epoch: 43/100 | step: 175/422 | loss: 2.226238489151001\n",
      "Epoch: 43/100 | step: 176/422 | loss: 1.6716177463531494\n",
      "Epoch: 43/100 | step: 177/422 | loss: 1.3833051919937134\n",
      "Epoch: 43/100 | step: 178/422 | loss: 1.5471924543380737\n",
      "Epoch: 43/100 | step: 179/422 | loss: 1.7513328790664673\n",
      "Epoch: 43/100 | step: 180/422 | loss: 1.6699612140655518\n",
      "Epoch: 43/100 | step: 181/422 | loss: 1.8867088556289673\n",
      "Epoch: 43/100 | step: 182/422 | loss: 1.3827993869781494\n",
      "Epoch: 43/100 | step: 183/422 | loss: 1.8998932838439941\n",
      "Epoch: 43/100 | step: 184/422 | loss: 2.207897186279297\n",
      "Epoch: 43/100 | step: 185/422 | loss: 1.6071445941925049\n",
      "Epoch: 43/100 | step: 186/422 | loss: 1.9444975852966309\n",
      "Epoch: 43/100 | step: 187/422 | loss: 1.7294632196426392\n",
      "Epoch: 43/100 | step: 188/422 | loss: 1.5638591051101685\n",
      "Epoch: 43/100 | step: 189/422 | loss: 1.6950675249099731\n",
      "Epoch: 43/100 | step: 190/422 | loss: 2.0074265003204346\n",
      "Epoch: 43/100 | step: 191/422 | loss: 1.609654188156128\n",
      "Epoch: 43/100 | step: 192/422 | loss: 1.8275123834609985\n",
      "Epoch: 43/100 | step: 193/422 | loss: 1.5726264715194702\n",
      "Epoch: 43/100 | step: 194/422 | loss: 1.5876891613006592\n",
      "Epoch: 43/100 | step: 195/422 | loss: 1.3715014457702637\n",
      "Epoch: 43/100 | step: 196/422 | loss: 1.4688595533370972\n",
      "Epoch: 43/100 | step: 197/422 | loss: 1.5572696924209595\n",
      "Epoch: 43/100 | step: 198/422 | loss: 2.082171678543091\n",
      "Epoch: 43/100 | step: 199/422 | loss: 1.3427691459655762\n",
      "Epoch: 43/100 | step: 200/422 | loss: 1.4417948722839355\n",
      "Epoch: 43/100 | step: 201/422 | loss: 1.5328216552734375\n",
      "Epoch: 43/100 | step: 202/422 | loss: 1.8001114130020142\n",
      "Epoch: 43/100 | step: 203/422 | loss: 1.6241480112075806\n",
      "Epoch: 43/100 | step: 204/422 | loss: 1.4346243143081665\n",
      "Epoch: 43/100 | step: 205/422 | loss: 1.2047020196914673\n",
      "Epoch: 43/100 | step: 206/422 | loss: 1.4849224090576172\n",
      "Epoch: 43/100 | step: 207/422 | loss: 1.4962081909179688\n",
      "Epoch: 43/100 | step: 208/422 | loss: 1.3691685199737549\n",
      "Epoch: 43/100 | step: 209/422 | loss: 1.7030385732650757\n",
      "Epoch: 43/100 | step: 210/422 | loss: 1.7157495021820068\n",
      "Epoch: 43/100 | step: 211/422 | loss: 1.1935844421386719\n",
      "Epoch: 43/100 | step: 212/422 | loss: 1.336618423461914\n",
      "Epoch: 43/100 | step: 213/422 | loss: 1.607254147529602\n",
      "Epoch: 43/100 | step: 214/422 | loss: 1.9805898666381836\n",
      "Epoch: 43/100 | step: 215/422 | loss: 1.645178198814392\n",
      "Epoch: 43/100 | step: 216/422 | loss: 1.8618849515914917\n",
      "Epoch: 43/100 | step: 217/422 | loss: 1.890135407447815\n",
      "Epoch: 43/100 | step: 218/422 | loss: 1.814509391784668\n",
      "Epoch: 43/100 | step: 219/422 | loss: 1.9346524477005005\n",
      "Epoch: 43/100 | step: 220/422 | loss: 1.9434632062911987\n",
      "Epoch: 43/100 | step: 221/422 | loss: 1.2795300483703613\n",
      "Epoch: 43/100 | step: 222/422 | loss: 1.73697829246521\n",
      "Epoch: 43/100 | step: 223/422 | loss: 1.7861558198928833\n",
      "Epoch: 43/100 | step: 224/422 | loss: 1.8738000392913818\n",
      "Epoch: 43/100 | step: 225/422 | loss: 1.6715281009674072\n",
      "Epoch: 43/100 | step: 226/422 | loss: 1.5428763628005981\n",
      "Epoch: 43/100 | step: 227/422 | loss: 1.42766535282135\n",
      "Epoch: 43/100 | step: 228/422 | loss: 2.0780229568481445\n",
      "Epoch: 43/100 | step: 229/422 | loss: 1.73460853099823\n",
      "Epoch: 43/100 | step: 230/422 | loss: 1.4984583854675293\n",
      "Epoch: 43/100 | step: 231/422 | loss: 2.2359676361083984\n",
      "Epoch: 43/100 | step: 232/422 | loss: 1.8133317232131958\n",
      "Epoch: 43/100 | step: 233/422 | loss: 1.3373501300811768\n",
      "Epoch: 43/100 | step: 234/422 | loss: 1.5592660903930664\n",
      "Epoch: 43/100 | step: 235/422 | loss: 1.3389405012130737\n",
      "Epoch: 43/100 | step: 236/422 | loss: 1.5080746412277222\n",
      "Epoch: 43/100 | step: 237/422 | loss: 1.9129406213760376\n",
      "Epoch: 43/100 | step: 238/422 | loss: 1.5226138830184937\n",
      "Epoch: 43/100 | step: 239/422 | loss: 1.7779405117034912\n",
      "Epoch: 43/100 | step: 240/422 | loss: 1.2960748672485352\n",
      "Epoch: 43/100 | step: 241/422 | loss: 1.7767013311386108\n",
      "Epoch: 43/100 | step: 242/422 | loss: 1.3722596168518066\n",
      "Epoch: 43/100 | step: 243/422 | loss: 0.9228610396385193\n",
      "Epoch: 43/100 | step: 244/422 | loss: 1.5948457717895508\n",
      "Epoch: 43/100 | step: 245/422 | loss: 1.8878157138824463\n",
      "Epoch: 43/100 | step: 246/422 | loss: 1.8336308002471924\n",
      "Epoch: 43/100 | step: 247/422 | loss: 2.13859224319458\n",
      "Epoch: 43/100 | step: 248/422 | loss: 1.2563602924346924\n",
      "Epoch: 43/100 | step: 249/422 | loss: 1.7997697591781616\n",
      "Epoch: 43/100 | step: 250/422 | loss: 2.30845308303833\n",
      "Epoch: 43/100 | step: 251/422 | loss: 1.8628352880477905\n",
      "Epoch: 43/100 | step: 252/422 | loss: 1.6043277978897095\n",
      "Epoch: 43/100 | step: 253/422 | loss: 1.482382893562317\n",
      "Epoch: 43/100 | step: 254/422 | loss: 1.6129467487335205\n",
      "Epoch: 43/100 | step: 255/422 | loss: 1.7162126302719116\n",
      "Epoch: 43/100 | step: 256/422 | loss: 1.955277681350708\n",
      "Epoch: 43/100 | step: 257/422 | loss: 1.8680051565170288\n",
      "Epoch: 43/100 | step: 258/422 | loss: 1.6785128116607666\n",
      "Epoch: 43/100 | step: 259/422 | loss: 1.6405251026153564\n",
      "Epoch: 43/100 | step: 260/422 | loss: 1.5549514293670654\n",
      "Epoch: 43/100 | step: 261/422 | loss: 1.6694520711898804\n",
      "Epoch: 43/100 | step: 262/422 | loss: 1.656750202178955\n",
      "Epoch: 43/100 | step: 263/422 | loss: 1.0234302282333374\n",
      "Epoch: 43/100 | step: 264/422 | loss: 2.0773696899414062\n",
      "Epoch: 43/100 | step: 265/422 | loss: 1.8289768695831299\n",
      "Epoch: 43/100 | step: 266/422 | loss: 1.463659644126892\n",
      "Epoch: 43/100 | step: 267/422 | loss: 1.5944105386734009\n",
      "Epoch: 43/100 | step: 268/422 | loss: 1.9528112411499023\n",
      "Epoch: 43/100 | step: 269/422 | loss: 1.4574050903320312\n",
      "Epoch: 43/100 | step: 270/422 | loss: 1.4490861892700195\n",
      "Epoch: 43/100 | step: 271/422 | loss: 1.7185943126678467\n",
      "Epoch: 43/100 | step: 272/422 | loss: 1.365321397781372\n",
      "Epoch: 43/100 | step: 273/422 | loss: 1.4946190118789673\n",
      "Epoch: 43/100 | step: 274/422 | loss: 1.744182825088501\n",
      "Epoch: 43/100 | step: 275/422 | loss: 2.2365832328796387\n",
      "Epoch: 43/100 | step: 276/422 | loss: 1.627640724182129\n",
      "Epoch: 43/100 | step: 277/422 | loss: 1.379178762435913\n",
      "Epoch: 43/100 | step: 278/422 | loss: 1.7865291833877563\n",
      "Epoch: 43/100 | step: 279/422 | loss: 1.4395976066589355\n",
      "Epoch: 43/100 | step: 280/422 | loss: 1.5260703563690186\n",
      "Epoch: 43/100 | step: 281/422 | loss: 2.3824150562286377\n",
      "Epoch: 43/100 | step: 282/422 | loss: 1.5871447324752808\n",
      "Epoch: 43/100 | step: 283/422 | loss: 1.0564219951629639\n",
      "Epoch: 43/100 | step: 284/422 | loss: 1.4026710987091064\n",
      "Epoch: 43/100 | step: 285/422 | loss: 1.6030806303024292\n",
      "Epoch: 43/100 | step: 286/422 | loss: 1.8999663591384888\n",
      "Epoch: 43/100 | step: 287/422 | loss: 1.443700909614563\n",
      "Epoch: 43/100 | step: 288/422 | loss: 2.2855782508850098\n",
      "Epoch: 43/100 | step: 289/422 | loss: 1.6652371883392334\n",
      "Epoch: 43/100 | step: 290/422 | loss: 1.4271109104156494\n",
      "Epoch: 43/100 | step: 291/422 | loss: 1.719152569770813\n",
      "Epoch: 43/100 | step: 292/422 | loss: 1.3100308179855347\n",
      "Epoch: 43/100 | step: 293/422 | loss: 1.608155608177185\n",
      "Epoch: 43/100 | step: 294/422 | loss: 1.2807047367095947\n",
      "Epoch: 43/100 | step: 295/422 | loss: 1.7022089958190918\n",
      "Epoch: 43/100 | step: 296/422 | loss: 1.5181865692138672\n",
      "Epoch: 43/100 | step: 297/422 | loss: 1.8212718963623047\n",
      "Epoch: 43/100 | step: 298/422 | loss: 1.4019442796707153\n",
      "Epoch: 43/100 | step: 299/422 | loss: 1.6321097612380981\n",
      "Epoch: 43/100 | step: 300/422 | loss: 1.7721918821334839\n",
      "Epoch: 43/100 | step: 301/422 | loss: 2.14589524269104\n",
      "Epoch: 43/100 | step: 302/422 | loss: 1.8771965503692627\n",
      "Epoch: 43/100 | step: 303/422 | loss: 2.0620598793029785\n",
      "Epoch: 43/100 | step: 304/422 | loss: 1.4201529026031494\n",
      "Epoch: 43/100 | step: 305/422 | loss: 1.612446665763855\n",
      "Epoch: 43/100 | step: 306/422 | loss: 1.8586949110031128\n",
      "Epoch: 43/100 | step: 307/422 | loss: 1.835369348526001\n",
      "Epoch: 43/100 | step: 308/422 | loss: 1.9793288707733154\n",
      "Epoch: 43/100 | step: 309/422 | loss: 1.8826922178268433\n",
      "Epoch: 43/100 | step: 310/422 | loss: 1.5752829313278198\n",
      "Epoch: 43/100 | step: 311/422 | loss: 1.7444024085998535\n",
      "Epoch: 43/100 | step: 312/422 | loss: 1.6107392311096191\n",
      "Epoch: 43/100 | step: 313/422 | loss: 2.2320399284362793\n",
      "Epoch: 43/100 | step: 314/422 | loss: 2.1177520751953125\n",
      "Epoch: 43/100 | step: 315/422 | loss: 1.6918714046478271\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 44/100 | step: 1/422 | loss: 0.9276889562606812\n",
      "Epoch: 44/100 | step: 2/422 | loss: 1.1734819412231445\n",
      "Epoch: 44/100 | step: 3/422 | loss: 1.6197348833084106\n",
      "Epoch: 44/100 | step: 4/422 | loss: 1.444793701171875\n",
      "Epoch: 44/100 | step: 5/422 | loss: 1.66191565990448\n",
      "Epoch: 44/100 | step: 6/422 | loss: 1.345224380493164\n",
      "Epoch: 44/100 | step: 7/422 | loss: 1.2664234638214111\n",
      "Epoch: 44/100 | step: 8/422 | loss: 1.2614825963974\n",
      "Epoch: 44/100 | step: 9/422 | loss: 1.611718773841858\n",
      "Epoch: 44/100 | step: 10/422 | loss: 1.5978484153747559\n",
      "Epoch: 44/100 | step: 11/422 | loss: 1.3421051502227783\n",
      "Epoch: 44/100 | step: 12/422 | loss: 0.8980199694633484\n",
      "Epoch: 44/100 | step: 13/422 | loss: 1.1726961135864258\n",
      "Epoch: 44/100 | step: 14/422 | loss: 1.3736110925674438\n",
      "Epoch: 44/100 | step: 15/422 | loss: 0.9842851161956787\n",
      "Epoch: 44/100 | step: 16/422 | loss: 1.3268836736679077\n",
      "Epoch: 44/100 | step: 17/422 | loss: 1.411665916442871\n",
      "Epoch: 44/100 | step: 18/422 | loss: 1.1047698259353638\n",
      "Epoch: 44/100 | step: 19/422 | loss: 1.3193541765213013\n",
      "Epoch: 44/100 | step: 20/422 | loss: 1.3982421159744263\n",
      "Epoch: 44/100 | step: 21/422 | loss: 1.698227882385254\n",
      "Epoch: 44/100 | step: 22/422 | loss: 2.054591178894043\n",
      "Epoch: 44/100 | step: 23/422 | loss: 1.3791779279708862\n",
      "Epoch: 44/100 | step: 24/422 | loss: 1.8192967176437378\n",
      "Epoch: 44/100 | step: 25/422 | loss: 1.383926272392273\n",
      "Epoch: 44/100 | step: 26/422 | loss: 1.2479946613311768\n",
      "Epoch: 44/100 | step: 27/422 | loss: 1.4728621244430542\n",
      "Epoch: 44/100 | step: 28/422 | loss: 1.448879599571228\n",
      "Epoch: 44/100 | step: 29/422 | loss: 1.307951807975769\n",
      "Epoch: 44/100 | step: 30/422 | loss: 1.3266632556915283\n",
      "Epoch: 44/100 | step: 31/422 | loss: 1.2490665912628174\n",
      "Epoch: 44/100 | step: 32/422 | loss: 1.7864917516708374\n",
      "Epoch: 44/100 | step: 33/422 | loss: 1.6083544492721558\n",
      "Epoch: 44/100 | step: 34/422 | loss: 1.4361287355422974\n",
      "Epoch: 44/100 | step: 35/422 | loss: 1.0894891023635864\n",
      "Epoch: 44/100 | step: 36/422 | loss: 1.4762110710144043\n",
      "Epoch: 44/100 | step: 37/422 | loss: 1.730688214302063\n",
      "Epoch: 44/100 | step: 38/422 | loss: 1.7176960706710815\n",
      "Epoch: 44/100 | step: 39/422 | loss: 1.4974406957626343\n",
      "Epoch: 44/100 | step: 40/422 | loss: 1.5440278053283691\n",
      "Epoch: 44/100 | step: 41/422 | loss: 1.3901764154434204\n",
      "Epoch: 44/100 | step: 42/422 | loss: 1.3362935781478882\n",
      "Epoch: 44/100 | step: 43/422 | loss: 1.4029029607772827\n",
      "Epoch: 44/100 | step: 44/422 | loss: 1.3307212591171265\n",
      "Epoch: 44/100 | step: 45/422 | loss: 1.5319916009902954\n",
      "Epoch: 44/100 | step: 46/422 | loss: 1.3877404928207397\n",
      "Epoch: 44/100 | step: 47/422 | loss: 0.9495251774787903\n",
      "Epoch: 44/100 | step: 48/422 | loss: 1.1707004308700562\n",
      "Epoch: 44/100 | step: 49/422 | loss: 1.7307662963867188\n",
      "Epoch: 44/100 | step: 50/422 | loss: 1.2940356731414795\n",
      "Epoch: 44/100 | step: 51/422 | loss: 1.7195202112197876\n",
      "Epoch: 44/100 | step: 52/422 | loss: 1.342390775680542\n",
      "Epoch: 44/100 | step: 53/422 | loss: 1.34878408908844\n",
      "Epoch: 44/100 | step: 54/422 | loss: 1.159139633178711\n",
      "Epoch: 44/100 | step: 55/422 | loss: 1.452514886856079\n",
      "Epoch: 44/100 | step: 56/422 | loss: 1.4550434350967407\n",
      "Epoch: 44/100 | step: 57/422 | loss: 1.2891457080841064\n",
      "Epoch: 44/100 | step: 58/422 | loss: 1.2701491117477417\n",
      "Epoch: 44/100 | step: 59/422 | loss: 1.3621220588684082\n",
      "Epoch: 44/100 | step: 60/422 | loss: 1.7124172449111938\n",
      "Epoch: 44/100 | step: 61/422 | loss: 1.8496159315109253\n",
      "Epoch: 44/100 | step: 62/422 | loss: 1.515642523765564\n",
      "Epoch: 44/100 | step: 63/422 | loss: 1.1624889373779297\n",
      "Epoch: 44/100 | step: 64/422 | loss: 1.4053727388381958\n",
      "Epoch: 44/100 | step: 65/422 | loss: 1.6958837509155273\n",
      "Epoch: 44/100 | step: 66/422 | loss: 1.388374924659729\n",
      "Epoch: 44/100 | step: 67/422 | loss: 1.5353494882583618\n",
      "Epoch: 44/100 | step: 68/422 | loss: 1.2635726928710938\n",
      "Epoch: 44/100 | step: 69/422 | loss: 1.2624677419662476\n",
      "Epoch: 44/100 | step: 70/422 | loss: 1.457779049873352\n",
      "Epoch: 44/100 | step: 71/422 | loss: 2.052621841430664\n",
      "Epoch: 44/100 | step: 72/422 | loss: 1.7239078283309937\n",
      "Epoch: 44/100 | step: 73/422 | loss: 1.0931068658828735\n",
      "Epoch: 44/100 | step: 74/422 | loss: 1.2161881923675537\n",
      "Epoch: 44/100 | step: 75/422 | loss: 1.9369451999664307\n",
      "Epoch: 44/100 | step: 76/422 | loss: 1.2759714126586914\n",
      "Epoch: 44/100 | step: 77/422 | loss: 1.5959851741790771\n",
      "Epoch: 44/100 | step: 78/422 | loss: 1.7653800249099731\n",
      "Epoch: 44/100 | step: 79/422 | loss: 1.1029692888259888\n",
      "Epoch: 44/100 | step: 80/422 | loss: 1.9801887273788452\n",
      "Epoch: 44/100 | step: 81/422 | loss: 1.3314553499221802\n",
      "Epoch: 44/100 | step: 82/422 | loss: 1.2672502994537354\n",
      "Epoch: 44/100 | step: 83/422 | loss: 2.016335964202881\n",
      "Epoch: 44/100 | step: 84/422 | loss: 1.7754970788955688\n",
      "Epoch: 44/100 | step: 85/422 | loss: 1.5836843252182007\n",
      "Epoch: 44/100 | step: 86/422 | loss: 1.523841381072998\n",
      "Epoch: 44/100 | step: 87/422 | loss: 1.4422955513000488\n",
      "Epoch: 44/100 | step: 88/422 | loss: 1.1634598970413208\n",
      "Epoch: 44/100 | step: 89/422 | loss: 1.4185597896575928\n",
      "Epoch: 44/100 | step: 90/422 | loss: 1.3077006340026855\n",
      "Epoch: 44/100 | step: 91/422 | loss: 1.6162586212158203\n",
      "Epoch: 44/100 | step: 92/422 | loss: 1.649832844734192\n",
      "Epoch: 44/100 | step: 93/422 | loss: 1.705114483833313\n",
      "Epoch: 44/100 | step: 94/422 | loss: 1.576051950454712\n",
      "Epoch: 44/100 | step: 95/422 | loss: 1.6221907138824463\n",
      "Epoch: 44/100 | step: 96/422 | loss: 1.2069989442825317\n",
      "Epoch: 44/100 | step: 97/422 | loss: 1.5686711072921753\n",
      "Epoch: 44/100 | step: 98/422 | loss: 1.2628511190414429\n",
      "Epoch: 44/100 | step: 99/422 | loss: 1.4575423002243042\n",
      "Epoch: 44/100 | step: 100/422 | loss: 1.4875531196594238\n",
      "Epoch: 44/100 | step: 101/422 | loss: 0.9968042373657227\n",
      "Epoch: 44/100 | step: 102/422 | loss: 1.5845834016799927\n",
      "Epoch: 44/100 | step: 103/422 | loss: 1.538097620010376\n",
      "Epoch: 44/100 | step: 104/422 | loss: 1.401206135749817\n",
      "Epoch: 44/100 | step: 105/422 | loss: 1.3934143781661987\n",
      "Epoch: 44/100 | step: 106/422 | loss: 1.5607954263687134\n",
      "Epoch: 44/100 | step: 107/422 | loss: 1.555958867073059\n",
      "Epoch: 44/100 | step: 108/422 | loss: 1.4990366697311401\n",
      "Epoch: 44/100 | step: 109/422 | loss: 1.613800048828125\n",
      "Epoch: 44/100 | step: 110/422 | loss: 1.7763434648513794\n",
      "Epoch: 44/100 | step: 111/422 | loss: 1.7050011157989502\n",
      "Epoch: 44/100 | step: 112/422 | loss: 1.5171133279800415\n",
      "Epoch: 44/100 | step: 113/422 | loss: 1.2830990552902222\n",
      "Epoch: 44/100 | step: 114/422 | loss: 1.4461740255355835\n",
      "Epoch: 44/100 | step: 115/422 | loss: 1.3062515258789062\n",
      "Epoch: 44/100 | step: 116/422 | loss: 1.2834407091140747\n",
      "Epoch: 44/100 | step: 117/422 | loss: 1.7218763828277588\n",
      "Epoch: 44/100 | step: 118/422 | loss: 1.2312192916870117\n",
      "Epoch: 44/100 | step: 119/422 | loss: 1.2794222831726074\n",
      "Epoch: 44/100 | step: 120/422 | loss: 2.0450661182403564\n",
      "Epoch: 44/100 | step: 121/422 | loss: 1.4851446151733398\n",
      "Epoch: 44/100 | step: 122/422 | loss: 1.6091126203536987\n",
      "Epoch: 44/100 | step: 123/422 | loss: 1.6655118465423584\n",
      "Epoch: 44/100 | step: 124/422 | loss: 1.8791378736495972\n",
      "Epoch: 44/100 | step: 125/422 | loss: 1.515183925628662\n",
      "Epoch: 44/100 | step: 126/422 | loss: 1.8073248863220215\n",
      "Epoch: 44/100 | step: 127/422 | loss: 2.0641582012176514\n",
      "Epoch: 44/100 | step: 128/422 | loss: 1.3336750268936157\n",
      "Epoch: 44/100 | step: 129/422 | loss: 1.8657878637313843\n",
      "Epoch: 44/100 | step: 130/422 | loss: 1.8300212621688843\n",
      "Epoch: 44/100 | step: 131/422 | loss: 1.3951796293258667\n",
      "Epoch: 44/100 | step: 132/422 | loss: 1.1631793975830078\n",
      "Epoch: 44/100 | step: 133/422 | loss: 1.369101643562317\n",
      "Epoch: 44/100 | step: 134/422 | loss: 1.6850653886795044\n",
      "Epoch: 44/100 | step: 135/422 | loss: 1.630006194114685\n",
      "Epoch: 44/100 | step: 136/422 | loss: 1.3695502281188965\n",
      "Epoch: 44/100 | step: 137/422 | loss: 2.223043203353882\n",
      "Epoch: 44/100 | step: 138/422 | loss: 1.3589328527450562\n",
      "Epoch: 44/100 | step: 139/422 | loss: 0.891678512096405\n",
      "Epoch: 44/100 | step: 140/422 | loss: 1.3871639966964722\n",
      "Epoch: 44/100 | step: 141/422 | loss: 1.3867943286895752\n",
      "Epoch: 44/100 | step: 142/422 | loss: 1.4085572957992554\n",
      "Epoch: 44/100 | step: 143/422 | loss: 1.2052513360977173\n",
      "Epoch: 44/100 | step: 144/422 | loss: 1.946399450302124\n",
      "Epoch: 44/100 | step: 145/422 | loss: 1.738737940788269\n",
      "Epoch: 44/100 | step: 146/422 | loss: 1.3832402229309082\n",
      "Epoch: 44/100 | step: 147/422 | loss: 1.2510499954223633\n",
      "Epoch: 44/100 | step: 148/422 | loss: 1.1789085865020752\n",
      "Epoch: 44/100 | step: 149/422 | loss: 1.6343708038330078\n",
      "Epoch: 44/100 | step: 150/422 | loss: 1.3941401243209839\n",
      "Epoch: 44/100 | step: 151/422 | loss: 1.1420561075210571\n",
      "Epoch: 44/100 | step: 152/422 | loss: 1.1700379848480225\n",
      "Epoch: 44/100 | step: 153/422 | loss: 1.2471383810043335\n",
      "Epoch: 44/100 | step: 154/422 | loss: 1.435270071029663\n",
      "Epoch: 44/100 | step: 155/422 | loss: 1.3686896562576294\n",
      "Epoch: 44/100 | step: 156/422 | loss: 1.3664722442626953\n",
      "Epoch: 44/100 | step: 157/422 | loss: 1.3404067754745483\n",
      "Epoch: 44/100 | step: 158/422 | loss: 1.1005737781524658\n",
      "Epoch: 44/100 | step: 159/422 | loss: 1.7596219778060913\n",
      "Epoch: 44/100 | step: 160/422 | loss: 1.6409202814102173\n",
      "Epoch: 44/100 | step: 161/422 | loss: 1.480780005455017\n",
      "Epoch: 44/100 | step: 162/422 | loss: 1.5849628448486328\n",
      "Epoch: 44/100 | step: 163/422 | loss: 1.6948089599609375\n",
      "Epoch: 44/100 | step: 164/422 | loss: 2.031093120574951\n",
      "Epoch: 44/100 | step: 165/422 | loss: 1.8090968132019043\n",
      "Epoch: 44/100 | step: 166/422 | loss: 1.7671633958816528\n",
      "Epoch: 44/100 | step: 167/422 | loss: 1.3212993144989014\n",
      "Epoch: 44/100 | step: 168/422 | loss: 1.7797198295593262\n",
      "Epoch: 44/100 | step: 169/422 | loss: 1.3902002573013306\n",
      "Epoch: 44/100 | step: 170/422 | loss: 1.1577941179275513\n",
      "Epoch: 44/100 | step: 171/422 | loss: 0.9696378707885742\n",
      "Epoch: 44/100 | step: 172/422 | loss: 1.7230697870254517\n",
      "Epoch: 44/100 | step: 173/422 | loss: 0.8462920188903809\n",
      "Epoch: 44/100 | step: 174/422 | loss: 1.8181926012039185\n",
      "Epoch: 44/100 | step: 175/422 | loss: 1.2250691652297974\n",
      "Epoch: 44/100 | step: 176/422 | loss: 1.1137380599975586\n",
      "Epoch: 44/100 | step: 177/422 | loss: 1.63301420211792\n",
      "Epoch: 44/100 | step: 178/422 | loss: 1.65409255027771\n",
      "Epoch: 44/100 | step: 179/422 | loss: 1.529402256011963\n",
      "Epoch: 44/100 | step: 180/422 | loss: 1.8528655767440796\n",
      "Epoch: 44/100 | step: 181/422 | loss: 1.7645732164382935\n",
      "Epoch: 44/100 | step: 182/422 | loss: 1.3155921697616577\n",
      "Epoch: 44/100 | step: 183/422 | loss: 1.8091238737106323\n",
      "Epoch: 44/100 | step: 184/422 | loss: 2.153477191925049\n",
      "Epoch: 44/100 | step: 185/422 | loss: 1.398666262626648\n",
      "Epoch: 44/100 | step: 186/422 | loss: 1.5597831010818481\n",
      "Epoch: 44/100 | step: 187/422 | loss: 1.7040643692016602\n",
      "Epoch: 44/100 | step: 188/422 | loss: 1.7710047960281372\n",
      "Epoch: 44/100 | step: 189/422 | loss: 2.0584471225738525\n",
      "Epoch: 44/100 | step: 190/422 | loss: 1.8825883865356445\n",
      "Epoch: 44/100 | step: 191/422 | loss: 1.321582555770874\n",
      "Epoch: 44/100 | step: 192/422 | loss: 1.6948378086090088\n",
      "Epoch: 44/100 | step: 193/422 | loss: 2.128464460372925\n",
      "Epoch: 44/100 | step: 194/422 | loss: 1.8362574577331543\n",
      "Epoch: 44/100 | step: 195/422 | loss: 1.6539771556854248\n",
      "Epoch: 44/100 | step: 196/422 | loss: 1.1819568872451782\n",
      "Epoch: 44/100 | step: 197/422 | loss: 1.480591893196106\n",
      "Epoch: 44/100 | step: 198/422 | loss: 1.5466276407241821\n",
      "Epoch: 44/100 | step: 199/422 | loss: 1.6376676559448242\n",
      "Epoch: 44/100 | step: 200/422 | loss: 1.515509843826294\n",
      "Epoch: 44/100 | step: 201/422 | loss: 1.8051526546478271\n",
      "Epoch: 44/100 | step: 202/422 | loss: 1.5053274631500244\n",
      "Epoch: 44/100 | step: 203/422 | loss: 1.8386136293411255\n",
      "Epoch: 44/100 | step: 204/422 | loss: 1.6939817667007446\n",
      "Epoch: 44/100 | step: 205/422 | loss: 1.335436463356018\n",
      "Epoch: 44/100 | step: 206/422 | loss: 1.3116999864578247\n",
      "Epoch: 44/100 | step: 207/422 | loss: 1.512778878211975\n",
      "Epoch: 44/100 | step: 208/422 | loss: 1.1630264520645142\n",
      "Epoch: 44/100 | step: 209/422 | loss: 1.147955298423767\n",
      "Epoch: 44/100 | step: 210/422 | loss: 1.5411620140075684\n",
      "Epoch: 44/100 | step: 211/422 | loss: 1.5235682725906372\n",
      "Epoch: 44/100 | step: 212/422 | loss: 1.472895860671997\n",
      "Epoch: 44/100 | step: 213/422 | loss: 1.9025344848632812\n",
      "Epoch: 44/100 | step: 214/422 | loss: 1.3801023960113525\n",
      "Epoch: 44/100 | step: 215/422 | loss: 1.7961474657058716\n",
      "Epoch: 44/100 | step: 216/422 | loss: 1.5453299283981323\n",
      "Epoch: 44/100 | step: 217/422 | loss: 1.425069808959961\n",
      "Epoch: 44/100 | step: 218/422 | loss: 1.6694504022598267\n",
      "Epoch: 44/100 | step: 219/422 | loss: 1.7540671825408936\n",
      "Epoch: 44/100 | step: 220/422 | loss: 1.4634718894958496\n",
      "Epoch: 44/100 | step: 221/422 | loss: 0.9604681730270386\n",
      "Epoch: 44/100 | step: 222/422 | loss: 1.66507089138031\n",
      "Epoch: 44/100 | step: 223/422 | loss: 1.2961727380752563\n",
      "Epoch: 44/100 | step: 224/422 | loss: 1.1325647830963135\n",
      "Epoch: 44/100 | step: 225/422 | loss: 1.482746958732605\n",
      "Epoch: 44/100 | step: 226/422 | loss: 1.3495752811431885\n",
      "Epoch: 44/100 | step: 227/422 | loss: 1.847546935081482\n",
      "Epoch: 44/100 | step: 228/422 | loss: 1.1137501001358032\n",
      "Epoch: 44/100 | step: 229/422 | loss: 1.7437119483947754\n",
      "Epoch: 44/100 | step: 230/422 | loss: 1.8656145334243774\n",
      "Epoch: 44/100 | step: 231/422 | loss: 1.5915935039520264\n",
      "Epoch: 44/100 | step: 232/422 | loss: 1.9310331344604492\n",
      "Epoch: 44/100 | step: 233/422 | loss: 1.4725360870361328\n",
      "Epoch: 44/100 | step: 234/422 | loss: 1.7334169149398804\n",
      "Epoch: 44/100 | step: 235/422 | loss: 1.285497784614563\n",
      "Epoch: 44/100 | step: 236/422 | loss: 1.7751848697662354\n",
      "Epoch: 44/100 | step: 237/422 | loss: 1.145732045173645\n",
      "Epoch: 44/100 | step: 238/422 | loss: 1.638588786125183\n",
      "Epoch: 44/100 | step: 239/422 | loss: 1.5136548280715942\n",
      "Epoch: 44/100 | step: 240/422 | loss: 1.9046294689178467\n",
      "Epoch: 44/100 | step: 241/422 | loss: 1.6568576097488403\n",
      "Epoch: 44/100 | step: 242/422 | loss: 1.4903541803359985\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 45/100 | step: 1/422 | loss: 1.6046077013015747\n",
      "Epoch: 45/100 | step: 2/422 | loss: 0.8107972741127014\n",
      "Epoch: 45/100 | step: 3/422 | loss: 1.1965904235839844\n",
      "Epoch: 45/100 | step: 4/422 | loss: 1.3094784021377563\n",
      "Epoch: 45/100 | step: 5/422 | loss: 1.046940565109253\n",
      "Epoch: 45/100 | step: 6/422 | loss: 1.4002525806427002\n",
      "Epoch: 45/100 | step: 7/422 | loss: 0.9136316776275635\n",
      "Epoch: 45/100 | step: 8/422 | loss: 1.287919521331787\n",
      "Epoch: 45/100 | step: 9/422 | loss: 0.9984753131866455\n",
      "Epoch: 45/100 | step: 10/422 | loss: 1.546095609664917\n",
      "Epoch: 45/100 | step: 11/422 | loss: 1.3956048488616943\n",
      "Epoch: 45/100 | step: 12/422 | loss: 1.4769022464752197\n",
      "Epoch: 45/100 | step: 13/422 | loss: 1.190118432044983\n",
      "Epoch: 45/100 | step: 14/422 | loss: 1.0806705951690674\n",
      "Epoch: 45/100 | step: 15/422 | loss: 1.4815168380737305\n",
      "Epoch: 45/100 | step: 16/422 | loss: 1.4894466400146484\n",
      "Epoch: 45/100 | step: 17/422 | loss: 1.5319933891296387\n",
      "Epoch: 45/100 | step: 18/422 | loss: 1.613631248474121\n",
      "Epoch: 45/100 | step: 19/422 | loss: 1.4327943325042725\n",
      "Epoch: 45/100 | step: 20/422 | loss: 0.9711851477622986\n",
      "Epoch: 45/100 | step: 21/422 | loss: 1.4277873039245605\n",
      "Epoch: 45/100 | step: 22/422 | loss: 1.6951713562011719\n",
      "Epoch: 45/100 | step: 23/422 | loss: 1.4362784624099731\n",
      "Epoch: 45/100 | step: 24/422 | loss: 1.5276960134506226\n",
      "Epoch: 45/100 | step: 25/422 | loss: 1.2521591186523438\n",
      "Epoch: 45/100 | step: 26/422 | loss: 1.5979403257369995\n",
      "Epoch: 45/100 | step: 27/422 | loss: 1.3863455057144165\n",
      "Epoch: 45/100 | step: 28/422 | loss: 1.141011118888855\n",
      "Epoch: 45/100 | step: 29/422 | loss: 0.8695088028907776\n",
      "Epoch: 45/100 | step: 30/422 | loss: 1.5594725608825684\n",
      "Epoch: 45/100 | step: 31/422 | loss: 1.2487748861312866\n",
      "Epoch: 45/100 | step: 32/422 | loss: 1.3356266021728516\n",
      "Epoch: 45/100 | step: 33/422 | loss: 1.3212921619415283\n",
      "Epoch: 45/100 | step: 34/422 | loss: 1.7921503782272339\n",
      "Epoch: 45/100 | step: 35/422 | loss: 1.50580632686615\n",
      "Epoch: 45/100 | step: 36/422 | loss: 1.743247628211975\n",
      "Epoch: 45/100 | step: 37/422 | loss: 1.547166347503662\n",
      "Epoch: 45/100 | step: 38/422 | loss: 1.0368986129760742\n",
      "Epoch: 45/100 | step: 39/422 | loss: 1.8622231483459473\n",
      "Epoch: 45/100 | step: 40/422 | loss: 1.6394150257110596\n",
      "Epoch: 45/100 | step: 41/422 | loss: 0.999424159526825\n",
      "Epoch: 45/100 | step: 42/422 | loss: 1.5625219345092773\n",
      "Epoch: 45/100 | step: 43/422 | loss: 1.6335264444351196\n",
      "Epoch: 45/100 | step: 44/422 | loss: 1.722459316253662\n",
      "Epoch: 45/100 | step: 45/422 | loss: 1.2612428665161133\n",
      "Epoch: 45/100 | step: 46/422 | loss: 1.0824992656707764\n",
      "Epoch: 45/100 | step: 47/422 | loss: 1.323837161064148\n",
      "Epoch: 45/100 | step: 48/422 | loss: 1.3864654302597046\n",
      "Epoch: 45/100 | step: 49/422 | loss: 1.4568262100219727\n",
      "Epoch: 45/100 | step: 50/422 | loss: 1.4728866815567017\n",
      "Epoch: 45/100 | step: 51/422 | loss: 1.1342480182647705\n",
      "Epoch: 45/100 | step: 52/422 | loss: 1.1922264099121094\n",
      "Epoch: 45/100 | step: 53/422 | loss: 1.2487938404083252\n",
      "Epoch: 45/100 | step: 54/422 | loss: 1.867920160293579\n",
      "Epoch: 45/100 | step: 55/422 | loss: 1.4712721109390259\n",
      "Epoch: 45/100 | step: 56/422 | loss: 1.4653966426849365\n",
      "Epoch: 45/100 | step: 57/422 | loss: 1.0687839984893799\n",
      "Epoch: 45/100 | step: 58/422 | loss: 1.6024612188339233\n",
      "Epoch: 45/100 | step: 59/422 | loss: 1.0621508359909058\n",
      "Epoch: 45/100 | step: 60/422 | loss: 1.527275562286377\n",
      "Epoch: 45/100 | step: 61/422 | loss: 1.3291903734207153\n",
      "Epoch: 45/100 | step: 62/422 | loss: 1.4982995986938477\n",
      "Epoch: 45/100 | step: 63/422 | loss: 1.092079520225525\n",
      "Epoch: 45/100 | step: 64/422 | loss: 1.4510514736175537\n",
      "Epoch: 45/100 | step: 65/422 | loss: 1.2633310556411743\n",
      "Epoch: 45/100 | step: 66/422 | loss: 1.4899237155914307\n",
      "Epoch: 45/100 | step: 67/422 | loss: 1.1298218965530396\n",
      "Epoch: 45/100 | step: 68/422 | loss: 0.9243399500846863\n",
      "Epoch: 45/100 | step: 69/422 | loss: 1.1183470487594604\n",
      "Epoch: 45/100 | step: 70/422 | loss: 1.2405390739440918\n",
      "Epoch: 45/100 | step: 71/422 | loss: 1.6798646450042725\n",
      "Epoch: 45/100 | step: 72/422 | loss: 1.0394911766052246\n",
      "Epoch: 45/100 | step: 73/422 | loss: 1.6322340965270996\n",
      "Epoch: 45/100 | step: 74/422 | loss: 1.2354234457015991\n",
      "Epoch: 45/100 | step: 75/422 | loss: 1.4412119388580322\n",
      "Epoch: 45/100 | step: 76/422 | loss: 1.6300684213638306\n",
      "Epoch: 45/100 | step: 77/422 | loss: 2.35235595703125\n",
      "Epoch: 45/100 | step: 78/422 | loss: 1.0917692184448242\n",
      "Epoch: 45/100 | step: 79/422 | loss: 1.5755006074905396\n",
      "Epoch: 45/100 | step: 80/422 | loss: 1.2810204029083252\n",
      "Epoch: 45/100 | step: 81/422 | loss: 1.4314459562301636\n",
      "Epoch: 45/100 | step: 82/422 | loss: 1.6213146448135376\n",
      "Epoch: 45/100 | step: 83/422 | loss: 1.6044728755950928\n",
      "Epoch: 45/100 | step: 84/422 | loss: 1.9561821222305298\n",
      "Epoch: 45/100 | step: 85/422 | loss: 1.0785740613937378\n",
      "Epoch: 45/100 | step: 86/422 | loss: 1.5256319046020508\n",
      "Epoch: 45/100 | step: 87/422 | loss: 1.2985572814941406\n",
      "Epoch: 45/100 | step: 88/422 | loss: 1.0358948707580566\n",
      "Epoch: 45/100 | step: 89/422 | loss: 1.864396572113037\n",
      "Epoch: 45/100 | step: 90/422 | loss: 1.0553292036056519\n",
      "Epoch: 45/100 | step: 91/422 | loss: 1.2677233219146729\n",
      "Epoch: 45/100 | step: 92/422 | loss: 1.286818027496338\n",
      "Epoch: 45/100 | step: 93/422 | loss: 1.6647855043411255\n",
      "Epoch: 45/100 | step: 94/422 | loss: 2.0042057037353516\n",
      "Epoch: 45/100 | step: 95/422 | loss: 2.0906341075897217\n",
      "Epoch: 45/100 | step: 96/422 | loss: 1.6930642127990723\n",
      "Epoch: 45/100 | step: 97/422 | loss: 1.3683700561523438\n",
      "Epoch: 45/100 | step: 98/422 | loss: 1.4978965520858765\n",
      "Epoch: 45/100 | step: 99/422 | loss: 1.4438384771347046\n",
      "Epoch: 45/100 | step: 100/422 | loss: 1.0022294521331787\n",
      "Epoch: 45/100 | step: 101/422 | loss: 2.121140480041504\n",
      "Epoch: 45/100 | step: 102/422 | loss: 1.9811444282531738\n",
      "Epoch: 45/100 | step: 103/422 | loss: 1.7612568140029907\n",
      "Epoch: 45/100 | step: 104/422 | loss: 2.156458854675293\n",
      "Epoch: 45/100 | step: 105/422 | loss: 1.9213446378707886\n",
      "Epoch: 45/100 | step: 106/422 | loss: 2.3868937492370605\n",
      "Epoch: 45/100 | step: 107/422 | loss: 1.2753502130508423\n",
      "Epoch: 45/100 | step: 108/422 | loss: 1.0858319997787476\n",
      "Epoch: 45/100 | step: 109/422 | loss: 1.3257545232772827\n",
      "Epoch: 45/100 | step: 110/422 | loss: 1.7882776260375977\n",
      "Epoch: 45/100 | step: 111/422 | loss: 1.3233145475387573\n",
      "Epoch: 45/100 | step: 112/422 | loss: 1.5333377122879028\n",
      "Epoch: 45/100 | step: 113/422 | loss: 2.0306026935577393\n",
      "Epoch: 45/100 | step: 114/422 | loss: 2.459914445877075\n",
      "Epoch: 45/100 | step: 115/422 | loss: 1.415677785873413\n",
      "Epoch: 45/100 | step: 116/422 | loss: 1.574805736541748\n",
      "Epoch: 45/100 | step: 117/422 | loss: 1.7975871562957764\n",
      "Epoch: 45/100 | step: 118/422 | loss: 1.7010823488235474\n",
      "Epoch: 45/100 | step: 119/422 | loss: 1.6350412368774414\n",
      "Epoch: 45/100 | step: 120/422 | loss: 1.2533748149871826\n",
      "Epoch: 45/100 | step: 121/422 | loss: 1.5141704082489014\n",
      "Epoch: 45/100 | step: 122/422 | loss: 1.3511160612106323\n",
      "Epoch: 45/100 | step: 123/422 | loss: 1.3170346021652222\n",
      "Epoch: 45/100 | step: 124/422 | loss: 0.9216717481613159\n",
      "Epoch: 45/100 | step: 125/422 | loss: 1.7438768148422241\n",
      "Epoch: 45/100 | step: 126/422 | loss: 1.4869370460510254\n",
      "Epoch: 45/100 | step: 127/422 | loss: 1.4971362352371216\n",
      "Epoch: 45/100 | step: 128/422 | loss: 1.430940866470337\n",
      "Epoch: 45/100 | step: 129/422 | loss: 1.0937799215316772\n",
      "Epoch: 45/100 | step: 130/422 | loss: 1.5571426153182983\n",
      "Epoch: 45/100 | step: 131/422 | loss: 1.3232618570327759\n",
      "Epoch: 45/100 | step: 132/422 | loss: 1.5932419300079346\n",
      "Epoch: 45/100 | step: 133/422 | loss: 1.3280640840530396\n",
      "Epoch: 45/100 | step: 134/422 | loss: 1.0969501733779907\n",
      "Epoch: 45/100 | step: 135/422 | loss: 1.516390323638916\n",
      "Epoch: 45/100 | step: 136/422 | loss: 0.9718526005744934\n",
      "Epoch: 45/100 | step: 137/422 | loss: 1.3422822952270508\n",
      "Epoch: 45/100 | step: 138/422 | loss: 1.6075116395950317\n",
      "Epoch: 45/100 | step: 139/422 | loss: 1.6193702220916748\n",
      "Epoch: 45/100 | step: 140/422 | loss: 1.3849033117294312\n",
      "Epoch: 45/100 | step: 141/422 | loss: 1.4477912187576294\n",
      "Epoch: 45/100 | step: 142/422 | loss: 1.2461967468261719\n",
      "Epoch: 45/100 | step: 143/422 | loss: 1.3027411699295044\n",
      "Epoch: 45/100 | step: 144/422 | loss: 1.1412112712860107\n",
      "Epoch: 45/100 | step: 145/422 | loss: 1.8630592823028564\n",
      "Epoch: 45/100 | step: 146/422 | loss: 2.048966646194458\n",
      "Epoch: 45/100 | step: 147/422 | loss: 1.6184731721878052\n",
      "Epoch: 45/100 | step: 148/422 | loss: 1.631898283958435\n",
      "Epoch: 45/100 | step: 149/422 | loss: 1.4982439279556274\n",
      "Epoch: 45/100 | step: 150/422 | loss: 1.8714807033538818\n",
      "Epoch: 45/100 | step: 151/422 | loss: 1.3680561780929565\n",
      "Epoch: 45/100 | step: 152/422 | loss: 1.4513587951660156\n",
      "Epoch: 45/100 | step: 153/422 | loss: 0.9525600075721741\n",
      "Epoch: 45/100 | step: 154/422 | loss: 1.2004717588424683\n",
      "Epoch: 45/100 | step: 155/422 | loss: 1.547583818435669\n",
      "Epoch: 45/100 | step: 156/422 | loss: 1.3657158613204956\n",
      "Epoch: 45/100 | step: 157/422 | loss: 1.8150006532669067\n",
      "Epoch: 45/100 | step: 158/422 | loss: 1.1493690013885498\n",
      "Epoch: 45/100 | step: 159/422 | loss: 1.306299090385437\n",
      "Epoch: 45/100 | step: 160/422 | loss: 1.4462568759918213\n",
      "Epoch: 45/100 | step: 161/422 | loss: 1.5723844766616821\n",
      "Epoch: 45/100 | step: 162/422 | loss: 1.134838581085205\n",
      "Epoch: 45/100 | step: 163/422 | loss: 1.961725115776062\n",
      "Epoch: 45/100 | step: 164/422 | loss: 1.4020171165466309\n",
      "Epoch: 45/100 | step: 165/422 | loss: 1.443440318107605\n",
      "Epoch: 45/100 | step: 166/422 | loss: 1.589242696762085\n",
      "Epoch: 45/100 | step: 167/422 | loss: 1.3579638004302979\n",
      "Epoch: 45/100 | step: 168/422 | loss: 1.171639323234558\n",
      "Epoch: 45/100 | step: 169/422 | loss: 1.6118735074996948\n",
      "Epoch: 45/100 | step: 170/422 | loss: 2.0046253204345703\n",
      "Epoch: 45/100 | step: 171/422 | loss: 2.0136849880218506\n",
      "Epoch: 45/100 | step: 172/422 | loss: 1.9691009521484375\n",
      "Epoch: 45/100 | step: 173/422 | loss: 1.2831205129623413\n",
      "Epoch: 45/100 | step: 174/422 | loss: 1.357126235961914\n",
      "Epoch: 45/100 | step: 175/422 | loss: 1.2076267004013062\n",
      "Epoch: 45/100 | step: 176/422 | loss: 1.69503915309906\n",
      "Epoch: 45/100 | step: 177/422 | loss: 1.438425064086914\n",
      "Epoch: 45/100 | step: 178/422 | loss: 1.66831636428833\n",
      "Epoch: 45/100 | step: 179/422 | loss: 0.9204134941101074\n",
      "Epoch: 45/100 | step: 180/422 | loss: 1.7873408794403076\n",
      "Epoch: 45/100 | step: 181/422 | loss: 1.3494173288345337\n",
      "Epoch: 45/100 | step: 182/422 | loss: 1.2922310829162598\n",
      "Epoch: 45/100 | step: 183/422 | loss: 1.2261598110198975\n",
      "Epoch: 45/100 | step: 184/422 | loss: 1.674187183380127\n",
      "Epoch: 45/100 | step: 185/422 | loss: 1.0846009254455566\n",
      "Epoch: 45/100 | step: 186/422 | loss: 1.3764315843582153\n",
      "Epoch: 45/100 | step: 187/422 | loss: 1.4405932426452637\n",
      "Epoch: 45/100 | step: 188/422 | loss: 1.3134328126907349\n",
      "Epoch: 45/100 | step: 189/422 | loss: 1.5763366222381592\n",
      "Epoch: 45/100 | step: 190/422 | loss: 1.4131200313568115\n",
      "Epoch: 45/100 | step: 191/422 | loss: 1.212114930152893\n",
      "Epoch: 45/100 | step: 192/422 | loss: 1.2323851585388184\n",
      "Epoch: 45/100 | step: 193/422 | loss: 1.2164705991744995\n",
      "Epoch: 45/100 | step: 194/422 | loss: 1.4767885208129883\n",
      "Epoch: 45/100 | step: 195/422 | loss: 1.613980770111084\n",
      "Epoch: 45/100 | step: 196/422 | loss: 1.2497283220291138\n",
      "Epoch: 45/100 | step: 197/422 | loss: 1.3049925565719604\n",
      "Epoch: 45/100 | step: 198/422 | loss: 2.116452932357788\n",
      "Epoch: 45/100 | step: 199/422 | loss: 1.4832494258880615\n",
      "Epoch: 45/100 | step: 200/422 | loss: 1.3667782545089722\n",
      "Epoch: 45/100 | step: 201/422 | loss: 1.8600608110427856\n",
      "Epoch: 45/100 | step: 202/422 | loss: 1.4953607320785522\n",
      "Epoch: 45/100 | step: 203/422 | loss: 1.8032050132751465\n",
      "Epoch: 45/100 | step: 204/422 | loss: 1.558964729309082\n",
      "Epoch: 45/100 | step: 205/422 | loss: 1.529241681098938\n",
      "Epoch: 45/100 | step: 206/422 | loss: 1.1868479251861572\n",
      "Epoch: 45/100 | step: 207/422 | loss: 1.5056217908859253\n",
      "Epoch: 45/100 | step: 208/422 | loss: 1.4059834480285645\n",
      "Epoch: 45/100 | step: 209/422 | loss: 1.3997019529342651\n",
      "Epoch: 45/100 | step: 210/422 | loss: 1.6594566106796265\n",
      "Epoch: 45/100 | step: 211/422 | loss: 2.2252180576324463\n",
      "Epoch: 45/100 | step: 212/422 | loss: 1.4933804273605347\n",
      "Epoch: 45/100 | step: 213/422 | loss: 1.4156619310379028\n",
      "Epoch: 45/100 | step: 214/422 | loss: 1.1662362813949585\n",
      "Epoch: 45/100 | step: 215/422 | loss: 2.0110347270965576\n",
      "Epoch: 45/100 | step: 216/422 | loss: 1.6554503440856934\n",
      "Epoch: 45/100 | step: 217/422 | loss: 1.3774958848953247\n",
      "Epoch: 45/100 | step: 218/422 | loss: 1.6054139137268066\n",
      "Epoch: 45/100 | step: 219/422 | loss: 1.7017468214035034\n",
      "Epoch: 45/100 | step: 220/422 | loss: 2.005300760269165\n",
      "Epoch: 45/100 | step: 221/422 | loss: 1.779125452041626\n",
      "Epoch: 45/100 | step: 222/422 | loss: 1.865240216255188\n",
      "Epoch: 45/100 | step: 223/422 | loss: 1.6467610597610474\n",
      "Epoch: 45/100 | step: 224/422 | loss: 1.569859266281128\n",
      "Epoch: 45/100 | step: 225/422 | loss: 2.001828908920288\n",
      "Epoch: 45/100 | step: 226/422 | loss: 1.6273219585418701\n",
      "Epoch: 45/100 | step: 227/422 | loss: 1.8297908306121826\n",
      "Epoch: 45/100 | step: 228/422 | loss: 1.7272311449050903\n",
      "Epoch: 45/100 | step: 229/422 | loss: 1.3139631748199463\n",
      "Epoch: 45/100 | step: 230/422 | loss: 1.1368342638015747\n",
      "Epoch: 45/100 | step: 231/422 | loss: 0.9525725841522217\n",
      "Epoch: 45/100 | step: 232/422 | loss: 2.1020514965057373\n",
      "Epoch: 45/100 | step: 233/422 | loss: 1.4097427129745483\n",
      "Epoch: 45/100 | step: 234/422 | loss: 1.7208962440490723\n",
      "Epoch: 45/100 | step: 235/422 | loss: 1.533029317855835\n",
      "Epoch: 45/100 | step: 236/422 | loss: 1.6813578605651855\n",
      "Epoch: 45/100 | step: 237/422 | loss: 1.2738673686981201\n",
      "Epoch: 45/100 | step: 238/422 | loss: 1.1453251838684082\n",
      "Epoch: 45/100 | step: 239/422 | loss: 1.1489475965499878\n",
      "Epoch: 45/100 | step: 240/422 | loss: 1.9138623476028442\n",
      "Epoch: 45/100 | step: 241/422 | loss: 2.0506160259246826\n",
      "Epoch: 45/100 | step: 242/422 | loss: 1.4598689079284668\n",
      "Epoch: 45/100 | step: 243/422 | loss: 1.6384625434875488\n",
      "Epoch: 45/100 | step: 244/422 | loss: 1.652101755142212\n",
      "Epoch: 45/100 | step: 245/422 | loss: 2.1397008895874023\n",
      "Epoch: 45/100 | step: 246/422 | loss: 1.5503093004226685\n",
      "Epoch: 45/100 | step: 247/422 | loss: 1.7316901683807373\n",
      "Epoch: 45/100 | step: 248/422 | loss: 1.4808409214019775\n",
      "Epoch: 45/100 | step: 249/422 | loss: 0.9886704087257385\n",
      "Epoch: 45/100 | step: 250/422 | loss: 1.5912803411483765\n",
      "Epoch: 45/100 | step: 251/422 | loss: 1.3149256706237793\n",
      "Epoch: 45/100 | step: 252/422 | loss: 1.2446383237838745\n",
      "Epoch: 45/100 | step: 253/422 | loss: 0.9474793076515198\n",
      "Epoch: 45/100 | step: 254/422 | loss: 1.5385075807571411\n",
      "Epoch: 45/100 | step: 255/422 | loss: 2.2856881618499756\n",
      "Epoch: 45/100 | step: 256/422 | loss: 1.6040247678756714\n",
      "Epoch: 45/100 | step: 257/422 | loss: 1.3952420949935913\n",
      "Epoch: 45/100 | step: 258/422 | loss: 0.9787126183509827\n",
      "Epoch: 45/100 | step: 259/422 | loss: 1.549752950668335\n",
      "Epoch: 45/100 | step: 260/422 | loss: 1.5588114261627197\n",
      "Epoch: 45/100 | step: 261/422 | loss: 1.3196980953216553\n",
      "Epoch: 45/100 | step: 262/422 | loss: 1.7067289352416992\n",
      "Epoch: 45/100 | step: 263/422 | loss: 1.2274441719055176\n",
      "Epoch: 45/100 | step: 264/422 | loss: 1.5038955211639404\n",
      "Epoch: 45/100 | step: 265/422 | loss: 1.4188487529754639\n",
      "Epoch: 45/100 | step: 266/422 | loss: 1.9881799221038818\n",
      "Epoch: 45/100 | step: 267/422 | loss: 1.7755637168884277\n",
      "Epoch: 45/100 | step: 268/422 | loss: 1.7545201778411865\n",
      "Epoch: 45/100 | step: 269/422 | loss: 1.1173887252807617\n",
      "Epoch: 45/100 | step: 270/422 | loss: 1.7443244457244873\n",
      "Epoch: 45/100 | step: 271/422 | loss: 1.1169310808181763\n",
      "Epoch: 45/100 | step: 272/422 | loss: 1.9470010995864868\n",
      "Epoch: 45/100 | step: 273/422 | loss: 1.4843841791152954\n",
      "Epoch: 45/100 | step: 274/422 | loss: 1.7120959758758545\n",
      "Epoch: 45/100 | step: 275/422 | loss: 1.20442795753479\n",
      "Epoch: 45/100 | step: 276/422 | loss: 1.5227686166763306\n",
      "Epoch: 45/100 | step: 277/422 | loss: 1.8909506797790527\n",
      "Epoch: 45/100 | step: 278/422 | loss: 1.5985753536224365\n",
      "Epoch: 45/100 | step: 279/422 | loss: 1.8817757368087769\n",
      "Epoch: 45/100 | step: 280/422 | loss: 1.26935875415802\n",
      "Epoch: 45/100 | step: 281/422 | loss: 1.3260252475738525\n",
      "Epoch: 45/100 | step: 282/422 | loss: 1.2623742818832397\n",
      "Epoch: 45/100 | step: 283/422 | loss: 1.7189757823944092\n",
      "Epoch: 45/100 | step: 284/422 | loss: 1.2632763385772705\n",
      "Epoch: 45/100 | step: 285/422 | loss: 1.5745656490325928\n",
      "Epoch: 45/100 | step: 286/422 | loss: 1.516135811805725\n",
      "Epoch: 45/100 | step: 287/422 | loss: 1.4694761037826538\n",
      "Epoch: 45/100 | step: 288/422 | loss: 1.4143508672714233\n",
      "Epoch: 45/100 | step: 289/422 | loss: 1.5369887351989746\n",
      "Epoch: 45/100 | step: 290/422 | loss: 1.8614164590835571\n",
      "Epoch: 45/100 | step: 291/422 | loss: 1.8107349872589111\n",
      "Epoch: 45/100 | step: 292/422 | loss: 1.4312443733215332\n",
      "Epoch: 45/100 | step: 293/422 | loss: 1.6422854661941528\n",
      "Epoch: 45/100 | step: 294/422 | loss: 1.193058729171753\n",
      "Epoch: 45/100 | step: 295/422 | loss: 1.393428921699524\n",
      "Epoch: 45/100 | step: 296/422 | loss: 1.938520908355713\n",
      "Epoch: 45/100 | step: 297/422 | loss: 1.6618237495422363\n",
      "Epoch: 45/100 | step: 298/422 | loss: 1.491801142692566\n",
      "Epoch: 45/100 | step: 299/422 | loss: 2.122138023376465\n",
      "Epoch: 45/100 | step: 300/422 | loss: 1.2715784311294556\n",
      "Epoch: 45/100 | step: 301/422 | loss: 1.5698318481445312\n",
      "Epoch: 45/100 | step: 302/422 | loss: 2.1203677654266357\n",
      "Epoch: 45/100 | step: 303/422 | loss: 1.5435134172439575\n",
      "Epoch: 45/100 | step: 304/422 | loss: 1.4713696241378784\n",
      "Epoch: 45/100 | step: 305/422 | loss: 1.218632698059082\n",
      "Epoch: 45/100 | step: 306/422 | loss: 1.1594336032867432\n",
      "Epoch: 45/100 | step: 307/422 | loss: 1.3430771827697754\n",
      "Epoch: 45/100 | step: 308/422 | loss: 1.3492753505706787\n",
      "Epoch: 45/100 | step: 309/422 | loss: 1.1508820056915283\n",
      "Epoch: 45/100 | step: 310/422 | loss: 1.2770837545394897\n",
      "Epoch: 45/100 | step: 311/422 | loss: 1.8585219383239746\n",
      "Epoch: 45/100 | step: 312/422 | loss: 1.7464309930801392\n",
      "Epoch: 45/100 | step: 313/422 | loss: 1.6238771677017212\n",
      "Epoch: 45/100 | step: 314/422 | loss: 1.2723281383514404\n",
      "Epoch: 45/100 | step: 315/422 | loss: 1.5245883464813232\n",
      "Epoch: 45/100 | step: 316/422 | loss: 1.0136812925338745\n",
      "Epoch: 45/100 | step: 317/422 | loss: 1.8959074020385742\n",
      "Epoch: 45/100 | step: 318/422 | loss: 1.4620907306671143\n",
      "Epoch: 45/100 | step: 319/422 | loss: 1.2837746143341064\n",
      "Epoch: 45/100 | step: 320/422 | loss: 1.1896418333053589\n",
      "Epoch: 45/100 | step: 321/422 | loss: 1.6519392728805542\n",
      "Epoch: 45/100 | step: 322/422 | loss: 1.293280839920044\n",
      "Epoch: 45/100 | step: 323/422 | loss: 1.7704459428787231\n",
      "Epoch: 45/100 | step: 324/422 | loss: 1.3850271701812744\n",
      "Epoch: 45/100 | step: 325/422 | loss: 1.4717575311660767\n",
      "Epoch: 45/100 | step: 326/422 | loss: 1.7968859672546387\n",
      "Epoch: 45/100 | step: 327/422 | loss: 0.8131240010261536\n",
      "Epoch: 45/100 | step: 328/422 | loss: 1.1135305166244507\n",
      "Epoch: 45/100 | step: 329/422 | loss: 1.3838404417037964\n",
      "Epoch: 45/100 | step: 330/422 | loss: 1.7143129110336304\n",
      "Epoch: 45/100 | step: 331/422 | loss: 1.0635493993759155\n",
      "Epoch: 45/100 | step: 332/422 | loss: 1.0169777870178223\n",
      "Epoch: 45/100 | step: 333/422 | loss: 1.1584210395812988\n",
      "Epoch: 45/100 | step: 334/422 | loss: 1.4599614143371582\n",
      "Epoch: 45/100 | step: 335/422 | loss: 1.2704613208770752\n",
      "Epoch: 45/100 | step: 336/422 | loss: 1.4560599327087402\n",
      "Epoch: 45/100 | step: 337/422 | loss: 1.4010905027389526\n",
      "Epoch: 45/100 | step: 338/422 | loss: 1.6443839073181152\n",
      "Epoch: 45/100 | step: 339/422 | loss: 1.1544300317764282\n",
      "Epoch: 45/100 | step: 340/422 | loss: 1.200528621673584\n",
      "Epoch: 45/100 | step: 341/422 | loss: 0.6712754964828491\n",
      "Epoch: 45/100 | step: 342/422 | loss: 1.1906208992004395\n",
      "Epoch: 45/100 | step: 343/422 | loss: 1.4960150718688965\n",
      "Epoch: 45/100 | step: 344/422 | loss: 1.9897457361221313\n",
      "Epoch: 45/100 | step: 345/422 | loss: 1.7357760667800903\n",
      "Epoch: 45/100 | step: 346/422 | loss: 1.7854286432266235\n",
      "Epoch: 45/100 | step: 347/422 | loss: 1.6044480800628662\n",
      "Epoch: 45/100 | step: 348/422 | loss: 1.54564368724823\n",
      "Epoch: 45/100 | step: 349/422 | loss: 1.6517730951309204\n",
      "Epoch: 45/100 | step: 350/422 | loss: 1.248782753944397\n",
      "Epoch: 45/100 | step: 351/422 | loss: 1.7520030736923218\n",
      "Epoch: 45/100 | step: 352/422 | loss: 1.2991900444030762\n",
      "Epoch: 45/100 | step: 353/422 | loss: 2.2376139163970947\n",
      "Epoch: 45/100 | step: 354/422 | loss: 1.489244818687439\n",
      "Epoch: 45/100 | step: 355/422 | loss: 1.7927674055099487\n",
      "Epoch: 45/100 | step: 356/422 | loss: 1.7392653226852417\n",
      "Epoch: 45/100 | step: 357/422 | loss: 1.4373879432678223\n",
      "Epoch: 45/100 | step: 358/422 | loss: 1.3127907514572144\n",
      "Epoch: 45/100 | step: 359/422 | loss: 1.7189686298370361\n",
      "Epoch: 45/100 | step: 360/422 | loss: 1.9293324947357178\n",
      "Epoch: 45/100 | step: 361/422 | loss: 1.2213712930679321\n",
      "Epoch: 45/100 | step: 362/422 | loss: 1.1653921604156494\n",
      "Epoch: 45/100 | step: 363/422 | loss: 1.836095929145813\n",
      "Epoch: 45/100 | step: 364/422 | loss: 1.7584326267242432\n",
      "Epoch: 45/100 | step: 365/422 | loss: 1.394978404045105\n",
      "Epoch: 45/100 | step: 366/422 | loss: 1.6120048761367798\n",
      "Epoch: 45/100 | step: 367/422 | loss: 1.3848202228546143\n",
      "Epoch: 45/100 | step: 368/422 | loss: 0.9514654278755188\n",
      "Epoch: 45/100 | step: 369/422 | loss: 1.318389892578125\n",
      "Epoch: 45/100 | step: 370/422 | loss: 1.7631235122680664\n",
      "Epoch: 45/100 | step: 371/422 | loss: 1.4923921823501587\n",
      "Epoch: 45/100 | step: 372/422 | loss: 1.5346897840499878\n",
      "Epoch: 45/100 | step: 373/422 | loss: 1.3374778032302856\n",
      "Epoch: 45/100 | step: 374/422 | loss: 1.8852423429489136\n",
      "Epoch: 45/100 | step: 375/422 | loss: 1.6458172798156738\n",
      "Epoch: 45/100 | step: 376/422 | loss: 1.333191990852356\n",
      "Epoch: 45/100 | step: 377/422 | loss: 1.9076213836669922\n",
      "Epoch: 45/100 | step: 378/422 | loss: 1.2873188257217407\n",
      "Epoch: 45/100 | step: 379/422 | loss: 1.3037408590316772\n",
      "Epoch: 45/100 | step: 380/422 | loss: 1.593651294708252\n",
      "Epoch: 45/100 | step: 381/422 | loss: 1.4617563486099243\n",
      "Epoch: 45/100 | step: 382/422 | loss: 1.265938401222229\n",
      "Epoch: 45/100 | step: 383/422 | loss: 1.070774793624878\n",
      "Epoch: 45/100 | step: 384/422 | loss: 1.4933475255966187\n",
      "Epoch: 45/100 | step: 385/422 | loss: 1.486416220664978\n",
      "Epoch: 45/100 | step: 386/422 | loss: 1.4272907972335815\n",
      "Epoch: 45/100 | step: 387/422 | loss: 1.3608094453811646\n",
      "Epoch: 45/100 | step: 388/422 | loss: 1.4388773441314697\n",
      "Epoch: 45/100 | step: 389/422 | loss: 1.189424753189087\n",
      "Epoch: 45/100 | step: 390/422 | loss: 1.174864649772644\n",
      "Epoch: 45/100 | step: 391/422 | loss: 1.118584394454956\n",
      "Epoch: 45/100 | step: 392/422 | loss: 1.5712448358535767\n",
      "Epoch: 45/100 | step: 393/422 | loss: 0.982630729675293\n",
      "Epoch: 45/100 | step: 394/422 | loss: 1.6808198690414429\n",
      "Epoch: 45/100 | step: 395/422 | loss: 1.234086275100708\n",
      "Epoch: 45/100 | step: 396/422 | loss: 1.6799548864364624\n",
      "Epoch: 45/100 | step: 397/422 | loss: 1.4043182134628296\n",
      "Epoch: 45/100 | step: 398/422 | loss: 1.5514904260635376\n",
      "Epoch: 45/100 | step: 399/422 | loss: 2.310114860534668\n",
      "Epoch: 45/100 | step: 400/422 | loss: 1.1797536611557007\n",
      "Epoch: 45/100 | step: 401/422 | loss: 1.1514359712600708\n",
      "Epoch: 45/100 | step: 402/422 | loss: 1.4794784784317017\n",
      "Epoch: 45/100 | step: 403/422 | loss: 1.5590295791625977\n",
      "Epoch: 45/100 | step: 404/422 | loss: 1.6601557731628418\n",
      "Epoch: 45/100 | step: 405/422 | loss: 1.8432080745697021\n",
      "Epoch: 45/100 | step: 406/422 | loss: 1.8711880445480347\n",
      "Epoch: 45/100 | step: 407/422 | loss: 2.0862433910369873\n",
      "Epoch: 45/100 | step: 408/422 | loss: 2.138739824295044\n",
      "Epoch: 45/100 | step: 409/422 | loss: 1.2967512607574463\n",
      "Epoch: 45/100 | step: 410/422 | loss: 1.5886378288269043\n",
      "Epoch: 45/100 | step: 411/422 | loss: 1.6128414869308472\n",
      "Epoch: 45/100 | step: 412/422 | loss: 1.177351951599121\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 46/100 | step: 1/422 | loss: 1.4422070980072021\n",
      "Epoch: 46/100 | step: 2/422 | loss: 1.1479967832565308\n",
      "Epoch: 46/100 | step: 3/422 | loss: 1.2369284629821777\n",
      "Epoch: 46/100 | step: 4/422 | loss: 1.2975549697875977\n",
      "Epoch: 46/100 | step: 5/422 | loss: 0.9548243880271912\n",
      "Epoch: 46/100 | step: 6/422 | loss: 1.1990184783935547\n",
      "Epoch: 46/100 | step: 7/422 | loss: 1.0940783023834229\n",
      "Epoch: 46/100 | step: 8/422 | loss: 0.9040439128875732\n",
      "Epoch: 46/100 | step: 9/422 | loss: 1.409525990486145\n",
      "Epoch: 46/100 | step: 10/422 | loss: 1.5990898609161377\n",
      "Epoch: 46/100 | step: 11/422 | loss: 1.2561249732971191\n",
      "Epoch: 46/100 | step: 12/422 | loss: 1.1147990226745605\n",
      "Epoch: 46/100 | step: 13/422 | loss: 1.2024253606796265\n",
      "Epoch: 46/100 | step: 14/422 | loss: 1.1950554847717285\n",
      "Epoch: 46/100 | step: 15/422 | loss: 1.2086174488067627\n",
      "Epoch: 46/100 | step: 16/422 | loss: 0.9556723833084106\n",
      "Epoch: 46/100 | step: 17/422 | loss: 0.8123981952667236\n",
      "Epoch: 46/100 | step: 18/422 | loss: 1.5109318494796753\n",
      "Epoch: 46/100 | step: 19/422 | loss: 1.5464026927947998\n",
      "Epoch: 46/100 | step: 20/422 | loss: 1.0409635305404663\n",
      "Epoch: 46/100 | step: 21/422 | loss: 0.9258549213409424\n",
      "Epoch: 46/100 | step: 22/422 | loss: 1.0167007446289062\n",
      "Epoch: 46/100 | step: 23/422 | loss: 1.0121155977249146\n",
      "Epoch: 46/100 | step: 24/422 | loss: 0.9076176285743713\n",
      "Epoch: 46/100 | step: 25/422 | loss: 1.172183632850647\n",
      "Epoch: 46/100 | step: 26/422 | loss: 1.540956974029541\n",
      "Epoch: 46/100 | step: 27/422 | loss: 1.3572371006011963\n",
      "Epoch: 46/100 | step: 28/422 | loss: 1.0357568264007568\n",
      "Epoch: 46/100 | step: 29/422 | loss: 1.144897222518921\n",
      "Epoch: 46/100 | step: 30/422 | loss: 1.557588815689087\n",
      "Epoch: 46/100 | step: 31/422 | loss: 1.212278127670288\n",
      "Epoch: 46/100 | step: 32/422 | loss: 0.914588212966919\n",
      "Epoch: 46/100 | step: 33/422 | loss: 0.9672118425369263\n",
      "Epoch: 46/100 | step: 34/422 | loss: 1.6478785276412964\n",
      "Epoch: 46/100 | step: 35/422 | loss: 1.3158652782440186\n",
      "Epoch: 46/100 | step: 36/422 | loss: 0.9616585373878479\n",
      "Epoch: 46/100 | step: 37/422 | loss: 1.1401335000991821\n",
      "Epoch: 46/100 | step: 38/422 | loss: 1.2410013675689697\n",
      "Epoch: 46/100 | step: 39/422 | loss: 1.6089727878570557\n",
      "Epoch: 46/100 | step: 40/422 | loss: 1.3281912803649902\n",
      "Epoch: 46/100 | step: 41/422 | loss: 1.1276438236236572\n",
      "Epoch: 46/100 | step: 42/422 | loss: 1.4818795919418335\n",
      "Epoch: 46/100 | step: 43/422 | loss: 1.1081379652023315\n",
      "Epoch: 46/100 | step: 44/422 | loss: 1.2401096820831299\n",
      "Epoch: 46/100 | step: 45/422 | loss: 1.029078483581543\n",
      "Epoch: 46/100 | step: 46/422 | loss: 1.264350414276123\n",
      "Epoch: 46/100 | step: 47/422 | loss: 1.1700059175491333\n",
      "Epoch: 46/100 | step: 48/422 | loss: 1.2228671312332153\n",
      "Epoch: 46/100 | step: 49/422 | loss: 1.4218525886535645\n",
      "Epoch: 46/100 | step: 50/422 | loss: 1.338036060333252\n",
      "Epoch: 46/100 | step: 51/422 | loss: 1.1157681941986084\n",
      "Epoch: 46/100 | step: 52/422 | loss: 1.089156985282898\n",
      "Epoch: 46/100 | step: 53/422 | loss: 1.6160888671875\n",
      "Epoch: 46/100 | step: 54/422 | loss: 1.435622215270996\n",
      "Epoch: 46/100 | step: 55/422 | loss: 1.1035895347595215\n",
      "Epoch: 46/100 | step: 56/422 | loss: 1.1053986549377441\n",
      "Epoch: 46/100 | step: 57/422 | loss: 1.1844139099121094\n",
      "Epoch: 46/100 | step: 58/422 | loss: 1.421109914779663\n",
      "Epoch: 46/100 | step: 59/422 | loss: 1.0340694189071655\n",
      "Epoch: 46/100 | step: 60/422 | loss: 1.191480278968811\n",
      "Epoch: 46/100 | step: 61/422 | loss: 1.391806721687317\n",
      "Epoch: 46/100 | step: 62/422 | loss: 1.4917585849761963\n",
      "Epoch: 46/100 | step: 63/422 | loss: 0.9480687975883484\n",
      "Epoch: 46/100 | step: 64/422 | loss: 0.9673903584480286\n",
      "Epoch: 46/100 | step: 65/422 | loss: 1.2874884605407715\n",
      "Epoch: 46/100 | step: 66/422 | loss: 0.8818351030349731\n",
      "Epoch: 46/100 | step: 67/422 | loss: 0.6836820244789124\n",
      "Epoch: 46/100 | step: 68/422 | loss: 1.1429939270019531\n",
      "Epoch: 46/100 | step: 69/422 | loss: 0.9209538698196411\n",
      "Epoch: 46/100 | step: 70/422 | loss: 1.2243777513504028\n",
      "Epoch: 46/100 | step: 71/422 | loss: 1.4677931070327759\n",
      "Epoch: 46/100 | step: 72/422 | loss: 1.2333157062530518\n",
      "Epoch: 46/100 | step: 73/422 | loss: 1.0361603498458862\n",
      "Epoch: 46/100 | step: 74/422 | loss: 1.366572618484497\n",
      "Epoch: 46/100 | step: 75/422 | loss: 1.8890745639801025\n",
      "Epoch: 46/100 | step: 76/422 | loss: 1.005864143371582\n",
      "Epoch: 46/100 | step: 77/422 | loss: 1.6050227880477905\n",
      "Epoch: 46/100 | step: 78/422 | loss: 1.2922004461288452\n",
      "Epoch: 46/100 | step: 79/422 | loss: 1.3803199529647827\n",
      "Epoch: 46/100 | step: 80/422 | loss: 1.4838778972625732\n",
      "Epoch: 46/100 | step: 81/422 | loss: 1.5251902341842651\n",
      "Epoch: 46/100 | step: 82/422 | loss: 2.1120736598968506\n",
      "Epoch: 46/100 | step: 83/422 | loss: 1.4823397397994995\n",
      "Epoch: 46/100 | step: 84/422 | loss: 0.7144171595573425\n",
      "Epoch: 46/100 | step: 85/422 | loss: 1.0915234088897705\n",
      "Epoch: 46/100 | step: 86/422 | loss: 1.3718554973602295\n",
      "Epoch: 46/100 | step: 87/422 | loss: 1.0027222633361816\n",
      "Epoch: 46/100 | step: 88/422 | loss: 1.5223934650421143\n",
      "Epoch: 46/100 | step: 89/422 | loss: 1.1569130420684814\n",
      "Epoch: 46/100 | step: 90/422 | loss: 1.4270215034484863\n",
      "Epoch: 46/100 | step: 91/422 | loss: 1.8233915567398071\n",
      "Epoch: 46/100 | step: 92/422 | loss: 1.332574725151062\n",
      "Epoch: 46/100 | step: 93/422 | loss: 1.2436740398406982\n",
      "Epoch: 46/100 | step: 94/422 | loss: 1.0846641063690186\n",
      "Epoch: 46/100 | step: 95/422 | loss: 0.921685516834259\n",
      "Epoch: 46/100 | step: 96/422 | loss: 1.0202810764312744\n",
      "Epoch: 46/100 | step: 97/422 | loss: 1.2508258819580078\n",
      "Epoch: 46/100 | step: 98/422 | loss: 0.9792079329490662\n",
      "Epoch: 46/100 | step: 99/422 | loss: 1.5807678699493408\n",
      "Epoch: 46/100 | step: 100/422 | loss: 1.0038161277770996\n",
      "Epoch: 46/100 | step: 101/422 | loss: 1.2167508602142334\n",
      "Epoch: 46/100 | step: 102/422 | loss: 1.0823254585266113\n",
      "Epoch: 46/100 | step: 103/422 | loss: 1.5963997840881348\n",
      "Epoch: 46/100 | step: 104/422 | loss: 1.4785659313201904\n",
      "Epoch: 46/100 | step: 105/422 | loss: 1.0550507307052612\n",
      "Epoch: 46/100 | step: 106/422 | loss: 1.245012879371643\n",
      "Epoch: 46/100 | step: 107/422 | loss: 1.3819561004638672\n",
      "Epoch: 46/100 | step: 108/422 | loss: 1.1995925903320312\n",
      "Epoch: 46/100 | step: 109/422 | loss: 1.2060027122497559\n",
      "Epoch: 46/100 | step: 110/422 | loss: 1.2278708219528198\n",
      "Epoch: 46/100 | step: 111/422 | loss: 1.0644994974136353\n",
      "Epoch: 46/100 | step: 112/422 | loss: 0.9106382727622986\n",
      "Epoch: 46/100 | step: 113/422 | loss: 1.4797229766845703\n",
      "Epoch: 46/100 | step: 114/422 | loss: 1.3126438856124878\n",
      "Epoch: 46/100 | step: 115/422 | loss: 1.3474633693695068\n",
      "Epoch: 46/100 | step: 116/422 | loss: 1.3292746543884277\n",
      "Epoch: 46/100 | step: 117/422 | loss: 1.2122300863265991\n",
      "Epoch: 46/100 | step: 118/422 | loss: 1.1403909921646118\n",
      "Epoch: 46/100 | step: 119/422 | loss: 1.2167021036148071\n",
      "Epoch: 46/100 | step: 120/422 | loss: 1.1952403783798218\n",
      "Epoch: 46/100 | step: 121/422 | loss: 1.0329769849777222\n",
      "Epoch: 46/100 | step: 122/422 | loss: 1.3576836585998535\n",
      "Epoch: 46/100 | step: 123/422 | loss: 0.7119627594947815\n",
      "Epoch: 46/100 | step: 124/422 | loss: 1.1520438194274902\n",
      "Epoch: 46/100 | step: 125/422 | loss: 1.2038823366165161\n",
      "Epoch: 46/100 | step: 126/422 | loss: 0.8734819293022156\n",
      "Epoch: 46/100 | step: 127/422 | loss: 1.0998716354370117\n",
      "Epoch: 46/100 | step: 128/422 | loss: 0.9955224990844727\n",
      "Epoch: 46/100 | step: 129/422 | loss: 1.3172279596328735\n",
      "Epoch: 46/100 | step: 130/422 | loss: 1.2449710369110107\n",
      "Epoch: 46/100 | step: 131/422 | loss: 1.7797644138336182\n",
      "Epoch: 46/100 | step: 132/422 | loss: 1.2925546169281006\n",
      "Epoch: 46/100 | step: 133/422 | loss: 1.7582483291625977\n",
      "Epoch: 46/100 | step: 134/422 | loss: 1.2834129333496094\n",
      "Epoch: 46/100 | step: 135/422 | loss: 1.4932548999786377\n",
      "Epoch: 46/100 | step: 136/422 | loss: 0.8991013765335083\n",
      "Epoch: 46/100 | step: 137/422 | loss: 1.5628522634506226\n",
      "Epoch: 46/100 | step: 138/422 | loss: 1.7197210788726807\n",
      "Epoch: 46/100 | step: 139/422 | loss: 1.0435487031936646\n",
      "Epoch: 46/100 | step: 140/422 | loss: 1.2917855978012085\n",
      "Epoch: 46/100 | step: 141/422 | loss: 1.2485909461975098\n",
      "Epoch: 46/100 | step: 142/422 | loss: 1.5523114204406738\n",
      "Epoch: 46/100 | step: 143/422 | loss: 1.2515939474105835\n",
      "Epoch: 46/100 | step: 144/422 | loss: 1.655776023864746\n",
      "Epoch: 46/100 | step: 145/422 | loss: 1.647566556930542\n",
      "Epoch: 46/100 | step: 146/422 | loss: 1.0292043685913086\n",
      "Epoch: 46/100 | step: 147/422 | loss: 1.0332856178283691\n",
      "Epoch: 46/100 | step: 148/422 | loss: 1.288199543952942\n",
      "Epoch: 46/100 | step: 149/422 | loss: 1.1860973834991455\n",
      "Epoch: 46/100 | step: 150/422 | loss: 1.5138087272644043\n",
      "Epoch: 46/100 | step: 151/422 | loss: 1.1723326444625854\n",
      "Epoch: 46/100 | step: 152/422 | loss: 1.3798632621765137\n",
      "Epoch: 46/100 | step: 153/422 | loss: 1.1769503355026245\n",
      "Epoch: 46/100 | step: 154/422 | loss: 1.0236918926239014\n",
      "Epoch: 46/100 | step: 155/422 | loss: 1.4472910165786743\n",
      "Epoch: 46/100 | step: 156/422 | loss: 1.4330154657363892\n",
      "Epoch: 46/100 | step: 157/422 | loss: 1.2753547430038452\n",
      "Epoch: 46/100 | step: 158/422 | loss: 1.2735499143600464\n",
      "Epoch: 46/100 | step: 159/422 | loss: 0.9948077201843262\n",
      "Epoch: 46/100 | step: 160/422 | loss: 1.1367719173431396\n",
      "Epoch: 46/100 | step: 161/422 | loss: 1.5166000127792358\n",
      "Epoch: 46/100 | step: 162/422 | loss: 1.8233636617660522\n",
      "Epoch: 46/100 | step: 163/422 | loss: 1.1493098735809326\n",
      "Epoch: 46/100 | step: 164/422 | loss: 1.188802719116211\n",
      "Epoch: 46/100 | step: 165/422 | loss: 1.20978581905365\n",
      "Epoch: 46/100 | step: 166/422 | loss: 1.2761069536209106\n",
      "Epoch: 46/100 | step: 167/422 | loss: 1.8990968465805054\n",
      "Epoch: 46/100 | step: 168/422 | loss: 1.362922191619873\n",
      "Epoch: 46/100 | step: 169/422 | loss: 1.2689567804336548\n",
      "Epoch: 46/100 | step: 170/422 | loss: 1.4631632566452026\n",
      "Epoch: 46/100 | step: 171/422 | loss: 1.162956953048706\n",
      "Epoch: 46/100 | step: 172/422 | loss: 1.4874435663223267\n",
      "Epoch: 46/100 | step: 173/422 | loss: 1.337357521057129\n",
      "Epoch: 46/100 | step: 174/422 | loss: 1.3571298122406006\n",
      "Epoch: 46/100 | step: 175/422 | loss: 1.5890473127365112\n",
      "Epoch: 46/100 | step: 176/422 | loss: 1.0844377279281616\n",
      "Epoch: 46/100 | step: 177/422 | loss: 1.819180965423584\n",
      "Epoch: 46/100 | step: 178/422 | loss: 0.8108296990394592\n",
      "Epoch: 46/100 | step: 179/422 | loss: 1.1899018287658691\n",
      "Epoch: 46/100 | step: 180/422 | loss: 1.001379370689392\n",
      "Epoch: 46/100 | step: 181/422 | loss: 0.7924826145172119\n",
      "Epoch: 46/100 | step: 182/422 | loss: 1.1131513118743896\n",
      "Epoch: 46/100 | step: 183/422 | loss: 1.911904215812683\n",
      "Epoch: 46/100 | step: 184/422 | loss: 1.6650959253311157\n",
      "Epoch: 46/100 | step: 185/422 | loss: 1.3674590587615967\n",
      "Epoch: 46/100 | step: 186/422 | loss: 1.0420482158660889\n",
      "Epoch: 46/100 | step: 187/422 | loss: 0.9893403053283691\n",
      "Epoch: 46/100 | step: 188/422 | loss: 1.6411672830581665\n",
      "Epoch: 46/100 | step: 189/422 | loss: 2.3726866245269775\n",
      "Epoch: 46/100 | step: 190/422 | loss: 0.9200108051300049\n",
      "Epoch: 46/100 | step: 191/422 | loss: 1.1825282573699951\n",
      "Epoch: 46/100 | step: 192/422 | loss: 1.6255006790161133\n",
      "Epoch: 46/100 | step: 193/422 | loss: 1.4058219194412231\n",
      "Epoch: 46/100 | step: 194/422 | loss: 1.2723817825317383\n",
      "Epoch: 46/100 | step: 195/422 | loss: 1.2145845890045166\n",
      "Epoch: 46/100 | step: 196/422 | loss: 1.2128435373306274\n",
      "Epoch: 46/100 | step: 197/422 | loss: 1.3326752185821533\n",
      "Epoch: 46/100 | step: 198/422 | loss: 0.9123418927192688\n",
      "Epoch: 46/100 | step: 199/422 | loss: 1.1648719310760498\n",
      "Epoch: 46/100 | step: 200/422 | loss: 1.1513267755508423\n",
      "Epoch: 46/100 | step: 201/422 | loss: 1.2986961603164673\n",
      "Epoch: 46/100 | step: 202/422 | loss: 1.6085782051086426\n",
      "Epoch: 46/100 | step: 203/422 | loss: 1.6315664052963257\n",
      "Epoch: 46/100 | step: 204/422 | loss: 1.5516997575759888\n",
      "Epoch: 46/100 | step: 205/422 | loss: 1.2345911264419556\n",
      "Epoch: 46/100 | step: 206/422 | loss: 1.4633616209030151\n",
      "Epoch: 46/100 | step: 207/422 | loss: 1.3580583333969116\n",
      "Epoch: 46/100 | step: 208/422 | loss: 1.5445778369903564\n",
      "Epoch: 46/100 | step: 209/422 | loss: 1.237326979637146\n",
      "Epoch: 46/100 | step: 210/422 | loss: 1.2522648572921753\n",
      "Epoch: 46/100 | step: 211/422 | loss: 1.0126519203186035\n",
      "Epoch: 46/100 | step: 212/422 | loss: 1.4722415208816528\n",
      "Epoch: 46/100 | step: 213/422 | loss: 1.6431457996368408\n",
      "Epoch: 46/100 | step: 214/422 | loss: 1.3848484754562378\n",
      "Epoch: 46/100 | step: 215/422 | loss: 1.4147803783416748\n",
      "Epoch: 46/100 | step: 216/422 | loss: 1.3589130640029907\n",
      "Epoch: 46/100 | step: 217/422 | loss: 2.4541869163513184\n",
      "Epoch: 46/100 | step: 218/422 | loss: 0.9768824577331543\n",
      "Epoch: 46/100 | step: 219/422 | loss: 1.735787034034729\n",
      "Epoch: 46/100 | step: 220/422 | loss: 1.1365463733673096\n",
      "Epoch: 46/100 | step: 221/422 | loss: 1.1116963624954224\n",
      "Epoch: 46/100 | step: 222/422 | loss: 1.8827639818191528\n",
      "Epoch: 46/100 | step: 223/422 | loss: 1.4895821809768677\n",
      "Epoch: 46/100 | step: 224/422 | loss: 1.708349585533142\n",
      "Epoch: 46/100 | step: 225/422 | loss: 1.094864010810852\n",
      "Epoch: 46/100 | step: 226/422 | loss: 1.0624840259552002\n",
      "Epoch: 46/100 | step: 227/422 | loss: 1.6752756834030151\n",
      "Epoch: 46/100 | step: 228/422 | loss: 1.1683295965194702\n",
      "Epoch: 46/100 | step: 229/422 | loss: 1.4227685928344727\n",
      "Epoch: 46/100 | step: 230/422 | loss: 1.3262202739715576\n",
      "Epoch: 46/100 | step: 231/422 | loss: 1.3833208084106445\n",
      "Epoch: 46/100 | step: 232/422 | loss: 1.3616946935653687\n",
      "Epoch: 46/100 | step: 233/422 | loss: 1.359702706336975\n",
      "Epoch: 46/100 | step: 234/422 | loss: 0.9720392823219299\n",
      "Epoch: 46/100 | step: 235/422 | loss: 1.4427082538604736\n",
      "Epoch: 46/100 | step: 236/422 | loss: 0.9251757264137268\n",
      "Epoch: 46/100 | step: 237/422 | loss: 1.475110650062561\n",
      "Epoch: 46/100 | step: 238/422 | loss: 1.5622715950012207\n",
      "Epoch: 46/100 | step: 239/422 | loss: 1.4723514318466187\n",
      "Epoch: 46/100 | step: 240/422 | loss: 1.4761079549789429\n",
      "Epoch: 46/100 | step: 241/422 | loss: 1.1698247194290161\n",
      "Epoch: 46/100 | step: 242/422 | loss: 1.4342819452285767\n",
      "Epoch: 46/100 | step: 243/422 | loss: 1.488574743270874\n",
      "Epoch: 46/100 | step: 244/422 | loss: 1.6821438074111938\n",
      "Epoch: 46/100 | step: 245/422 | loss: 1.7903035879135132\n",
      "Epoch: 46/100 | step: 246/422 | loss: 1.3164944648742676\n",
      "Epoch: 46/100 | step: 247/422 | loss: 1.4110493659973145\n",
      "Epoch: 46/100 | step: 248/422 | loss: 1.4732328653335571\n",
      "Epoch: 46/100 | step: 249/422 | loss: 0.9602968096733093\n",
      "Epoch: 46/100 | step: 250/422 | loss: 1.483386516571045\n",
      "Epoch: 46/100 | step: 251/422 | loss: 1.4558570384979248\n",
      "Epoch: 46/100 | step: 252/422 | loss: 1.4270879030227661\n",
      "Epoch: 46/100 | step: 253/422 | loss: 1.2803863286972046\n",
      "Epoch: 46/100 | step: 254/422 | loss: 1.1840351819992065\n",
      "Epoch: 46/100 | step: 255/422 | loss: 1.3407856225967407\n",
      "Epoch: 46/100 | step: 256/422 | loss: 1.3750678300857544\n",
      "Epoch: 46/100 | step: 257/422 | loss: 1.9562008380889893\n",
      "Epoch: 46/100 | step: 258/422 | loss: 1.3601272106170654\n",
      "Epoch: 46/100 | step: 259/422 | loss: 1.3617627620697021\n",
      "Epoch: 46/100 | step: 260/422 | loss: 1.5255568027496338\n",
      "Epoch: 46/100 | step: 261/422 | loss: 1.275041103363037\n",
      "Epoch: 46/100 | step: 262/422 | loss: 1.7866301536560059\n",
      "Epoch: 46/100 | step: 263/422 | loss: 1.2291420698165894\n",
      "Epoch: 46/100 | step: 264/422 | loss: 1.8429679870605469\n",
      "Epoch: 46/100 | step: 265/422 | loss: 1.7778314352035522\n",
      "Epoch: 46/100 | step: 266/422 | loss: 1.1242073774337769\n",
      "Epoch: 46/100 | step: 267/422 | loss: 1.397265911102295\n",
      "Epoch: 46/100 | step: 268/422 | loss: 1.4629212617874146\n",
      "Epoch: 46/100 | step: 269/422 | loss: 1.2176735401153564\n",
      "Epoch: 46/100 | step: 270/422 | loss: 1.5936228036880493\n",
      "Epoch: 46/100 | step: 271/422 | loss: 1.6861839294433594\n",
      "Epoch: 46/100 | step: 272/422 | loss: 1.1451971530914307\n",
      "Epoch: 46/100 | step: 273/422 | loss: 1.146282434463501\n",
      "Epoch: 46/100 | step: 274/422 | loss: 1.581152081489563\n",
      "Epoch: 46/100 | step: 275/422 | loss: 1.2913395166397095\n",
      "Epoch: 46/100 | step: 276/422 | loss: 1.4786899089813232\n",
      "Epoch: 46/100 | step: 277/422 | loss: 1.631291389465332\n",
      "Epoch: 46/100 | step: 278/422 | loss: 1.7214678525924683\n",
      "Epoch: 46/100 | step: 279/422 | loss: 1.5711978673934937\n",
      "Epoch: 46/100 | step: 280/422 | loss: 1.8066058158874512\n",
      "Epoch: 46/100 | step: 281/422 | loss: 1.2409361600875854\n",
      "Epoch: 46/100 | step: 282/422 | loss: 1.4096157550811768\n",
      "Epoch: 46/100 | step: 283/422 | loss: 1.3567696809768677\n",
      "Epoch: 46/100 | step: 284/422 | loss: 1.1989668607711792\n",
      "Epoch: 46/100 | step: 285/422 | loss: 1.1148449182510376\n",
      "Epoch: 46/100 | step: 286/422 | loss: 1.1985613107681274\n",
      "Epoch: 46/100 | step: 287/422 | loss: 0.8117653727531433\n",
      "Epoch: 46/100 | step: 288/422 | loss: 1.1504926681518555\n",
      "Epoch: 46/100 | step: 289/422 | loss: 0.9407957196235657\n",
      "Epoch: 46/100 | step: 290/422 | loss: 1.314756989479065\n",
      "Epoch: 46/100 | step: 291/422 | loss: 1.3202654123306274\n",
      "Epoch: 46/100 | step: 292/422 | loss: 1.1695195436477661\n",
      "Epoch: 46/100 | step: 293/422 | loss: 1.8587453365325928\n",
      "Epoch: 46/100 | step: 294/422 | loss: 1.8742024898529053\n",
      "Epoch: 46/100 | step: 295/422 | loss: 1.2706224918365479\n",
      "Epoch: 46/100 | step: 296/422 | loss: 1.381677508354187\n",
      "Epoch: 46/100 | step: 297/422 | loss: 1.4294384717941284\n",
      "Epoch: 46/100 | step: 298/422 | loss: 1.4175037145614624\n",
      "Epoch: 46/100 | step: 299/422 | loss: 1.2516944408416748\n",
      "Epoch: 46/100 | step: 300/422 | loss: 1.605201005935669\n",
      "Epoch: 46/100 | step: 301/422 | loss: 1.2625195980072021\n",
      "Epoch: 46/100 | step: 302/422 | loss: 1.1276620626449585\n",
      "Epoch: 46/100 | step: 303/422 | loss: 1.3084564208984375\n",
      "Epoch: 46/100 | step: 304/422 | loss: 1.347634196281433\n",
      "Epoch: 46/100 | step: 305/422 | loss: 1.8275060653686523\n",
      "Epoch: 46/100 | step: 306/422 | loss: 0.9483962655067444\n",
      "Epoch: 46/100 | step: 307/422 | loss: 1.329591155052185\n",
      "Epoch: 46/100 | step: 308/422 | loss: 0.8250630497932434\n",
      "Epoch: 46/100 | step: 309/422 | loss: 0.8788422346115112\n",
      "Epoch: 46/100 | step: 310/422 | loss: 1.1990063190460205\n",
      "Epoch: 46/100 | step: 311/422 | loss: 1.3326562643051147\n",
      "Epoch: 46/100 | step: 312/422 | loss: 1.4394936561584473\n",
      "Epoch: 46/100 | step: 313/422 | loss: 1.289376974105835\n",
      "Epoch: 46/100 | step: 314/422 | loss: 1.1498537063598633\n",
      "Epoch: 46/100 | step: 315/422 | loss: 1.3233743906021118\n",
      "Epoch: 46/100 | step: 316/422 | loss: 1.5568445920944214\n",
      "Epoch: 46/100 | step: 317/422 | loss: 0.8492060899734497\n",
      "Epoch: 46/100 | step: 318/422 | loss: 0.9423113465309143\n",
      "Epoch: 46/100 | step: 319/422 | loss: 1.2242720127105713\n",
      "Epoch: 46/100 | step: 320/422 | loss: 1.3701306581497192\n",
      "Epoch: 46/100 | step: 321/422 | loss: 1.1587456464767456\n",
      "Epoch: 46/100 | step: 322/422 | loss: 1.0949774980545044\n",
      "Epoch: 46/100 | step: 323/422 | loss: 1.0644092559814453\n",
      "Epoch: 46/100 | step: 324/422 | loss: 1.3290432691574097\n",
      "Epoch: 46/100 | step: 325/422 | loss: 1.2659801244735718\n",
      "Epoch: 46/100 | step: 326/422 | loss: 1.0127800703048706\n",
      "Epoch: 46/100 | step: 327/422 | loss: 1.6640690565109253\n",
      "Epoch: 46/100 | step: 328/422 | loss: 1.5991321802139282\n",
      "Epoch: 46/100 | step: 329/422 | loss: 1.4965219497680664\n",
      "Epoch: 46/100 | step: 330/422 | loss: 1.2862865924835205\n",
      "Epoch: 46/100 | step: 331/422 | loss: 1.6890205144882202\n",
      "Epoch: 46/100 | step: 332/422 | loss: 1.2384698390960693\n",
      "Epoch: 46/100 | step: 333/422 | loss: 1.8025363683700562\n",
      "Epoch: 46/100 | step: 334/422 | loss: 1.5212100744247437\n",
      "Epoch: 46/100 | step: 335/422 | loss: 0.9663363099098206\n",
      "Epoch: 46/100 | step: 336/422 | loss: 1.4716839790344238\n",
      "Epoch: 46/100 | step: 337/422 | loss: 1.274672269821167\n",
      "Epoch: 46/100 | step: 338/422 | loss: 1.229073405265808\n",
      "Epoch: 46/100 | step: 339/422 | loss: 1.2717094421386719\n",
      "Epoch: 46/100 | step: 340/422 | loss: 1.2248809337615967\n",
      "Epoch: 46/100 | step: 341/422 | loss: 1.2318994998931885\n",
      "Epoch: 46/100 | step: 342/422 | loss: 1.3553861379623413\n",
      "Epoch: 46/100 | step: 343/422 | loss: 1.4086456298828125\n",
      "Epoch: 46/100 | step: 344/422 | loss: 1.1797349452972412\n",
      "Epoch: 46/100 | step: 345/422 | loss: 1.0274410247802734\n",
      "Epoch: 46/100 | step: 346/422 | loss: 1.3272778987884521\n",
      "Epoch: 46/100 | step: 347/422 | loss: 1.6159474849700928\n",
      "Epoch: 46/100 | step: 348/422 | loss: 1.1956700086593628\n",
      "Epoch: 46/100 | step: 349/422 | loss: 1.380724310874939\n",
      "Epoch: 46/100 | step: 350/422 | loss: 1.273422122001648\n",
      "Epoch: 46/100 | step: 351/422 | loss: 1.2645299434661865\n",
      "Epoch: 46/100 | step: 352/422 | loss: 1.286497950553894\n",
      "Epoch: 46/100 | step: 353/422 | loss: 1.0381567478179932\n",
      "Epoch: 46/100 | step: 354/422 | loss: 1.567248821258545\n",
      "Epoch: 46/100 | step: 355/422 | loss: 1.821183681488037\n",
      "Epoch: 46/100 | step: 356/422 | loss: 1.750872015953064\n",
      "Epoch: 46/100 | step: 357/422 | loss: 1.0633693933486938\n",
      "Epoch: 46/100 | step: 358/422 | loss: 1.3315589427947998\n",
      "Epoch: 46/100 | step: 359/422 | loss: 1.0006691217422485\n",
      "Epoch: 46/100 | step: 360/422 | loss: 1.8045281171798706\n",
      "Epoch: 46/100 | step: 361/422 | loss: 1.8543645143508911\n",
      "Epoch: 46/100 | step: 362/422 | loss: 1.5120854377746582\n",
      "Epoch: 46/100 | step: 363/422 | loss: 1.358390212059021\n",
      "Epoch: 46/100 | step: 364/422 | loss: 1.490303874015808\n",
      "Epoch: 46/100 | step: 365/422 | loss: 1.5659973621368408\n",
      "Epoch: 46/100 | step: 366/422 | loss: 1.4522514343261719\n",
      "Epoch: 46/100 | step: 367/422 | loss: 1.60628080368042\n",
      "Epoch: 46/100 | step: 368/422 | loss: 1.432610273361206\n",
      "Epoch: 46/100 | step: 369/422 | loss: 1.7308049201965332\n",
      "Epoch: 46/100 | step: 370/422 | loss: 1.3930754661560059\n",
      "Epoch: 46/100 | step: 371/422 | loss: 1.2231353521347046\n",
      "Epoch: 46/100 | step: 372/422 | loss: 1.2526118755340576\n",
      "Epoch: 46/100 | step: 373/422 | loss: 1.335784673690796\n",
      "Epoch: 46/100 | step: 374/422 | loss: 1.3295804262161255\n",
      "Epoch: 46/100 | step: 375/422 | loss: 0.9093776345252991\n",
      "Epoch: 46/100 | step: 376/422 | loss: 1.9494699239730835\n",
      "Epoch: 46/100 | step: 377/422 | loss: 0.8554688692092896\n",
      "Epoch: 46/100 | step: 378/422 | loss: 1.5121928453445435\n",
      "Epoch: 46/100 | step: 379/422 | loss: 1.1222317218780518\n",
      "Epoch: 46/100 | step: 380/422 | loss: 1.325722098350525\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 47/100 | step: 1/422 | loss: 1.196784257888794\n",
      "Epoch: 47/100 | step: 2/422 | loss: 1.2502546310424805\n",
      "Epoch: 47/100 | step: 3/422 | loss: 1.2111369371414185\n",
      "Epoch: 47/100 | step: 4/422 | loss: 0.9011556506156921\n",
      "Epoch: 47/100 | step: 5/422 | loss: 1.6513285636901855\n",
      "Epoch: 47/100 | step: 6/422 | loss: 1.5734649896621704\n",
      "Epoch: 47/100 | step: 7/422 | loss: 0.9784443378448486\n",
      "Epoch: 47/100 | step: 8/422 | loss: 1.6709356307983398\n",
      "Epoch: 47/100 | step: 9/422 | loss: 0.9800649285316467\n",
      "Epoch: 47/100 | step: 10/422 | loss: 0.9081661105155945\n",
      "Epoch: 47/100 | step: 11/422 | loss: 0.8410281538963318\n",
      "Epoch: 47/100 | step: 12/422 | loss: 0.917340874671936\n",
      "Epoch: 47/100 | step: 13/422 | loss: 1.0412821769714355\n",
      "Epoch: 47/100 | step: 14/422 | loss: 1.2617740631103516\n",
      "Epoch: 47/100 | step: 15/422 | loss: 0.9297285079956055\n",
      "Epoch: 47/100 | step: 16/422 | loss: 1.243380069732666\n",
      "Epoch: 47/100 | step: 17/422 | loss: 1.2841994762420654\n",
      "Epoch: 47/100 | step: 18/422 | loss: 1.3317317962646484\n",
      "Epoch: 47/100 | step: 19/422 | loss: 0.9296343922615051\n",
      "Epoch: 47/100 | step: 20/422 | loss: 0.8363516926765442\n",
      "Epoch: 47/100 | step: 21/422 | loss: 1.0004183053970337\n",
      "Epoch: 47/100 | step: 22/422 | loss: 0.7303746342658997\n",
      "Epoch: 47/100 | step: 23/422 | loss: 0.9719510674476624\n",
      "Epoch: 47/100 | step: 24/422 | loss: 1.3191437721252441\n",
      "Epoch: 47/100 | step: 25/422 | loss: 1.227323055267334\n",
      "Epoch: 47/100 | step: 26/422 | loss: 1.3985228538513184\n",
      "Epoch: 47/100 | step: 27/422 | loss: 0.9957955479621887\n",
      "Epoch: 47/100 | step: 28/422 | loss: 1.331042766571045\n",
      "Epoch: 47/100 | step: 29/422 | loss: 0.6118876338005066\n",
      "Epoch: 47/100 | step: 30/422 | loss: 1.2880918979644775\n",
      "Epoch: 47/100 | step: 31/422 | loss: 0.6947017908096313\n",
      "Epoch: 47/100 | step: 32/422 | loss: 1.2253137826919556\n",
      "Epoch: 47/100 | step: 33/422 | loss: 0.9413836002349854\n",
      "Epoch: 47/100 | step: 34/422 | loss: 1.0633960962295532\n",
      "Epoch: 47/100 | step: 35/422 | loss: 0.5047929286956787\n",
      "Epoch: 47/100 | step: 36/422 | loss: 1.406087040901184\n",
      "Epoch: 47/100 | step: 37/422 | loss: 1.0659247636795044\n",
      "Epoch: 47/100 | step: 38/422 | loss: 1.0527551174163818\n",
      "Epoch: 47/100 | step: 39/422 | loss: 0.9503944516181946\n",
      "Epoch: 47/100 | step: 40/422 | loss: 0.9743768572807312\n",
      "Epoch: 47/100 | step: 41/422 | loss: 1.4095654487609863\n",
      "Epoch: 47/100 | step: 42/422 | loss: 0.9820685386657715\n",
      "Epoch: 47/100 | step: 43/422 | loss: 0.973049521446228\n",
      "Epoch: 47/100 | step: 44/422 | loss: 0.8134046792984009\n",
      "Epoch: 47/100 | step: 45/422 | loss: 1.1313022375106812\n",
      "Epoch: 47/100 | step: 46/422 | loss: 1.2217466831207275\n",
      "Epoch: 47/100 | step: 47/422 | loss: 1.44303297996521\n",
      "Epoch: 47/100 | step: 48/422 | loss: 1.4163159132003784\n",
      "Epoch: 47/100 | step: 49/422 | loss: 1.263684630393982\n",
      "Epoch: 47/100 | step: 50/422 | loss: 1.7374507188796997\n",
      "Epoch: 47/100 | step: 51/422 | loss: 1.1294262409210205\n",
      "Epoch: 47/100 | step: 52/422 | loss: 1.589992880821228\n",
      "Epoch: 47/100 | step: 53/422 | loss: 1.3618510961532593\n",
      "Epoch: 47/100 | step: 54/422 | loss: 1.6334433555603027\n",
      "Epoch: 47/100 | step: 55/422 | loss: 1.9181735515594482\n",
      "Epoch: 47/100 | step: 56/422 | loss: 0.8779057860374451\n",
      "Epoch: 47/100 | step: 57/422 | loss: 0.9629858732223511\n",
      "Epoch: 47/100 | step: 58/422 | loss: 1.3843084573745728\n",
      "Epoch: 47/100 | step: 59/422 | loss: 1.0036896467208862\n",
      "Epoch: 47/100 | step: 60/422 | loss: 1.704918384552002\n",
      "Epoch: 47/100 | step: 61/422 | loss: 1.16801118850708\n",
      "Epoch: 47/100 | step: 62/422 | loss: 1.2237845659255981\n",
      "Epoch: 47/100 | step: 63/422 | loss: 1.1756963729858398\n",
      "Epoch: 47/100 | step: 64/422 | loss: 1.1065186262130737\n",
      "Epoch: 47/100 | step: 65/422 | loss: 1.3380929231643677\n",
      "Epoch: 47/100 | step: 66/422 | loss: 1.0887759923934937\n",
      "Epoch: 47/100 | step: 67/422 | loss: 1.3385299444198608\n",
      "Epoch: 47/100 | step: 68/422 | loss: 1.0128870010375977\n",
      "Epoch: 47/100 | step: 69/422 | loss: 1.384661078453064\n",
      "Epoch: 47/100 | step: 70/422 | loss: 2.0612921714782715\n",
      "Epoch: 47/100 | step: 71/422 | loss: 1.1424134969711304\n",
      "Epoch: 47/100 | step: 72/422 | loss: 1.1053638458251953\n",
      "Epoch: 47/100 | step: 73/422 | loss: 1.0880793333053589\n",
      "Epoch: 47/100 | step: 74/422 | loss: 1.4221018552780151\n",
      "Epoch: 47/100 | step: 75/422 | loss: 1.179053544998169\n",
      "Epoch: 47/100 | step: 76/422 | loss: 1.0095804929733276\n",
      "Epoch: 47/100 | step: 77/422 | loss: 1.3371291160583496\n",
      "Epoch: 47/100 | step: 78/422 | loss: 1.551403284072876\n",
      "Epoch: 47/100 | step: 79/422 | loss: 1.091951847076416\n",
      "Epoch: 47/100 | step: 80/422 | loss: 0.8384950160980225\n",
      "Epoch: 47/100 | step: 81/422 | loss: 1.1070207357406616\n",
      "Epoch: 47/100 | step: 82/422 | loss: 0.7669265866279602\n",
      "Epoch: 47/100 | step: 83/422 | loss: 0.9652788639068604\n",
      "Epoch: 47/100 | step: 84/422 | loss: 1.6708462238311768\n",
      "Epoch: 47/100 | step: 85/422 | loss: 1.285415768623352\n",
      "Epoch: 47/100 | step: 86/422 | loss: 0.90416419506073\n",
      "Epoch: 47/100 | step: 87/422 | loss: 0.9725722670555115\n",
      "Epoch: 47/100 | step: 88/422 | loss: 1.380001187324524\n",
      "Epoch: 47/100 | step: 89/422 | loss: 0.928942859172821\n",
      "Epoch: 47/100 | step: 90/422 | loss: 1.209598183631897\n",
      "Epoch: 47/100 | step: 91/422 | loss: 1.124988079071045\n",
      "Epoch: 47/100 | step: 92/422 | loss: 1.2257001399993896\n",
      "Epoch: 47/100 | step: 93/422 | loss: 1.3048522472381592\n",
      "Epoch: 47/100 | step: 94/422 | loss: 1.1505966186523438\n",
      "Epoch: 47/100 | step: 95/422 | loss: 1.131617784500122\n",
      "Epoch: 47/100 | step: 96/422 | loss: 1.4739409685134888\n",
      "Epoch: 47/100 | step: 97/422 | loss: 1.073947787284851\n",
      "Epoch: 47/100 | step: 98/422 | loss: 1.2942591905593872\n",
      "Epoch: 47/100 | step: 99/422 | loss: 0.966385543346405\n",
      "Epoch: 47/100 | step: 100/422 | loss: 1.1345176696777344\n",
      "Epoch: 47/100 | step: 101/422 | loss: 1.0205992460250854\n",
      "Epoch: 47/100 | step: 102/422 | loss: 0.9081999659538269\n",
      "Epoch: 47/100 | step: 103/422 | loss: 0.8861692547798157\n",
      "Epoch: 47/100 | step: 104/422 | loss: 1.3135452270507812\n",
      "Epoch: 47/100 | step: 105/422 | loss: 1.1936516761779785\n",
      "Epoch: 47/100 | step: 106/422 | loss: 1.9166394472122192\n",
      "Epoch: 47/100 | step: 107/422 | loss: 1.36849844455719\n",
      "Epoch: 47/100 | step: 108/422 | loss: 1.2691736221313477\n",
      "Epoch: 47/100 | step: 109/422 | loss: 0.7366133332252502\n",
      "Epoch: 47/100 | step: 110/422 | loss: 1.1758633852005005\n",
      "Epoch: 47/100 | step: 111/422 | loss: 1.0313982963562012\n",
      "Epoch: 47/100 | step: 112/422 | loss: 1.1379212141036987\n",
      "Epoch: 47/100 | step: 113/422 | loss: 1.1207423210144043\n",
      "Epoch: 47/100 | step: 114/422 | loss: 1.0164141654968262\n",
      "Epoch: 47/100 | step: 115/422 | loss: 1.0746203660964966\n",
      "Epoch: 47/100 | step: 116/422 | loss: 1.885371446609497\n",
      "Epoch: 47/100 | step: 117/422 | loss: 1.1928597688674927\n",
      "Epoch: 47/100 | step: 118/422 | loss: 1.023336410522461\n",
      "Epoch: 47/100 | step: 119/422 | loss: 1.168777346611023\n",
      "Epoch: 47/100 | step: 120/422 | loss: 1.093518853187561\n",
      "Epoch: 47/100 | step: 121/422 | loss: 1.0864077806472778\n",
      "Epoch: 47/100 | step: 122/422 | loss: 1.1120294332504272\n",
      "Epoch: 47/100 | step: 123/422 | loss: 0.8406727313995361\n",
      "Epoch: 47/100 | step: 124/422 | loss: 0.7361024022102356\n",
      "Epoch: 47/100 | step: 125/422 | loss: 0.8759862780570984\n",
      "Epoch: 47/100 | step: 126/422 | loss: 1.2962557077407837\n",
      "Epoch: 47/100 | step: 127/422 | loss: 0.9217196106910706\n",
      "Epoch: 47/100 | step: 128/422 | loss: 1.1799932718276978\n",
      "Epoch: 47/100 | step: 129/422 | loss: 1.0962402820587158\n",
      "Epoch: 47/100 | step: 130/422 | loss: 0.7832233905792236\n",
      "Epoch: 47/100 | step: 131/422 | loss: 1.4203780889511108\n",
      "Epoch: 47/100 | step: 132/422 | loss: 1.224352478981018\n",
      "Epoch: 47/100 | step: 133/422 | loss: 1.4337260723114014\n",
      "Epoch: 47/100 | step: 134/422 | loss: 0.9903336763381958\n",
      "Epoch: 47/100 | step: 135/422 | loss: 1.0939825773239136\n",
      "Epoch: 47/100 | step: 136/422 | loss: 1.656799077987671\n",
      "Epoch: 47/100 | step: 137/422 | loss: 1.395650029182434\n",
      "Epoch: 47/100 | step: 138/422 | loss: 0.9461554288864136\n",
      "Epoch: 47/100 | step: 139/422 | loss: 1.4985547065734863\n",
      "Epoch: 47/100 | step: 140/422 | loss: 1.6853035688400269\n",
      "Epoch: 47/100 | step: 141/422 | loss: 1.0159294605255127\n",
      "Epoch: 47/100 | step: 142/422 | loss: 0.8351174592971802\n",
      "Epoch: 47/100 | step: 143/422 | loss: 1.3378275632858276\n",
      "Epoch: 47/100 | step: 144/422 | loss: 1.5786157846450806\n",
      "Epoch: 47/100 | step: 145/422 | loss: 1.3497620820999146\n",
      "Epoch: 47/100 | step: 146/422 | loss: 1.5101039409637451\n",
      "Epoch: 47/100 | step: 147/422 | loss: 1.5098533630371094\n",
      "Epoch: 47/100 | step: 148/422 | loss: 1.3183588981628418\n",
      "Epoch: 47/100 | step: 149/422 | loss: 1.3284415006637573\n",
      "Epoch: 47/100 | step: 150/422 | loss: 0.9140127301216125\n",
      "Epoch: 47/100 | step: 151/422 | loss: 1.0348843336105347\n",
      "Epoch: 47/100 | step: 152/422 | loss: 0.5658841133117676\n",
      "Epoch: 47/100 | step: 153/422 | loss: 1.6315964460372925\n",
      "Epoch: 47/100 | step: 154/422 | loss: 1.5082453489303589\n",
      "Epoch: 47/100 | step: 155/422 | loss: 0.6407081484794617\n",
      "Epoch: 47/100 | step: 156/422 | loss: 1.3848106861114502\n",
      "Epoch: 47/100 | step: 157/422 | loss: 1.6043634414672852\n",
      "Epoch: 47/100 | step: 158/422 | loss: 1.0367648601531982\n",
      "Epoch: 47/100 | step: 159/422 | loss: 0.9924246668815613\n",
      "Epoch: 47/100 | step: 160/422 | loss: 1.0719499588012695\n",
      "Epoch: 47/100 | step: 161/422 | loss: 0.8494853377342224\n",
      "Epoch: 47/100 | step: 162/422 | loss: 1.166956901550293\n",
      "Epoch: 47/100 | step: 163/422 | loss: 1.4873731136322021\n",
      "Epoch: 47/100 | step: 164/422 | loss: 1.425866961479187\n",
      "Epoch: 47/100 | step: 165/422 | loss: 0.9845196604728699\n",
      "Epoch: 47/100 | step: 166/422 | loss: 1.0030410289764404\n",
      "Epoch: 47/100 | step: 167/422 | loss: 1.2494741678237915\n",
      "Epoch: 47/100 | step: 168/422 | loss: 0.9014241695404053\n",
      "Epoch: 47/100 | step: 169/422 | loss: 0.865070641040802\n",
      "Epoch: 47/100 | step: 170/422 | loss: 1.151038646697998\n",
      "Epoch: 47/100 | step: 171/422 | loss: 1.1397818326950073\n",
      "Epoch: 47/100 | step: 172/422 | loss: 1.479263186454773\n",
      "Epoch: 47/100 | step: 173/422 | loss: 0.9779996275901794\n",
      "Epoch: 47/100 | step: 174/422 | loss: 1.5616717338562012\n",
      "Epoch: 47/100 | step: 175/422 | loss: 1.3620572090148926\n",
      "Epoch: 47/100 | step: 176/422 | loss: 1.3892163038253784\n",
      "Epoch: 47/100 | step: 177/422 | loss: 1.2793152332305908\n",
      "Epoch: 47/100 | step: 178/422 | loss: 1.6335736513137817\n",
      "Epoch: 47/100 | step: 179/422 | loss: 1.0645904541015625\n",
      "Epoch: 47/100 | step: 180/422 | loss: 1.2161961793899536\n",
      "Epoch: 47/100 | step: 181/422 | loss: 1.1687012910842896\n",
      "Epoch: 47/100 | step: 182/422 | loss: 1.2696887254714966\n",
      "Epoch: 47/100 | step: 183/422 | loss: 1.5695335865020752\n",
      "Epoch: 47/100 | step: 184/422 | loss: 1.3696224689483643\n",
      "Epoch: 47/100 | step: 185/422 | loss: 1.1980140209197998\n",
      "Epoch: 47/100 | step: 186/422 | loss: 1.5406967401504517\n",
      "Epoch: 47/100 | step: 187/422 | loss: 0.7972503304481506\n",
      "Epoch: 47/100 | step: 188/422 | loss: 1.1221972703933716\n",
      "Epoch: 47/100 | step: 189/422 | loss: 1.13991117477417\n",
      "Epoch: 47/100 | step: 190/422 | loss: 1.3356602191925049\n",
      "Epoch: 47/100 | step: 191/422 | loss: 1.4772422313690186\n",
      "Epoch: 47/100 | step: 192/422 | loss: 1.194279432296753\n",
      "Epoch: 47/100 | step: 193/422 | loss: 1.3800915479660034\n",
      "Epoch: 47/100 | step: 194/422 | loss: 1.0424360036849976\n",
      "Epoch: 47/100 | step: 195/422 | loss: 1.534698486328125\n",
      "Epoch: 47/100 | step: 196/422 | loss: 1.0592151880264282\n",
      "Epoch: 47/100 | step: 197/422 | loss: 1.2494597434997559\n",
      "Epoch: 47/100 | step: 198/422 | loss: 0.8604035973548889\n",
      "Epoch: 47/100 | step: 199/422 | loss: 0.9910770654678345\n",
      "Epoch: 47/100 | step: 200/422 | loss: 1.0171270370483398\n",
      "Epoch: 47/100 | step: 201/422 | loss: 0.8053105473518372\n",
      "Epoch: 47/100 | step: 202/422 | loss: 0.9617142081260681\n",
      "Epoch: 47/100 | step: 203/422 | loss: 1.0280247926712036\n",
      "Epoch: 47/100 | step: 204/422 | loss: 1.246612548828125\n",
      "Epoch: 47/100 | step: 205/422 | loss: 1.2556843757629395\n",
      "Epoch: 47/100 | step: 206/422 | loss: 1.0348221063613892\n",
      "Epoch: 47/100 | step: 207/422 | loss: 0.9517332911491394\n",
      "Epoch: 47/100 | step: 208/422 | loss: 1.250844120979309\n",
      "Epoch: 47/100 | step: 209/422 | loss: 1.2975226640701294\n",
      "Epoch: 47/100 | step: 210/422 | loss: 1.1495314836502075\n",
      "Epoch: 47/100 | step: 211/422 | loss: 1.126530408859253\n",
      "Epoch: 47/100 | step: 212/422 | loss: 1.0368659496307373\n",
      "Epoch: 47/100 | step: 213/422 | loss: 1.0208368301391602\n",
      "Epoch: 47/100 | step: 214/422 | loss: 0.8272661566734314\n",
      "Epoch: 47/100 | step: 215/422 | loss: 1.2753044366836548\n",
      "Epoch: 47/100 | step: 216/422 | loss: 1.3937804698944092\n",
      "Epoch: 47/100 | step: 217/422 | loss: 0.8463909029960632\n",
      "Epoch: 47/100 | step: 218/422 | loss: 1.2972508668899536\n",
      "Epoch: 47/100 | step: 219/422 | loss: 0.9325377345085144\n",
      "Epoch: 47/100 | step: 220/422 | loss: 1.0215169191360474\n",
      "Epoch: 47/100 | step: 221/422 | loss: 1.1444892883300781\n",
      "Epoch: 47/100 | step: 222/422 | loss: 0.9858514070510864\n",
      "Epoch: 47/100 | step: 223/422 | loss: 1.30574631690979\n",
      "Epoch: 47/100 | step: 224/422 | loss: 1.1689420938491821\n",
      "Epoch: 47/100 | step: 225/422 | loss: 1.2018016576766968\n",
      "Epoch: 47/100 | step: 226/422 | loss: 1.4327425956726074\n",
      "Epoch: 47/100 | step: 227/422 | loss: 0.958430290222168\n",
      "Epoch: 47/100 | step: 228/422 | loss: 1.0706721544265747\n",
      "Epoch: 47/100 | step: 229/422 | loss: 1.5585992336273193\n",
      "Epoch: 47/100 | step: 230/422 | loss: 1.1639221906661987\n",
      "Epoch: 47/100 | step: 231/422 | loss: 1.0605478286743164\n",
      "Epoch: 47/100 | step: 232/422 | loss: 0.9926271438598633\n",
      "Epoch: 47/100 | step: 233/422 | loss: 1.5813344717025757\n",
      "Epoch: 47/100 | step: 234/422 | loss: 0.974561333656311\n",
      "Epoch: 47/100 | step: 235/422 | loss: 1.4972707033157349\n",
      "Epoch: 47/100 | step: 236/422 | loss: 1.386834979057312\n",
      "Epoch: 47/100 | step: 237/422 | loss: 0.9096618294715881\n",
      "Epoch: 47/100 | step: 238/422 | loss: 1.260890245437622\n",
      "Epoch: 47/100 | step: 239/422 | loss: 1.1017378568649292\n",
      "Epoch: 47/100 | step: 240/422 | loss: 1.8324284553527832\n",
      "Epoch: 47/100 | step: 241/422 | loss: 1.2945557832717896\n",
      "Epoch: 47/100 | step: 242/422 | loss: 0.9170184135437012\n",
      "Epoch: 47/100 | step: 243/422 | loss: 1.0385822057724\n",
      "Epoch: 47/100 | step: 244/422 | loss: 1.1083918809890747\n",
      "Epoch: 47/100 | step: 245/422 | loss: 1.1683274507522583\n",
      "Epoch: 47/100 | step: 246/422 | loss: 1.03919517993927\n",
      "Epoch: 47/100 | step: 247/422 | loss: 1.5746493339538574\n",
      "Epoch: 47/100 | step: 248/422 | loss: 0.9559618234634399\n",
      "Epoch: 47/100 | step: 249/422 | loss: 0.9825409054756165\n",
      "Epoch: 47/100 | step: 250/422 | loss: 1.089911699295044\n",
      "Epoch: 47/100 | step: 251/422 | loss: 0.8181129693984985\n",
      "Epoch: 47/100 | step: 252/422 | loss: 0.919912576675415\n",
      "Epoch: 47/100 | step: 253/422 | loss: 1.272091031074524\n",
      "Epoch: 47/100 | step: 254/422 | loss: 1.0507628917694092\n",
      "Epoch: 47/100 | step: 255/422 | loss: 1.2488489151000977\n",
      "Epoch: 47/100 | step: 256/422 | loss: 0.9873729944229126\n",
      "Epoch: 47/100 | step: 257/422 | loss: 1.0619163513183594\n",
      "Epoch: 47/100 | step: 258/422 | loss: 1.0498018264770508\n",
      "Epoch: 47/100 | step: 259/422 | loss: 1.147196650505066\n",
      "Epoch: 47/100 | step: 260/422 | loss: 1.077588438987732\n",
      "Epoch: 47/100 | step: 261/422 | loss: 0.9449499249458313\n",
      "Epoch: 47/100 | step: 262/422 | loss: 0.9978694915771484\n",
      "Epoch: 47/100 | step: 263/422 | loss: 1.6072781085968018\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 48/100 | step: 1/422 | loss: 1.137756586074829\n",
      "Epoch: 48/100 | step: 2/422 | loss: 1.0734138488769531\n",
      "Epoch: 48/100 | step: 3/422 | loss: 1.1097432374954224\n",
      "Epoch: 48/100 | step: 4/422 | loss: 1.0260220766067505\n",
      "Epoch: 48/100 | step: 5/422 | loss: 1.3195509910583496\n",
      "Epoch: 48/100 | step: 6/422 | loss: 1.1690921783447266\n",
      "Epoch: 48/100 | step: 7/422 | loss: 0.807612955570221\n",
      "Epoch: 48/100 | step: 8/422 | loss: 1.2242743968963623\n",
      "Epoch: 48/100 | step: 9/422 | loss: 1.0539777278900146\n",
      "Epoch: 48/100 | step: 10/422 | loss: 0.7826573848724365\n",
      "Epoch: 48/100 | step: 11/422 | loss: 0.9055036306381226\n",
      "Epoch: 48/100 | step: 12/422 | loss: 0.9083414077758789\n",
      "Epoch: 48/100 | step: 13/422 | loss: 0.977135956287384\n",
      "Epoch: 48/100 | step: 14/422 | loss: 1.0431238412857056\n",
      "Epoch: 48/100 | step: 15/422 | loss: 1.3566455841064453\n",
      "Epoch: 48/100 | step: 16/422 | loss: 0.9300410151481628\n",
      "Epoch: 48/100 | step: 17/422 | loss: 0.9278923273086548\n",
      "Epoch: 48/100 | step: 18/422 | loss: 1.4107388257980347\n",
      "Epoch: 48/100 | step: 19/422 | loss: 1.2046735286712646\n",
      "Epoch: 48/100 | step: 20/422 | loss: 1.0063310861587524\n",
      "Epoch: 48/100 | step: 21/422 | loss: 1.0407721996307373\n",
      "Epoch: 48/100 | step: 22/422 | loss: 0.9636271595954895\n",
      "Epoch: 48/100 | step: 23/422 | loss: 1.3244044780731201\n",
      "Epoch: 48/100 | step: 24/422 | loss: 1.0571256875991821\n",
      "Epoch: 48/100 | step: 25/422 | loss: 1.422226905822754\n",
      "Epoch: 48/100 | step: 26/422 | loss: 1.7116060256958008\n",
      "Epoch: 48/100 | step: 27/422 | loss: 1.3518915176391602\n",
      "Epoch: 48/100 | step: 28/422 | loss: 0.6564323306083679\n",
      "Epoch: 48/100 | step: 29/422 | loss: 1.5220898389816284\n",
      "Epoch: 48/100 | step: 30/422 | loss: 1.035500407218933\n",
      "Epoch: 48/100 | step: 31/422 | loss: 0.8207300901412964\n",
      "Epoch: 48/100 | step: 32/422 | loss: 1.2227709293365479\n",
      "Epoch: 48/100 | step: 33/422 | loss: 1.0898404121398926\n",
      "Epoch: 48/100 | step: 34/422 | loss: 1.4270597696304321\n",
      "Epoch: 48/100 | step: 35/422 | loss: 1.3179410696029663\n",
      "Epoch: 48/100 | step: 36/422 | loss: 1.0908293724060059\n",
      "Epoch: 48/100 | step: 37/422 | loss: 1.1721433401107788\n",
      "Epoch: 48/100 | step: 38/422 | loss: 0.9591447710990906\n",
      "Epoch: 48/100 | step: 39/422 | loss: 1.2123302221298218\n",
      "Epoch: 48/100 | step: 40/422 | loss: 1.7563650608062744\n",
      "Epoch: 48/100 | step: 41/422 | loss: 0.8711832761764526\n",
      "Epoch: 48/100 | step: 42/422 | loss: 1.401004433631897\n",
      "Epoch: 48/100 | step: 43/422 | loss: 0.7171330451965332\n",
      "Epoch: 48/100 | step: 44/422 | loss: 1.5710113048553467\n",
      "Epoch: 48/100 | step: 45/422 | loss: 0.9930914044380188\n",
      "Epoch: 48/100 | step: 46/422 | loss: 1.0275137424468994\n",
      "Epoch: 48/100 | step: 47/422 | loss: 1.4238018989562988\n",
      "Epoch: 48/100 | step: 48/422 | loss: 1.1533652544021606\n",
      "Epoch: 48/100 | step: 49/422 | loss: 0.8652452826499939\n",
      "Epoch: 48/100 | step: 50/422 | loss: 0.9959602355957031\n",
      "Epoch: 48/100 | step: 51/422 | loss: 0.859253466129303\n",
      "Epoch: 48/100 | step: 52/422 | loss: 0.9911113381385803\n",
      "Epoch: 48/100 | step: 53/422 | loss: 0.6708825826644897\n",
      "Epoch: 48/100 | step: 54/422 | loss: 1.2181119918823242\n",
      "Epoch: 48/100 | step: 55/422 | loss: 0.8631361126899719\n",
      "Epoch: 48/100 | step: 56/422 | loss: 1.126448631286621\n",
      "Epoch: 48/100 | step: 57/422 | loss: 1.3538320064544678\n",
      "Epoch: 48/100 | step: 58/422 | loss: 0.9725391864776611\n",
      "Epoch: 48/100 | step: 59/422 | loss: 0.8170576691627502\n",
      "Epoch: 48/100 | step: 60/422 | loss: 1.1342612504959106\n",
      "Epoch: 48/100 | step: 61/422 | loss: 1.0267008543014526\n",
      "Epoch: 48/100 | step: 62/422 | loss: 1.1719456911087036\n",
      "Epoch: 48/100 | step: 63/422 | loss: 1.2948030233383179\n",
      "Epoch: 48/100 | step: 64/422 | loss: 1.0917326211929321\n",
      "Epoch: 48/100 | step: 65/422 | loss: 1.4055874347686768\n",
      "Epoch: 48/100 | step: 66/422 | loss: 0.9316310882568359\n",
      "Epoch: 48/100 | step: 67/422 | loss: 0.7995781898498535\n",
      "Epoch: 48/100 | step: 68/422 | loss: 0.6587595343589783\n",
      "Epoch: 48/100 | step: 69/422 | loss: 1.068473219871521\n",
      "Epoch: 48/100 | step: 70/422 | loss: 0.5627061128616333\n",
      "Epoch: 48/100 | step: 71/422 | loss: 0.8459827899932861\n",
      "Epoch: 48/100 | step: 72/422 | loss: 0.7635721564292908\n",
      "Epoch: 48/100 | step: 73/422 | loss: 1.039125919342041\n",
      "Epoch: 48/100 | step: 74/422 | loss: 1.0009069442749023\n",
      "Epoch: 48/100 | step: 75/422 | loss: 0.932232141494751\n",
      "Epoch: 48/100 | step: 76/422 | loss: 0.9354309439659119\n",
      "Epoch: 48/100 | step: 77/422 | loss: 1.061381459236145\n",
      "Epoch: 48/100 | step: 78/422 | loss: 1.0491249561309814\n",
      "Epoch: 48/100 | step: 79/422 | loss: 1.343014121055603\n",
      "Epoch: 48/100 | step: 80/422 | loss: 0.9272311329841614\n",
      "Epoch: 48/100 | step: 81/422 | loss: 0.8963375687599182\n",
      "Epoch: 48/100 | step: 82/422 | loss: 1.1167939901351929\n",
      "Epoch: 48/100 | step: 83/422 | loss: 0.8298752903938293\n",
      "Epoch: 48/100 | step: 84/422 | loss: 0.9667171239852905\n",
      "Epoch: 48/100 | step: 85/422 | loss: 0.9368749856948853\n",
      "Epoch: 48/100 | step: 86/422 | loss: 1.0617215633392334\n",
      "Epoch: 48/100 | step: 87/422 | loss: 1.209593415260315\n",
      "Epoch: 48/100 | step: 88/422 | loss: 1.138431429862976\n",
      "Epoch: 48/100 | step: 89/422 | loss: 0.8410726189613342\n",
      "Epoch: 48/100 | step: 90/422 | loss: 1.3118836879730225\n",
      "Epoch: 48/100 | step: 91/422 | loss: 0.9578312039375305\n",
      "Epoch: 48/100 | step: 92/422 | loss: 0.7512084245681763\n",
      "Epoch: 48/100 | step: 93/422 | loss: 1.2264554500579834\n",
      "Epoch: 48/100 | step: 94/422 | loss: 1.4250450134277344\n",
      "Epoch: 48/100 | step: 95/422 | loss: 1.0926874876022339\n",
      "Epoch: 48/100 | step: 96/422 | loss: 0.9696071147918701\n",
      "Epoch: 48/100 | step: 97/422 | loss: 0.9730890989303589\n",
      "Epoch: 48/100 | step: 98/422 | loss: 0.8780204653739929\n",
      "Epoch: 48/100 | step: 99/422 | loss: 0.7463191747665405\n",
      "Epoch: 48/100 | step: 100/422 | loss: 1.1106008291244507\n",
      "Epoch: 48/100 | step: 101/422 | loss: 0.8126523494720459\n",
      "Epoch: 48/100 | step: 102/422 | loss: 1.238476276397705\n",
      "Epoch: 48/100 | step: 103/422 | loss: 0.9949697256088257\n",
      "Epoch: 48/100 | step: 104/422 | loss: 1.269524335861206\n",
      "Epoch: 48/100 | step: 105/422 | loss: 1.4571411609649658\n",
      "Epoch: 48/100 | step: 106/422 | loss: 0.8589293360710144\n",
      "Epoch: 48/100 | step: 107/422 | loss: 1.3141038417816162\n",
      "Epoch: 48/100 | step: 108/422 | loss: 1.37483811378479\n",
      "Epoch: 48/100 | step: 109/422 | loss: 1.4102073907852173\n",
      "Epoch: 48/100 | step: 110/422 | loss: 1.136941909790039\n",
      "Epoch: 48/100 | step: 111/422 | loss: 1.2864631414413452\n",
      "Epoch: 48/100 | step: 112/422 | loss: 1.2053662538528442\n",
      "Epoch: 48/100 | step: 113/422 | loss: 1.2935378551483154\n",
      "Epoch: 48/100 | step: 114/422 | loss: 1.3628791570663452\n",
      "Epoch: 48/100 | step: 115/422 | loss: 1.0795704126358032\n",
      "Epoch: 48/100 | step: 116/422 | loss: 1.5419195890426636\n",
      "Epoch: 48/100 | step: 117/422 | loss: 1.5368345975875854\n",
      "Epoch: 48/100 | step: 118/422 | loss: 1.7004278898239136\n",
      "Epoch: 48/100 | step: 119/422 | loss: 1.0378960371017456\n",
      "Epoch: 48/100 | step: 120/422 | loss: 1.2715264558792114\n",
      "Epoch: 48/100 | step: 121/422 | loss: 0.920145571231842\n",
      "Epoch: 48/100 | step: 122/422 | loss: 1.4170782566070557\n",
      "Epoch: 48/100 | step: 123/422 | loss: 1.1852248907089233\n",
      "Epoch: 48/100 | step: 124/422 | loss: 1.032595157623291\n",
      "Epoch: 48/100 | step: 125/422 | loss: 1.0374741554260254\n",
      "Epoch: 48/100 | step: 126/422 | loss: 1.1394293308258057\n",
      "Epoch: 48/100 | step: 127/422 | loss: 1.0049585103988647\n",
      "Epoch: 48/100 | step: 128/422 | loss: 1.1821577548980713\n",
      "Epoch: 48/100 | step: 129/422 | loss: 0.7023360133171082\n",
      "Epoch: 48/100 | step: 130/422 | loss: 0.9996771812438965\n",
      "Epoch: 48/100 | step: 131/422 | loss: 0.8712741732597351\n",
      "Epoch: 48/100 | step: 132/422 | loss: 0.6748419404029846\n",
      "Epoch: 48/100 | step: 133/422 | loss: 0.981293797492981\n",
      "Epoch: 48/100 | step: 134/422 | loss: 1.319242000579834\n",
      "Epoch: 48/100 | step: 135/422 | loss: 0.43564078211784363\n",
      "Epoch: 48/100 | step: 136/422 | loss: 1.111212134361267\n",
      "Epoch: 48/100 | step: 137/422 | loss: 0.9325165748596191\n",
      "Epoch: 48/100 | step: 138/422 | loss: 0.9716548323631287\n",
      "Epoch: 48/100 | step: 139/422 | loss: 1.2404415607452393\n",
      "Epoch: 48/100 | step: 140/422 | loss: 0.6670872569084167\n",
      "Epoch: 48/100 | step: 141/422 | loss: 0.7700269222259521\n",
      "Epoch: 48/100 | step: 142/422 | loss: 1.5697295665740967\n",
      "Epoch: 48/100 | step: 143/422 | loss: 1.5439982414245605\n",
      "Epoch: 48/100 | step: 144/422 | loss: 1.1612197160720825\n",
      "Epoch: 48/100 | step: 145/422 | loss: 1.0723612308502197\n",
      "Epoch: 48/100 | step: 146/422 | loss: 1.2194411754608154\n",
      "Epoch: 48/100 | step: 147/422 | loss: 0.8193877339363098\n",
      "Epoch: 48/100 | step: 148/422 | loss: 0.8997883200645447\n",
      "Epoch: 48/100 | step: 149/422 | loss: 1.2262842655181885\n",
      "Epoch: 48/100 | step: 150/422 | loss: 1.3881157636642456\n",
      "Epoch: 48/100 | step: 151/422 | loss: 1.208133339881897\n",
      "Epoch: 48/100 | step: 152/422 | loss: 1.1609270572662354\n",
      "Epoch: 48/100 | step: 153/422 | loss: 1.089592695236206\n",
      "Epoch: 48/100 | step: 154/422 | loss: 1.1883838176727295\n",
      "Epoch: 48/100 | step: 155/422 | loss: 1.1107853651046753\n",
      "Epoch: 48/100 | step: 156/422 | loss: 0.7886340618133545\n",
      "Epoch: 48/100 | step: 157/422 | loss: 1.1087733507156372\n",
      "Epoch: 48/100 | step: 158/422 | loss: 0.9193038940429688\n",
      "Epoch: 48/100 | step: 159/422 | loss: 0.7942383885383606\n",
      "Epoch: 48/100 | step: 160/422 | loss: 0.8319568634033203\n",
      "Epoch: 48/100 | step: 161/422 | loss: 0.7694722414016724\n",
      "Epoch: 48/100 | step: 162/422 | loss: 0.7762100696563721\n",
      "Epoch: 48/100 | step: 163/422 | loss: 1.2325637340545654\n",
      "Epoch: 48/100 | step: 164/422 | loss: 1.3225773572921753\n",
      "Epoch: 48/100 | step: 165/422 | loss: 0.9909430146217346\n",
      "Epoch: 48/100 | step: 166/422 | loss: 1.2626091241836548\n",
      "Epoch: 48/100 | step: 167/422 | loss: 0.8873118758201599\n",
      "Epoch: 48/100 | step: 168/422 | loss: 1.3043594360351562\n",
      "Epoch: 48/100 | step: 169/422 | loss: 0.9521822333335876\n",
      "Epoch: 48/100 | step: 170/422 | loss: 1.2918564081192017\n",
      "Epoch: 48/100 | step: 171/422 | loss: 0.898293137550354\n",
      "Epoch: 48/100 | step: 172/422 | loss: 1.1250746250152588\n",
      "Epoch: 48/100 | step: 173/422 | loss: 0.9313536882400513\n",
      "Epoch: 48/100 | step: 174/422 | loss: 1.1819612979888916\n",
      "Epoch: 48/100 | step: 175/422 | loss: 1.072298526763916\n",
      "Epoch: 48/100 | step: 176/422 | loss: 1.1182363033294678\n",
      "Epoch: 48/100 | step: 177/422 | loss: 0.8234326839447021\n",
      "Epoch: 48/100 | step: 178/422 | loss: 0.7833093404769897\n",
      "Epoch: 48/100 | step: 179/422 | loss: 0.8617098331451416\n",
      "Epoch: 48/100 | step: 180/422 | loss: 0.9747645258903503\n",
      "Epoch: 48/100 | step: 181/422 | loss: 0.9913010001182556\n",
      "Epoch: 48/100 | step: 182/422 | loss: 0.786118745803833\n",
      "Epoch: 48/100 | step: 183/422 | loss: 0.860614001750946\n",
      "Epoch: 48/100 | step: 184/422 | loss: 1.6976242065429688\n",
      "Epoch: 48/100 | step: 185/422 | loss: 0.9335458278656006\n",
      "Epoch: 48/100 | step: 186/422 | loss: 1.611271858215332\n",
      "Epoch: 48/100 | step: 187/422 | loss: 1.1828174591064453\n",
      "Epoch: 48/100 | step: 188/422 | loss: 1.2240461111068726\n",
      "Epoch: 48/100 | step: 189/422 | loss: 0.6907200813293457\n",
      "Epoch: 48/100 | step: 190/422 | loss: 1.640334129333496\n",
      "Epoch: 48/100 | step: 191/422 | loss: 1.01820969581604\n",
      "Epoch: 48/100 | step: 192/422 | loss: 1.0612728595733643\n",
      "Epoch: 48/100 | step: 193/422 | loss: 1.7001274824142456\n",
      "Epoch: 48/100 | step: 194/422 | loss: 1.0055582523345947\n",
      "Epoch: 48/100 | step: 195/422 | loss: 1.0950592756271362\n",
      "Epoch: 48/100 | step: 196/422 | loss: 1.2193905115127563\n",
      "Epoch: 48/100 | step: 197/422 | loss: 0.9357176423072815\n",
      "Epoch: 48/100 | step: 198/422 | loss: 1.2466940879821777\n",
      "Epoch: 48/100 | step: 199/422 | loss: 1.42825448513031\n",
      "Epoch: 48/100 | step: 200/422 | loss: 0.9194765686988831\n",
      "Epoch: 48/100 | step: 201/422 | loss: 1.5219565629959106\n",
      "Epoch: 48/100 | step: 202/422 | loss: 1.033441424369812\n",
      "Epoch: 48/100 | step: 203/422 | loss: 1.2552860975265503\n",
      "Epoch: 48/100 | step: 204/422 | loss: 1.2719929218292236\n",
      "Epoch: 48/100 | step: 205/422 | loss: 1.0197986364364624\n",
      "Epoch: 48/100 | step: 206/422 | loss: 1.1683326959609985\n",
      "Epoch: 48/100 | step: 207/422 | loss: 0.8498722910881042\n",
      "Epoch: 48/100 | step: 208/422 | loss: 1.293681025505066\n",
      "Epoch: 48/100 | step: 209/422 | loss: 0.7375150322914124\n",
      "Epoch: 48/100 | step: 210/422 | loss: 1.0314571857452393\n",
      "Epoch: 48/100 | step: 211/422 | loss: 1.056863784790039\n",
      "Epoch: 48/100 | step: 212/422 | loss: 1.296036958694458\n",
      "Epoch: 48/100 | step: 213/422 | loss: 0.893742024898529\n",
      "Epoch: 48/100 | step: 214/422 | loss: 1.1265952587127686\n",
      "Epoch: 48/100 | step: 215/422 | loss: 1.252244472503662\n",
      "Epoch: 48/100 | step: 216/422 | loss: 1.1017016172409058\n",
      "Epoch: 48/100 | step: 217/422 | loss: 0.9406084418296814\n",
      "Epoch: 48/100 | step: 218/422 | loss: 1.1113029718399048\n",
      "Epoch: 48/100 | step: 219/422 | loss: 1.1197338104248047\n",
      "Epoch: 48/100 | step: 220/422 | loss: 1.7560741901397705\n",
      "Epoch: 48/100 | step: 221/422 | loss: 1.0541075468063354\n",
      "Epoch: 48/100 | step: 222/422 | loss: 1.113179087638855\n",
      "Epoch: 48/100 | step: 223/422 | loss: 1.1617506742477417\n",
      "Epoch: 48/100 | step: 224/422 | loss: 1.1599721908569336\n",
      "Epoch: 48/100 | step: 225/422 | loss: 1.0130901336669922\n",
      "Epoch: 48/100 | step: 226/422 | loss: 0.9119263291358948\n",
      "Epoch: 48/100 | step: 227/422 | loss: 0.6961609721183777\n",
      "Epoch: 48/100 | step: 228/422 | loss: 0.9159168601036072\n",
      "Epoch: 48/100 | step: 229/422 | loss: 1.083632469177246\n",
      "Epoch: 48/100 | step: 230/422 | loss: 0.8864421844482422\n",
      "Epoch: 48/100 | step: 231/422 | loss: 1.0074107646942139\n",
      "Epoch: 48/100 | step: 232/422 | loss: 0.7641092538833618\n",
      "Epoch: 48/100 | step: 233/422 | loss: 1.0485070943832397\n",
      "Epoch: 48/100 | step: 234/422 | loss: 1.3114625215530396\n",
      "Epoch: 48/100 | step: 235/422 | loss: 1.4207508563995361\n",
      "Epoch: 48/100 | step: 236/422 | loss: 1.0656042098999023\n",
      "Epoch: 48/100 | step: 237/422 | loss: 1.3864487409591675\n",
      "Epoch: 48/100 | step: 238/422 | loss: 0.9034039378166199\n",
      "Epoch: 48/100 | step: 239/422 | loss: 0.9983334541320801\n",
      "Epoch: 48/100 | step: 240/422 | loss: 1.1118276119232178\n",
      "Epoch: 48/100 | step: 241/422 | loss: 0.968277633190155\n",
      "Epoch: 48/100 | step: 242/422 | loss: 1.1761372089385986\n",
      "Epoch: 48/100 | step: 243/422 | loss: 0.8001348972320557\n",
      "Epoch: 48/100 | step: 244/422 | loss: 1.143101692199707\n",
      "Epoch: 48/100 | step: 245/422 | loss: 0.7048260569572449\n",
      "Epoch: 48/100 | step: 246/422 | loss: 0.7805870175361633\n",
      "Epoch: 48/100 | step: 247/422 | loss: 0.8384264707565308\n",
      "Epoch: 48/100 | step: 248/422 | loss: 0.9356576800346375\n",
      "Epoch: 48/100 | step: 249/422 | loss: 0.7730531692504883\n",
      "Epoch: 48/100 | step: 250/422 | loss: 0.7815910577774048\n",
      "Epoch: 48/100 | step: 251/422 | loss: 0.7990519404411316\n",
      "Epoch: 48/100 | step: 252/422 | loss: 0.8675206303596497\n",
      "Epoch: 48/100 | step: 253/422 | loss: 1.3807097673416138\n",
      "Epoch: 48/100 | step: 254/422 | loss: 1.4253361225128174\n",
      "Epoch: 48/100 | step: 255/422 | loss: 0.5624208450317383\n",
      "Epoch: 48/100 | step: 256/422 | loss: 1.4165388345718384\n",
      "Epoch: 48/100 | step: 257/422 | loss: 1.2222366333007812\n",
      "Epoch: 48/100 | step: 258/422 | loss: 1.0247129201889038\n",
      "Epoch: 48/100 | step: 259/422 | loss: 0.9217198491096497\n",
      "Epoch: 48/100 | step: 260/422 | loss: 0.9258605241775513\n",
      "Epoch: 48/100 | step: 261/422 | loss: 1.2332969903945923\n",
      "Epoch: 48/100 | step: 262/422 | loss: 1.585457682609558\n",
      "Epoch: 48/100 | step: 263/422 | loss: 0.6843994855880737\n",
      "Epoch: 48/100 | step: 264/422 | loss: 1.1382405757904053\n",
      "Epoch: 48/100 | step: 265/422 | loss: 1.248565912246704\n",
      "Epoch: 48/100 | step: 266/422 | loss: 1.0819340944290161\n",
      "Epoch: 48/100 | step: 267/422 | loss: 1.1509591341018677\n",
      "Epoch: 48/100 | step: 268/422 | loss: 1.1150437593460083\n",
      "Epoch: 48/100 | step: 269/422 | loss: 1.0891798734664917\n",
      "Epoch: 48/100 | step: 270/422 | loss: 1.123772144317627\n",
      "Epoch: 48/100 | step: 271/422 | loss: 1.1583919525146484\n",
      "Epoch: 48/100 | step: 272/422 | loss: 1.4241228103637695\n",
      "Epoch: 48/100 | step: 273/422 | loss: 1.181341528892517\n",
      "Epoch: 48/100 | step: 274/422 | loss: 1.2475924491882324\n",
      "Epoch: 48/100 | step: 275/422 | loss: 0.8042065501213074\n",
      "Epoch: 48/100 | step: 276/422 | loss: 1.0547552108764648\n",
      "Epoch: 48/100 | step: 277/422 | loss: 1.1246941089630127\n",
      "Epoch: 48/100 | step: 278/422 | loss: 1.0381317138671875\n",
      "Epoch: 48/100 | step: 279/422 | loss: 1.4682353734970093\n",
      "Epoch: 48/100 | step: 280/422 | loss: 1.398392677307129\n",
      "Epoch: 48/100 | step: 281/422 | loss: 0.8472461104393005\n",
      "Epoch: 48/100 | step: 282/422 | loss: 0.9056314826011658\n",
      "Epoch: 48/100 | step: 283/422 | loss: 1.176936149597168\n",
      "Epoch: 48/100 | step: 284/422 | loss: 1.1474735736846924\n",
      "Epoch: 48/100 | step: 285/422 | loss: 1.0694129467010498\n",
      "Epoch: 48/100 | step: 286/422 | loss: 1.235995888710022\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 49/100 | step: 1/422 | loss: 0.6136678457260132\n",
      "Epoch: 49/100 | step: 2/422 | loss: 0.7028064131736755\n",
      "Epoch: 49/100 | step: 3/422 | loss: 0.9896687865257263\n",
      "Epoch: 49/100 | step: 4/422 | loss: 1.4257808923721313\n",
      "Epoch: 49/100 | step: 5/422 | loss: 1.222267746925354\n",
      "Epoch: 49/100 | step: 6/422 | loss: 1.0854448080062866\n",
      "Epoch: 49/100 | step: 7/422 | loss: 0.9620547890663147\n",
      "Epoch: 49/100 | step: 8/422 | loss: 0.7593881487846375\n",
      "Epoch: 49/100 | step: 9/422 | loss: 1.0098142623901367\n",
      "Epoch: 49/100 | step: 10/422 | loss: 0.642431378364563\n",
      "Epoch: 49/100 | step: 11/422 | loss: 1.2254143953323364\n",
      "Epoch: 49/100 | step: 12/422 | loss: 0.9913098216056824\n",
      "Epoch: 49/100 | step: 13/422 | loss: 0.8405594229698181\n",
      "Epoch: 49/100 | step: 14/422 | loss: 0.9426043033599854\n",
      "Epoch: 49/100 | step: 15/422 | loss: 1.0973756313323975\n",
      "Epoch: 49/100 | step: 16/422 | loss: 1.0477346181869507\n",
      "Epoch: 49/100 | step: 17/422 | loss: 1.2175556421279907\n",
      "Epoch: 49/100 | step: 18/422 | loss: 0.938751757144928\n",
      "Epoch: 49/100 | step: 19/422 | loss: 1.0403598546981812\n",
      "Epoch: 49/100 | step: 20/422 | loss: 0.6530861258506775\n",
      "Epoch: 49/100 | step: 21/422 | loss: 0.4968978464603424\n",
      "Epoch: 49/100 | step: 22/422 | loss: 0.8608767986297607\n",
      "Epoch: 49/100 | step: 23/422 | loss: 0.942263662815094\n",
      "Epoch: 49/100 | step: 24/422 | loss: 1.0988467931747437\n",
      "Epoch: 49/100 | step: 25/422 | loss: 0.9410236477851868\n",
      "Epoch: 49/100 | step: 26/422 | loss: 0.7988129258155823\n",
      "Epoch: 49/100 | step: 27/422 | loss: 0.9570844769477844\n",
      "Epoch: 49/100 | step: 28/422 | loss: 0.838173508644104\n",
      "Epoch: 49/100 | step: 29/422 | loss: 0.9780482053756714\n",
      "Epoch: 49/100 | step: 30/422 | loss: 0.9469371438026428\n",
      "Epoch: 49/100 | step: 31/422 | loss: 0.8756515383720398\n",
      "Epoch: 49/100 | step: 32/422 | loss: 0.9076583981513977\n",
      "Epoch: 49/100 | step: 33/422 | loss: 0.6433897018432617\n",
      "Epoch: 49/100 | step: 34/422 | loss: 1.1357439756393433\n",
      "Epoch: 49/100 | step: 35/422 | loss: 1.2029950618743896\n",
      "Epoch: 49/100 | step: 36/422 | loss: 0.7808718681335449\n",
      "Epoch: 49/100 | step: 37/422 | loss: 0.6107860803604126\n",
      "Epoch: 49/100 | step: 38/422 | loss: 0.5664226412773132\n",
      "Epoch: 49/100 | step: 39/422 | loss: 0.8022458553314209\n",
      "Epoch: 49/100 | step: 40/422 | loss: 0.57557213306427\n",
      "Epoch: 49/100 | step: 41/422 | loss: 0.7288445234298706\n",
      "Epoch: 49/100 | step: 42/422 | loss: 1.2373210191726685\n",
      "Epoch: 49/100 | step: 43/422 | loss: 0.8425363898277283\n",
      "Epoch: 49/100 | step: 44/422 | loss: 0.8693526387214661\n",
      "Epoch: 49/100 | step: 45/422 | loss: 0.6963555812835693\n",
      "Epoch: 49/100 | step: 46/422 | loss: 0.7545233964920044\n",
      "Epoch: 49/100 | step: 47/422 | loss: 0.9350347518920898\n",
      "Epoch: 49/100 | step: 48/422 | loss: 1.2990837097167969\n",
      "Epoch: 49/100 | step: 49/422 | loss: 1.1770484447479248\n",
      "Epoch: 49/100 | step: 50/422 | loss: 1.0309302806854248\n",
      "Epoch: 49/100 | step: 51/422 | loss: 0.36731186509132385\n",
      "Epoch: 49/100 | step: 52/422 | loss: 0.9464704394340515\n",
      "Epoch: 49/100 | step: 53/422 | loss: 0.9504161477088928\n",
      "Epoch: 49/100 | step: 54/422 | loss: 1.0149599313735962\n",
      "Epoch: 49/100 | step: 55/422 | loss: 0.8472482562065125\n",
      "Epoch: 49/100 | step: 56/422 | loss: 0.5832498073577881\n",
      "Epoch: 49/100 | step: 57/422 | loss: 0.9226034283638\n",
      "Epoch: 49/100 | step: 58/422 | loss: 1.1126246452331543\n",
      "Epoch: 49/100 | step: 59/422 | loss: 0.9731482863426208\n",
      "Epoch: 49/100 | step: 60/422 | loss: 0.8492090702056885\n",
      "Epoch: 49/100 | step: 61/422 | loss: 1.23922860622406\n",
      "Epoch: 49/100 | step: 62/422 | loss: 1.2266358137130737\n",
      "Epoch: 49/100 | step: 63/422 | loss: 0.8816397786140442\n",
      "Epoch: 49/100 | step: 64/422 | loss: 0.6652047038078308\n",
      "Epoch: 49/100 | step: 65/422 | loss: 0.5132196545600891\n",
      "Epoch: 49/100 | step: 66/422 | loss: 0.9385795593261719\n",
      "Epoch: 49/100 | step: 67/422 | loss: 0.7568403482437134\n",
      "Epoch: 49/100 | step: 68/422 | loss: 1.0477627515792847\n",
      "Epoch: 49/100 | step: 69/422 | loss: 1.3079620599746704\n",
      "Epoch: 49/100 | step: 70/422 | loss: 1.7220408916473389\n",
      "Epoch: 49/100 | step: 71/422 | loss: 1.238824725151062\n",
      "Epoch: 49/100 | step: 72/422 | loss: 1.2270056009292603\n",
      "Epoch: 49/100 | step: 73/422 | loss: 0.7608343362808228\n",
      "Epoch: 49/100 | step: 74/422 | loss: 0.6196726560592651\n",
      "Epoch: 49/100 | step: 75/422 | loss: 1.0662167072296143\n",
      "Epoch: 49/100 | step: 76/422 | loss: 1.2815966606140137\n",
      "Epoch: 49/100 | step: 77/422 | loss: 1.106479287147522\n",
      "Epoch: 49/100 | step: 78/422 | loss: 0.9283910989761353\n",
      "Epoch: 49/100 | step: 79/422 | loss: 0.8726778030395508\n",
      "Epoch: 49/100 | step: 80/422 | loss: 1.0880180597305298\n",
      "Epoch: 49/100 | step: 81/422 | loss: 0.9339250922203064\n",
      "Epoch: 49/100 | step: 82/422 | loss: 0.603818416595459\n",
      "Epoch: 49/100 | step: 83/422 | loss: 0.8509837985038757\n",
      "Epoch: 49/100 | step: 84/422 | loss: 1.038788914680481\n",
      "Epoch: 49/100 | step: 85/422 | loss: 1.1488319635391235\n",
      "Epoch: 49/100 | step: 86/422 | loss: 0.8750287294387817\n",
      "Epoch: 49/100 | step: 87/422 | loss: 0.7934726476669312\n",
      "Epoch: 49/100 | step: 88/422 | loss: 0.7464987635612488\n",
      "Epoch: 49/100 | step: 89/422 | loss: 0.7487282156944275\n",
      "Epoch: 49/100 | step: 90/422 | loss: 0.9155948162078857\n",
      "Epoch: 49/100 | step: 91/422 | loss: 1.052720069885254\n",
      "Epoch: 49/100 | step: 92/422 | loss: 1.0078192949295044\n",
      "Epoch: 49/100 | step: 93/422 | loss: 0.6922008395195007\n",
      "Epoch: 49/100 | step: 94/422 | loss: 0.7764737606048584\n",
      "Epoch: 49/100 | step: 95/422 | loss: 0.8211444616317749\n",
      "Epoch: 49/100 | step: 96/422 | loss: 1.2049756050109863\n",
      "Epoch: 49/100 | step: 97/422 | loss: 1.4135518074035645\n",
      "Epoch: 49/100 | step: 98/422 | loss: 1.4350789785385132\n",
      "Epoch: 49/100 | step: 99/422 | loss: 0.9205682277679443\n",
      "Epoch: 49/100 | step: 100/422 | loss: 0.7703841328620911\n",
      "Epoch: 49/100 | step: 101/422 | loss: 0.6711195707321167\n",
      "Epoch: 49/100 | step: 102/422 | loss: 0.7698126435279846\n",
      "Epoch: 49/100 | step: 103/422 | loss: 1.1189634799957275\n",
      "Epoch: 49/100 | step: 104/422 | loss: 1.6620745658874512\n",
      "Epoch: 49/100 | step: 105/422 | loss: 1.3409408330917358\n",
      "Epoch: 49/100 | step: 106/422 | loss: 0.8645058274269104\n",
      "Epoch: 49/100 | step: 107/422 | loss: 1.226313829421997\n",
      "Epoch: 49/100 | step: 108/422 | loss: 0.9408829212188721\n",
      "Epoch: 49/100 | step: 109/422 | loss: 0.7086762189865112\n",
      "Epoch: 49/100 | step: 110/422 | loss: 1.1378165483474731\n",
      "Epoch: 49/100 | step: 111/422 | loss: 0.8900754451751709\n",
      "Epoch: 49/100 | step: 112/422 | loss: 0.7782792448997498\n",
      "Epoch: 49/100 | step: 113/422 | loss: 1.6878007650375366\n",
      "Epoch: 49/100 | step: 114/422 | loss: 0.807516872882843\n",
      "Epoch: 49/100 | step: 115/422 | loss: 0.8233407735824585\n",
      "Epoch: 49/100 | step: 116/422 | loss: 0.9501593112945557\n",
      "Epoch: 49/100 | step: 117/422 | loss: 1.0293437242507935\n",
      "Epoch: 49/100 | step: 118/422 | loss: 1.6016192436218262\n",
      "Epoch: 49/100 | step: 119/422 | loss: 1.199527621269226\n",
      "Epoch: 49/100 | step: 120/422 | loss: 0.7957392334938049\n",
      "Epoch: 49/100 | step: 121/422 | loss: 1.3465453386306763\n",
      "Epoch: 49/100 | step: 122/422 | loss: 0.8769753575325012\n",
      "Epoch: 49/100 | step: 123/422 | loss: 0.6400551795959473\n",
      "Epoch: 49/100 | step: 124/422 | loss: 1.210728645324707\n",
      "Epoch: 49/100 | step: 125/422 | loss: 1.1954309940338135\n",
      "Epoch: 49/100 | step: 126/422 | loss: 0.8886350989341736\n",
      "Epoch: 49/100 | step: 127/422 | loss: 1.376882791519165\n",
      "Epoch: 49/100 | step: 128/422 | loss: 0.9388331770896912\n",
      "Epoch: 49/100 | step: 129/422 | loss: 0.8109868168830872\n",
      "Epoch: 49/100 | step: 130/422 | loss: 0.9683557152748108\n",
      "Epoch: 49/100 | step: 131/422 | loss: 0.8462228775024414\n",
      "Epoch: 49/100 | step: 132/422 | loss: 1.0949995517730713\n",
      "Epoch: 49/100 | step: 133/422 | loss: 1.0302424430847168\n",
      "Epoch: 49/100 | step: 134/422 | loss: 0.8659899234771729\n",
      "Epoch: 49/100 | step: 135/422 | loss: 0.7600080966949463\n",
      "Epoch: 49/100 | step: 136/422 | loss: 1.3325138092041016\n",
      "Epoch: 49/100 | step: 137/422 | loss: 1.2027314901351929\n",
      "Epoch: 49/100 | step: 138/422 | loss: 0.7164917588233948\n",
      "Epoch: 49/100 | step: 139/422 | loss: 0.7539864182472229\n",
      "Epoch: 49/100 | step: 140/422 | loss: 0.855911910533905\n",
      "Epoch: 49/100 | step: 141/422 | loss: 1.4795905351638794\n",
      "Epoch: 49/100 | step: 142/422 | loss: 0.7817601561546326\n",
      "Epoch: 49/100 | step: 143/422 | loss: 1.4878031015396118\n",
      "Epoch: 49/100 | step: 144/422 | loss: 0.7393102049827576\n",
      "Epoch: 49/100 | step: 145/422 | loss: 0.6596561670303345\n",
      "Epoch: 49/100 | step: 146/422 | loss: 0.5674471259117126\n",
      "Epoch: 49/100 | step: 147/422 | loss: 0.7713497877120972\n",
      "Epoch: 49/100 | step: 148/422 | loss: 0.964152991771698\n",
      "Epoch: 49/100 | step: 149/422 | loss: 1.0404423475265503\n",
      "Epoch: 49/100 | step: 150/422 | loss: 0.7324773073196411\n",
      "Epoch: 49/100 | step: 151/422 | loss: 0.7181084752082825\n",
      "Epoch: 49/100 | step: 152/422 | loss: 1.2529360055923462\n",
      "Epoch: 49/100 | step: 153/422 | loss: 0.8479280471801758\n",
      "Epoch: 49/100 | step: 154/422 | loss: 0.9000486135482788\n",
      "Epoch: 49/100 | step: 155/422 | loss: 1.2596524953842163\n",
      "Epoch: 49/100 | step: 156/422 | loss: 0.9684205651283264\n",
      "Epoch: 49/100 | step: 157/422 | loss: 1.1163618564605713\n",
      "Epoch: 49/100 | step: 158/422 | loss: 0.8275423049926758\n",
      "Epoch: 49/100 | step: 159/422 | loss: 1.1134750843048096\n",
      "Epoch: 49/100 | step: 160/422 | loss: 0.9798563122749329\n",
      "Epoch: 49/100 | step: 161/422 | loss: 1.3423150777816772\n",
      "Epoch: 49/100 | step: 162/422 | loss: 0.5966648459434509\n",
      "Epoch: 49/100 | step: 163/422 | loss: 0.8360997438430786\n",
      "Epoch: 49/100 | step: 164/422 | loss: 1.0101737976074219\n",
      "Epoch: 49/100 | step: 165/422 | loss: 0.6275527477264404\n",
      "Epoch: 49/100 | step: 166/422 | loss: 0.8848433494567871\n",
      "Epoch: 49/100 | step: 167/422 | loss: 1.5870550870895386\n",
      "Epoch: 49/100 | step: 168/422 | loss: 0.7538598775863647\n",
      "Epoch: 49/100 | step: 169/422 | loss: 0.6682766079902649\n",
      "Epoch: 49/100 | step: 170/422 | loss: 0.9639047980308533\n",
      "Epoch: 49/100 | step: 171/422 | loss: 1.5963773727416992\n",
      "Epoch: 49/100 | step: 172/422 | loss: 1.2106823921203613\n",
      "Epoch: 49/100 | step: 173/422 | loss: 0.9307451844215393\n",
      "Epoch: 49/100 | step: 174/422 | loss: 0.8896361589431763\n",
      "Epoch: 49/100 | step: 175/422 | loss: 0.9253251552581787\n",
      "Epoch: 49/100 | step: 176/422 | loss: 1.127706527709961\n",
      "Epoch: 49/100 | step: 177/422 | loss: 0.7806181311607361\n",
      "Epoch: 49/100 | step: 178/422 | loss: 1.2612833976745605\n",
      "Epoch: 49/100 | step: 179/422 | loss: 1.1835284233093262\n",
      "Epoch: 49/100 | step: 180/422 | loss: 1.3079754114151\n",
      "Epoch: 49/100 | step: 181/422 | loss: 0.7869908809661865\n",
      "Epoch: 49/100 | step: 182/422 | loss: 0.6856068968772888\n",
      "Epoch: 49/100 | step: 183/422 | loss: 0.9286985397338867\n",
      "Epoch: 49/100 | step: 184/422 | loss: 0.9139783382415771\n",
      "Epoch: 49/100 | step: 185/422 | loss: 1.5621347427368164\n",
      "Epoch: 49/100 | step: 186/422 | loss: 1.0274873971939087\n",
      "Epoch: 49/100 | step: 187/422 | loss: 1.0064598321914673\n",
      "Epoch: 49/100 | step: 188/422 | loss: 1.182118535041809\n",
      "Epoch: 49/100 | step: 189/422 | loss: 1.3511499166488647\n",
      "Epoch: 49/100 | step: 190/422 | loss: 0.8712391257286072\n",
      "Epoch: 49/100 | step: 191/422 | loss: 1.4533206224441528\n",
      "Epoch: 49/100 | step: 192/422 | loss: 0.6749078035354614\n",
      "Epoch: 49/100 | step: 193/422 | loss: 1.1511048078536987\n",
      "Epoch: 49/100 | step: 194/422 | loss: 0.9118928909301758\n",
      "Epoch: 49/100 | step: 195/422 | loss: 0.7103956341743469\n",
      "Epoch: 49/100 | step: 196/422 | loss: 1.2150458097457886\n",
      "Epoch: 49/100 | step: 197/422 | loss: 1.2912044525146484\n",
      "Epoch: 49/100 | step: 198/422 | loss: 1.667041540145874\n",
      "Epoch: 49/100 | step: 199/422 | loss: 1.0323799848556519\n",
      "Epoch: 49/100 | step: 200/422 | loss: 1.9263514280319214\n",
      "Epoch: 49/100 | step: 201/422 | loss: 1.3094446659088135\n",
      "Epoch: 49/100 | step: 202/422 | loss: 1.7397433519363403\n",
      "Epoch: 49/100 | step: 203/422 | loss: 1.382709264755249\n",
      "Epoch: 49/100 | step: 204/422 | loss: 1.0318999290466309\n",
      "Epoch: 49/100 | step: 205/422 | loss: 1.0082529783248901\n",
      "Epoch: 49/100 | step: 206/422 | loss: 1.1102018356323242\n",
      "Epoch: 49/100 | step: 207/422 | loss: 1.3031553030014038\n",
      "Epoch: 49/100 | step: 208/422 | loss: 1.20212721824646\n",
      "Epoch: 49/100 | step: 209/422 | loss: 0.8937026262283325\n",
      "Epoch: 49/100 | step: 210/422 | loss: 0.9393190741539001\n",
      "Epoch: 49/100 | step: 211/422 | loss: 1.2367289066314697\n",
      "Epoch: 49/100 | step: 212/422 | loss: 1.0910134315490723\n",
      "Epoch: 49/100 | step: 213/422 | loss: 0.8781907558441162\n",
      "Epoch: 49/100 | step: 214/422 | loss: 1.2812628746032715\n",
      "Epoch: 49/100 | step: 215/422 | loss: 0.6966181397438049\n",
      "Epoch: 49/100 | step: 216/422 | loss: 0.8010686039924622\n",
      "Epoch: 49/100 | step: 217/422 | loss: 1.469865083694458\n",
      "Epoch: 49/100 | step: 218/422 | loss: 1.327310562133789\n",
      "Epoch: 49/100 | step: 219/422 | loss: 0.951086699962616\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 50/100 | step: 1/422 | loss: 0.9793533682823181\n",
      "Epoch: 50/100 | step: 2/422 | loss: 0.8296467661857605\n",
      "Epoch: 50/100 | step: 3/422 | loss: 0.6229938268661499\n",
      "Epoch: 50/100 | step: 4/422 | loss: 0.9279574751853943\n",
      "Epoch: 50/100 | step: 5/422 | loss: 0.9474285244941711\n",
      "Epoch: 50/100 | step: 6/422 | loss: 0.8886739611625671\n",
      "Epoch: 50/100 | step: 7/422 | loss: 1.0046303272247314\n",
      "Epoch: 50/100 | step: 8/422 | loss: 0.4980599880218506\n",
      "Epoch: 50/100 | step: 9/422 | loss: 0.7675871253013611\n",
      "Epoch: 50/100 | step: 10/422 | loss: 0.48178631067276\n",
      "Epoch: 50/100 | step: 11/422 | loss: 0.72940593957901\n",
      "Epoch: 50/100 | step: 12/422 | loss: 0.64088374376297\n",
      "Epoch: 50/100 | step: 13/422 | loss: 0.651766836643219\n",
      "Epoch: 50/100 | step: 14/422 | loss: 1.4940285682678223\n",
      "Epoch: 50/100 | step: 15/422 | loss: 0.6652042865753174\n",
      "Epoch: 50/100 | step: 16/422 | loss: 0.7622701525688171\n",
      "Epoch: 50/100 | step: 17/422 | loss: 0.7893646955490112\n",
      "Epoch: 50/100 | step: 18/422 | loss: 0.6646608710289001\n",
      "Epoch: 50/100 | step: 19/422 | loss: 0.695081353187561\n",
      "Epoch: 50/100 | step: 20/422 | loss: 0.5710536241531372\n",
      "Epoch: 50/100 | step: 21/422 | loss: 0.9935786724090576\n",
      "Epoch: 50/100 | step: 22/422 | loss: 0.904699981212616\n",
      "Epoch: 50/100 | step: 23/422 | loss: 0.911251425743103\n",
      "Epoch: 50/100 | step: 24/422 | loss: 0.45940861105918884\n",
      "Epoch: 50/100 | step: 25/422 | loss: 0.586700439453125\n",
      "Epoch: 50/100 | step: 26/422 | loss: 0.7989218235015869\n",
      "Epoch: 50/100 | step: 27/422 | loss: 0.7141611576080322\n",
      "Epoch: 50/100 | step: 28/422 | loss: 0.9691023826599121\n",
      "Epoch: 50/100 | step: 29/422 | loss: 0.6740131974220276\n",
      "Epoch: 50/100 | step: 30/422 | loss: 0.972326397895813\n",
      "Epoch: 50/100 | step: 31/422 | loss: 1.2296115159988403\n",
      "Epoch: 50/100 | step: 32/422 | loss: 1.2165745496749878\n",
      "Epoch: 50/100 | step: 33/422 | loss: 0.48041456937789917\n",
      "Epoch: 50/100 | step: 34/422 | loss: 0.8440662026405334\n",
      "Epoch: 50/100 | step: 35/422 | loss: 0.797555685043335\n",
      "Epoch: 50/100 | step: 36/422 | loss: 0.7225756645202637\n",
      "Epoch: 50/100 | step: 37/422 | loss: 0.7805014252662659\n",
      "Epoch: 50/100 | step: 38/422 | loss: 1.0620771646499634\n",
      "Epoch: 50/100 | step: 39/422 | loss: 0.9582532048225403\n",
      "Epoch: 50/100 | step: 40/422 | loss: 0.7445282340049744\n",
      "Epoch: 50/100 | step: 41/422 | loss: 0.8260104060173035\n",
      "Epoch: 50/100 | step: 42/422 | loss: 1.3542327880859375\n",
      "Epoch: 50/100 | step: 43/422 | loss: 0.9179251194000244\n",
      "Epoch: 50/100 | step: 44/422 | loss: 1.1600341796875\n",
      "Epoch: 50/100 | step: 45/422 | loss: 0.5807981491088867\n",
      "Epoch: 50/100 | step: 46/422 | loss: 1.1006437540054321\n",
      "Epoch: 50/100 | step: 47/422 | loss: 0.9495620727539062\n",
      "Epoch: 50/100 | step: 48/422 | loss: 0.7498330473899841\n",
      "Epoch: 50/100 | step: 49/422 | loss: 0.801105797290802\n",
      "Epoch: 50/100 | step: 50/422 | loss: 1.3415892124176025\n",
      "Epoch: 50/100 | step: 51/422 | loss: 1.1634690761566162\n",
      "Epoch: 50/100 | step: 52/422 | loss: 0.763289213180542\n",
      "Epoch: 50/100 | step: 53/422 | loss: 1.0197566747665405\n",
      "Epoch: 50/100 | step: 54/422 | loss: 0.9904499053955078\n",
      "Epoch: 50/100 | step: 55/422 | loss: 1.413109540939331\n",
      "Epoch: 50/100 | step: 56/422 | loss: 0.9872247576713562\n",
      "Epoch: 50/100 | step: 57/422 | loss: 1.0772570371627808\n",
      "Epoch: 50/100 | step: 58/422 | loss: 0.7843456864356995\n",
      "Epoch: 50/100 | step: 59/422 | loss: 1.20794677734375\n",
      "Epoch: 50/100 | step: 60/422 | loss: 0.8500576615333557\n",
      "Epoch: 50/100 | step: 61/422 | loss: 0.6406521797180176\n",
      "Epoch: 50/100 | step: 62/422 | loss: 0.8456460237503052\n",
      "Epoch: 50/100 | step: 63/422 | loss: 1.3418959379196167\n",
      "Epoch: 50/100 | step: 64/422 | loss: 0.8535107970237732\n",
      "Epoch: 50/100 | step: 65/422 | loss: 1.0753164291381836\n",
      "Epoch: 50/100 | step: 66/422 | loss: 1.078615665435791\n",
      "Epoch: 50/100 | step: 67/422 | loss: 1.3660049438476562\n",
      "Epoch: 50/100 | step: 68/422 | loss: 0.8174546957015991\n",
      "Epoch: 50/100 | step: 69/422 | loss: 0.9691634178161621\n",
      "Epoch: 50/100 | step: 70/422 | loss: 0.8534476161003113\n",
      "Epoch: 50/100 | step: 71/422 | loss: 0.8837956786155701\n",
      "Epoch: 50/100 | step: 72/422 | loss: 0.37918713688850403\n",
      "Epoch: 50/100 | step: 73/422 | loss: 0.9709464311599731\n",
      "Epoch: 50/100 | step: 74/422 | loss: 1.1436859369277954\n",
      "Epoch: 50/100 | step: 75/422 | loss: 1.0930695533752441\n",
      "Epoch: 50/100 | step: 76/422 | loss: 1.5005652904510498\n",
      "Epoch: 50/100 | step: 77/422 | loss: 0.7916406989097595\n",
      "Epoch: 50/100 | step: 78/422 | loss: 1.2523118257522583\n",
      "Epoch: 50/100 | step: 79/422 | loss: 0.6560795307159424\n",
      "Epoch: 50/100 | step: 80/422 | loss: 0.6623849868774414\n",
      "Epoch: 50/100 | step: 81/422 | loss: 1.0048282146453857\n",
      "Epoch: 50/100 | step: 82/422 | loss: 1.087841272354126\n",
      "Epoch: 50/100 | step: 83/422 | loss: 0.8594934940338135\n",
      "Epoch: 50/100 | step: 84/422 | loss: 0.8834805488586426\n",
      "Epoch: 50/100 | step: 85/422 | loss: 1.1079964637756348\n",
      "Epoch: 50/100 | step: 86/422 | loss: 0.6763303875923157\n",
      "Epoch: 50/100 | step: 87/422 | loss: 0.9972284436225891\n",
      "Epoch: 50/100 | step: 88/422 | loss: 1.423094391822815\n",
      "Epoch: 50/100 | step: 89/422 | loss: 0.9900385141372681\n",
      "Epoch: 50/100 | step: 90/422 | loss: 0.8882516026496887\n",
      "Epoch: 50/100 | step: 91/422 | loss: 0.553233802318573\n",
      "Epoch: 50/100 | step: 92/422 | loss: 0.6163690686225891\n",
      "Epoch: 50/100 | step: 93/422 | loss: 0.8157241344451904\n",
      "Epoch: 50/100 | step: 94/422 | loss: 0.7204574942588806\n",
      "Epoch: 50/100 | step: 95/422 | loss: 1.082993745803833\n",
      "Epoch: 50/100 | step: 96/422 | loss: 1.44001042842865\n",
      "Epoch: 50/100 | step: 97/422 | loss: 0.8963927626609802\n",
      "Epoch: 50/100 | step: 98/422 | loss: 0.8017613887786865\n",
      "Epoch: 50/100 | step: 99/422 | loss: 0.6798959970474243\n",
      "Epoch: 50/100 | step: 100/422 | loss: 0.9105060696601868\n",
      "Epoch: 50/100 | step: 101/422 | loss: 1.3655812740325928\n",
      "Epoch: 50/100 | step: 102/422 | loss: 1.1885393857955933\n",
      "Epoch: 50/100 | step: 103/422 | loss: 1.530448317527771\n",
      "Epoch: 50/100 | step: 104/422 | loss: 0.7433183789253235\n",
      "Epoch: 50/100 | step: 105/422 | loss: 0.8156368136405945\n",
      "Epoch: 50/100 | step: 106/422 | loss: 0.7072486877441406\n",
      "Epoch: 50/100 | step: 107/422 | loss: 0.61251300573349\n",
      "Epoch: 50/100 | step: 108/422 | loss: 0.8021687269210815\n",
      "Epoch: 50/100 | step: 109/422 | loss: 0.809222400188446\n",
      "Epoch: 50/100 | step: 110/422 | loss: 0.6709830164909363\n",
      "Epoch: 50/100 | step: 111/422 | loss: 0.8204733729362488\n",
      "Epoch: 50/100 | step: 112/422 | loss: 0.8953840136528015\n",
      "Epoch: 50/100 | step: 113/422 | loss: 1.351568579673767\n",
      "Epoch: 50/100 | step: 114/422 | loss: 1.0998040437698364\n",
      "Epoch: 50/100 | step: 115/422 | loss: 1.1311273574829102\n",
      "Epoch: 50/100 | step: 116/422 | loss: 1.307105302810669\n",
      "Epoch: 50/100 | step: 117/422 | loss: 0.9730464220046997\n",
      "Epoch: 50/100 | step: 118/422 | loss: 0.8969881534576416\n",
      "Epoch: 50/100 | step: 119/422 | loss: 0.6754644513130188\n",
      "Epoch: 50/100 | step: 120/422 | loss: 0.9466913938522339\n",
      "Epoch: 50/100 | step: 121/422 | loss: 1.014032006263733\n",
      "Epoch: 50/100 | step: 122/422 | loss: 0.7823895812034607\n",
      "Epoch: 50/100 | step: 123/422 | loss: 0.8457139730453491\n",
      "Epoch: 50/100 | step: 124/422 | loss: 1.0220592021942139\n",
      "Epoch: 50/100 | step: 125/422 | loss: 0.5874701738357544\n",
      "Epoch: 50/100 | step: 126/422 | loss: 1.2931305170059204\n",
      "Epoch: 50/100 | step: 127/422 | loss: 1.0695796012878418\n",
      "Epoch: 50/100 | step: 128/422 | loss: 0.3671189248561859\n",
      "Epoch: 50/100 | step: 129/422 | loss: 1.0600188970565796\n",
      "Epoch: 50/100 | step: 130/422 | loss: 0.9529908299446106\n",
      "Epoch: 50/100 | step: 131/422 | loss: 1.2341724634170532\n",
      "Epoch: 50/100 | step: 132/422 | loss: 0.965222179889679\n",
      "Epoch: 50/100 | step: 133/422 | loss: 1.0933693647384644\n",
      "Epoch: 50/100 | step: 134/422 | loss: 0.8011348247528076\n",
      "Epoch: 50/100 | step: 135/422 | loss: 1.0707751512527466\n",
      "Epoch: 50/100 | step: 136/422 | loss: 1.0352036952972412\n",
      "Epoch: 50/100 | step: 137/422 | loss: 0.8173871636390686\n",
      "Epoch: 50/100 | step: 138/422 | loss: 0.8075063228607178\n",
      "Epoch: 50/100 | step: 139/422 | loss: 1.0700724124908447\n",
      "Epoch: 50/100 | step: 140/422 | loss: 1.1382659673690796\n",
      "Epoch: 50/100 | step: 141/422 | loss: 1.099940538406372\n",
      "Epoch: 50/100 | step: 142/422 | loss: 1.3987706899642944\n",
      "Epoch: 50/100 | step: 143/422 | loss: 0.8456528782844543\n",
      "Epoch: 50/100 | step: 144/422 | loss: 0.8667382597923279\n",
      "Epoch: 50/100 | step: 145/422 | loss: 0.7723276019096375\n",
      "Epoch: 50/100 | step: 146/422 | loss: 0.751218855381012\n",
      "Epoch: 50/100 | step: 147/422 | loss: 0.859879195690155\n",
      "Epoch: 50/100 | step: 148/422 | loss: 0.8932015895843506\n",
      "Epoch: 50/100 | step: 149/422 | loss: 1.0365986824035645\n",
      "Epoch: 50/100 | step: 150/422 | loss: 0.7181130051612854\n",
      "Epoch: 50/100 | step: 151/422 | loss: 0.9708796739578247\n",
      "Epoch: 50/100 | step: 152/422 | loss: 0.4073794484138489\n",
      "Epoch: 50/100 | step: 153/422 | loss: 0.6367754340171814\n",
      "Epoch: 50/100 | step: 154/422 | loss: 1.0381665229797363\n",
      "Epoch: 50/100 | step: 155/422 | loss: 0.9863750338554382\n",
      "Epoch: 50/100 | step: 156/422 | loss: 1.121650218963623\n",
      "Epoch: 50/100 | step: 157/422 | loss: 0.6231760382652283\n",
      "Epoch: 50/100 | step: 158/422 | loss: 0.8586815595626831\n",
      "Epoch: 50/100 | step: 159/422 | loss: 0.9331148862838745\n",
      "Epoch: 50/100 | step: 160/422 | loss: 1.2888280153274536\n",
      "Epoch: 50/100 | step: 161/422 | loss: 0.7373294234275818\n",
      "Epoch: 50/100 | step: 162/422 | loss: 1.2316796779632568\n",
      "Epoch: 50/100 | step: 163/422 | loss: 0.9958555698394775\n",
      "Epoch: 50/100 | step: 164/422 | loss: 0.7663087248802185\n",
      "Epoch: 50/100 | step: 165/422 | loss: 1.1087449789047241\n",
      "Epoch: 50/100 | step: 166/422 | loss: 0.888662576675415\n",
      "Epoch: 50/100 | step: 167/422 | loss: 1.1498987674713135\n",
      "Epoch: 50/100 | step: 168/422 | loss: 1.1927745342254639\n",
      "Epoch: 50/100 | step: 169/422 | loss: 1.106763482093811\n",
      "Epoch: 50/100 | step: 170/422 | loss: 0.9915526509284973\n",
      "Epoch: 50/100 | step: 171/422 | loss: 0.8103488683700562\n",
      "Epoch: 50/100 | step: 172/422 | loss: 0.7035045623779297\n",
      "Epoch: 50/100 | step: 173/422 | loss: 1.152694821357727\n",
      "Epoch: 50/100 | step: 174/422 | loss: 0.8009085655212402\n",
      "Epoch: 50/100 | step: 175/422 | loss: 0.9736190438270569\n",
      "Epoch: 50/100 | step: 176/422 | loss: 1.133585810661316\n",
      "Epoch: 50/100 | step: 177/422 | loss: 0.8157773613929749\n",
      "Epoch: 50/100 | step: 178/422 | loss: 0.7738571166992188\n",
      "Epoch: 50/100 | step: 179/422 | loss: 0.8219355344772339\n",
      "Epoch: 50/100 | step: 180/422 | loss: 1.0736178159713745\n",
      "Epoch: 50/100 | step: 181/422 | loss: 1.157336950302124\n",
      "Epoch: 50/100 | step: 182/422 | loss: 0.9982181787490845\n",
      "Epoch: 50/100 | step: 183/422 | loss: 0.8802012801170349\n",
      "Epoch: 50/100 | step: 184/422 | loss: 1.1945548057556152\n",
      "Epoch: 50/100 | step: 185/422 | loss: 1.1558890342712402\n",
      "Epoch: 50/100 | step: 186/422 | loss: 1.101488709449768\n",
      "Epoch: 50/100 | step: 187/422 | loss: 0.5971956849098206\n",
      "Epoch: 50/100 | step: 188/422 | loss: 0.9655287265777588\n",
      "Epoch: 50/100 | step: 189/422 | loss: 0.6655927300453186\n",
      "Epoch: 50/100 | step: 190/422 | loss: 0.8716529011726379\n",
      "Epoch: 50/100 | step: 191/422 | loss: 0.7782092690467834\n",
      "Epoch: 50/100 | step: 192/422 | loss: 0.8024003505706787\n",
      "Epoch: 50/100 | step: 193/422 | loss: 0.8832728266716003\n",
      "Epoch: 50/100 | step: 194/422 | loss: 0.9248096346855164\n",
      "Epoch: 50/100 | step: 195/422 | loss: 0.6638623476028442\n",
      "Epoch: 50/100 | step: 196/422 | loss: 0.851427435874939\n",
      "Epoch: 50/100 | step: 197/422 | loss: 0.655085563659668\n",
      "Epoch: 50/100 | step: 198/422 | loss: 1.0773290395736694\n",
      "Epoch: 50/100 | step: 199/422 | loss: 0.5856750011444092\n",
      "Epoch: 50/100 | step: 200/422 | loss: 0.7457523345947266\n",
      "Epoch: 50/100 | step: 201/422 | loss: 1.2566020488739014\n",
      "Epoch: 50/100 | step: 202/422 | loss: 0.6561259627342224\n",
      "Epoch: 50/100 | step: 203/422 | loss: 0.6759430170059204\n",
      "Epoch: 50/100 | step: 204/422 | loss: 0.8686112761497498\n",
      "Epoch: 50/100 | step: 205/422 | loss: 0.7246760129928589\n",
      "Epoch: 50/100 | step: 206/422 | loss: 0.86280757188797\n",
      "Epoch: 50/100 | step: 207/422 | loss: 1.109741449356079\n",
      "Epoch: 50/100 | step: 208/422 | loss: 1.1545467376708984\n",
      "Epoch: 50/100 | step: 209/422 | loss: 0.9603962302207947\n",
      "Epoch: 50/100 | step: 210/422 | loss: 0.7795384526252747\n",
      "Epoch: 50/100 | step: 211/422 | loss: 0.6262030005455017\n",
      "Epoch: 50/100 | step: 212/422 | loss: 0.6520689129829407\n",
      "Epoch: 50/100 | step: 213/422 | loss: 1.2965928316116333\n",
      "Epoch: 50/100 | step: 214/422 | loss: 1.0809139013290405\n",
      "Epoch: 50/100 | step: 215/422 | loss: 0.6055519580841064\n",
      "Epoch: 50/100 | step: 216/422 | loss: 1.0664201974868774\n",
      "Epoch: 50/100 | step: 217/422 | loss: 1.0398677587509155\n",
      "Epoch: 50/100 | step: 218/422 | loss: 1.160014271736145\n",
      "Epoch: 50/100 | step: 219/422 | loss: 1.4067835807800293\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 51/100 | step: 1/422 | loss: 0.7654032111167908\n",
      "Epoch: 51/100 | step: 2/422 | loss: 0.7111932635307312\n",
      "Epoch: 51/100 | step: 3/422 | loss: 0.7812588810920715\n",
      "Epoch: 51/100 | step: 4/422 | loss: 0.9066272974014282\n",
      "Epoch: 51/100 | step: 5/422 | loss: 0.4212912321090698\n",
      "Epoch: 51/100 | step: 6/422 | loss: 0.5836170315742493\n",
      "Epoch: 51/100 | step: 7/422 | loss: 0.5322510600090027\n",
      "Epoch: 51/100 | step: 8/422 | loss: 0.8738366961479187\n",
      "Epoch: 51/100 | step: 9/422 | loss: 0.6670734286308289\n",
      "Epoch: 51/100 | step: 10/422 | loss: 1.1779863834381104\n",
      "Epoch: 51/100 | step: 11/422 | loss: 1.1333369016647339\n",
      "Epoch: 51/100 | step: 12/422 | loss: 0.967337965965271\n",
      "Epoch: 51/100 | step: 13/422 | loss: 0.9654717445373535\n",
      "Epoch: 51/100 | step: 14/422 | loss: 1.1381428241729736\n",
      "Epoch: 51/100 | step: 15/422 | loss: 0.8624947667121887\n",
      "Epoch: 51/100 | step: 16/422 | loss: 1.1321494579315186\n",
      "Epoch: 51/100 | step: 17/422 | loss: 0.6875540614128113\n",
      "Epoch: 51/100 | step: 18/422 | loss: 1.239948034286499\n",
      "Epoch: 51/100 | step: 19/422 | loss: 1.086188793182373\n",
      "Epoch: 51/100 | step: 20/422 | loss: 0.6558148860931396\n",
      "Epoch: 51/100 | step: 21/422 | loss: 0.9747505187988281\n",
      "Epoch: 51/100 | step: 22/422 | loss: 0.8750659227371216\n",
      "Epoch: 51/100 | step: 23/422 | loss: 0.6989773511886597\n",
      "Epoch: 51/100 | step: 24/422 | loss: 1.2700470685958862\n",
      "Epoch: 51/100 | step: 25/422 | loss: 0.6714065670967102\n",
      "Epoch: 51/100 | step: 26/422 | loss: 0.3409774899482727\n",
      "Epoch: 51/100 | step: 27/422 | loss: 0.5779306292533875\n",
      "Epoch: 51/100 | step: 28/422 | loss: 0.743484616279602\n",
      "Epoch: 51/100 | step: 29/422 | loss: 0.6177192330360413\n",
      "Epoch: 51/100 | step: 30/422 | loss: 0.4058515131473541\n",
      "Epoch: 51/100 | step: 31/422 | loss: 1.0365679264068604\n",
      "Epoch: 51/100 | step: 32/422 | loss: 0.41979286074638367\n",
      "Epoch: 51/100 | step: 33/422 | loss: 0.8323298096656799\n",
      "Epoch: 51/100 | step: 34/422 | loss: 0.6806530952453613\n",
      "Epoch: 51/100 | step: 35/422 | loss: 0.6922488808631897\n",
      "Epoch: 51/100 | step: 36/422 | loss: 0.576938271522522\n",
      "Epoch: 51/100 | step: 37/422 | loss: 0.5087252855300903\n",
      "Epoch: 51/100 | step: 38/422 | loss: 0.8190528154373169\n",
      "Epoch: 51/100 | step: 39/422 | loss: 1.0999120473861694\n",
      "Epoch: 51/100 | step: 40/422 | loss: 0.8011898994445801\n",
      "Epoch: 51/100 | step: 41/422 | loss: 1.1673814058303833\n",
      "Epoch: 51/100 | step: 42/422 | loss: 0.8158242106437683\n",
      "Epoch: 51/100 | step: 43/422 | loss: 1.1370099782943726\n",
      "Epoch: 51/100 | step: 44/422 | loss: 0.4490956664085388\n",
      "Epoch: 51/100 | step: 45/422 | loss: 0.8496643304824829\n",
      "Epoch: 51/100 | step: 46/422 | loss: 0.8781045079231262\n",
      "Epoch: 51/100 | step: 47/422 | loss: 0.914841890335083\n",
      "Epoch: 51/100 | step: 48/422 | loss: 0.8975569009780884\n",
      "Epoch: 51/100 | step: 49/422 | loss: 0.39295390248298645\n",
      "Epoch: 51/100 | step: 50/422 | loss: 0.5702421069145203\n",
      "Epoch: 51/100 | step: 51/422 | loss: 0.8342925906181335\n",
      "Epoch: 51/100 | step: 52/422 | loss: 0.9573956727981567\n",
      "Epoch: 51/100 | step: 53/422 | loss: 0.8296986818313599\n",
      "Epoch: 51/100 | step: 54/422 | loss: 0.8358021974563599\n",
      "Epoch: 51/100 | step: 55/422 | loss: 0.9445096850395203\n",
      "Epoch: 51/100 | step: 56/422 | loss: 0.8124880790710449\n",
      "Epoch: 51/100 | step: 57/422 | loss: 1.3057690858840942\n",
      "Epoch: 51/100 | step: 58/422 | loss: 0.9557917714118958\n",
      "Epoch: 51/100 | step: 59/422 | loss: 0.7537298798561096\n",
      "Epoch: 51/100 | step: 60/422 | loss: 0.6159435510635376\n",
      "Epoch: 51/100 | step: 61/422 | loss: 1.0256130695343018\n",
      "Epoch: 51/100 | step: 62/422 | loss: 0.979132354259491\n",
      "Epoch: 51/100 | step: 63/422 | loss: 0.5844807028770447\n",
      "Epoch: 51/100 | step: 64/422 | loss: 0.9980274438858032\n",
      "Epoch: 51/100 | step: 65/422 | loss: 0.7356197237968445\n",
      "Epoch: 51/100 | step: 66/422 | loss: 0.9604364633560181\n",
      "Epoch: 51/100 | step: 67/422 | loss: 1.0081976652145386\n",
      "Epoch: 51/100 | step: 68/422 | loss: 0.8769056797027588\n",
      "Epoch: 51/100 | step: 69/422 | loss: 0.7914119362831116\n",
      "Epoch: 51/100 | step: 70/422 | loss: 0.8000813126564026\n",
      "Epoch: 51/100 | step: 71/422 | loss: 0.7325307130813599\n",
      "Epoch: 51/100 | step: 72/422 | loss: 0.9801865220069885\n",
      "Epoch: 51/100 | step: 73/422 | loss: 0.71100914478302\n",
      "Epoch: 51/100 | step: 74/422 | loss: 1.0606200695037842\n",
      "Epoch: 51/100 | step: 75/422 | loss: 1.1647562980651855\n",
      "Epoch: 51/100 | step: 76/422 | loss: 1.3453181982040405\n",
      "Epoch: 51/100 | step: 77/422 | loss: 0.780526876449585\n",
      "Epoch: 51/100 | step: 78/422 | loss: 0.8944380283355713\n",
      "Epoch: 51/100 | step: 79/422 | loss: 0.8935816287994385\n",
      "Epoch: 51/100 | step: 80/422 | loss: 0.8451975584030151\n",
      "Epoch: 51/100 | step: 81/422 | loss: 0.7892045378684998\n",
      "Epoch: 51/100 | step: 82/422 | loss: 1.2155749797821045\n",
      "Epoch: 51/100 | step: 83/422 | loss: 0.9918359518051147\n",
      "Epoch: 51/100 | step: 84/422 | loss: 0.7502766251564026\n",
      "Epoch: 51/100 | step: 85/422 | loss: 1.134858250617981\n",
      "Epoch: 51/100 | step: 86/422 | loss: 0.7791410088539124\n",
      "Epoch: 51/100 | step: 87/422 | loss: 0.5265060663223267\n",
      "Epoch: 51/100 | step: 88/422 | loss: 0.6827222108840942\n",
      "Epoch: 51/100 | step: 89/422 | loss: 1.022552490234375\n",
      "Epoch: 51/100 | step: 90/422 | loss: 0.659388542175293\n",
      "Epoch: 51/100 | step: 91/422 | loss: 0.8623597621917725\n",
      "Epoch: 51/100 | step: 92/422 | loss: 0.7572200894355774\n",
      "Epoch: 51/100 | step: 93/422 | loss: 0.4801369309425354\n",
      "Epoch: 51/100 | step: 94/422 | loss: 0.777420699596405\n",
      "Epoch: 51/100 | step: 95/422 | loss: 0.8813047409057617\n",
      "Epoch: 51/100 | step: 96/422 | loss: 1.2000092267990112\n",
      "Epoch: 51/100 | step: 97/422 | loss: 0.8999338150024414\n",
      "Epoch: 51/100 | step: 98/422 | loss: 0.8393511176109314\n",
      "Epoch: 51/100 | step: 99/422 | loss: 1.0903587341308594\n",
      "Epoch: 51/100 | step: 100/422 | loss: 0.6534562110900879\n",
      "Epoch: 51/100 | step: 101/422 | loss: 0.9497315883636475\n",
      "Epoch: 51/100 | step: 102/422 | loss: 0.7943585515022278\n",
      "Epoch: 51/100 | step: 103/422 | loss: 0.638321042060852\n",
      "Epoch: 51/100 | step: 104/422 | loss: 0.878703773021698\n",
      "Epoch: 51/100 | step: 105/422 | loss: 0.5726094245910645\n",
      "Epoch: 51/100 | step: 106/422 | loss: 0.701439619064331\n",
      "Epoch: 51/100 | step: 107/422 | loss: 0.8021808862686157\n",
      "Epoch: 51/100 | step: 108/422 | loss: 0.6781825423240662\n",
      "Epoch: 51/100 | step: 109/422 | loss: 1.023855209350586\n",
      "Epoch: 51/100 | step: 110/422 | loss: 0.7317466139793396\n",
      "Epoch: 51/100 | step: 111/422 | loss: 0.652942419052124\n",
      "Epoch: 51/100 | step: 112/422 | loss: 0.8389090299606323\n",
      "Epoch: 51/100 | step: 113/422 | loss: 0.8558369874954224\n",
      "Epoch: 51/100 | step: 114/422 | loss: 0.7413316965103149\n",
      "Epoch: 51/100 | step: 115/422 | loss: 0.7796985507011414\n",
      "Epoch: 51/100 | step: 116/422 | loss: 0.5694959163665771\n",
      "Epoch: 51/100 | step: 117/422 | loss: 0.5716868042945862\n",
      "Epoch: 51/100 | step: 118/422 | loss: 0.5699685215950012\n",
      "Epoch: 51/100 | step: 119/422 | loss: 0.7175680994987488\n",
      "Epoch: 51/100 | step: 120/422 | loss: 0.8019881844520569\n",
      "Epoch: 51/100 | step: 121/422 | loss: 0.7653149962425232\n",
      "Epoch: 51/100 | step: 122/422 | loss: 0.8459904789924622\n",
      "Epoch: 51/100 | step: 123/422 | loss: 0.6390380263328552\n",
      "Epoch: 51/100 | step: 124/422 | loss: 0.48329025506973267\n",
      "Epoch: 51/100 | step: 125/422 | loss: 0.5284481048583984\n",
      "Epoch: 51/100 | step: 126/422 | loss: 0.5778471827507019\n",
      "Epoch: 51/100 | step: 127/422 | loss: 0.904698371887207\n",
      "Epoch: 51/100 | step: 128/422 | loss: 0.7852235436439514\n",
      "Epoch: 51/100 | step: 129/422 | loss: 1.0246710777282715\n",
      "Epoch: 51/100 | step: 130/422 | loss: 0.7559151649475098\n",
      "Epoch: 51/100 | step: 131/422 | loss: 0.6768174767494202\n",
      "Epoch: 51/100 | step: 132/422 | loss: 0.9061897397041321\n",
      "Epoch: 51/100 | step: 133/422 | loss: 0.6308095455169678\n",
      "Epoch: 51/100 | step: 134/422 | loss: 0.5743653774261475\n",
      "Epoch: 51/100 | step: 135/422 | loss: 1.1988627910614014\n",
      "Epoch: 51/100 | step: 136/422 | loss: 1.2932322025299072\n",
      "Epoch: 51/100 | step: 137/422 | loss: 1.0526070594787598\n",
      "Epoch: 51/100 | step: 138/422 | loss: 0.8780205845832825\n",
      "Epoch: 51/100 | step: 139/422 | loss: 0.771483838558197\n",
      "Epoch: 51/100 | step: 140/422 | loss: 0.7723867893218994\n",
      "Epoch: 51/100 | step: 141/422 | loss: 0.6766377687454224\n",
      "Epoch: 51/100 | step: 142/422 | loss: 0.7997565865516663\n",
      "Epoch: 51/100 | step: 143/422 | loss: 0.5315137505531311\n",
      "Epoch: 51/100 | step: 144/422 | loss: 0.8327425718307495\n",
      "Epoch: 51/100 | step: 145/422 | loss: 0.6581583619117737\n",
      "Epoch: 51/100 | step: 146/422 | loss: 0.8143512010574341\n",
      "Epoch: 51/100 | step: 147/422 | loss: 1.2154765129089355\n",
      "Epoch: 51/100 | step: 148/422 | loss: 0.9090959429740906\n",
      "Epoch: 51/100 | step: 149/422 | loss: 0.9156054258346558\n",
      "Epoch: 51/100 | step: 150/422 | loss: 0.8374176621437073\n",
      "Epoch: 51/100 | step: 151/422 | loss: 0.6099437475204468\n",
      "Epoch: 51/100 | step: 152/422 | loss: 1.4197195768356323\n",
      "Epoch: 51/100 | step: 153/422 | loss: 1.2335566282272339\n",
      "Epoch: 51/100 | step: 154/422 | loss: 1.121570110321045\n",
      "Epoch: 51/100 | step: 155/422 | loss: 1.1509203910827637\n",
      "Epoch: 51/100 | step: 156/422 | loss: 0.773712694644928\n",
      "Epoch: 51/100 | step: 157/422 | loss: 0.8644437193870544\n",
      "Epoch: 51/100 | step: 158/422 | loss: 1.4282459020614624\n",
      "Epoch: 51/100 | step: 159/422 | loss: 1.0009334087371826\n",
      "Epoch: 51/100 | step: 160/422 | loss: 1.0470337867736816\n",
      "Epoch: 51/100 | step: 161/422 | loss: 0.8320978879928589\n",
      "Epoch: 51/100 | step: 162/422 | loss: 1.0515692234039307\n",
      "Epoch: 51/100 | step: 163/422 | loss: 0.6710794568061829\n",
      "Epoch: 51/100 | step: 164/422 | loss: 0.7729462385177612\n",
      "Epoch: 51/100 | step: 165/422 | loss: 1.4294798374176025\n",
      "Epoch: 51/100 | step: 166/422 | loss: 0.4438273012638092\n",
      "Epoch: 51/100 | step: 167/422 | loss: 0.7599722743034363\n",
      "Epoch: 51/100 | step: 168/422 | loss: 0.5294889211654663\n",
      "Epoch: 51/100 | step: 169/422 | loss: 1.0172351598739624\n",
      "Epoch: 51/100 | step: 170/422 | loss: 0.5607877969741821\n",
      "Epoch: 51/100 | step: 171/422 | loss: 1.3765480518341064\n",
      "Epoch: 51/100 | step: 172/422 | loss: 0.521586537361145\n",
      "Epoch: 51/100 | step: 173/422 | loss: 0.6576172113418579\n",
      "Epoch: 51/100 | step: 174/422 | loss: 1.2825793027877808\n",
      "Epoch: 51/100 | step: 175/422 | loss: 0.7604954838752747\n",
      "Epoch: 51/100 | step: 176/422 | loss: 0.7185807824134827\n",
      "Epoch: 51/100 | step: 177/422 | loss: 1.2356740236282349\n",
      "Epoch: 51/100 | step: 178/422 | loss: 0.9767516851425171\n",
      "Epoch: 51/100 | step: 179/422 | loss: 0.7062363028526306\n",
      "Epoch: 51/100 | step: 180/422 | loss: 1.101889729499817\n",
      "Epoch: 51/100 | step: 181/422 | loss: 0.7168089747428894\n",
      "Epoch: 51/100 | step: 182/422 | loss: 0.627156138420105\n",
      "Epoch: 51/100 | step: 183/422 | loss: 1.1157089471817017\n",
      "Epoch: 51/100 | step: 184/422 | loss: 0.8090029358863831\n",
      "Epoch: 51/100 | step: 185/422 | loss: 0.5584558248519897\n",
      "Epoch: 51/100 | step: 186/422 | loss: 0.5261639356613159\n",
      "Epoch: 51/100 | step: 187/422 | loss: 1.2415516376495361\n",
      "Epoch: 51/100 | step: 188/422 | loss: 1.2454288005828857\n",
      "Epoch: 51/100 | step: 189/422 | loss: 1.2514525651931763\n",
      "Epoch: 51/100 | step: 190/422 | loss: 0.7741822004318237\n",
      "Epoch: 51/100 | step: 191/422 | loss: 1.406720519065857\n",
      "Epoch: 51/100 | step: 192/422 | loss: 1.1489222049713135\n",
      "Epoch: 51/100 | step: 193/422 | loss: 0.5366670489311218\n",
      "Epoch: 51/100 | step: 194/422 | loss: 0.7983201146125793\n",
      "Epoch: 51/100 | step: 195/422 | loss: 0.7713466286659241\n",
      "Epoch: 51/100 | step: 196/422 | loss: 1.0066043138504028\n",
      "Epoch: 51/100 | step: 197/422 | loss: 0.4401479959487915\n",
      "Epoch: 51/100 | step: 198/422 | loss: 0.7883868217468262\n",
      "Epoch: 51/100 | step: 199/422 | loss: 0.5086620450019836\n",
      "Epoch: 51/100 | step: 200/422 | loss: 0.9779731035232544\n",
      "Epoch: 51/100 | step: 201/422 | loss: 0.6225495338439941\n",
      "Epoch: 51/100 | step: 202/422 | loss: 0.7631273865699768\n",
      "Epoch: 51/100 | step: 203/422 | loss: 1.078555703163147\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 52/100 | step: 1/422 | loss: 0.49304649233818054\n",
      "Epoch: 52/100 | step: 2/422 | loss: 0.860025942325592\n",
      "Epoch: 52/100 | step: 3/422 | loss: 0.9103496670722961\n",
      "Epoch: 52/100 | step: 4/422 | loss: 0.7845731973648071\n",
      "Epoch: 52/100 | step: 5/422 | loss: 0.5994922518730164\n",
      "Epoch: 52/100 | step: 6/422 | loss: 0.846264123916626\n",
      "Epoch: 52/100 | step: 7/422 | loss: 0.4822901785373688\n",
      "Epoch: 52/100 | step: 8/422 | loss: 0.6882554292678833\n",
      "Epoch: 52/100 | step: 9/422 | loss: 0.41434022784233093\n",
      "Epoch: 52/100 | step: 10/422 | loss: 0.6487879157066345\n",
      "Epoch: 52/100 | step: 11/422 | loss: 0.7231917977333069\n",
      "Epoch: 52/100 | step: 12/422 | loss: 0.5047910213470459\n",
      "Epoch: 52/100 | step: 13/422 | loss: 0.7072756290435791\n",
      "Epoch: 52/100 | step: 14/422 | loss: 0.6777015328407288\n",
      "Epoch: 52/100 | step: 15/422 | loss: 0.46227672696113586\n",
      "Epoch: 52/100 | step: 16/422 | loss: 0.5114099383354187\n",
      "Epoch: 52/100 | step: 17/422 | loss: 0.6257104277610779\n",
      "Epoch: 52/100 | step: 18/422 | loss: 0.859097421169281\n",
      "Epoch: 52/100 | step: 19/422 | loss: 0.7607782483100891\n",
      "Epoch: 52/100 | step: 20/422 | loss: 0.7340501546859741\n",
      "Epoch: 52/100 | step: 21/422 | loss: 0.6236834526062012\n",
      "Epoch: 52/100 | step: 22/422 | loss: 0.8314363360404968\n",
      "Epoch: 52/100 | step: 23/422 | loss: 0.6567570567131042\n",
      "Epoch: 52/100 | step: 24/422 | loss: 0.48681920766830444\n",
      "Epoch: 52/100 | step: 25/422 | loss: 0.5210834741592407\n",
      "Epoch: 52/100 | step: 26/422 | loss: 1.013014316558838\n",
      "Epoch: 52/100 | step: 27/422 | loss: 0.7592884302139282\n",
      "Epoch: 52/100 | step: 28/422 | loss: 1.1867362260818481\n",
      "Epoch: 52/100 | step: 29/422 | loss: 0.9908195734024048\n",
      "Epoch: 52/100 | step: 30/422 | loss: 0.4169401228427887\n",
      "Epoch: 52/100 | step: 31/422 | loss: 0.7230402827262878\n",
      "Epoch: 52/100 | step: 32/422 | loss: 0.9093588590621948\n",
      "Epoch: 52/100 | step: 33/422 | loss: 0.6870613098144531\n",
      "Epoch: 52/100 | step: 34/422 | loss: 0.6666467189788818\n",
      "Epoch: 52/100 | step: 35/422 | loss: 0.7324416041374207\n",
      "Epoch: 52/100 | step: 36/422 | loss: 0.8280234932899475\n",
      "Epoch: 52/100 | step: 37/422 | loss: 0.699591875076294\n",
      "Epoch: 52/100 | step: 38/422 | loss: 0.48717400431632996\n",
      "Epoch: 52/100 | step: 39/422 | loss: 0.6238229870796204\n",
      "Epoch: 52/100 | step: 40/422 | loss: 0.6524503827095032\n",
      "Epoch: 52/100 | step: 41/422 | loss: 0.42154762148857117\n",
      "Epoch: 52/100 | step: 42/422 | loss: 0.728794515132904\n",
      "Epoch: 52/100 | step: 43/422 | loss: 0.709059476852417\n",
      "Epoch: 52/100 | step: 44/422 | loss: 0.7646996378898621\n",
      "Epoch: 52/100 | step: 45/422 | loss: 0.996337354183197\n",
      "Epoch: 52/100 | step: 46/422 | loss: 0.7487238049507141\n",
      "Epoch: 52/100 | step: 47/422 | loss: 0.5937316417694092\n",
      "Epoch: 52/100 | step: 48/422 | loss: 0.7734195590019226\n",
      "Epoch: 52/100 | step: 49/422 | loss: 0.517350435256958\n",
      "Epoch: 52/100 | step: 50/422 | loss: 0.9043537974357605\n",
      "Epoch: 52/100 | step: 51/422 | loss: 0.6292112469673157\n",
      "Epoch: 52/100 | step: 52/422 | loss: 0.9363874197006226\n",
      "Epoch: 52/100 | step: 53/422 | loss: 0.5619586110115051\n",
      "Epoch: 52/100 | step: 54/422 | loss: 0.7488164305686951\n",
      "Epoch: 52/100 | step: 55/422 | loss: 0.916217565536499\n",
      "Epoch: 52/100 | step: 56/422 | loss: 1.453965187072754\n",
      "Epoch: 52/100 | step: 57/422 | loss: 0.6091418862342834\n",
      "Epoch: 52/100 | step: 58/422 | loss: 0.9689459800720215\n",
      "Epoch: 52/100 | step: 59/422 | loss: 0.7940724492073059\n",
      "Epoch: 52/100 | step: 60/422 | loss: 0.9096246957778931\n",
      "Epoch: 52/100 | step: 61/422 | loss: 0.699161171913147\n",
      "Epoch: 52/100 | step: 62/422 | loss: 0.7559708952903748\n",
      "Epoch: 52/100 | step: 63/422 | loss: 0.8637971878051758\n",
      "Epoch: 52/100 | step: 64/422 | loss: 0.9652517437934875\n",
      "Epoch: 52/100 | step: 65/422 | loss: 0.7118848562240601\n",
      "Epoch: 52/100 | step: 66/422 | loss: 0.9567180275917053\n",
      "Epoch: 52/100 | step: 67/422 | loss: 0.4109950363636017\n",
      "Epoch: 52/100 | step: 68/422 | loss: 0.545503556728363\n",
      "Epoch: 52/100 | step: 69/422 | loss: 1.0426870584487915\n",
      "Epoch: 52/100 | step: 70/422 | loss: 0.8277378082275391\n",
      "Epoch: 52/100 | step: 71/422 | loss: 0.8105100989341736\n",
      "Epoch: 52/100 | step: 72/422 | loss: 0.7572179436683655\n",
      "Epoch: 52/100 | step: 73/422 | loss: 0.8174471855163574\n",
      "Epoch: 52/100 | step: 74/422 | loss: 1.0855281352996826\n",
      "Epoch: 52/100 | step: 75/422 | loss: 0.5762313008308411\n",
      "Epoch: 52/100 | step: 76/422 | loss: 0.8026710152626038\n",
      "Epoch: 52/100 | step: 77/422 | loss: 1.0715255737304688\n",
      "Epoch: 52/100 | step: 78/422 | loss: 1.4816304445266724\n",
      "Epoch: 52/100 | step: 79/422 | loss: 0.7191950678825378\n",
      "Epoch: 52/100 | step: 80/422 | loss: 0.9037830829620361\n",
      "Epoch: 52/100 | step: 81/422 | loss: 1.3296295404434204\n",
      "Epoch: 52/100 | step: 82/422 | loss: 1.560402512550354\n",
      "Epoch: 52/100 | step: 83/422 | loss: 0.5720775127410889\n",
      "Epoch: 52/100 | step: 84/422 | loss: 1.091363549232483\n",
      "Epoch: 52/100 | step: 85/422 | loss: 0.9741218686103821\n",
      "Epoch: 52/100 | step: 86/422 | loss: 1.0336517095565796\n",
      "Epoch: 52/100 | step: 87/422 | loss: 0.7225558757781982\n",
      "Epoch: 52/100 | step: 88/422 | loss: 0.6757009029388428\n",
      "Epoch: 52/100 | step: 89/422 | loss: 1.1825153827667236\n",
      "Epoch: 52/100 | step: 90/422 | loss: 0.9517180919647217\n",
      "Epoch: 52/100 | step: 91/422 | loss: 0.8069390654563904\n",
      "Epoch: 52/100 | step: 92/422 | loss: 1.0327814817428589\n",
      "Epoch: 52/100 | step: 93/422 | loss: 0.8252397179603577\n",
      "Epoch: 52/100 | step: 94/422 | loss: 1.1979572772979736\n",
      "Epoch: 52/100 | step: 95/422 | loss: 1.0401414632797241\n",
      "Epoch: 52/100 | step: 96/422 | loss: 0.7945057153701782\n",
      "Epoch: 52/100 | step: 97/422 | loss: 1.1431549787521362\n",
      "Epoch: 52/100 | step: 98/422 | loss: 0.5264500379562378\n",
      "Epoch: 52/100 | step: 99/422 | loss: 0.7068291306495667\n",
      "Epoch: 52/100 | step: 100/422 | loss: 0.7148520946502686\n",
      "Epoch: 52/100 | step: 101/422 | loss: 0.7994475364685059\n",
      "Epoch: 52/100 | step: 102/422 | loss: 0.5502333641052246\n",
      "Epoch: 52/100 | step: 103/422 | loss: 0.7589216232299805\n",
      "Epoch: 52/100 | step: 104/422 | loss: 0.80185467004776\n",
      "Epoch: 52/100 | step: 105/422 | loss: 0.8745936751365662\n",
      "Epoch: 52/100 | step: 106/422 | loss: 0.7250505685806274\n",
      "Epoch: 52/100 | step: 107/422 | loss: 0.9437234997749329\n",
      "Epoch: 52/100 | step: 108/422 | loss: 0.9504714608192444\n",
      "Epoch: 52/100 | step: 109/422 | loss: 0.8445765376091003\n",
      "Epoch: 52/100 | step: 110/422 | loss: 0.6895933151245117\n",
      "Epoch: 52/100 | step: 111/422 | loss: 0.8870441913604736\n",
      "Epoch: 52/100 | step: 112/422 | loss: 0.8446375131607056\n",
      "Epoch: 52/100 | step: 113/422 | loss: 1.154659628868103\n",
      "Epoch: 52/100 | step: 114/422 | loss: 1.1867411136627197\n",
      "Epoch: 52/100 | step: 115/422 | loss: 0.805070698261261\n",
      "Epoch: 52/100 | step: 116/422 | loss: 0.49326643347740173\n",
      "Epoch: 52/100 | step: 117/422 | loss: 0.8575825691223145\n",
      "Epoch: 52/100 | step: 118/422 | loss: 0.8471975922584534\n",
      "Epoch: 52/100 | step: 119/422 | loss: 0.6374778151512146\n",
      "Epoch: 52/100 | step: 120/422 | loss: 0.7264281511306763\n",
      "Epoch: 52/100 | step: 121/422 | loss: 0.9540873765945435\n",
      "Epoch: 52/100 | step: 122/422 | loss: 0.802259087562561\n",
      "Epoch: 52/100 | step: 123/422 | loss: 0.7350204586982727\n",
      "Epoch: 52/100 | step: 124/422 | loss: 0.8106964230537415\n",
      "Epoch: 52/100 | step: 125/422 | loss: 1.002874493598938\n",
      "Epoch: 52/100 | step: 126/422 | loss: 1.060421347618103\n",
      "Epoch: 52/100 | step: 127/422 | loss: 0.7241595983505249\n",
      "Epoch: 52/100 | step: 128/422 | loss: 0.6711050271987915\n",
      "Epoch: 52/100 | step: 129/422 | loss: 0.7232267260551453\n",
      "Epoch: 52/100 | step: 130/422 | loss: 0.6833876371383667\n",
      "Epoch: 52/100 | step: 131/422 | loss: 1.0140011310577393\n",
      "Epoch: 52/100 | step: 132/422 | loss: 0.46427828073501587\n",
      "Epoch: 52/100 | step: 133/422 | loss: 0.6899781227111816\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 53/100 | step: 1/422 | loss: 0.7560174465179443\n",
      "Epoch: 53/100 | step: 2/422 | loss: 0.654617428779602\n",
      "Epoch: 53/100 | step: 3/422 | loss: 1.2313655614852905\n",
      "Epoch: 53/100 | step: 4/422 | loss: 0.9931650757789612\n",
      "Epoch: 53/100 | step: 5/422 | loss: 0.6338374614715576\n",
      "Epoch: 53/100 | step: 6/422 | loss: 0.877839207649231\n",
      "Epoch: 53/100 | step: 7/422 | loss: 0.6755221486091614\n",
      "Epoch: 53/100 | step: 8/422 | loss: 1.6173533201217651\n",
      "Epoch: 53/100 | step: 9/422 | loss: 0.6979830265045166\n",
      "Epoch: 53/100 | step: 10/422 | loss: 1.1023873090744019\n",
      "Epoch: 53/100 | step: 11/422 | loss: 0.8453254103660583\n",
      "Epoch: 53/100 | step: 12/422 | loss: 0.3642881512641907\n",
      "Epoch: 53/100 | step: 13/422 | loss: 0.6439040303230286\n",
      "Epoch: 53/100 | step: 14/422 | loss: 0.5582329034805298\n",
      "Epoch: 53/100 | step: 15/422 | loss: 0.44558021426200867\n",
      "Epoch: 53/100 | step: 16/422 | loss: 1.1233958005905151\n",
      "Epoch: 53/100 | step: 17/422 | loss: 0.4052360951900482\n",
      "Epoch: 53/100 | step: 18/422 | loss: 0.8295987248420715\n",
      "Epoch: 53/100 | step: 19/422 | loss: 0.8985588550567627\n",
      "Epoch: 53/100 | step: 20/422 | loss: 0.8062496781349182\n",
      "Epoch: 53/100 | step: 21/422 | loss: 0.8194356560707092\n",
      "Epoch: 53/100 | step: 22/422 | loss: 0.6351243257522583\n",
      "Epoch: 53/100 | step: 23/422 | loss: 0.5079242587089539\n",
      "Epoch: 53/100 | step: 24/422 | loss: 0.670721709728241\n",
      "Epoch: 53/100 | step: 25/422 | loss: 0.648345947265625\n",
      "Epoch: 53/100 | step: 26/422 | loss: 1.021314263343811\n",
      "Epoch: 53/100 | step: 27/422 | loss: 0.9117166996002197\n",
      "Epoch: 53/100 | step: 28/422 | loss: 0.6618165373802185\n",
      "Epoch: 53/100 | step: 29/422 | loss: 0.5511479377746582\n",
      "Epoch: 53/100 | step: 30/422 | loss: 0.4630547761917114\n",
      "Epoch: 53/100 | step: 31/422 | loss: 0.5571061372756958\n",
      "Epoch: 53/100 | step: 32/422 | loss: 0.5852391719818115\n",
      "Epoch: 53/100 | step: 33/422 | loss: 0.719504714012146\n",
      "Epoch: 53/100 | step: 34/422 | loss: 0.8904618620872498\n",
      "Epoch: 53/100 | step: 35/422 | loss: 1.0329245328903198\n",
      "Epoch: 53/100 | step: 36/422 | loss: 0.6971099376678467\n",
      "Epoch: 53/100 | step: 37/422 | loss: 0.6526870727539062\n",
      "Epoch: 53/100 | step: 38/422 | loss: 1.0137237310409546\n",
      "Epoch: 53/100 | step: 39/422 | loss: 0.5785424709320068\n",
      "Epoch: 53/100 | step: 40/422 | loss: 1.1828539371490479\n",
      "Epoch: 53/100 | step: 41/422 | loss: 1.095933437347412\n",
      "Epoch: 53/100 | step: 42/422 | loss: 0.5016277432441711\n",
      "Epoch: 53/100 | step: 43/422 | loss: 0.8076975345611572\n",
      "Epoch: 53/100 | step: 44/422 | loss: 1.2048711776733398\n",
      "Epoch: 53/100 | step: 45/422 | loss: 1.3223271369934082\n",
      "Epoch: 53/100 | step: 46/422 | loss: 1.217075228691101\n",
      "Epoch: 53/100 | step: 47/422 | loss: 1.0317150354385376\n",
      "Epoch: 53/100 | step: 48/422 | loss: 0.7517936825752258\n",
      "Epoch: 53/100 | step: 49/422 | loss: 0.4455779790878296\n",
      "Epoch: 53/100 | step: 50/422 | loss: 0.7906661033630371\n",
      "Epoch: 53/100 | step: 51/422 | loss: 0.9280804991722107\n",
      "Epoch: 53/100 | step: 52/422 | loss: 0.5912594795227051\n",
      "Epoch: 53/100 | step: 53/422 | loss: 1.0061075687408447\n",
      "Epoch: 53/100 | step: 54/422 | loss: 0.8094462156295776\n",
      "Epoch: 53/100 | step: 55/422 | loss: 0.8307957649230957\n",
      "Epoch: 53/100 | step: 56/422 | loss: 0.8460946679115295\n",
      "Epoch: 53/100 | step: 57/422 | loss: 0.5220722556114197\n",
      "Epoch: 53/100 | step: 58/422 | loss: 0.8032357692718506\n",
      "Epoch: 53/100 | step: 59/422 | loss: 0.7941277623176575\n",
      "Epoch: 53/100 | step: 60/422 | loss: 0.615558922290802\n",
      "Epoch: 53/100 | step: 61/422 | loss: 0.6391568183898926\n",
      "Epoch: 53/100 | step: 62/422 | loss: 0.8632005453109741\n",
      "Epoch: 53/100 | step: 63/422 | loss: 1.0447719097137451\n",
      "Epoch: 53/100 | step: 64/422 | loss: 0.9416693449020386\n",
      "Epoch: 53/100 | step: 65/422 | loss: 0.6512112021446228\n",
      "Epoch: 53/100 | step: 66/422 | loss: 0.9166485071182251\n",
      "Epoch: 53/100 | step: 67/422 | loss: 0.3706953227519989\n",
      "Epoch: 53/100 | step: 68/422 | loss: 0.8120839595794678\n",
      "Epoch: 53/100 | step: 69/422 | loss: 1.0534987449645996\n",
      "Epoch: 53/100 | step: 70/422 | loss: 0.7393203973770142\n",
      "Epoch: 53/100 | step: 71/422 | loss: 0.6186359524726868\n",
      "Epoch: 53/100 | step: 72/422 | loss: 0.9240685701370239\n",
      "Epoch: 53/100 | step: 73/422 | loss: 1.1813231706619263\n",
      "Epoch: 53/100 | step: 74/422 | loss: 1.466589093208313\n",
      "Epoch: 53/100 | step: 75/422 | loss: 0.7432844042778015\n",
      "Epoch: 53/100 | step: 76/422 | loss: 0.7692262530326843\n",
      "Epoch: 53/100 | step: 77/422 | loss: 1.0512385368347168\n",
      "Epoch: 53/100 | step: 78/422 | loss: 1.11495041847229\n",
      "Epoch: 53/100 | step: 79/422 | loss: 0.7879538536071777\n",
      "Epoch: 53/100 | step: 80/422 | loss: 0.7533360719680786\n",
      "Epoch: 53/100 | step: 81/422 | loss: 0.48550060391426086\n",
      "Epoch: 53/100 | step: 82/422 | loss: 0.6143537163734436\n",
      "Epoch: 53/100 | step: 83/422 | loss: 0.5358597040176392\n",
      "Epoch: 53/100 | step: 84/422 | loss: 1.0484371185302734\n",
      "Epoch: 53/100 | step: 85/422 | loss: 1.1007840633392334\n",
      "Epoch: 53/100 | step: 86/422 | loss: 0.7064594626426697\n",
      "Epoch: 53/100 | step: 87/422 | loss: 0.9294379353523254\n",
      "Epoch: 53/100 | step: 88/422 | loss: 0.5277295112609863\n",
      "Epoch: 53/100 | step: 89/422 | loss: 0.9089539647102356\n",
      "Epoch: 53/100 | step: 90/422 | loss: 0.7793400883674622\n",
      "Epoch: 53/100 | step: 91/422 | loss: 0.7248018980026245\n",
      "Epoch: 53/100 | step: 92/422 | loss: 1.2691491842269897\n",
      "Epoch: 53/100 | step: 93/422 | loss: 0.7909642457962036\n",
      "Epoch: 53/100 | step: 94/422 | loss: 0.9323678612709045\n",
      "Epoch: 53/100 | step: 95/422 | loss: 0.6847458481788635\n",
      "Epoch: 53/100 | step: 96/422 | loss: 1.0270918607711792\n",
      "Epoch: 53/100 | step: 97/422 | loss: 0.5818802714347839\n",
      "Epoch: 53/100 | step: 98/422 | loss: 0.4258459508419037\n",
      "Epoch: 53/100 | step: 99/422 | loss: 0.793947696685791\n",
      "Epoch: 53/100 | step: 100/422 | loss: 0.9745730757713318\n",
      "Epoch: 53/100 | step: 101/422 | loss: 0.8059953451156616\n",
      "Epoch: 53/100 | step: 102/422 | loss: 0.6389801502227783\n",
      "Epoch: 53/100 | step: 103/422 | loss: 0.4848352372646332\n",
      "Epoch: 53/100 | step: 104/422 | loss: 0.7576857209205627\n",
      "Epoch: 53/100 | step: 105/422 | loss: 1.1976624727249146\n",
      "Epoch: 53/100 | step: 106/422 | loss: 0.6924315094947815\n",
      "Epoch: 53/100 | step: 107/422 | loss: 1.2484548091888428\n",
      "Epoch: 53/100 | step: 108/422 | loss: 0.534763753414154\n",
      "Epoch: 53/100 | step: 109/422 | loss: 0.8406227231025696\n",
      "Epoch: 53/100 | step: 110/422 | loss: 0.9083812832832336\n",
      "Epoch: 53/100 | step: 111/422 | loss: 0.8161429762840271\n",
      "Epoch: 53/100 | step: 112/422 | loss: 0.37450912594795227\n",
      "Epoch: 53/100 | step: 113/422 | loss: 0.7910698652267456\n",
      "Epoch: 53/100 | step: 114/422 | loss: 0.9707565307617188\n",
      "Epoch: 53/100 | step: 115/422 | loss: 0.6223841905593872\n",
      "Epoch: 53/100 | step: 116/422 | loss: 0.6638163924217224\n",
      "Epoch: 53/100 | step: 117/422 | loss: 0.41970837116241455\n",
      "Epoch: 53/100 | step: 118/422 | loss: 0.6314501166343689\n",
      "Epoch: 53/100 | step: 119/422 | loss: 0.7173177599906921\n",
      "Epoch: 53/100 | step: 120/422 | loss: 0.5481062531471252\n",
      "Epoch: 53/100 | step: 121/422 | loss: 0.6439064741134644\n",
      "Epoch: 53/100 | step: 122/422 | loss: 0.948930561542511\n",
      "Epoch: 53/100 | step: 123/422 | loss: 0.5985102653503418\n",
      "Epoch: 53/100 | step: 124/422 | loss: 0.7037148475646973\n",
      "Epoch: 53/100 | step: 125/422 | loss: 0.501770555973053\n",
      "Epoch: 53/100 | step: 126/422 | loss: 0.7902228236198425\n",
      "Epoch: 53/100 | step: 127/422 | loss: 0.7748517990112305\n",
      "Epoch: 53/100 | step: 128/422 | loss: 0.773844301700592\n",
      "Epoch: 53/100 | step: 129/422 | loss: 0.5027350187301636\n",
      "Epoch: 53/100 | step: 130/422 | loss: 0.5526400804519653\n",
      "Epoch: 53/100 | step: 131/422 | loss: 0.6050894856452942\n",
      "Epoch: 53/100 | step: 132/422 | loss: 1.0819774866104126\n",
      "Epoch: 53/100 | step: 133/422 | loss: 0.8968575596809387\n",
      "Epoch: 53/100 | step: 134/422 | loss: 0.7632459998130798\n",
      "Epoch: 53/100 | step: 135/422 | loss: 1.2674306631088257\n",
      "Epoch: 53/100 | step: 136/422 | loss: 0.8166453242301941\n",
      "Epoch: 53/100 | step: 137/422 | loss: 0.6842219233512878\n",
      "Epoch: 53/100 | step: 138/422 | loss: 0.8582367300987244\n",
      "Epoch: 53/100 | step: 139/422 | loss: 0.5900742411613464\n",
      "Epoch: 53/100 | step: 140/422 | loss: 0.5087615847587585\n",
      "Epoch: 53/100 | step: 141/422 | loss: 0.5947389602661133\n",
      "Epoch: 53/100 | step: 142/422 | loss: 0.5830698609352112\n",
      "Epoch: 53/100 | step: 143/422 | loss: 0.7089192867279053\n",
      "Epoch: 53/100 | step: 144/422 | loss: 0.7131104469299316\n",
      "Epoch: 53/100 | step: 145/422 | loss: 0.6339399814605713\n",
      "Epoch: 53/100 | step: 146/422 | loss: 0.5037601590156555\n",
      "Epoch: 53/100 | step: 147/422 | loss: 0.714140772819519\n",
      "Epoch: 53/100 | step: 148/422 | loss: 0.8987890481948853\n",
      "Epoch: 53/100 | step: 149/422 | loss: 1.0452899932861328\n",
      "Epoch: 53/100 | step: 150/422 | loss: 0.8506272435188293\n",
      "Epoch: 53/100 | step: 151/422 | loss: 0.8866370320320129\n",
      "Epoch: 53/100 | step: 152/422 | loss: 0.9888860583305359\n",
      "Epoch: 53/100 | step: 153/422 | loss: 0.8690370321273804\n",
      "Epoch: 53/100 | step: 154/422 | loss: 0.9957450032234192\n",
      "Epoch: 53/100 | step: 155/422 | loss: 0.782036542892456\n",
      "Epoch: 53/100 | step: 156/422 | loss: 0.756605327129364\n",
      "Epoch: 53/100 | step: 157/422 | loss: 0.4338911175727844\n",
      "Epoch: 53/100 | step: 158/422 | loss: 0.879008948802948\n",
      "Epoch: 53/100 | step: 159/422 | loss: 0.7942018508911133\n",
      "Epoch: 53/100 | step: 160/422 | loss: 0.6681615114212036\n",
      "Epoch: 53/100 | step: 161/422 | loss: 0.7226605415344238\n",
      "Epoch: 53/100 | step: 162/422 | loss: 0.7009636759757996\n",
      "Epoch: 53/100 | step: 163/422 | loss: 0.5476474165916443\n",
      "Epoch: 53/100 | step: 164/422 | loss: 0.5949015617370605\n",
      "Epoch: 53/100 | step: 165/422 | loss: 1.0182738304138184\n",
      "Epoch: 53/100 | step: 166/422 | loss: 1.0487290620803833\n",
      "Epoch: 53/100 | step: 167/422 | loss: 0.9967700242996216\n",
      "Epoch: 53/100 | step: 168/422 | loss: 0.4392552673816681\n",
      "Epoch: 53/100 | step: 169/422 | loss: 1.1495031118392944\n",
      "Epoch: 53/100 | step: 170/422 | loss: 0.8471435308456421\n",
      "Epoch: 53/100 | step: 171/422 | loss: 0.9765405654907227\n",
      "Epoch: 53/100 | step: 172/422 | loss: 0.6954193711280823\n",
      "Epoch: 53/100 | step: 173/422 | loss: 0.9735000133514404\n",
      "Epoch: 53/100 | step: 174/422 | loss: 1.0326262712478638\n",
      "Epoch: 53/100 | step: 175/422 | loss: 0.7425506114959717\n",
      "Epoch: 53/100 | step: 176/422 | loss: 0.5264282822608948\n",
      "Epoch: 53/100 | step: 177/422 | loss: 0.9422475695610046\n",
      "Epoch: 53/100 | step: 178/422 | loss: 0.7687488198280334\n",
      "Epoch: 53/100 | step: 179/422 | loss: 0.7641210556030273\n",
      "Epoch: 53/100 | step: 180/422 | loss: 0.553695797920227\n",
      "Epoch: 53/100 | step: 181/422 | loss: 0.8146914839744568\n",
      "Epoch: 53/100 | step: 182/422 | loss: 0.46883246302604675\n",
      "Epoch: 53/100 | step: 183/422 | loss: 0.43366551399230957\n",
      "Epoch: 53/100 | step: 184/422 | loss: 0.5797950029373169\n",
      "Epoch: 53/100 | step: 185/422 | loss: 0.7965049743652344\n",
      "Epoch: 53/100 | step: 186/422 | loss: 0.8395607471466064\n",
      "Epoch: 53/100 | step: 187/422 | loss: 0.9236981868743896\n",
      "Epoch: 53/100 | step: 188/422 | loss: 0.8894806504249573\n",
      "Epoch: 53/100 | step: 189/422 | loss: 1.1392370462417603\n",
      "Epoch: 53/100 | step: 190/422 | loss: 1.019199252128601\n",
      "Epoch: 53/100 | step: 191/422 | loss: 1.086499571800232\n",
      "Epoch: 53/100 | step: 192/422 | loss: 0.9824783205986023\n",
      "Epoch: 53/100 | step: 193/422 | loss: 0.8216809034347534\n",
      "Epoch: 53/100 | step: 194/422 | loss: 0.7953386902809143\n",
      "Epoch: 53/100 | step: 195/422 | loss: 0.877736508846283\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 54/100 | step: 1/422 | loss: 0.4386047422885895\n",
      "Epoch: 54/100 | step: 2/422 | loss: 0.8500452041625977\n",
      "Epoch: 54/100 | step: 3/422 | loss: 0.6906492114067078\n",
      "Epoch: 54/100 | step: 4/422 | loss: 0.7684597969055176\n",
      "Epoch: 54/100 | step: 5/422 | loss: 0.48875659704208374\n",
      "Epoch: 54/100 | step: 6/422 | loss: 0.5118264555931091\n",
      "Epoch: 54/100 | step: 7/422 | loss: 0.5067741870880127\n",
      "Epoch: 54/100 | step: 8/422 | loss: 0.5831055045127869\n",
      "Epoch: 54/100 | step: 9/422 | loss: 0.7250024676322937\n",
      "Epoch: 54/100 | step: 10/422 | loss: 0.5466196537017822\n",
      "Epoch: 54/100 | step: 11/422 | loss: 0.5839153528213501\n",
      "Epoch: 54/100 | step: 12/422 | loss: 0.7515978217124939\n",
      "Epoch: 54/100 | step: 13/422 | loss: 0.8882227540016174\n",
      "Epoch: 54/100 | step: 14/422 | loss: 0.6271194219589233\n",
      "Epoch: 54/100 | step: 15/422 | loss: 0.6117376685142517\n",
      "Epoch: 54/100 | step: 16/422 | loss: 0.5981346964836121\n",
      "Epoch: 54/100 | step: 17/422 | loss: 0.733932614326477\n",
      "Epoch: 54/100 | step: 18/422 | loss: 0.7763564586639404\n",
      "Epoch: 54/100 | step: 19/422 | loss: 0.8140666484832764\n",
      "Epoch: 54/100 | step: 20/422 | loss: 0.5326106548309326\n",
      "Epoch: 54/100 | step: 21/422 | loss: 0.8031792640686035\n",
      "Epoch: 54/100 | step: 22/422 | loss: 0.22276842594146729\n",
      "Epoch: 54/100 | step: 23/422 | loss: 0.3847368657588959\n",
      "Epoch: 54/100 | step: 24/422 | loss: 0.9128087759017944\n",
      "Epoch: 54/100 | step: 25/422 | loss: 0.5845369696617126\n",
      "Epoch: 54/100 | step: 26/422 | loss: 0.6530620455741882\n",
      "Epoch: 54/100 | step: 27/422 | loss: 0.9608339667320251\n",
      "Epoch: 54/100 | step: 28/422 | loss: 0.9467678070068359\n",
      "Epoch: 54/100 | step: 29/422 | loss: 0.6327375173568726\n",
      "Epoch: 54/100 | step: 30/422 | loss: 0.4484240710735321\n",
      "Epoch: 54/100 | step: 31/422 | loss: 0.8114025592803955\n",
      "Epoch: 54/100 | step: 32/422 | loss: 0.6005043387413025\n",
      "Epoch: 54/100 | step: 33/422 | loss: 0.7695369720458984\n",
      "Epoch: 54/100 | step: 34/422 | loss: 0.5789453983306885\n",
      "Epoch: 54/100 | step: 35/422 | loss: 0.648560106754303\n",
      "Epoch: 54/100 | step: 36/422 | loss: 1.1387732028961182\n",
      "Epoch: 54/100 | step: 37/422 | loss: 0.8986610770225525\n",
      "Epoch: 54/100 | step: 38/422 | loss: 1.1251004934310913\n",
      "Epoch: 54/100 | step: 39/422 | loss: 0.7223902344703674\n",
      "Epoch: 54/100 | step: 40/422 | loss: 0.8679200410842896\n",
      "Epoch: 54/100 | step: 41/422 | loss: 0.37540143728256226\n",
      "Epoch: 54/100 | step: 42/422 | loss: 0.8399439454078674\n",
      "Epoch: 54/100 | step: 43/422 | loss: 0.8130924105644226\n",
      "Epoch: 54/100 | step: 44/422 | loss: 0.8673883080482483\n",
      "Epoch: 54/100 | step: 45/422 | loss: 0.4382340610027313\n",
      "Epoch: 54/100 | step: 46/422 | loss: 0.4348643124103546\n",
      "Epoch: 54/100 | step: 47/422 | loss: 0.4814174473285675\n",
      "Epoch: 54/100 | step: 48/422 | loss: 0.6691848039627075\n",
      "Epoch: 54/100 | step: 49/422 | loss: 0.6631287336349487\n",
      "Epoch: 54/100 | step: 50/422 | loss: 0.840945303440094\n",
      "Epoch: 54/100 | step: 51/422 | loss: 0.8994534015655518\n",
      "Epoch: 54/100 | step: 52/422 | loss: 0.5296738743782043\n",
      "Epoch: 54/100 | step: 53/422 | loss: 0.5060544610023499\n",
      "Epoch: 54/100 | step: 54/422 | loss: 0.5084778666496277\n",
      "Epoch: 54/100 | step: 55/422 | loss: 0.6701933145523071\n",
      "Epoch: 54/100 | step: 56/422 | loss: 0.5281124711036682\n",
      "Epoch: 54/100 | step: 57/422 | loss: 1.147535800933838\n",
      "Epoch: 54/100 | step: 58/422 | loss: 0.7094018459320068\n",
      "Epoch: 54/100 | step: 59/422 | loss: 0.7077773809432983\n",
      "Epoch: 54/100 | step: 60/422 | loss: 0.6944481730461121\n",
      "Epoch: 54/100 | step: 61/422 | loss: 1.3222166299819946\n",
      "Epoch: 54/100 | step: 62/422 | loss: 1.0451961755752563\n",
      "Epoch: 54/100 | step: 63/422 | loss: 0.5673177242279053\n",
      "Epoch: 54/100 | step: 64/422 | loss: 0.707092821598053\n",
      "Epoch: 54/100 | step: 65/422 | loss: 0.37508705258369446\n",
      "Epoch: 54/100 | step: 66/422 | loss: 0.7127586603164673\n",
      "Epoch: 54/100 | step: 67/422 | loss: 0.6122693419456482\n",
      "Epoch: 54/100 | step: 68/422 | loss: 0.7700210809707642\n",
      "Epoch: 54/100 | step: 69/422 | loss: 0.534979522228241\n",
      "Epoch: 54/100 | step: 70/422 | loss: 0.7275457382202148\n",
      "Epoch: 54/100 | step: 71/422 | loss: 0.6872768998146057\n",
      "Epoch: 54/100 | step: 72/422 | loss: 0.6664733290672302\n",
      "Epoch: 54/100 | step: 73/422 | loss: 0.3121511936187744\n",
      "Epoch: 54/100 | step: 74/422 | loss: 0.8154233694076538\n",
      "Epoch: 54/100 | step: 75/422 | loss: 0.4687885344028473\n",
      "Epoch: 54/100 | step: 76/422 | loss: 0.6625968217849731\n",
      "Epoch: 54/100 | step: 77/422 | loss: 0.5788991451263428\n",
      "Epoch: 54/100 | step: 78/422 | loss: 0.581261396408081\n",
      "Epoch: 54/100 | step: 79/422 | loss: 1.0319174528121948\n",
      "Epoch: 54/100 | step: 80/422 | loss: 1.093009114265442\n",
      "Epoch: 54/100 | step: 81/422 | loss: 0.43669354915618896\n",
      "Epoch: 54/100 | step: 82/422 | loss: 0.5736783742904663\n",
      "Epoch: 54/100 | step: 83/422 | loss: 0.3514816164970398\n",
      "Epoch: 54/100 | step: 84/422 | loss: 0.5090237259864807\n",
      "Epoch: 54/100 | step: 85/422 | loss: 0.4952673614025116\n",
      "Epoch: 54/100 | step: 86/422 | loss: 1.040432333946228\n",
      "Epoch: 54/100 | step: 87/422 | loss: 0.6493078470230103\n",
      "Epoch: 54/100 | step: 88/422 | loss: 0.4570733308792114\n",
      "Epoch: 54/100 | step: 89/422 | loss: 0.643610417842865\n",
      "Epoch: 54/100 | step: 90/422 | loss: 0.504135251045227\n",
      "Epoch: 54/100 | step: 91/422 | loss: 0.42616915702819824\n",
      "Epoch: 54/100 | step: 92/422 | loss: 0.7851681113243103\n",
      "Epoch: 54/100 | step: 93/422 | loss: 0.5157762765884399\n",
      "Epoch: 54/100 | step: 94/422 | loss: 0.8292301297187805\n",
      "Epoch: 54/100 | step: 95/422 | loss: 0.4415912628173828\n",
      "Epoch: 54/100 | step: 96/422 | loss: 0.389624685049057\n",
      "Epoch: 54/100 | step: 97/422 | loss: 0.9897790551185608\n",
      "Epoch: 54/100 | step: 98/422 | loss: 0.6242468357086182\n",
      "Epoch: 54/100 | step: 99/422 | loss: 0.9810654520988464\n",
      "Epoch: 54/100 | step: 100/422 | loss: 1.1405659914016724\n",
      "Epoch: 54/100 | step: 101/422 | loss: 0.7210626602172852\n",
      "Epoch: 54/100 | step: 102/422 | loss: 0.3917590081691742\n",
      "Epoch: 54/100 | step: 103/422 | loss: 0.8798267841339111\n",
      "Epoch: 54/100 | step: 104/422 | loss: 0.6876986026763916\n",
      "Epoch: 54/100 | step: 105/422 | loss: 0.6362559199333191\n",
      "Epoch: 54/100 | step: 106/422 | loss: 0.8920682668685913\n",
      "Epoch: 54/100 | step: 107/422 | loss: 0.9011862874031067\n",
      "Epoch: 54/100 | step: 108/422 | loss: 1.0595982074737549\n",
      "Epoch: 54/100 | step: 109/422 | loss: 0.7528361678123474\n",
      "Epoch: 54/100 | step: 110/422 | loss: 0.7019956111907959\n",
      "Epoch: 54/100 | step: 111/422 | loss: 0.7709512710571289\n",
      "Epoch: 54/100 | step: 112/422 | loss: 0.6252175569534302\n",
      "Epoch: 54/100 | step: 113/422 | loss: 1.0982627868652344\n",
      "Epoch: 54/100 | step: 114/422 | loss: 0.46903711557388306\n",
      "Epoch: 54/100 | step: 115/422 | loss: 0.6243209838867188\n",
      "Epoch: 54/100 | step: 116/422 | loss: 1.146371841430664\n",
      "Epoch: 54/100 | step: 117/422 | loss: 0.7007793188095093\n",
      "Epoch: 54/100 | step: 118/422 | loss: 0.6678179502487183\n",
      "Epoch: 54/100 | step: 119/422 | loss: 0.5606363415718079\n",
      "Epoch: 54/100 | step: 120/422 | loss: 0.6756790280342102\n",
      "Epoch: 54/100 | step: 121/422 | loss: 0.7554230690002441\n",
      "Epoch: 54/100 | step: 122/422 | loss: 0.6411768198013306\n",
      "Epoch: 54/100 | step: 123/422 | loss: 0.8397355675697327\n",
      "Epoch: 54/100 | step: 124/422 | loss: 0.5690120458602905\n",
      "Epoch: 54/100 | step: 125/422 | loss: 0.4387266933917999\n",
      "Epoch: 54/100 | step: 126/422 | loss: 0.5540119409561157\n",
      "Epoch: 54/100 | step: 127/422 | loss: 0.8733916282653809\n",
      "Epoch: 54/100 | step: 128/422 | loss: 0.8795149326324463\n",
      "Epoch: 54/100 | step: 129/422 | loss: 1.0059889554977417\n",
      "Epoch: 54/100 | step: 130/422 | loss: 0.8956450819969177\n",
      "Epoch: 54/100 | step: 131/422 | loss: 0.8830989003181458\n",
      "Epoch: 54/100 | step: 132/422 | loss: 1.360587239265442\n",
      "Epoch: 54/100 | step: 133/422 | loss: 0.4287892282009125\n",
      "Epoch: 54/100 | step: 134/422 | loss: 0.4456994831562042\n",
      "Epoch: 54/100 | step: 135/422 | loss: 0.4865244925022125\n",
      "Epoch: 54/100 | step: 136/422 | loss: 0.7605515718460083\n",
      "Epoch: 54/100 | step: 137/422 | loss: 0.33843305706977844\n",
      "Epoch: 54/100 | step: 138/422 | loss: 0.5110465884208679\n",
      "Epoch: 54/100 | step: 139/422 | loss: 0.5486299395561218\n",
      "Epoch: 54/100 | step: 140/422 | loss: 0.5118967890739441\n",
      "Epoch: 54/100 | step: 141/422 | loss: 0.5712469220161438\n",
      "Epoch: 54/100 | step: 142/422 | loss: 0.8270795345306396\n",
      "Epoch: 54/100 | step: 143/422 | loss: 1.0805308818817139\n",
      "Epoch: 54/100 | step: 144/422 | loss: 0.7361933588981628\n",
      "Epoch: 54/100 | step: 145/422 | loss: 0.6363580226898193\n",
      "Epoch: 54/100 | step: 146/422 | loss: 0.6869605779647827\n",
      "Epoch: 54/100 | step: 147/422 | loss: 0.7603678703308105\n",
      "Epoch: 54/100 | step: 148/422 | loss: 0.8650574088096619\n",
      "Epoch: 54/100 | step: 149/422 | loss: 0.578333854675293\n",
      "Epoch: 54/100 | step: 150/422 | loss: 0.5330506563186646\n",
      "Epoch: 54/100 | step: 151/422 | loss: 0.42032331228256226\n",
      "Epoch: 54/100 | step: 152/422 | loss: 0.9543595314025879\n",
      "Epoch: 54/100 | step: 153/422 | loss: 1.0253167152404785\n",
      "Epoch: 54/100 | step: 154/422 | loss: 0.8851437568664551\n",
      "Epoch: 54/100 | step: 155/422 | loss: 0.6786920428276062\n",
      "Epoch: 54/100 | step: 156/422 | loss: 0.4435514807701111\n",
      "Epoch: 54/100 | step: 157/422 | loss: 0.861804723739624\n",
      "Epoch: 54/100 | step: 158/422 | loss: 1.0632731914520264\n",
      "Epoch: 54/100 | step: 159/422 | loss: 0.879736602306366\n",
      "Epoch: 54/100 | step: 160/422 | loss: 1.027154803276062\n",
      "Epoch: 54/100 | step: 161/422 | loss: 0.6170968413352966\n",
      "Epoch: 54/100 | step: 162/422 | loss: 0.5846688151359558\n",
      "Epoch: 54/100 | step: 163/422 | loss: 0.4928499460220337\n",
      "Epoch: 54/100 | step: 164/422 | loss: 0.820384681224823\n",
      "Epoch: 54/100 | step: 165/422 | loss: 0.9115908145904541\n",
      "Epoch: 54/100 | step: 166/422 | loss: 0.5181490778923035\n",
      "Epoch: 54/100 | step: 167/422 | loss: 0.6629236936569214\n",
      "Epoch: 54/100 | step: 168/422 | loss: 0.605647087097168\n",
      "Epoch: 54/100 | step: 169/422 | loss: 0.3814390301704407\n",
      "Epoch: 54/100 | step: 170/422 | loss: 0.5863252282142639\n",
      "Epoch: 54/100 | step: 171/422 | loss: 0.35842153429985046\n",
      "Epoch: 54/100 | step: 172/422 | loss: 0.8612140417098999\n",
      "Epoch: 54/100 | step: 173/422 | loss: 0.5637776255607605\n",
      "Epoch: 54/100 | step: 174/422 | loss: 0.5552656054496765\n",
      "Epoch: 54/100 | step: 175/422 | loss: 0.9146227836608887\n",
      "Epoch: 54/100 | step: 176/422 | loss: 0.8020994663238525\n",
      "Epoch: 54/100 | step: 177/422 | loss: 0.9523633122444153\n",
      "Epoch: 54/100 | step: 178/422 | loss: 0.8241393566131592\n",
      "Epoch: 54/100 | step: 179/422 | loss: 0.9882689714431763\n",
      "Epoch: 54/100 | step: 180/422 | loss: 0.6963614225387573\n",
      "Epoch: 54/100 | step: 181/422 | loss: 0.8963748216629028\n",
      "Epoch: 54/100 | step: 182/422 | loss: 0.9564674496650696\n",
      "Epoch: 54/100 | step: 183/422 | loss: 0.8759474158287048\n",
      "Epoch: 54/100 | step: 184/422 | loss: 0.648713231086731\n",
      "Epoch: 54/100 | step: 185/422 | loss: 0.4152294397354126\n",
      "Epoch: 54/100 | step: 186/422 | loss: 0.8158287405967712\n",
      "Epoch: 54/100 | step: 187/422 | loss: 0.49009037017822266\n",
      "Epoch: 54/100 | step: 188/422 | loss: 0.6977972984313965\n",
      "Epoch: 54/100 | step: 189/422 | loss: 0.7086054086685181\n",
      "Epoch: 54/100 | step: 190/422 | loss: 0.7440366744995117\n",
      "Epoch: 54/100 | step: 191/422 | loss: 0.645431637763977\n",
      "Epoch: 54/100 | step: 192/422 | loss: 0.7011513113975525\n",
      "Epoch: 54/100 | step: 193/422 | loss: 1.5530028343200684\n",
      "Epoch: 54/100 | step: 194/422 | loss: 0.5597212910652161\n",
      "Epoch: 54/100 | step: 195/422 | loss: 0.9367853403091431\n",
      "Epoch: 54/100 | step: 196/422 | loss: 1.1311010122299194\n",
      "Epoch: 54/100 | step: 197/422 | loss: 0.7831348180770874\n",
      "Epoch: 54/100 | step: 198/422 | loss: 0.7131083011627197\n",
      "Epoch: 54/100 | step: 199/422 | loss: 0.7823524475097656\n",
      "Epoch: 54/100 | step: 200/422 | loss: 0.6752329468727112\n",
      "Epoch: 54/100 | step: 201/422 | loss: 0.847389280796051\n",
      "Epoch: 54/100 | step: 202/422 | loss: 0.5150777101516724\n",
      "Epoch: 54/100 | step: 203/422 | loss: 0.5322491526603699\n",
      "Epoch: 54/100 | step: 204/422 | loss: 0.6801759600639343\n",
      "Epoch: 54/100 | step: 205/422 | loss: 0.9138652086257935\n",
      "Epoch: 54/100 | step: 206/422 | loss: 0.931607723236084\n",
      "Epoch: 54/100 | step: 207/422 | loss: 1.162057638168335\n",
      "Epoch: 54/100 | step: 208/422 | loss: 0.8500139713287354\n",
      "Epoch: 54/100 | step: 209/422 | loss: 0.8349975347518921\n",
      "Epoch: 54/100 | step: 210/422 | loss: 0.9205549955368042\n",
      "Epoch: 54/100 | step: 211/422 | loss: 0.7888379693031311\n",
      "Epoch: 54/100 | step: 212/422 | loss: 0.8020519018173218\n",
      "Epoch: 54/100 | step: 213/422 | loss: 0.6612460613250732\n",
      "Epoch: 54/100 | step: 214/422 | loss: 0.6516412496566772\n",
      "Epoch: 54/100 | step: 215/422 | loss: 0.688240110874176\n",
      "Epoch: 54/100 | step: 216/422 | loss: 0.5174804925918579\n",
      "Epoch: 54/100 | step: 217/422 | loss: 0.6487734317779541\n",
      "Epoch: 54/100 | step: 218/422 | loss: 0.631497323513031\n",
      "Epoch: 54/100 | step: 219/422 | loss: 0.7725685834884644\n",
      "Epoch: 54/100 | step: 220/422 | loss: 0.7992462515830994\n",
      "Epoch: 54/100 | step: 221/422 | loss: 0.4275284707546234\n",
      "Epoch: 54/100 | step: 222/422 | loss: 0.8921896815299988\n",
      "Epoch: 54/100 | step: 223/422 | loss: 1.1095925569534302\n",
      "Epoch: 54/100 | step: 224/422 | loss: 0.9460227489471436\n",
      "Epoch: 54/100 | step: 225/422 | loss: 0.6760937571525574\n",
      "Epoch: 54/100 | step: 226/422 | loss: 0.8608568906784058\n",
      "Epoch: 54/100 | step: 227/422 | loss: 0.6103343367576599\n",
      "Epoch: 54/100 | step: 228/422 | loss: 0.7647653818130493\n",
      "Epoch: 54/100 | step: 229/422 | loss: 0.914889395236969\n",
      "Epoch: 54/100 | step: 230/422 | loss: 1.1450183391571045\n",
      "Epoch: 54/100 | step: 231/422 | loss: 0.8668603897094727\n",
      "Epoch: 54/100 | step: 232/422 | loss: 0.5818240642547607\n",
      "Epoch: 54/100 | step: 233/422 | loss: 0.7695638537406921\n",
      "Epoch: 54/100 | step: 234/422 | loss: 0.7014154195785522\n",
      "Epoch: 54/100 | step: 235/422 | loss: 0.5668230652809143\n",
      "Epoch: 54/100 | step: 236/422 | loss: 0.65776526927948\n",
      "Epoch: 54/100 | step: 237/422 | loss: 0.7147589921951294\n",
      "Epoch: 54/100 | step: 238/422 | loss: 0.6162285804748535\n",
      "Epoch: 54/100 | step: 239/422 | loss: 0.6749345064163208\n",
      "Epoch: 54/100 | step: 240/422 | loss: 0.5129469037055969\n",
      "Epoch: 54/100 | step: 241/422 | loss: 0.7996772527694702\n",
      "Epoch: 54/100 | step: 242/422 | loss: 0.8183198571205139\n",
      "Epoch: 54/100 | step: 243/422 | loss: 0.4390468895435333\n",
      "Epoch: 54/100 | step: 244/422 | loss: 1.385276198387146\n",
      "Epoch: 54/100 | step: 245/422 | loss: 0.8427901864051819\n",
      "Epoch: 54/100 | step: 246/422 | loss: 0.5316391587257385\n",
      "Epoch: 54/100 | step: 247/422 | loss: 0.39650827646255493\n",
      "Epoch: 54/100 | step: 248/422 | loss: 0.5632304549217224\n",
      "Epoch: 54/100 | step: 249/422 | loss: 0.882621705532074\n",
      "Epoch: 54/100 | step: 250/422 | loss: 0.6305012702941895\n",
      "Epoch: 54/100 | step: 251/422 | loss: 0.7203988432884216\n",
      "Epoch: 54/100 | step: 252/422 | loss: 0.8670065402984619\n",
      "Epoch: 54/100 | step: 253/422 | loss: 0.6210081577301025\n",
      "Epoch: 54/100 | step: 254/422 | loss: 0.6912221312522888\n",
      "Epoch: 54/100 | step: 255/422 | loss: 0.5764750838279724\n",
      "Epoch: 54/100 | step: 256/422 | loss: 0.7726996541023254\n",
      "Epoch: 54/100 | step: 257/422 | loss: 0.7390216588973999\n",
      "Epoch: 54/100 | step: 258/422 | loss: 0.875338077545166\n",
      "Epoch: 54/100 | step: 259/422 | loss: 0.5435789227485657\n",
      "Epoch: 54/100 | step: 260/422 | loss: 0.9801932573318481\n",
      "Epoch: 54/100 | step: 261/422 | loss: 0.7615597248077393\n",
      "Epoch: 54/100 | step: 262/422 | loss: 0.546305239200592\n",
      "Epoch: 54/100 | step: 263/422 | loss: 0.44586482644081116\n",
      "Epoch: 54/100 | step: 264/422 | loss: 0.5258432626724243\n",
      "Epoch: 54/100 | step: 265/422 | loss: 0.5089917778968811\n",
      "Epoch: 54/100 | step: 266/422 | loss: 0.821450412273407\n",
      "Epoch: 54/100 | step: 267/422 | loss: 0.744750440120697\n",
      "Epoch: 54/100 | step: 268/422 | loss: 0.5922366380691528\n",
      "Epoch: 54/100 | step: 269/422 | loss: 0.6755125522613525\n",
      "Epoch: 54/100 | step: 270/422 | loss: 0.6017209887504578\n",
      "Epoch: 54/100 | step: 271/422 | loss: 0.46600890159606934\n",
      "Epoch: 54/100 | step: 272/422 | loss: 0.40172868967056274\n",
      "Epoch: 54/100 | step: 273/422 | loss: 0.7540676593780518\n",
      "Epoch: 54/100 | step: 274/422 | loss: 0.8389264941215515\n",
      "Epoch: 54/100 | step: 275/422 | loss: 0.7768211960792542\n",
      "Epoch: 54/100 | step: 276/422 | loss: 0.7662831544876099\n",
      "Epoch: 54/100 | step: 277/422 | loss: 0.47845524549484253\n",
      "Epoch: 54/100 | step: 278/422 | loss: 0.5123316049575806\n",
      "Epoch: 54/100 | step: 279/422 | loss: 0.5490782856941223\n",
      "Epoch: 54/100 | step: 280/422 | loss: 0.5686574578285217\n",
      "Epoch: 54/100 | step: 281/422 | loss: 0.8089588284492493\n",
      "Epoch: 54/100 | step: 282/422 | loss: 0.7770317196846008\n",
      "Epoch: 54/100 | step: 283/422 | loss: 0.9076804518699646\n",
      "Epoch: 54/100 | step: 284/422 | loss: 0.8049528002738953\n",
      "Epoch: 54/100 | step: 285/422 | loss: 0.9698036313056946\n",
      "Epoch: 54/100 | step: 286/422 | loss: 0.8833511471748352\n",
      "Epoch: 54/100 | step: 287/422 | loss: 0.7284463047981262\n",
      "Epoch: 54/100 | step: 288/422 | loss: 1.3623719215393066\n",
      "Epoch: 54/100 | step: 289/422 | loss: 1.071013331413269\n",
      "Epoch: 54/100 | step: 290/422 | loss: 0.9133596420288086\n",
      "Epoch: 54/100 | step: 291/422 | loss: 0.604049801826477\n",
      "Epoch: 54/100 | step: 292/422 | loss: 0.8556972742080688\n",
      "Epoch: 54/100 | step: 293/422 | loss: 0.5451622009277344\n",
      "Epoch: 54/100 | step: 294/422 | loss: 0.5397318601608276\n",
      "Epoch: 54/100 | step: 295/422 | loss: 1.0805667638778687\n",
      "Epoch: 54/100 | step: 296/422 | loss: 0.6589822769165039\n",
      "Epoch: 54/100 | step: 297/422 | loss: 0.6087365746498108\n",
      "Epoch: 54/100 | step: 298/422 | loss: 0.6122760772705078\n",
      "Epoch: 54/100 | step: 299/422 | loss: 0.5342286229133606\n",
      "Epoch: 54/100 | step: 300/422 | loss: 0.886017918586731\n",
      "Epoch: 54/100 | step: 301/422 | loss: 0.5479986667633057\n",
      "Epoch: 54/100 | step: 302/422 | loss: 0.9524945020675659\n",
      "Epoch: 54/100 | step: 303/422 | loss: 0.7540660500526428\n",
      "Epoch: 54/100 | step: 304/422 | loss: 0.5449129343032837\n",
      "Epoch: 54/100 | step: 305/422 | loss: 0.6124463677406311\n",
      "Epoch: 54/100 | step: 306/422 | loss: 0.698345959186554\n",
      "Epoch: 54/100 | step: 307/422 | loss: 0.5779940485954285\n",
      "Epoch: 54/100 | step: 308/422 | loss: 0.694580078125\n",
      "Epoch: 54/100 | step: 309/422 | loss: 0.7351069450378418\n",
      "Epoch: 54/100 | step: 310/422 | loss: 0.5203993320465088\n",
      "Epoch: 54/100 | step: 311/422 | loss: 0.6947715878486633\n",
      "Epoch: 54/100 | step: 312/422 | loss: 0.5670625567436218\n",
      "Epoch: 54/100 | step: 313/422 | loss: 0.7994278073310852\n",
      "Epoch: 54/100 | step: 314/422 | loss: 0.549500584602356\n",
      "Epoch: 54/100 | step: 315/422 | loss: 1.126321792602539\n",
      "Epoch: 54/100 | step: 316/422 | loss: 0.6665040254592896\n",
      "Epoch: 54/100 | step: 317/422 | loss: 0.6910587549209595\n",
      "Epoch: 54/100 | step: 318/422 | loss: 0.48440077900886536\n",
      "Epoch: 54/100 | step: 319/422 | loss: 0.715079128742218\n",
      "Epoch: 54/100 | step: 320/422 | loss: 0.692926287651062\n",
      "Epoch: 54/100 | step: 321/422 | loss: 0.9888154864311218\n",
      "Epoch: 54/100 | step: 322/422 | loss: 1.0062775611877441\n",
      "Epoch: 54/100 | step: 323/422 | loss: 0.7294130325317383\n",
      "Epoch: 54/100 | step: 324/422 | loss: 1.0023772716522217\n",
      "Epoch: 54/100 | step: 325/422 | loss: 0.5987321138381958\n",
      "Epoch: 54/100 | step: 326/422 | loss: 1.1086210012435913\n",
      "Epoch: 54/100 | step: 327/422 | loss: 0.9131452441215515\n",
      "Epoch: 54/100 | step: 328/422 | loss: 0.47886210680007935\n",
      "Epoch: 54/100 | step: 329/422 | loss: 0.7440711259841919\n",
      "Epoch: 54/100 | step: 330/422 | loss: 0.7124444246292114\n",
      "Epoch: 54/100 | step: 331/422 | loss: 0.5878632664680481\n",
      "Epoch: 54/100 | step: 332/422 | loss: 1.028239130973816\n",
      "Epoch: 54/100 | step: 333/422 | loss: 0.7336282134056091\n",
      "Epoch: 54/100 | step: 334/422 | loss: 0.9380654692649841\n",
      "Epoch: 54/100 | step: 335/422 | loss: 0.5900744199752808\n",
      "Epoch: 54/100 | step: 336/422 | loss: 0.6296117305755615\n",
      "Epoch: 54/100 | step: 337/422 | loss: 0.7856223583221436\n",
      "Epoch: 54/100 | step: 338/422 | loss: 0.7572924494743347\n",
      "Epoch: 54/100 | step: 339/422 | loss: 0.7293829321861267\n",
      "Epoch: 54/100 | step: 340/422 | loss: 0.6331740617752075\n",
      "Epoch: 54/100 | step: 341/422 | loss: 0.7952820062637329\n",
      "Epoch: 54/100 | step: 342/422 | loss: 1.041330099105835\n",
      "Epoch: 54/100 | step: 343/422 | loss: 0.781020998954773\n",
      "Epoch: 54/100 | step: 344/422 | loss: 0.7320697903633118\n",
      "Epoch: 54/100 | step: 345/422 | loss: 0.8606294989585876\n",
      "Epoch: 54/100 | step: 346/422 | loss: 0.909123420715332\n",
      "Epoch: 54/100 | step: 347/422 | loss: 0.5129537582397461\n",
      "Epoch: 54/100 | step: 348/422 | loss: 1.0133150815963745\n",
      "Epoch: 54/100 | step: 349/422 | loss: 0.5723080635070801\n",
      "Epoch: 54/100 | step: 350/422 | loss: 0.8649744391441345\n",
      "Epoch: 54/100 | step: 351/422 | loss: 0.44128578901290894\n",
      "Epoch: 54/100 | step: 352/422 | loss: 0.728696346282959\n",
      "Epoch: 54/100 | step: 353/422 | loss: 1.1149678230285645\n",
      "Epoch: 54/100 | step: 354/422 | loss: 0.6544197201728821\n",
      "Epoch: 54/100 | step: 355/422 | loss: 0.8891723155975342\n",
      "Epoch: 54/100 | step: 356/422 | loss: 0.6131038665771484\n",
      "Epoch: 54/100 | step: 357/422 | loss: 0.6957006454467773\n",
      "Epoch: 54/100 | step: 358/422 | loss: 0.7786514759063721\n",
      "Epoch: 54/100 | step: 359/422 | loss: 0.5975781083106995\n",
      "Epoch: 54/100 | step: 360/422 | loss: 0.6132897734642029\n",
      "Epoch: 54/100 | step: 361/422 | loss: 0.7510310411453247\n",
      "Epoch: 54/100 | step: 362/422 | loss: 0.7651611566543579\n",
      "Epoch: 54/100 | step: 363/422 | loss: 0.7990801930427551\n",
      "Epoch: 54/100 | step: 364/422 | loss: 0.9797618389129639\n",
      "Epoch: 54/100 | step: 365/422 | loss: 0.42361581325531006\n",
      "Epoch: 54/100 | step: 366/422 | loss: 0.9283342361450195\n",
      "Epoch: 54/100 | step: 367/422 | loss: 0.6333088278770447\n",
      "Epoch: 54/100 | step: 368/422 | loss: 0.9881150126457214\n",
      "Epoch: 54/100 | step: 369/422 | loss: 0.5516241192817688\n",
      "Epoch: 54/100 | step: 370/422 | loss: 0.8546075224876404\n",
      "Epoch: 54/100 | step: 371/422 | loss: 0.8248065710067749\n",
      "Epoch: 54/100 | step: 372/422 | loss: 0.7497807741165161\n",
      "Epoch: 54/100 | step: 373/422 | loss: 0.8543457984924316\n",
      "Epoch: 54/100 | step: 374/422 | loss: 0.9682303071022034\n",
      "Error occurred during training: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple\n",
      "Epoch: 55/100 | step: 1/422 | loss: 0.41124817728996277\n",
      "Epoch: 55/100 | step: 2/422 | loss: 0.4847959280014038\n",
      "Epoch: 55/100 | step: 3/422 | loss: 0.5869329571723938\n",
      "Epoch: 55/100 | step: 4/422 | loss: 0.44342875480651855\n",
      "Epoch: 55/100 | step: 5/422 | loss: 0.3752659857273102\n",
      "Epoch: 55/100 | step: 6/422 | loss: 0.5242394804954529\n",
      "Epoch: 55/100 | step: 7/422 | loss: 0.5785950422286987\n",
      "Epoch: 55/100 | step: 8/422 | loss: 0.6567029356956482\n",
      "Epoch: 55/100 | step: 9/422 | loss: 0.3944227993488312\n",
      "Epoch: 55/100 | step: 10/422 | loss: 0.6165695786476135\n",
      "Epoch: 55/100 | step: 11/422 | loss: 0.5551750063896179\n",
      "Epoch: 55/100 | step: 12/422 | loss: 0.4486493170261383\n",
      "Epoch: 55/100 | step: 13/422 | loss: 0.35151922702789307\n",
      "Epoch: 55/100 | step: 14/422 | loss: 0.6907199621200562\n",
      "Epoch: 55/100 | step: 15/422 | loss: 0.5161228775978088\n",
      "Epoch: 55/100 | step: 16/422 | loss: 0.466031551361084\n",
      "Epoch: 55/100 | step: 17/422 | loss: 0.7888922095298767\n",
      "Epoch: 55/100 | step: 18/422 | loss: 0.4397803843021393\n",
      "Epoch: 55/100 | step: 19/422 | loss: 0.33492934703826904\n",
      "Epoch: 55/100 | step: 20/422 | loss: 0.5540685057640076\n",
      "Epoch: 55/100 | step: 21/422 | loss: 0.5764952301979065\n",
      "Epoch: 55/100 | step: 22/422 | loss: 0.7975310683250427\n",
      "Epoch: 55/100 | step: 23/422 | loss: 0.3062778413295746\n",
      "Epoch: 55/100 | step: 24/422 | loss: 0.34753283858299255\n",
      "Epoch: 55/100 | step: 25/422 | loss: 0.7194238901138306\n",
      "Epoch: 55/100 | step: 26/422 | loss: 0.568768322467804\n",
      "Epoch: 55/100 | step: 27/422 | loss: 0.6161856651306152\n",
      "Epoch: 55/100 | step: 28/422 | loss: 0.6008089184761047\n",
      "Epoch: 55/100 | step: 29/422 | loss: 0.6606029272079468\n",
      "Epoch: 55/100 | step: 30/422 | loss: 0.39486393332481384\n",
      "Epoch: 55/100 | step: 31/422 | loss: 0.8323520421981812\n",
      "Epoch: 55/100 | step: 32/422 | loss: 0.4774743914604187\n",
      "Epoch: 55/100 | step: 33/422 | loss: 0.7593659162521362\n",
      "Epoch: 55/100 | step: 34/422 | loss: 0.5590272545814514\n",
      "Epoch: 55/100 | step: 35/422 | loss: 0.6534676551818848\n",
      "Epoch: 55/100 | step: 36/422 | loss: 0.7853729724884033\n",
      "Epoch: 55/100 | step: 37/422 | loss: 0.5190634727478027\n",
      "Epoch: 55/100 | step: 38/422 | loss: 0.4563724994659424\n",
      "Epoch: 55/100 | step: 39/422 | loss: 0.47821611166000366\n",
      "Epoch: 55/100 | step: 40/422 | loss: 0.21161052584648132\n",
      "Epoch: 55/100 | step: 41/422 | loss: 0.6209443807601929\n",
      "Epoch: 55/100 | step: 42/422 | loss: 0.4683607518672943\n",
      "Epoch: 55/100 | step: 43/422 | loss: 0.32694679498672485\n",
      "Epoch: 55/100 | step: 44/422 | loss: 0.34006166458129883\n",
      "Epoch: 55/100 | step: 45/422 | loss: 0.44684314727783203\n",
      "Epoch: 55/100 | step: 46/422 | loss: 0.6038437485694885\n",
      "Epoch: 55/100 | step: 47/422 | loss: 0.6457754969596863\n",
      "Epoch: 55/100 | step: 48/422 | loss: 0.5120013952255249\n",
      "Epoch: 55/100 | step: 49/422 | loss: 0.6762059926986694\n",
      "Epoch: 55/100 | step: 50/422 | loss: 0.4990955591201782\n",
      "Epoch: 55/100 | step: 51/422 | loss: 0.3917705714702606\n",
      "Epoch: 55/100 | step: 52/422 | loss: 0.28388187289237976\n",
      "Epoch: 55/100 | step: 53/422 | loss: 0.4279133379459381\n",
      "Epoch: 55/100 | step: 54/422 | loss: 0.5042191743850708\n",
      "Epoch: 55/100 | step: 55/422 | loss: 0.44344857335090637\n",
      "Epoch: 55/100 | step: 56/422 | loss: 0.8155145645141602\n",
      "Epoch: 55/100 | step: 57/422 | loss: 0.36743342876434326\n",
      "Epoch: 55/100 | step: 58/422 | loss: 0.40599867701530457\n",
      "Epoch: 55/100 | step: 59/422 | loss: 0.5219730734825134\n",
      "Epoch: 55/100 | step: 60/422 | loss: 0.9209315180778503\n",
      "Epoch: 55/100 | step: 61/422 | loss: 0.7713111639022827\n",
      "Epoch: 55/100 | step: 62/422 | loss: 0.5129266977310181\n",
      "Epoch: 55/100 | step: 63/422 | loss: 0.4465063512325287\n",
      "Epoch: 55/100 | step: 64/422 | loss: 0.4446677565574646\n",
      "Epoch: 55/100 | step: 65/422 | loss: 0.446755975484848\n",
      "Epoch: 55/100 | step: 66/422 | loss: 0.5276486873626709\n",
      "Epoch: 55/100 | step: 67/422 | loss: 0.6279470324516296\n",
      "Epoch: 55/100 | step: 68/422 | loss: 0.36382803320884705\n",
      "Epoch: 55/100 | step: 69/422 | loss: 0.5662806630134583\n",
      "Epoch: 55/100 | step: 70/422 | loss: 0.5360440611839294\n",
      "Epoch: 55/100 | step: 71/422 | loss: 0.7450453042984009\n",
      "Epoch: 55/100 | step: 72/422 | loss: 0.9560319185256958\n",
      "Epoch: 55/100 | step: 73/422 | loss: 0.6156351566314697\n",
      "Epoch: 55/100 | step: 74/422 | loss: 0.3474040627479553\n",
      "Epoch: 55/100 | step: 75/422 | loss: 0.7938336133956909\n",
      "Epoch: 55/100 | step: 76/422 | loss: 0.5890117287635803\n",
      "Epoch: 55/100 | step: 77/422 | loss: 0.9154368042945862\n",
      "Epoch: 55/100 | step: 78/422 | loss: 0.2585832476615906\n",
      "Epoch: 55/100 | step: 79/422 | loss: 0.45189398527145386\n",
      "Epoch: 55/100 | step: 80/422 | loss: 1.1424797773361206\n",
      "Epoch: 55/100 | step: 81/422 | loss: 0.43483251333236694\n",
      "Epoch: 55/100 | step: 82/422 | loss: 0.5004473328590393\n",
      "Epoch: 55/100 | step: 83/422 | loss: 0.958512544631958\n",
      "Epoch: 55/100 | step: 84/422 | loss: 0.3012820780277252\n",
      "Epoch: 55/100 | step: 85/422 | loss: 1.136459469795227\n",
      "Epoch: 55/100 | step: 86/422 | loss: 0.4691152274608612\n",
      "Epoch: 55/100 | step: 87/422 | loss: 0.595514178276062\n",
      "Epoch: 55/100 | step: 88/422 | loss: 0.6283771991729736\n",
      "Epoch: 55/100 | step: 89/422 | loss: 0.6058638691902161\n",
      "Epoch: 55/100 | step: 90/422 | loss: 0.8540183901786804\n",
      "Epoch: 55/100 | step: 91/422 | loss: 0.34406882524490356\n",
      "Epoch: 55/100 | step: 92/422 | loss: 0.5214810371398926\n",
      "Epoch: 55/100 | step: 93/422 | loss: 0.5726252794265747\n",
      "Epoch: 55/100 | step: 94/422 | loss: 0.39826831221580505\n",
      "Epoch: 55/100 | step: 95/422 | loss: 0.5412949323654175\n",
      "Epoch: 55/100 | step: 96/422 | loss: 0.6653950810432434\n",
      "Epoch: 55/100 | step: 97/422 | loss: 0.7644891142845154\n",
      "Epoch: 55/100 | step: 98/422 | loss: 0.7379186153411865\n",
      "Epoch: 55/100 | step: 99/422 | loss: 0.8918013572692871\n",
      "Epoch: 55/100 | step: 100/422 | loss: 0.3593938648700714\n",
      "Epoch: 55/100 | step: 101/422 | loss: 0.7093904614448547\n",
      "Epoch: 55/100 | step: 102/422 | loss: 0.8600011467933655\n",
      "Epoch: 55/100 | step: 103/422 | loss: 0.5746921896934509\n",
      "Epoch: 55/100 | step: 104/422 | loss: 0.37533456087112427\n",
      "Epoch: 55/100 | step: 105/422 | loss: 0.583939254283905\n",
      "Epoch: 55/100 | step: 106/422 | loss: 0.8676905632019043\n",
      "Epoch: 55/100 | step: 107/422 | loss: 0.5539892315864563\n",
      "Epoch: 55/100 | step: 108/422 | loss: 0.8634526133537292\n",
      "Epoch: 55/100 | step: 109/422 | loss: 0.8253232836723328\n",
      "Epoch: 55/100 | step: 110/422 | loss: 0.7727535963058472\n",
      "Epoch: 55/100 | step: 111/422 | loss: 0.703328549861908\n",
      "Epoch: 55/100 | step: 112/422 | loss: 0.737406313419342\n",
      "Epoch: 55/100 | step: 113/422 | loss: 0.49521416425704956\n",
      "Epoch: 55/100 | step: 114/422 | loss: 0.4915938079357147\n",
      "Epoch: 55/100 | step: 115/422 | loss: 0.7951987981796265\n",
      "Epoch: 55/100 | step: 116/422 | loss: 1.0472482442855835\n",
      "Epoch: 55/100 | step: 117/422 | loss: 0.8597472310066223\n",
      "Epoch: 55/100 | step: 118/422 | loss: 0.6668347120285034\n",
      "Epoch: 55/100 | step: 119/422 | loss: 0.6842678189277649\n",
      "Epoch: 55/100 | step: 120/422 | loss: 0.29510024189949036\n",
      "Epoch: 55/100 | step: 121/422 | loss: 0.8337293863296509\n",
      "Epoch: 55/100 | step: 122/422 | loss: 0.6102344393730164\n",
      "Epoch: 55/100 | step: 123/422 | loss: 0.4901009202003479\n",
      "Epoch: 55/100 | step: 124/422 | loss: 0.5532224178314209\n",
      "Epoch: 55/100 | step: 125/422 | loss: 0.47653982043266296\n",
      "Epoch: 55/100 | step: 126/422 | loss: 0.8084810972213745\n",
      "Epoch: 55/100 | step: 127/422 | loss: 0.7240738868713379\n",
      "Epoch: 55/100 | step: 128/422 | loss: 0.5131410956382751\n",
      "Epoch: 55/100 | step: 129/422 | loss: 0.7600856423377991\n",
      "Epoch: 55/100 | step: 130/422 | loss: 0.4416389763355255\n",
      "Epoch: 55/100 | step: 131/422 | loss: 0.6935838460922241\n",
      "Epoch: 55/100 | step: 132/422 | loss: 0.9795758724212646\n",
      "Epoch: 55/100 | step: 133/422 | loss: 0.4221446216106415\n",
      "Epoch: 55/100 | step: 134/422 | loss: 1.0050128698349\n",
      "Epoch: 55/100 | step: 135/422 | loss: 0.6712974309921265\n",
      "Epoch: 55/100 | step: 136/422 | loss: 0.8413424491882324\n",
      "Epoch: 55/100 | step: 137/422 | loss: 0.32336336374282837\n",
      "Epoch: 55/100 | step: 138/422 | loss: 0.49623897671699524\n",
      "Epoch: 55/100 | step: 139/422 | loss: 0.38510340452194214\n",
      "Epoch: 55/100 | step: 140/422 | loss: 0.5049011707305908\n",
      "Epoch: 55/100 | step: 141/422 | loss: 0.543956458568573\n",
      "Epoch: 55/100 | step: 142/422 | loss: 0.5752721428871155\n",
      "Epoch: 55/100 | step: 143/422 | loss: 0.33729296922683716\n",
      "Epoch: 55/100 | step: 144/422 | loss: 0.2098078727722168\n",
      "Epoch: 55/100 | step: 145/422 | loss: 0.5109100937843323\n",
      "Epoch: 55/100 | step: 146/422 | loss: 0.5052118897438049\n",
      "Epoch: 55/100 | step: 147/422 | loss: 0.6562667489051819\n",
      "Epoch: 55/100 | step: 148/422 | loss: 0.8662371635437012\n",
      "Epoch: 55/100 | step: 149/422 | loss: 0.26015231013298035\n",
      "Epoch: 55/100 | step: 150/422 | loss: 0.29378318786621094\n",
      "Epoch: 55/100 | step: 151/422 | loss: 0.4706859886646271\n",
      "Epoch: 55/100 | step: 152/422 | loss: 0.6980566382408142\n",
      "Epoch: 55/100 | step: 153/422 | loss: 0.3317835330963135\n",
      "Epoch: 55/100 | step: 154/422 | loss: 0.3544110655784607\n",
      "Epoch: 55/100 | step: 155/422 | loss: 0.5366770625114441\n",
      "Epoch: 55/100 | step: 156/422 | loss: 0.435946524143219\n",
      "Epoch: 55/100 | step: 157/422 | loss: 0.46147388219833374\n",
      "Epoch: 55/100 | step: 158/422 | loss: 0.8326209187507629\n",
      "Epoch: 55/100 | step: 159/422 | loss: 0.6196863651275635\n",
      "Epoch: 55/100 | step: 160/422 | loss: 0.3878394365310669\n",
      "Epoch: 55/100 | step: 161/422 | loss: 0.4553579092025757\n",
      "Epoch: 55/100 | step: 162/422 | loss: 0.7748012542724609\n",
      "Epoch: 55/100 | step: 163/422 | loss: 0.46725672483444214\n",
      "Epoch: 55/100 | step: 164/422 | loss: 0.7565039396286011\n",
      "Epoch: 55/100 | step: 165/422 | loss: 0.35749587416648865\n",
      "Epoch: 55/100 | step: 166/422 | loss: 0.8999127149581909\n",
      "Epoch: 55/100 | step: 167/422 | loss: 0.3818344473838806\n",
      "Epoch: 55/100 | step: 168/422 | loss: 0.465013325214386\n",
      "Epoch: 55/100 | step: 169/422 | loss: 0.5386136770248413\n",
      "Epoch: 55/100 | step: 170/422 | loss: 0.5493379235267639\n",
      "Epoch: 55/100 | step: 171/422 | loss: 0.3431088626384735\n",
      "Epoch: 55/100 | step: 172/422 | loss: 0.25178760290145874\n",
      "Epoch: 55/100 | step: 173/422 | loss: 0.3705405592918396\n",
      "Epoch: 55/100 | step: 174/422 | loss: 0.3598492741584778\n",
      "Epoch: 55/100 | step: 175/422 | loss: 0.40648385882377625\n",
      "Epoch: 55/100 | step: 176/422 | loss: 0.8543359041213989\n",
      "Epoch: 55/100 | step: 177/422 | loss: 0.3187008798122406\n",
      "Epoch: 55/100 | step: 178/422 | loss: 0.38887274265289307\n",
      "Epoch: 55/100 | step: 179/422 | loss: 0.39051353931427\n",
      "Epoch: 55/100 | step: 180/422 | loss: 0.5897697806358337\n",
      "Epoch: 55/100 | step: 181/422 | loss: 0.4059298038482666\n",
      "Epoch: 55/100 | step: 182/422 | loss: 0.3617321848869324\n",
      "Epoch: 55/100 | step: 183/422 | loss: 0.4467531740665436\n",
      "Epoch: 55/100 | step: 184/422 | loss: 0.38204947113990784\n",
      "Epoch: 55/100 | step: 185/422 | loss: 0.394275426864624\n",
      "Epoch: 55/100 | step: 186/422 | loss: 0.3535454571247101\n",
      "Epoch: 55/100 | step: 187/422 | loss: 0.7762743234634399\n",
      "Epoch: 55/100 | step: 188/422 | loss: 0.5496313571929932\n",
      "Epoch: 55/100 | step: 189/422 | loss: 0.3276234269142151\n",
      "Epoch: 55/100 | step: 190/422 | loss: 0.8094585537910461\n",
      "Epoch: 55/100 | step: 191/422 | loss: 0.5777848958969116\n",
      "Epoch: 55/100 | step: 192/422 | loss: 0.4804489016532898\n",
      "Epoch: 55/100 | step: 193/422 | loss: 0.5122817158699036\n",
      "Epoch: 55/100 | step: 194/422 | loss: 0.8697112798690796\n",
      "Epoch: 55/100 | step: 195/422 | loss: 0.6838628649711609\n",
      "Epoch: 55/100 | step: 196/422 | loss: 0.4369177222251892\n",
      "Epoch: 55/100 | step: 197/422 | loss: 1.16771399974823\n",
      "Epoch: 55/100 | step: 198/422 | loss: 0.9363712072372437\n",
      "Epoch: 55/100 | step: 199/422 | loss: 0.5121356248855591\n",
      "Epoch: 55/100 | step: 200/422 | loss: 0.7353543043136597\n",
      "Epoch: 55/100 | step: 201/422 | loss: 0.7254884243011475\n",
      "Epoch: 55/100 | step: 202/422 | loss: 0.8709619045257568\n",
      "Epoch: 55/100 | step: 203/422 | loss: 0.3581324517726898\n",
      "Epoch: 55/100 | step: 204/422 | loss: 0.7918744087219238\n",
      "Epoch: 55/100 | step: 205/422 | loss: 0.4236961901187897\n",
      "Epoch: 55/100 | step: 206/422 | loss: 0.29278919100761414\n",
      "Epoch: 55/100 | step: 207/422 | loss: 0.5468299388885498\n",
      "Epoch: 55/100 | step: 208/422 | loss: 0.4616020619869232\n",
      "Epoch: 55/100 | step: 209/422 | loss: 0.573270320892334\n",
      "Epoch: 55/100 | step: 210/422 | loss: 0.4221175014972687\n",
      "Epoch: 55/100 | step: 211/422 | loss: 0.31925538182258606\n",
      "Epoch: 55/100 | step: 212/422 | loss: 0.6590408086776733\n",
      "Epoch: 55/100 | step: 213/422 | loss: 0.5322545170783997\n",
      "Epoch: 55/100 | step: 214/422 | loss: 1.015986680984497\n",
      "Epoch: 55/100 | step: 215/422 | loss: 0.3189822733402252\n",
      "Epoch: 55/100 | step: 216/422 | loss: 0.5579296350479126\n",
      "Epoch: 55/100 | step: 217/422 | loss: 0.6384294629096985\n",
      "Epoch: 55/100 | step: 218/422 | loss: 0.8020554184913635\n",
      "Epoch: 55/100 | step: 219/422 | loss: 0.5409038662910461\n",
      "Epoch: 55/100 | step: 220/422 | loss: 0.5393630266189575\n",
      "Epoch: 55/100 | step: 221/422 | loss: 0.7685672044754028\n",
      "Epoch: 55/100 | step: 222/422 | loss: 0.916799783706665\n",
      "Epoch: 55/100 | step: 223/422 | loss: 0.4367746114730835\n",
      "Epoch: 55/100 | step: 224/422 | loss: 0.3787872791290283\n",
      "Epoch: 55/100 | step: 225/422 | loss: 0.739658534526825\n",
      "Epoch: 55/100 | step: 226/422 | loss: 0.5190165042877197\n",
      "Epoch: 55/100 | step: 227/422 | loss: 0.4024970233440399\n",
      "Epoch: 55/100 | step: 228/422 | loss: 0.5310837626457214\n",
      "Epoch: 55/100 | step: 229/422 | loss: 0.6515836715698242\n",
      "Epoch: 55/100 | step: 230/422 | loss: 0.5314761400222778\n",
      "Epoch: 55/100 | step: 231/422 | loss: 0.511981725692749\n",
      "Epoch: 55/100 | step: 232/422 | loss: 0.5425572991371155\n",
      "Epoch: 55/100 | step: 233/422 | loss: 0.3066622316837311\n",
      "Epoch: 55/100 | step: 234/422 | loss: 0.41042566299438477\n",
      "Epoch: 55/100 | step: 235/422 | loss: 0.6041569709777832\n",
      "Epoch: 55/100 | step: 236/422 | loss: 0.6704303026199341\n",
      "Epoch: 55/100 | step: 237/422 | loss: 0.6480850577354431\n",
      "Epoch: 55/100 | step: 238/422 | loss: 0.6931805610656738\n",
      "Epoch: 55/100 | step: 239/422 | loss: 0.6383446455001831\n",
      "Epoch: 55/100 | step: 240/422 | loss: 0.43775564432144165\n",
      "Epoch: 55/100 | step: 241/422 | loss: 0.42697644233703613\n",
      "Epoch: 55/100 | step: 242/422 | loss: 0.6920971274375916\n",
      "Epoch: 55/100 | step: 243/422 | loss: 0.7448116540908813\n",
      "Epoch: 55/100 | step: 244/422 | loss: 0.8310468196868896\n",
      "Epoch: 55/100 | step: 245/422 | loss: 1.4186022281646729\n",
      "Epoch: 55/100 | step: 246/422 | loss: 0.5877593755722046\n",
      "Epoch: 55/100 | step: 247/422 | loss: 0.410637766122818\n",
      "Epoch: 55/100 | step: 248/422 | loss: 0.7842922806739807\n",
      "Epoch: 55/100 | step: 249/422 | loss: 0.9932979941368103\n",
      "Epoch: 55/100 | step: 250/422 | loss: 0.551702082157135\n",
      "Epoch: 55/100 | step: 251/422 | loss: 0.72568678855896\n",
      "Epoch: 55/100 | step: 252/422 | loss: 0.48325881361961365\n",
      "Epoch: 55/100 | step: 253/422 | loss: 0.48074427247047424\n",
      "Epoch: 55/100 | step: 254/422 | loss: 0.5322049856185913\n",
      "Epoch: 55/100 | step: 255/422 | loss: 0.7694647312164307\n",
      "Epoch: 55/100 | step: 256/422 | loss: 0.4920281171798706\n",
      "Epoch: 55/100 | step: 257/422 | loss: 0.7148361802101135\n",
      "Epoch: 55/100 | step: 258/422 | loss: 0.7075990438461304\n",
      "Epoch: 55/100 | step: 259/422 | loss: 0.3854573369026184\n",
      "Epoch: 55/100 | step: 260/422 | loss: 0.5703637599945068\n",
      "Epoch: 55/100 | step: 261/422 | loss: 0.7754874229431152\n",
      "Epoch: 55/100 | step: 262/422 | loss: 0.7135190963745117\n",
      "Epoch: 55/100 | step: 263/422 | loss: 0.6551308035850525\n",
      "Epoch: 55/100 | step: 264/422 | loss: 0.416921466588974\n",
      "Epoch: 55/100 | step: 265/422 | loss: 0.6261013150215149\n",
      "Epoch: 55/100 | step: 266/422 | loss: 0.5909643769264221\n",
      "Epoch: 55/100 | step: 267/422 | loss: 0.41802000999450684\n",
      "Epoch: 55/100 | step: 268/422 | loss: 0.4535612463951111\n",
      "Epoch: 55/100 | step: 269/422 | loss: 0.46877360343933105\n",
      "Epoch: 55/100 | step: 270/422 | loss: 0.5951489210128784\n",
      "Epoch: 55/100 | step: 271/422 | loss: 0.5167527198791504\n",
      "Epoch: 55/100 | step: 272/422 | loss: 0.49070096015930176\n",
      "Epoch: 55/100 | step: 273/422 | loss: 0.37325045466423035\n",
      "Epoch: 55/100 | step: 274/422 | loss: 0.25745105743408203\n",
      "Epoch: 55/100 | step: 275/422 | loss: 0.7235830426216125\n",
      "Epoch: 55/100 | step: 276/422 | loss: 1.0968044996261597\n",
      "Epoch: 55/100 | step: 277/422 | loss: 1.0303620100021362\n",
      "Epoch: 55/100 | step: 278/422 | loss: 0.7324461936950684\n",
      "Epoch: 55/100 | step: 279/422 | loss: 0.5031611323356628\n",
      "Epoch: 55/100 | step: 280/422 | loss: 0.9679909944534302\n",
      "Epoch: 55/100 | step: 281/422 | loss: 0.7244199514389038\n",
      "Epoch: 55/100 | step: 282/422 | loss: 1.3692512512207031\n",
      "Epoch: 55/100 | step: 283/422 | loss: 0.8796173930168152\n",
      "Epoch: 55/100 | step: 284/422 | loss: 0.7553375959396362\n",
      "Epoch: 55/100 | step: 285/422 | loss: 0.6251481175422668\n",
      "Epoch: 55/100 | step: 286/422 | loss: 0.9699170589447021\n",
      "Epoch: 55/100 | step: 287/422 | loss: 0.5873976945877075\n",
      "Epoch: 55/100 | step: 288/422 | loss: 0.5635275840759277\n",
      "Epoch: 55/100 | step: 289/422 | loss: 0.5535337924957275\n",
      "Epoch: 55/100 | step: 290/422 | loss: 0.9481330513954163\n",
      "Epoch: 55/100 | step: 291/422 | loss: 0.6098198294639587\n",
      "Epoch: 55/100 | step: 292/422 | loss: 0.7409452199935913\n",
      "Epoch: 55/100 | step: 293/422 | loss: 0.40681931376457214\n",
      "Epoch: 55/100 | step: 294/422 | loss: 1.1261475086212158\n",
      "Epoch: 55/100 | step: 295/422 | loss: 0.4699704051017761\n",
      "Epoch: 55/100 | step: 296/422 | loss: 0.3507348895072937\n",
      "Epoch: 55/100 | step: 297/422 | loss: 0.739815890789032\n",
      "Epoch: 55/100 | step: 298/422 | loss: 0.6172371506690979\n",
      "Epoch: 55/100 | step: 299/422 | loss: 0.6344726085662842\n",
      "Epoch: 55/100 | step: 300/422 | loss: 0.6246471405029297\n",
      "Epoch: 55/100 | step: 301/422 | loss: 0.4923301637172699\n",
      "Epoch: 55/100 | step: 302/422 | loss: 0.7279086709022522\n",
      "Epoch: 55/100 | step: 303/422 | loss: 0.6640631556510925\n",
      "Epoch: 55/100 | step: 304/422 | loss: 0.6693747043609619\n",
      "Epoch: 55/100 | step: 305/422 | loss: 0.3862597346305847\n",
      "Epoch: 55/100 | step: 306/422 | loss: 0.46439358592033386\n",
      "Epoch: 55/100 | step: 307/422 | loss: 0.3802972137928009\n",
      "Epoch: 55/100 | step: 308/422 | loss: 0.5999530553817749\n",
      "Epoch: 55/100 | step: 309/422 | loss: 0.452237069606781\n",
      "Epoch: 55/100 | step: 310/422 | loss: 0.4400947391986847\n",
      "Epoch: 55/100 | step: 311/422 | loss: 0.7500909566879272\n",
      "Epoch: 55/100 | step: 312/422 | loss: 0.6086611747741699\n",
      "Epoch: 55/100 | step: 313/422 | loss: 0.5663493275642395\n",
      "Epoch: 55/100 | step: 314/422 | loss: 0.6671029925346375\n",
      "Epoch: 55/100 | step: 315/422 | loss: 0.7060379385948181\n",
      "Epoch: 55/100 | step: 316/422 | loss: 0.8357621431350708\n",
      "Epoch: 55/100 | step: 317/422 | loss: 0.6305550932884216\n",
      "Epoch: 55/100 | step: 318/422 | loss: 0.612077534198761\n",
      "Epoch: 55/100 | step: 319/422 | loss: 0.4062284231185913\n",
      "Epoch: 55/100 | step: 320/422 | loss: 0.7756971716880798\n",
      "Epoch: 55/100 | step: 321/422 | loss: 0.8981272578239441\n",
      "Epoch: 55/100 | step: 322/422 | loss: 0.7151612043380737\n",
      "Epoch: 55/100 | step: 323/422 | loss: 0.7387790083885193\n",
      "Epoch: 55/100 | step: 324/422 | loss: 0.8102049231529236\n",
      "Epoch: 55/100 | step: 325/422 | loss: 0.3789321780204773\n",
      "Epoch: 55/100 | step: 326/422 | loss: 0.3969321846961975\n",
      "Epoch: 55/100 | step: 327/422 | loss: 0.6566946506500244\n",
      "Epoch: 55/100 | step: 328/422 | loss: 0.5821524262428284\n",
      "Epoch: 55/100 | step: 329/422 | loss: 0.46368101239204407\n",
      "Epoch: 55/100 | step: 330/422 | loss: 0.5087650418281555\n",
      "Epoch: 55/100 | step: 331/422 | loss: 0.7896194458007812\n",
      "Epoch: 55/100 | step: 332/422 | loss: 0.5091713070869446\n",
      "Epoch: 55/100 | step: 333/422 | loss: 0.5077995657920837\n",
      "Epoch: 55/100 | step: 334/422 | loss: 0.7754244804382324\n",
      "Epoch: 55/100 | step: 335/422 | loss: 1.0283193588256836\n",
      "Epoch: 55/100 | step: 336/422 | loss: 0.8348937034606934\n",
      "Epoch: 55/100 | step: 337/422 | loss: 1.0259268283843994\n",
      "Epoch: 55/100 | step: 338/422 | loss: 0.5480862259864807\n",
      "Epoch: 55/100 | step: 339/422 | loss: 0.6276378631591797\n",
      "Epoch: 55/100 | step: 340/422 | loss: 0.36084556579589844\n",
      "Epoch: 55/100 | step: 341/422 | loss: 0.36795520782470703\n",
      "Epoch: 55/100 | step: 342/422 | loss: 0.5468538403511047\n",
      "Epoch: 55/100 | step: 343/422 | loss: 0.653184711933136\n",
      "Epoch: 55/100 | step: 344/422 | loss: 0.2923682630062103\n",
      "Epoch: 55/100 | step: 345/422 | loss: 0.860105574131012\n",
      "Epoch: 55/100 | step: 346/422 | loss: 0.3944927752017975\n",
      "Epoch: 55/100 | step: 347/422 | loss: 0.5792369246482849\n",
      "Epoch: 55/100 | step: 348/422 | loss: 0.9225224256515503\n",
      "Epoch: 55/100 | step: 349/422 | loss: 0.5598436594009399\n",
      "Epoch: 55/100 | step: 350/422 | loss: 0.6249501705169678\n",
      "Epoch: 55/100 | step: 351/422 | loss: 0.9971878528594971\n",
      "Epoch: 55/100 | step: 352/422 | loss: 0.6148431897163391\n",
      "Epoch: 55/100 | step: 353/422 | loss: 0.26859280467033386\n",
      "Epoch: 55/100 | step: 354/422 | loss: 1.2649112939834595\n",
      "Epoch: 55/100 | step: 355/422 | loss: 0.9217965602874756\n",
      "Epoch: 55/100 | step: 356/422 | loss: 0.8047366738319397\n",
      "Epoch: 55/100 | step: 357/422 | loss: 0.7260236144065857\n",
      "Epoch: 55/100 | step: 358/422 | loss: 0.9639824628829956\n",
      "Epoch: 55/100 | step: 359/422 | loss: 0.4573526382446289\n",
      "Epoch: 55/100 | step: 360/422 | loss: 0.42773130536079407\n",
      "Epoch: 55/100 | step: 361/422 | loss: 0.6175225377082825\n",
      "Epoch: 55/100 | step: 362/422 | loss: 0.8841351866722107\n",
      "Epoch: 55/100 | step: 363/422 | loss: 1.0731723308563232\n",
      "Epoch: 55/100 | step: 364/422 | loss: 0.9425289034843445\n",
      "Epoch: 55/100 | step: 365/422 | loss: 0.6806555986404419\n",
      "Epoch: 55/100 | step: 366/422 | loss: 0.8262420892715454\n",
      "Epoch: 55/100 | step: 367/422 | loss: 0.6416305303573608\n",
      "Epoch: 55/100 | step: 368/422 | loss: 0.5673137903213501\n",
      "Epoch: 55/100 | step: 369/422 | loss: 0.4941459894180298\n",
      "Epoch: 55/100 | step: 370/422 | loss: 0.4946101903915405\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 56/100 | step: 1/422 | loss: 0.5328460335731506\n",
      "Epoch: 56/100 | step: 2/422 | loss: 0.5569889545440674\n",
      "Epoch: 56/100 | step: 3/422 | loss: 0.459244042634964\n",
      "Epoch: 56/100 | step: 4/422 | loss: 0.4323941171169281\n",
      "Epoch: 56/100 | step: 5/422 | loss: 0.6066985726356506\n",
      "Epoch: 56/100 | step: 6/422 | loss: 0.6776347160339355\n",
      "Epoch: 56/100 | step: 7/422 | loss: 0.579003095626831\n",
      "Epoch: 56/100 | step: 8/422 | loss: 0.3596712648868561\n",
      "Epoch: 56/100 | step: 9/422 | loss: 0.8247735500335693\n",
      "Epoch: 56/100 | step: 10/422 | loss: 0.485845685005188\n",
      "Epoch: 56/100 | step: 11/422 | loss: 0.4400293231010437\n",
      "Epoch: 56/100 | step: 12/422 | loss: 0.660698652267456\n",
      "Epoch: 56/100 | step: 13/422 | loss: 0.4427403211593628\n",
      "Epoch: 56/100 | step: 14/422 | loss: 0.6823089718818665\n",
      "Epoch: 56/100 | step: 15/422 | loss: 0.25153329968452454\n",
      "Epoch: 56/100 | step: 16/422 | loss: 0.5081130266189575\n",
      "Epoch: 56/100 | step: 17/422 | loss: 0.44713810086250305\n",
      "Epoch: 56/100 | step: 18/422 | loss: 0.2946714460849762\n",
      "Epoch: 56/100 | step: 19/422 | loss: 0.22910071909427643\n",
      "Epoch: 56/100 | step: 20/422 | loss: 0.30441078543663025\n",
      "Epoch: 56/100 | step: 21/422 | loss: 0.4297792911529541\n",
      "Epoch: 56/100 | step: 22/422 | loss: 0.22505667805671692\n",
      "Epoch: 56/100 | step: 23/422 | loss: 0.31824031472206116\n",
      "Epoch: 56/100 | step: 24/422 | loss: 0.5202110409736633\n",
      "Epoch: 56/100 | step: 25/422 | loss: 0.4324463903903961\n",
      "Epoch: 56/100 | step: 26/422 | loss: 0.7736742496490479\n",
      "Epoch: 56/100 | step: 27/422 | loss: 0.4102356731891632\n",
      "Epoch: 56/100 | step: 28/422 | loss: 0.3849955201148987\n",
      "Epoch: 56/100 | step: 29/422 | loss: 0.5087582468986511\n",
      "Epoch: 56/100 | step: 30/422 | loss: 0.3981649875640869\n",
      "Epoch: 56/100 | step: 31/422 | loss: 0.25848889350891113\n",
      "Epoch: 56/100 | step: 32/422 | loss: 0.34431731700897217\n",
      "Epoch: 56/100 | step: 33/422 | loss: 0.4582820534706116\n",
      "Epoch: 56/100 | step: 34/422 | loss: 0.2618332803249359\n",
      "Epoch: 56/100 | step: 35/422 | loss: 0.5829136371612549\n",
      "Epoch: 56/100 | step: 36/422 | loss: 0.7518333792686462\n",
      "Epoch: 56/100 | step: 37/422 | loss: 0.37687885761260986\n",
      "Epoch: 56/100 | step: 38/422 | loss: 0.3150344491004944\n",
      "Epoch: 56/100 | step: 39/422 | loss: 0.45734262466430664\n",
      "Epoch: 56/100 | step: 40/422 | loss: 0.826481282711029\n",
      "Epoch: 56/100 | step: 41/422 | loss: 0.49602195620536804\n",
      "Epoch: 56/100 | step: 42/422 | loss: 0.1942441612482071\n",
      "Epoch: 56/100 | step: 43/422 | loss: 0.639905571937561\n",
      "Epoch: 56/100 | step: 44/422 | loss: 0.4415978491306305\n",
      "Epoch: 56/100 | step: 45/422 | loss: 0.8252130150794983\n",
      "Epoch: 56/100 | step: 46/422 | loss: 0.5097312331199646\n",
      "Epoch: 56/100 | step: 47/422 | loss: 0.40317222476005554\n",
      "Epoch: 56/100 | step: 48/422 | loss: 0.7044851779937744\n",
      "Epoch: 56/100 | step: 49/422 | loss: 0.6316379308700562\n",
      "Epoch: 56/100 | step: 50/422 | loss: 0.5824962258338928\n",
      "Epoch: 56/100 | step: 51/422 | loss: 0.6110365390777588\n",
      "Epoch: 56/100 | step: 52/422 | loss: 0.6457993984222412\n",
      "Epoch: 56/100 | step: 53/422 | loss: 0.537912130355835\n",
      "Epoch: 56/100 | step: 54/422 | loss: 0.4962771236896515\n",
      "Epoch: 56/100 | step: 55/422 | loss: 0.36399006843566895\n",
      "Epoch: 56/100 | step: 56/422 | loss: 0.4168661832809448\n",
      "Epoch: 56/100 | step: 57/422 | loss: 0.46645107865333557\n",
      "Epoch: 56/100 | step: 58/422 | loss: 0.35898005962371826\n",
      "Epoch: 56/100 | step: 59/422 | loss: 1.0587953329086304\n",
      "Epoch: 56/100 | step: 60/422 | loss: 0.5605932474136353\n",
      "Epoch: 56/100 | step: 61/422 | loss: 0.9048355221748352\n",
      "Epoch: 56/100 | step: 62/422 | loss: 0.4878009855747223\n",
      "Epoch: 56/100 | step: 63/422 | loss: 0.6388397812843323\n",
      "Epoch: 56/100 | step: 64/422 | loss: 0.4913812279701233\n",
      "Epoch: 56/100 | step: 65/422 | loss: 0.3576635718345642\n",
      "Epoch: 56/100 | step: 66/422 | loss: 0.5291866660118103\n",
      "Epoch: 56/100 | step: 67/422 | loss: 0.6232049465179443\n",
      "Epoch: 56/100 | step: 68/422 | loss: 0.44869232177734375\n",
      "Epoch: 56/100 | step: 69/422 | loss: 0.30697396397590637\n",
      "Epoch: 56/100 | step: 70/422 | loss: 0.24972528219223022\n",
      "Epoch: 56/100 | step: 71/422 | loss: 0.634938657283783\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 57/100 | step: 1/422 | loss: 0.6141440868377686\n",
      "Epoch: 57/100 | step: 2/422 | loss: 0.3062468469142914\n",
      "Epoch: 57/100 | step: 3/422 | loss: 0.26979222893714905\n",
      "Epoch: 57/100 | step: 4/422 | loss: 0.6443518400192261\n",
      "Epoch: 57/100 | step: 5/422 | loss: 0.511203408241272\n",
      "Epoch: 57/100 | step: 6/422 | loss: 0.8995181918144226\n",
      "Epoch: 57/100 | step: 7/422 | loss: 0.6640978455543518\n",
      "Epoch: 57/100 | step: 8/422 | loss: 0.7394576072692871\n",
      "Epoch: 57/100 | step: 9/422 | loss: 0.6073681712150574\n",
      "Epoch: 57/100 | step: 10/422 | loss: 0.42280593514442444\n",
      "Epoch: 57/100 | step: 11/422 | loss: 0.4950862228870392\n",
      "Epoch: 57/100 | step: 12/422 | loss: 0.4395277798175812\n",
      "Epoch: 57/100 | step: 13/422 | loss: 0.34068694710731506\n",
      "Epoch: 57/100 | step: 14/422 | loss: 0.3792398273944855\n",
      "Epoch: 57/100 | step: 15/422 | loss: 0.32956331968307495\n",
      "Epoch: 57/100 | step: 16/422 | loss: 0.4849582016468048\n",
      "Epoch: 57/100 | step: 17/422 | loss: 0.305046021938324\n",
      "Epoch: 57/100 | step: 18/422 | loss: 0.2331409752368927\n",
      "Epoch: 57/100 | step: 19/422 | loss: 0.4832223653793335\n",
      "Epoch: 57/100 | step: 20/422 | loss: 0.32007500529289246\n",
      "Epoch: 57/100 | step: 21/422 | loss: 0.584021270275116\n",
      "Epoch: 57/100 | step: 22/422 | loss: 0.2895873188972473\n",
      "Epoch: 57/100 | step: 23/422 | loss: 0.24665482342243195\n",
      "Epoch: 57/100 | step: 24/422 | loss: 0.2545178234577179\n",
      "Epoch: 57/100 | step: 25/422 | loss: 0.4185604453086853\n",
      "Epoch: 57/100 | step: 26/422 | loss: 0.40255188941955566\n",
      "Epoch: 57/100 | step: 27/422 | loss: 0.3486901521682739\n",
      "Epoch: 57/100 | step: 28/422 | loss: 0.6041284799575806\n",
      "Epoch: 57/100 | step: 29/422 | loss: 0.47780641913414\n",
      "Epoch: 57/100 | step: 30/422 | loss: 0.3572186231613159\n",
      "Epoch: 57/100 | step: 31/422 | loss: 0.3346318304538727\n",
      "Epoch: 57/100 | step: 32/422 | loss: 0.39070290327072144\n",
      "Epoch: 57/100 | step: 33/422 | loss: 0.3622230291366577\n",
      "Epoch: 57/100 | step: 34/422 | loss: 0.6840758323669434\n",
      "Epoch: 57/100 | step: 35/422 | loss: 0.3932395279407501\n",
      "Epoch: 57/100 | step: 36/422 | loss: 0.3985118865966797\n",
      "Epoch: 57/100 | step: 37/422 | loss: 0.36815345287323\n",
      "Epoch: 57/100 | step: 38/422 | loss: 0.4627732038497925\n",
      "Epoch: 57/100 | step: 39/422 | loss: 0.43505316972732544\n",
      "Epoch: 57/100 | step: 40/422 | loss: 0.6881981492042542\n",
      "Epoch: 57/100 | step: 41/422 | loss: 0.48112916946411133\n",
      "Epoch: 57/100 | step: 42/422 | loss: 0.5326923131942749\n",
      "Epoch: 57/100 | step: 43/422 | loss: 0.3833659589290619\n",
      "Epoch: 57/100 | step: 44/422 | loss: 0.45871224999427795\n",
      "Epoch: 57/100 | step: 45/422 | loss: 0.4172125458717346\n",
      "Epoch: 57/100 | step: 46/422 | loss: 0.6777323484420776\n",
      "Epoch: 57/100 | step: 47/422 | loss: 0.5413581132888794\n",
      "Epoch: 57/100 | step: 48/422 | loss: 0.5298383831977844\n",
      "Epoch: 57/100 | step: 49/422 | loss: 0.5669857263565063\n",
      "Epoch: 57/100 | step: 50/422 | loss: 0.6140768527984619\n",
      "Epoch: 57/100 | step: 51/422 | loss: 0.42296457290649414\n",
      "Epoch: 57/100 | step: 52/422 | loss: 0.2960301637649536\n",
      "Epoch: 57/100 | step: 53/422 | loss: 0.31990107893943787\n",
      "Epoch: 57/100 | step: 54/422 | loss: 0.1925034373998642\n",
      "Epoch: 57/100 | step: 55/422 | loss: 0.4118873178958893\n",
      "Epoch: 57/100 | step: 56/422 | loss: 0.7854050993919373\n",
      "Epoch: 57/100 | step: 57/422 | loss: 0.3825867772102356\n",
      "Epoch: 57/100 | step: 58/422 | loss: 0.39040669798851013\n",
      "Epoch: 57/100 | step: 59/422 | loss: 0.44134292006492615\n",
      "Epoch: 57/100 | step: 60/422 | loss: 0.31002557277679443\n",
      "Epoch: 57/100 | step: 61/422 | loss: 0.32649263739585876\n",
      "Epoch: 57/100 | step: 62/422 | loss: 0.4894486963748932\n",
      "Epoch: 57/100 | step: 63/422 | loss: 0.32173359394073486\n",
      "Epoch: 57/100 | step: 64/422 | loss: 0.21642550826072693\n",
      "Epoch: 57/100 | step: 65/422 | loss: 0.3913591504096985\n",
      "Epoch: 57/100 | step: 66/422 | loss: 0.22709517180919647\n",
      "Epoch: 57/100 | step: 67/422 | loss: 0.5287235975265503\n",
      "Epoch: 57/100 | step: 68/422 | loss: 0.5492183566093445\n",
      "Epoch: 57/100 | step: 69/422 | loss: 0.24000124633312225\n",
      "Epoch: 57/100 | step: 70/422 | loss: 0.5239885449409485\n",
      "Epoch: 57/100 | step: 71/422 | loss: 0.9301080703735352\n",
      "Epoch: 57/100 | step: 72/422 | loss: 0.4150902330875397\n",
      "Epoch: 57/100 | step: 73/422 | loss: 0.4159810543060303\n",
      "Epoch: 57/100 | step: 74/422 | loss: 0.2904501259326935\n",
      "Epoch: 57/100 | step: 75/422 | loss: 0.305078387260437\n",
      "Epoch: 57/100 | step: 76/422 | loss: 0.5149863958358765\n",
      "Epoch: 57/100 | step: 77/422 | loss: 0.4184754490852356\n",
      "Epoch: 57/100 | step: 78/422 | loss: 1.0193310976028442\n",
      "Epoch: 57/100 | step: 79/422 | loss: 0.4611627757549286\n",
      "Epoch: 57/100 | step: 80/422 | loss: 0.34029796719551086\n",
      "Epoch: 57/100 | step: 81/422 | loss: 0.27757886052131653\n",
      "Epoch: 57/100 | step: 82/422 | loss: 0.5551623702049255\n",
      "Epoch: 57/100 | step: 83/422 | loss: 0.49880409240722656\n",
      "Epoch: 57/100 | step: 84/422 | loss: 0.2845860421657562\n",
      "Epoch: 57/100 | step: 85/422 | loss: 0.6047422289848328\n",
      "Epoch: 57/100 | step: 86/422 | loss: 0.5041176676750183\n",
      "Epoch: 57/100 | step: 87/422 | loss: 0.5825106501579285\n",
      "Epoch: 57/100 | step: 88/422 | loss: 0.4523051381111145\n",
      "Epoch: 57/100 | step: 89/422 | loss: 0.3612627387046814\n",
      "Epoch: 57/100 | step: 90/422 | loss: 0.4676967263221741\n",
      "Epoch: 57/100 | step: 91/422 | loss: 0.24249380826950073\n",
      "Epoch: 57/100 | step: 92/422 | loss: 0.357896625995636\n",
      "Epoch: 57/100 | step: 93/422 | loss: 0.45890378952026367\n",
      "Epoch: 57/100 | step: 94/422 | loss: 0.8435233235359192\n",
      "Epoch: 57/100 | step: 95/422 | loss: 0.5595830082893372\n",
      "Epoch: 57/100 | step: 96/422 | loss: 1.1419702768325806\n",
      "Epoch: 57/100 | step: 97/422 | loss: 1.5521067380905151\n",
      "Epoch: 57/100 | step: 98/422 | loss: 0.2439599633216858\n",
      "Epoch: 57/100 | step: 99/422 | loss: 0.30526190996170044\n",
      "Epoch: 57/100 | step: 100/422 | loss: 0.5454391241073608\n",
      "Epoch: 57/100 | step: 101/422 | loss: 0.6329165697097778\n",
      "Epoch: 57/100 | step: 102/422 | loss: 0.862038254737854\n",
      "Epoch: 57/100 | step: 103/422 | loss: 0.6539727449417114\n",
      "Epoch: 57/100 | step: 104/422 | loss: 0.6290110349655151\n",
      "Epoch: 57/100 | step: 105/422 | loss: 0.40838977694511414\n",
      "Epoch: 57/100 | step: 106/422 | loss: 0.31955787539482117\n",
      "Epoch: 57/100 | step: 107/422 | loss: 0.568000078201294\n",
      "Epoch: 57/100 | step: 108/422 | loss: 0.4410768449306488\n",
      "Epoch: 57/100 | step: 109/422 | loss: 0.43079516291618347\n",
      "Epoch: 57/100 | step: 110/422 | loss: 0.44660693407058716\n",
      "Epoch: 57/100 | step: 111/422 | loss: 0.27685195207595825\n",
      "Epoch: 57/100 | step: 112/422 | loss: 0.3784700036048889\n",
      "Epoch: 57/100 | step: 113/422 | loss: 0.2801593542098999\n",
      "Epoch: 57/100 | step: 114/422 | loss: 0.2699725031852722\n",
      "Epoch: 57/100 | step: 115/422 | loss: 0.36156773567199707\n",
      "Epoch: 57/100 | step: 116/422 | loss: 0.35494935512542725\n",
      "Epoch: 57/100 | step: 117/422 | loss: 0.260596364736557\n",
      "Epoch: 57/100 | step: 118/422 | loss: 0.8284998536109924\n",
      "Epoch: 57/100 | step: 119/422 | loss: 0.540215790271759\n",
      "Epoch: 57/100 | step: 120/422 | loss: 0.4155343174934387\n",
      "Epoch: 57/100 | step: 121/422 | loss: 0.3680003583431244\n",
      "Epoch: 57/100 | step: 122/422 | loss: 0.4946058988571167\n",
      "Epoch: 57/100 | step: 123/422 | loss: 0.48324212431907654\n",
      "Epoch: 57/100 | step: 124/422 | loss: 0.5086861848831177\n",
      "Epoch: 57/100 | step: 125/422 | loss: 0.28207656741142273\n",
      "Epoch: 57/100 | step: 126/422 | loss: 0.2020878791809082\n",
      "Epoch: 57/100 | step: 127/422 | loss: 0.30238795280456543\n",
      "Epoch: 57/100 | step: 128/422 | loss: 0.34322234988212585\n",
      "Epoch: 57/100 | step: 129/422 | loss: 0.3138026297092438\n",
      "Epoch: 57/100 | step: 130/422 | loss: 0.48191648721694946\n",
      "Epoch: 57/100 | step: 131/422 | loss: 0.4607720673084259\n",
      "Epoch: 57/100 | step: 132/422 | loss: 0.42590823769569397\n",
      "Epoch: 57/100 | step: 133/422 | loss: 0.3458126187324524\n",
      "Epoch: 57/100 | step: 134/422 | loss: 0.5145512223243713\n",
      "Epoch: 57/100 | step: 135/422 | loss: 0.3810374140739441\n",
      "Epoch: 57/100 | step: 136/422 | loss: 0.5038447380065918\n",
      "Epoch: 57/100 | step: 137/422 | loss: 0.3532550036907196\n",
      "Epoch: 57/100 | step: 138/422 | loss: 0.299140602350235\n",
      "Epoch: 57/100 | step: 139/422 | loss: 0.35763129591941833\n",
      "Epoch: 57/100 | step: 140/422 | loss: 0.31196317076683044\n",
      "Epoch: 57/100 | step: 141/422 | loss: 0.4297952353954315\n",
      "Epoch: 57/100 | step: 142/422 | loss: 0.2497764527797699\n",
      "Epoch: 57/100 | step: 143/422 | loss: 0.5227908492088318\n",
      "Epoch: 57/100 | step: 144/422 | loss: 0.5233004689216614\n",
      "Epoch: 57/100 | step: 145/422 | loss: 0.3686674237251282\n",
      "Epoch: 57/100 | step: 146/422 | loss: 0.27268186211586\n",
      "Epoch: 57/100 | step: 147/422 | loss: 0.6896542310714722\n",
      "Epoch: 57/100 | step: 148/422 | loss: 0.9113417863845825\n",
      "Epoch: 57/100 | step: 149/422 | loss: 0.33247509598731995\n",
      "Epoch: 57/100 | step: 150/422 | loss: 0.2972109913825989\n",
      "Epoch: 57/100 | step: 151/422 | loss: 0.2979242503643036\n",
      "Epoch: 57/100 | step: 152/422 | loss: 0.3219624161720276\n",
      "Epoch: 57/100 | step: 153/422 | loss: 0.4826156795024872\n",
      "Epoch: 57/100 | step: 154/422 | loss: 0.3333601951599121\n",
      "Epoch: 57/100 | step: 155/422 | loss: 0.4880209267139435\n",
      "Epoch: 57/100 | step: 156/422 | loss: 0.5877330899238586\n",
      "Epoch: 57/100 | step: 157/422 | loss: 0.2448919117450714\n",
      "Epoch: 57/100 | step: 158/422 | loss: 0.7927327752113342\n",
      "Epoch: 57/100 | step: 159/422 | loss: 0.5070015788078308\n",
      "Epoch: 57/100 | step: 160/422 | loss: 0.6060091853141785\n",
      "Epoch: 57/100 | step: 161/422 | loss: 0.5636212229728699\n",
      "Epoch: 57/100 | step: 162/422 | loss: 0.5882580876350403\n",
      "Epoch: 57/100 | step: 163/422 | loss: 0.7177002429962158\n",
      "Epoch: 57/100 | step: 164/422 | loss: 0.9328519701957703\n",
      "Epoch: 57/100 | step: 165/422 | loss: 0.6430016160011292\n",
      "Epoch: 57/100 | step: 166/422 | loss: 0.7139379382133484\n",
      "Epoch: 57/100 | step: 167/422 | loss: 0.5110584497451782\n",
      "Epoch: 57/100 | step: 168/422 | loss: 0.7825771570205688\n",
      "Epoch: 57/100 | step: 169/422 | loss: 0.7673834562301636\n",
      "Epoch: 57/100 | step: 170/422 | loss: 0.25283631682395935\n",
      "Epoch: 57/100 | step: 171/422 | loss: 1.1004871129989624\n",
      "Epoch: 57/100 | step: 172/422 | loss: 1.1453218460083008\n",
      "Epoch: 57/100 | step: 173/422 | loss: 0.6517290472984314\n",
      "Epoch: 57/100 | step: 174/422 | loss: 0.9219648838043213\n",
      "Epoch: 57/100 | step: 175/422 | loss: 0.7866359353065491\n",
      "Epoch: 57/100 | step: 176/422 | loss: 0.6148229241371155\n",
      "Epoch: 57/100 | step: 177/422 | loss: 0.5827622413635254\n",
      "Epoch: 57/100 | step: 178/422 | loss: 0.8867881298065186\n",
      "Epoch: 57/100 | step: 179/422 | loss: 0.27750924229621887\n",
      "Epoch: 57/100 | step: 180/422 | loss: 0.488908588886261\n",
      "Epoch: 57/100 | step: 181/422 | loss: 0.23601526021957397\n",
      "Epoch: 57/100 | step: 182/422 | loss: 0.5480244159698486\n",
      "Epoch: 57/100 | step: 183/422 | loss: 0.32791250944137573\n",
      "Epoch: 57/100 | step: 184/422 | loss: 0.6465946435928345\n",
      "Epoch: 57/100 | step: 185/422 | loss: 0.5538736581802368\n",
      "Epoch: 57/100 | step: 186/422 | loss: 0.3587935268878937\n",
      "Epoch: 57/100 | step: 187/422 | loss: 0.23874101042747498\n",
      "Epoch: 57/100 | step: 188/422 | loss: 0.4932993948459625\n",
      "Epoch: 57/100 | step: 189/422 | loss: 0.6026070713996887\n",
      "Epoch: 57/100 | step: 190/422 | loss: 0.4529981017112732\n",
      "Epoch: 57/100 | step: 191/422 | loss: 0.22466668486595154\n",
      "Epoch: 57/100 | step: 192/422 | loss: 0.4175390303134918\n",
      "Epoch: 57/100 | step: 193/422 | loss: 0.41760310530662537\n",
      "Epoch: 57/100 | step: 194/422 | loss: 0.46916645765304565\n",
      "Epoch: 57/100 | step: 195/422 | loss: 0.25823304057121277\n",
      "Epoch: 57/100 | step: 196/422 | loss: 0.502039909362793\n",
      "Epoch: 57/100 | step: 197/422 | loss: 0.32299503684043884\n",
      "Epoch: 57/100 | step: 198/422 | loss: 0.34502941370010376\n",
      "Epoch: 57/100 | step: 199/422 | loss: 0.23546868562698364\n",
      "Epoch: 57/100 | step: 200/422 | loss: 0.30529314279556274\n",
      "Epoch: 57/100 | step: 201/422 | loss: 0.5390527248382568\n",
      "Epoch: 57/100 | step: 202/422 | loss: 0.43774670362472534\n",
      "Epoch: 57/100 | step: 203/422 | loss: 0.2363271415233612\n",
      "Epoch: 57/100 | step: 204/422 | loss: 0.4671776592731476\n",
      "Epoch: 57/100 | step: 205/422 | loss: 0.4490276277065277\n",
      "Epoch: 57/100 | step: 206/422 | loss: 0.4617789089679718\n",
      "Epoch: 57/100 | step: 207/422 | loss: 0.19936445355415344\n",
      "Epoch: 57/100 | step: 208/422 | loss: 0.3700012266635895\n",
      "Epoch: 57/100 | step: 209/422 | loss: 0.5239678621292114\n",
      "Epoch: 57/100 | step: 210/422 | loss: 0.4628771245479584\n",
      "Epoch: 57/100 | step: 211/422 | loss: 0.5398054122924805\n",
      "Epoch: 57/100 | step: 212/422 | loss: 0.46207356452941895\n",
      "Epoch: 57/100 | step: 213/422 | loss: 0.7395621538162231\n",
      "Epoch: 57/100 | step: 214/422 | loss: 0.4512006640434265\n",
      "Epoch: 57/100 | step: 215/422 | loss: 0.5196313261985779\n",
      "Epoch: 57/100 | step: 216/422 | loss: 0.4576977491378784\n",
      "Epoch: 57/100 | step: 217/422 | loss: 0.3211653232574463\n",
      "Epoch: 57/100 | step: 218/422 | loss: 0.5217797160148621\n",
      "Epoch: 57/100 | step: 219/422 | loss: 0.4689994752407074\n",
      "Epoch: 57/100 | step: 220/422 | loss: 0.42368873953819275\n",
      "Epoch: 57/100 | step: 221/422 | loss: 0.34419891238212585\n",
      "Epoch: 57/100 | step: 222/422 | loss: 0.2614096701145172\n",
      "Epoch: 57/100 | step: 223/422 | loss: 0.5027558207511902\n",
      "Epoch: 57/100 | step: 224/422 | loss: 0.9580283164978027\n",
      "Epoch: 57/100 | step: 225/422 | loss: 0.5133220553398132\n",
      "Epoch: 57/100 | step: 226/422 | loss: 0.5198271870613098\n",
      "Epoch: 57/100 | step: 227/422 | loss: 0.3648121953010559\n",
      "Epoch: 57/100 | step: 228/422 | loss: 0.8967829346656799\n",
      "Epoch: 57/100 | step: 229/422 | loss: 0.36030030250549316\n",
      "Epoch: 57/100 | step: 230/422 | loss: 0.5063573122024536\n",
      "Epoch: 57/100 | step: 231/422 | loss: 0.7569867372512817\n",
      "Epoch: 57/100 | step: 232/422 | loss: 0.6459188461303711\n",
      "Epoch: 57/100 | step: 233/422 | loss: 0.42181023955345154\n",
      "Epoch: 57/100 | step: 234/422 | loss: 0.6291632652282715\n",
      "Epoch: 57/100 | step: 235/422 | loss: 0.6897199749946594\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 58/100 | step: 1/422 | loss: 0.8927237391471863\n",
      "Epoch: 58/100 | step: 2/422 | loss: 0.8161565065383911\n",
      "Epoch: 58/100 | step: 3/422 | loss: 0.6503689885139465\n",
      "Epoch: 58/100 | step: 4/422 | loss: 0.36396101117134094\n",
      "Epoch: 58/100 | step: 5/422 | loss: 0.3447181284427643\n",
      "Epoch: 58/100 | step: 6/422 | loss: 0.4553246796131134\n",
      "Epoch: 58/100 | step: 7/422 | loss: 0.4690069556236267\n",
      "Epoch: 58/100 | step: 8/422 | loss: 0.29916948080062866\n",
      "Epoch: 58/100 | step: 9/422 | loss: 0.44399937987327576\n",
      "Epoch: 58/100 | step: 10/422 | loss: 0.2334594577550888\n",
      "Epoch: 58/100 | step: 11/422 | loss: 0.3252216875553131\n",
      "Epoch: 58/100 | step: 12/422 | loss: 0.21361340582370758\n",
      "Epoch: 58/100 | step: 13/422 | loss: 0.17169338464736938\n",
      "Epoch: 58/100 | step: 14/422 | loss: 0.3651057183742523\n",
      "Epoch: 58/100 | step: 15/422 | loss: 0.2602272629737854\n",
      "Epoch: 58/100 | step: 16/422 | loss: 0.40020403265953064\n",
      "Epoch: 58/100 | step: 17/422 | loss: 0.4284369647502899\n",
      "Epoch: 58/100 | step: 18/422 | loss: 0.5543540716171265\n",
      "Epoch: 58/100 | step: 19/422 | loss: 0.31153130531311035\n",
      "Epoch: 58/100 | step: 20/422 | loss: 0.2890131175518036\n",
      "Epoch: 58/100 | step: 21/422 | loss: 0.24386069178581238\n",
      "Epoch: 58/100 | step: 22/422 | loss: 0.7651459574699402\n",
      "Epoch: 58/100 | step: 23/422 | loss: 0.22532564401626587\n",
      "Epoch: 58/100 | step: 24/422 | loss: 0.46802493929862976\n",
      "Epoch: 58/100 | step: 25/422 | loss: 0.35649701952934265\n",
      "Epoch: 58/100 | step: 26/422 | loss: 0.5846115946769714\n",
      "Epoch: 58/100 | step: 27/422 | loss: 0.5897283554077148\n",
      "Epoch: 58/100 | step: 28/422 | loss: 0.41867774724960327\n",
      "Epoch: 58/100 | step: 29/422 | loss: 0.5801853537559509\n",
      "Epoch: 58/100 | step: 30/422 | loss: 0.270657479763031\n",
      "Epoch: 58/100 | step: 31/422 | loss: 0.28735360503196716\n",
      "Epoch: 58/100 | step: 32/422 | loss: 0.5326880216598511\n",
      "Epoch: 58/100 | step: 33/422 | loss: 0.2962267994880676\n",
      "Epoch: 58/100 | step: 34/422 | loss: 0.29446908831596375\n",
      "Epoch: 58/100 | step: 35/422 | loss: 0.4229753315448761\n",
      "Epoch: 58/100 | step: 36/422 | loss: 0.23648618161678314\n",
      "Epoch: 58/100 | step: 37/422 | loss: 0.605591356754303\n",
      "Epoch: 58/100 | step: 38/422 | loss: 0.31070244312286377\n",
      "Epoch: 58/100 | step: 39/422 | loss: 0.3873095214366913\n",
      "Epoch: 58/100 | step: 40/422 | loss: 0.3298429250717163\n",
      "Epoch: 58/100 | step: 41/422 | loss: 0.21240869164466858\n",
      "Epoch: 58/100 | step: 42/422 | loss: 0.377828985452652\n",
      "Epoch: 58/100 | step: 43/422 | loss: 0.3588310182094574\n",
      "Epoch: 58/100 | step: 44/422 | loss: 0.2487727701663971\n",
      "Epoch: 58/100 | step: 45/422 | loss: 0.18308904767036438\n",
      "Epoch: 58/100 | step: 46/422 | loss: 0.33253127336502075\n",
      "Epoch: 58/100 | step: 47/422 | loss: 0.4838394224643707\n",
      "Epoch: 58/100 | step: 48/422 | loss: 0.3468922972679138\n",
      "Epoch: 58/100 | step: 49/422 | loss: 0.45722734928131104\n",
      "Epoch: 58/100 | step: 50/422 | loss: 0.5392739176750183\n",
      "Epoch: 58/100 | step: 51/422 | loss: 0.6388737559318542\n",
      "Epoch: 58/100 | step: 52/422 | loss: 0.4505341649055481\n",
      "Epoch: 58/100 | step: 53/422 | loss: 0.2400413155555725\n",
      "Epoch: 58/100 | step: 54/422 | loss: 0.24135783314704895\n",
      "Epoch: 58/100 | step: 55/422 | loss: 0.3351406753063202\n",
      "Epoch: 58/100 | step: 56/422 | loss: 0.30570706725120544\n",
      "Epoch: 58/100 | step: 57/422 | loss: 0.4124011695384979\n",
      "Epoch: 58/100 | step: 58/422 | loss: 0.48673442006111145\n",
      "Epoch: 58/100 | step: 59/422 | loss: 0.30291327834129333\n",
      "Epoch: 58/100 | step: 60/422 | loss: 0.48337435722351074\n",
      "Epoch: 58/100 | step: 61/422 | loss: 0.3139817714691162\n",
      "Epoch: 58/100 | step: 62/422 | loss: 0.35485512018203735\n",
      "Epoch: 58/100 | step: 63/422 | loss: 0.6400521397590637\n",
      "Epoch: 58/100 | step: 64/422 | loss: 0.5334638357162476\n",
      "Epoch: 58/100 | step: 65/422 | loss: 0.30566659569740295\n",
      "Epoch: 58/100 | step: 66/422 | loss: 0.6878343820571899\n",
      "Epoch: 58/100 | step: 67/422 | loss: 0.37703943252563477\n",
      "Epoch: 58/100 | step: 68/422 | loss: 0.572509765625\n",
      "Epoch: 58/100 | step: 69/422 | loss: 0.4307265877723694\n",
      "Epoch: 58/100 | step: 70/422 | loss: 0.4929463565349579\n",
      "Epoch: 58/100 | step: 71/422 | loss: 0.4608853757381439\n",
      "Epoch: 58/100 | step: 72/422 | loss: 0.4742184579372406\n",
      "Epoch: 58/100 | step: 73/422 | loss: 0.5643603205680847\n",
      "Epoch: 58/100 | step: 74/422 | loss: 0.4404735863208771\n",
      "Epoch: 58/100 | step: 75/422 | loss: 0.7088868618011475\n",
      "Epoch: 58/100 | step: 76/422 | loss: 0.37985509634017944\n",
      "Epoch: 58/100 | step: 77/422 | loss: 0.8636342287063599\n",
      "Epoch: 58/100 | step: 78/422 | loss: 0.6579174995422363\n",
      "Epoch: 58/100 | step: 79/422 | loss: 0.39214327931404114\n",
      "Epoch: 58/100 | step: 80/422 | loss: 0.28131943941116333\n",
      "Epoch: 58/100 | step: 81/422 | loss: 0.29013460874557495\n",
      "Epoch: 58/100 | step: 82/422 | loss: 0.27380895614624023\n",
      "Epoch: 58/100 | step: 83/422 | loss: 0.17939668893814087\n",
      "Epoch: 58/100 | step: 84/422 | loss: 0.8784156441688538\n",
      "Epoch: 58/100 | step: 85/422 | loss: 0.4601099193096161\n",
      "Epoch: 58/100 | step: 86/422 | loss: 0.6465036273002625\n",
      "Epoch: 58/100 | step: 87/422 | loss: 0.3471415340900421\n",
      "Epoch: 58/100 | step: 88/422 | loss: 0.3635396957397461\n",
      "Epoch: 58/100 | step: 89/422 | loss: 0.39481794834136963\n",
      "Epoch: 58/100 | step: 90/422 | loss: 0.27748996019363403\n",
      "Epoch: 58/100 | step: 91/422 | loss: 0.36622101068496704\n",
      "Epoch: 58/100 | step: 92/422 | loss: 0.2789846360683441\n",
      "Epoch: 58/100 | step: 93/422 | loss: 0.37495383620262146\n",
      "Epoch: 58/100 | step: 94/422 | loss: 0.37836265563964844\n",
      "Epoch: 58/100 | step: 95/422 | loss: 0.30260801315307617\n",
      "Epoch: 58/100 | step: 96/422 | loss: 0.5617828965187073\n",
      "Epoch: 58/100 | step: 97/422 | loss: 0.28058457374572754\n",
      "Epoch: 58/100 | step: 98/422 | loss: 0.5469310879707336\n",
      "Epoch: 58/100 | step: 99/422 | loss: 0.4967856705188751\n",
      "Epoch: 58/100 | step: 100/422 | loss: 0.3504888117313385\n",
      "Epoch: 58/100 | step: 101/422 | loss: 0.34984225034713745\n",
      "Epoch: 58/100 | step: 102/422 | loss: 0.347171425819397\n",
      "Epoch: 58/100 | step: 103/422 | loss: 0.49941349029541016\n",
      "Epoch: 58/100 | step: 104/422 | loss: 0.3229008615016937\n",
      "Epoch: 58/100 | step: 105/422 | loss: 0.30390241742134094\n",
      "Epoch: 58/100 | step: 106/422 | loss: 0.4338960647583008\n",
      "Epoch: 58/100 | step: 107/422 | loss: 0.19343027472496033\n",
      "Epoch: 58/100 | step: 108/422 | loss: 0.25253885984420776\n",
      "Epoch: 58/100 | step: 109/422 | loss: 0.3746199309825897\n",
      "Epoch: 58/100 | step: 110/422 | loss: 0.5487975478172302\n",
      "Epoch: 58/100 | step: 111/422 | loss: 0.33880358934402466\n",
      "Epoch: 58/100 | step: 112/422 | loss: 0.2415793091058731\n",
      "Epoch: 58/100 | step: 113/422 | loss: 0.3248574733734131\n",
      "Epoch: 58/100 | step: 114/422 | loss: 0.30193641781806946\n",
      "Epoch: 58/100 | step: 115/422 | loss: 0.47437673807144165\n",
      "Epoch: 58/100 | step: 116/422 | loss: 0.3246474862098694\n",
      "Epoch: 58/100 | step: 117/422 | loss: 0.20683039724826813\n",
      "Epoch: 58/100 | step: 118/422 | loss: 0.5469496250152588\n",
      "Epoch: 58/100 | step: 119/422 | loss: 0.33620691299438477\n",
      "Epoch: 58/100 | step: 120/422 | loss: 0.5537977814674377\n",
      "Epoch: 58/100 | step: 121/422 | loss: 0.3272966146469116\n",
      "Epoch: 58/100 | step: 122/422 | loss: 0.6394729018211365\n",
      "Epoch: 58/100 | step: 123/422 | loss: 0.5970185995101929\n",
      "Epoch: 58/100 | step: 124/422 | loss: 0.3973410129547119\n",
      "Epoch: 58/100 | step: 125/422 | loss: 0.36316290497779846\n",
      "Epoch: 58/100 | step: 126/422 | loss: 0.5903908610343933\n",
      "Epoch: 58/100 | step: 127/422 | loss: 0.26965057849884033\n",
      "Epoch: 58/100 | step: 128/422 | loss: 0.4247424304485321\n",
      "Epoch: 58/100 | step: 129/422 | loss: 0.28252407908439636\n",
      "Epoch: 58/100 | step: 130/422 | loss: 0.5976782441139221\n",
      "Epoch: 58/100 | step: 131/422 | loss: 0.2652451694011688\n",
      "Epoch: 58/100 | step: 132/422 | loss: 0.5311145186424255\n",
      "Epoch: 58/100 | step: 133/422 | loss: 0.2540711760520935\n",
      "Epoch: 58/100 | step: 134/422 | loss: 0.8783698081970215\n",
      "Epoch: 58/100 | step: 135/422 | loss: 0.3345623016357422\n",
      "Epoch: 58/100 | step: 136/422 | loss: 0.3289848864078522\n",
      "Epoch: 58/100 | step: 137/422 | loss: 0.597771167755127\n",
      "Epoch: 58/100 | step: 138/422 | loss: 0.4850108325481415\n",
      "Epoch: 58/100 | step: 139/422 | loss: 0.3045275807380676\n",
      "Epoch: 58/100 | step: 140/422 | loss: 0.31629207730293274\n",
      "Epoch: 58/100 | step: 141/422 | loss: 0.37930187582969666\n",
      "Epoch: 58/100 | step: 142/422 | loss: 0.509975254535675\n",
      "Epoch: 58/100 | step: 143/422 | loss: 0.6452014446258545\n",
      "Epoch: 58/100 | step: 144/422 | loss: 0.4566878080368042\n",
      "Epoch: 58/100 | step: 145/422 | loss: 0.47423288226127625\n",
      "Epoch: 58/100 | step: 146/422 | loss: 0.22181583940982819\n",
      "Epoch: 58/100 | step: 147/422 | loss: 0.33145031332969666\n",
      "Epoch: 58/100 | step: 148/422 | loss: 0.6315603256225586\n",
      "Epoch: 58/100 | step: 149/422 | loss: 0.17911510169506073\n",
      "Epoch: 58/100 | step: 150/422 | loss: 0.4718766212463379\n",
      "Epoch: 58/100 | step: 151/422 | loss: 0.6406891942024231\n",
      "Epoch: 58/100 | step: 152/422 | loss: 0.6906279921531677\n",
      "Epoch: 58/100 | step: 153/422 | loss: 0.5612754225730896\n",
      "Epoch: 58/100 | step: 154/422 | loss: 0.14468994736671448\n",
      "Epoch: 58/100 | step: 155/422 | loss: 0.6234206557273865\n",
      "Epoch: 58/100 | step: 156/422 | loss: 0.5186975598335266\n",
      "Epoch: 58/100 | step: 157/422 | loss: 0.7514325976371765\n",
      "Epoch: 58/100 | step: 158/422 | loss: 0.4772060215473175\n",
      "Epoch: 58/100 | step: 159/422 | loss: 0.44119372963905334\n",
      "Epoch: 58/100 | step: 160/422 | loss: 0.6441512703895569\n",
      "Epoch: 58/100 | step: 161/422 | loss: 0.29596012830734253\n",
      "Epoch: 58/100 | step: 162/422 | loss: 0.5790168642997742\n",
      "Epoch: 58/100 | step: 163/422 | loss: 0.3683837950229645\n",
      "Epoch: 58/100 | step: 164/422 | loss: 0.38150912523269653\n",
      "Epoch: 58/100 | step: 165/422 | loss: 0.7251887917518616\n",
      "Epoch: 58/100 | step: 166/422 | loss: 0.43476736545562744\n",
      "Epoch: 58/100 | step: 167/422 | loss: 0.29778599739074707\n",
      "Epoch: 58/100 | step: 168/422 | loss: 0.35318702459335327\n",
      "Epoch: 58/100 | step: 169/422 | loss: 0.2672232389450073\n",
      "Epoch: 58/100 | step: 170/422 | loss: 0.2684260904788971\n",
      "Epoch: 58/100 | step: 171/422 | loss: 0.4150894582271576\n",
      "Epoch: 58/100 | step: 172/422 | loss: 0.3099300265312195\n",
      "Epoch: 58/100 | step: 173/422 | loss: 0.37350785732269287\n",
      "Epoch: 58/100 | step: 174/422 | loss: 0.43057307600975037\n",
      "Epoch: 58/100 | step: 175/422 | loss: 0.3161211311817169\n",
      "Epoch: 58/100 | step: 176/422 | loss: 0.3861187696456909\n",
      "Epoch: 58/100 | step: 177/422 | loss: 0.45772260427474976\n",
      "Epoch: 58/100 | step: 178/422 | loss: 0.36656877398490906\n",
      "Epoch: 58/100 | step: 179/422 | loss: 0.40776070952415466\n",
      "Epoch: 58/100 | step: 180/422 | loss: 0.24793143570423126\n",
      "Epoch: 58/100 | step: 181/422 | loss: 0.31552019715309143\n",
      "Epoch: 58/100 | step: 182/422 | loss: 0.21071785688400269\n",
      "Epoch: 58/100 | step: 183/422 | loss: 0.6599295735359192\n",
      "Epoch: 58/100 | step: 184/422 | loss: 0.3466091454029083\n",
      "Epoch: 58/100 | step: 185/422 | loss: 0.2368597686290741\n",
      "Epoch: 58/100 | step: 186/422 | loss: 0.41043996810913086\n",
      "Epoch: 58/100 | step: 187/422 | loss: 0.41664838790893555\n",
      "Epoch: 58/100 | step: 188/422 | loss: 0.2103777825832367\n",
      "Epoch: 58/100 | step: 189/422 | loss: 0.31551265716552734\n",
      "Epoch: 58/100 | step: 190/422 | loss: 0.22931905090808868\n",
      "Epoch: 58/100 | step: 191/422 | loss: 0.2817581593990326\n",
      "Epoch: 58/100 | step: 192/422 | loss: 0.8973472118377686\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 59/100 | step: 1/422 | loss: 0.2867518961429596\n",
      "Epoch: 59/100 | step: 2/422 | loss: 0.5912362337112427\n",
      "Epoch: 59/100 | step: 3/422 | loss: 0.21668601036071777\n",
      "Epoch: 59/100 | step: 4/422 | loss: 0.4585989713668823\n",
      "Epoch: 59/100 | step: 5/422 | loss: 0.386189728975296\n",
      "Epoch: 59/100 | step: 6/422 | loss: 0.34738296270370483\n",
      "Epoch: 59/100 | step: 7/422 | loss: 0.322806179523468\n",
      "Epoch: 59/100 | step: 8/422 | loss: 0.2020464837551117\n",
      "Epoch: 59/100 | step: 9/422 | loss: 0.4619463384151459\n",
      "Epoch: 59/100 | step: 10/422 | loss: 0.20339952409267426\n",
      "Epoch: 59/100 | step: 11/422 | loss: 0.5114670395851135\n",
      "Epoch: 59/100 | step: 12/422 | loss: 0.6661974787712097\n",
      "Epoch: 59/100 | step: 13/422 | loss: 0.1065119206905365\n",
      "Epoch: 59/100 | step: 14/422 | loss: 0.256576806306839\n",
      "Epoch: 59/100 | step: 15/422 | loss: 0.24959731101989746\n",
      "Epoch: 59/100 | step: 16/422 | loss: 0.4248620569705963\n",
      "Epoch: 59/100 | step: 17/422 | loss: 0.17274487018585205\n",
      "Epoch: 59/100 | step: 18/422 | loss: 0.336198091506958\n",
      "Epoch: 59/100 | step: 19/422 | loss: 0.17256788909435272\n",
      "Epoch: 59/100 | step: 20/422 | loss: 0.4358896315097809\n",
      "Epoch: 59/100 | step: 21/422 | loss: 0.38046059012413025\n",
      "Epoch: 59/100 | step: 22/422 | loss: 0.23433510959148407\n",
      "Epoch: 59/100 | step: 23/422 | loss: 0.43981194496154785\n",
      "Epoch: 59/100 | step: 24/422 | loss: 0.3575516641139984\n",
      "Epoch: 59/100 | step: 25/422 | loss: 0.235752135515213\n",
      "Epoch: 59/100 | step: 26/422 | loss: 0.1939702033996582\n",
      "Epoch: 59/100 | step: 27/422 | loss: 0.7874802350997925\n",
      "Epoch: 59/100 | step: 28/422 | loss: 0.35834795236587524\n",
      "Epoch: 59/100 | step: 29/422 | loss: 0.394757479429245\n",
      "Epoch: 59/100 | step: 30/422 | loss: 0.6126033663749695\n",
      "Epoch: 59/100 | step: 31/422 | loss: 0.33169907331466675\n",
      "Epoch: 59/100 | step: 32/422 | loss: 0.31218740344047546\n",
      "Epoch: 59/100 | step: 33/422 | loss: 0.22392910718917847\n",
      "Epoch: 59/100 | step: 34/422 | loss: 0.2891770601272583\n",
      "Epoch: 59/100 | step: 35/422 | loss: 0.5143837928771973\n",
      "Epoch: 59/100 | step: 36/422 | loss: 0.3261902928352356\n",
      "Epoch: 59/100 | step: 37/422 | loss: 0.3281576633453369\n",
      "Epoch: 59/100 | step: 38/422 | loss: 0.6933576464653015\n",
      "Epoch: 59/100 | step: 39/422 | loss: 0.5658383369445801\n",
      "Epoch: 59/100 | step: 40/422 | loss: 0.7194132208824158\n",
      "Epoch: 59/100 | step: 41/422 | loss: 0.0807953029870987\n",
      "Epoch: 59/100 | step: 42/422 | loss: 0.19708237051963806\n",
      "Epoch: 59/100 | step: 43/422 | loss: 0.22833235561847687\n",
      "Epoch: 59/100 | step: 44/422 | loss: 0.2637377679347992\n",
      "Epoch: 59/100 | step: 45/422 | loss: 0.4240264892578125\n",
      "Epoch: 59/100 | step: 46/422 | loss: 0.6239350438117981\n",
      "Epoch: 59/100 | step: 47/422 | loss: 0.658852219581604\n",
      "Epoch: 59/100 | step: 48/422 | loss: 0.25748181343078613\n",
      "Epoch: 59/100 | step: 49/422 | loss: 0.542719841003418\n",
      "Epoch: 59/100 | step: 50/422 | loss: 0.236275777220726\n",
      "Epoch: 59/100 | step: 51/422 | loss: 0.5539368391036987\n",
      "Epoch: 59/100 | step: 52/422 | loss: 0.37716636061668396\n",
      "Epoch: 59/100 | step: 53/422 | loss: 0.326931357383728\n",
      "Epoch: 59/100 | step: 54/422 | loss: 0.4709555506706238\n",
      "Epoch: 59/100 | step: 55/422 | loss: 0.5486108660697937\n",
      "Epoch: 59/100 | step: 56/422 | loss: 0.2668952941894531\n",
      "Epoch: 59/100 | step: 57/422 | loss: 0.5437250733375549\n",
      "Epoch: 59/100 | step: 58/422 | loss: 0.43452873826026917\n",
      "Epoch: 59/100 | step: 59/422 | loss: 0.27391329407691956\n",
      "Epoch: 59/100 | step: 60/422 | loss: 0.2339727282524109\n",
      "Epoch: 59/100 | step: 61/422 | loss: 0.45045509934425354\n",
      "Epoch: 59/100 | step: 62/422 | loss: 0.3260534107685089\n",
      "Epoch: 59/100 | step: 63/422 | loss: 0.31931015849113464\n",
      "Epoch: 59/100 | step: 64/422 | loss: 0.4968548119068146\n",
      "Epoch: 59/100 | step: 65/422 | loss: 0.32403603196144104\n",
      "Epoch: 59/100 | step: 66/422 | loss: 0.311136931180954\n",
      "Epoch: 59/100 | step: 67/422 | loss: 0.5281882882118225\n",
      "Epoch: 59/100 | step: 68/422 | loss: 0.26513418555259705\n",
      "Epoch: 59/100 | step: 69/422 | loss: 0.3155014216899872\n",
      "Epoch: 59/100 | step: 70/422 | loss: 0.6917593479156494\n",
      "Epoch: 59/100 | step: 71/422 | loss: 0.4018208384513855\n",
      "Epoch: 59/100 | step: 72/422 | loss: 0.3725612163543701\n",
      "Epoch: 59/100 | step: 73/422 | loss: 0.3674146234989166\n",
      "Epoch: 59/100 | step: 74/422 | loss: 0.7775039076805115\n",
      "Epoch: 59/100 | step: 75/422 | loss: 0.6864471435546875\n",
      "Epoch: 59/100 | step: 76/422 | loss: 0.22447852790355682\n",
      "Epoch: 59/100 | step: 77/422 | loss: 0.3163260221481323\n",
      "Epoch: 59/100 | step: 78/422 | loss: 0.2324487268924713\n",
      "Epoch: 59/100 | step: 79/422 | loss: 0.6216012835502625\n",
      "Epoch: 59/100 | step: 80/422 | loss: 0.2552618980407715\n",
      "Epoch: 59/100 | step: 81/422 | loss: 0.2643332779407501\n",
      "Epoch: 59/100 | step: 82/422 | loss: 0.27488234639167786\n",
      "Epoch: 59/100 | step: 83/422 | loss: 0.1691940873861313\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 60/100 | step: 1/422 | loss: 0.1672838032245636\n",
      "Epoch: 60/100 | step: 2/422 | loss: 0.36265259981155396\n",
      "Epoch: 60/100 | step: 3/422 | loss: 0.20493097603321075\n",
      "Epoch: 60/100 | step: 4/422 | loss: 0.19742432236671448\n",
      "Epoch: 60/100 | step: 5/422 | loss: 0.4403250813484192\n",
      "Epoch: 60/100 | step: 6/422 | loss: 0.3788200616836548\n",
      "Epoch: 60/100 | step: 7/422 | loss: 0.28749650716781616\n",
      "Epoch: 60/100 | step: 8/422 | loss: 0.36408624053001404\n",
      "Epoch: 60/100 | step: 9/422 | loss: 0.21242260932922363\n",
      "Epoch: 60/100 | step: 10/422 | loss: 0.7891450524330139\n",
      "Epoch: 60/100 | step: 11/422 | loss: 0.3402574062347412\n",
      "Epoch: 60/100 | step: 12/422 | loss: 0.26760363578796387\n",
      "Epoch: 60/100 | step: 13/422 | loss: 0.3692421317100525\n",
      "Epoch: 60/100 | step: 14/422 | loss: 0.3386463522911072\n",
      "Epoch: 60/100 | step: 15/422 | loss: 0.38237205147743225\n",
      "Epoch: 60/100 | step: 16/422 | loss: 0.5974633693695068\n",
      "Epoch: 60/100 | step: 17/422 | loss: 0.6433630585670471\n",
      "Epoch: 60/100 | step: 18/422 | loss: 1.5698792934417725\n",
      "Epoch: 60/100 | step: 19/422 | loss: 0.3160933554172516\n",
      "Epoch: 60/100 | step: 20/422 | loss: 0.3679194748401642\n",
      "Epoch: 60/100 | step: 21/422 | loss: 0.21738608181476593\n",
      "Epoch: 60/100 | step: 22/422 | loss: 0.27028730511665344\n",
      "Epoch: 60/100 | step: 23/422 | loss: 0.17803426086902618\n",
      "Epoch: 60/100 | step: 24/422 | loss: 0.3660773038864136\n",
      "Epoch: 60/100 | step: 25/422 | loss: 0.42062363028526306\n",
      "Epoch: 60/100 | step: 26/422 | loss: 0.43820902705192566\n",
      "Epoch: 60/100 | step: 27/422 | loss: 0.2306261509656906\n",
      "Epoch: 60/100 | step: 28/422 | loss: 0.44810205698013306\n",
      "Epoch: 60/100 | step: 29/422 | loss: 0.47726523876190186\n",
      "Epoch: 60/100 | step: 30/422 | loss: 0.17420430481433868\n",
      "Epoch: 60/100 | step: 31/422 | loss: 0.3073275685310364\n",
      "Epoch: 60/100 | step: 32/422 | loss: 0.28345000743865967\n",
      "Epoch: 60/100 | step: 33/422 | loss: 0.44465330243110657\n",
      "Epoch: 60/100 | step: 34/422 | loss: 0.448063462972641\n",
      "Epoch: 60/100 | step: 35/422 | loss: 0.4029449224472046\n",
      "Epoch: 60/100 | step: 36/422 | loss: 0.41050344705581665\n",
      "Epoch: 60/100 | step: 37/422 | loss: 0.44872841238975525\n",
      "Epoch: 60/100 | step: 38/422 | loss: 0.2740616798400879\n",
      "Epoch: 60/100 | step: 39/422 | loss: 0.4946466088294983\n",
      "Epoch: 60/100 | step: 40/422 | loss: 0.3894001543521881\n",
      "Epoch: 60/100 | step: 41/422 | loss: 0.5797394514083862\n",
      "Epoch: 60/100 | step: 42/422 | loss: 0.2291649878025055\n",
      "Epoch: 60/100 | step: 43/422 | loss: 0.24258379638195038\n",
      "Epoch: 60/100 | step: 44/422 | loss: 0.7021920680999756\n",
      "Epoch: 60/100 | step: 45/422 | loss: 0.8251262307167053\n",
      "Epoch: 60/100 | step: 46/422 | loss: 0.2523633539676666\n",
      "Epoch: 60/100 | step: 47/422 | loss: 0.1907457560300827\n",
      "Epoch: 60/100 | step: 48/422 | loss: 0.13800305128097534\n",
      "Epoch: 60/100 | step: 49/422 | loss: 0.33639246225357056\n",
      "Epoch: 60/100 | step: 50/422 | loss: 0.16115820407867432\n",
      "Epoch: 60/100 | step: 51/422 | loss: 0.2608504891395569\n",
      "Epoch: 60/100 | step: 52/422 | loss: 0.27205953001976013\n",
      "Epoch: 60/100 | step: 53/422 | loss: 0.3514057397842407\n",
      "Epoch: 60/100 | step: 54/422 | loss: 0.34376752376556396\n",
      "Epoch: 60/100 | step: 55/422 | loss: 0.49312734603881836\n",
      "Epoch: 60/100 | step: 56/422 | loss: 0.4052247703075409\n",
      "Epoch: 60/100 | step: 57/422 | loss: 0.1973804235458374\n",
      "Epoch: 60/100 | step: 58/422 | loss: 0.306061714887619\n",
      "Epoch: 60/100 | step: 59/422 | loss: 0.3100923001766205\n",
      "Epoch: 60/100 | step: 60/422 | loss: 0.5093935132026672\n",
      "Epoch: 60/100 | step: 61/422 | loss: 0.18531638383865356\n",
      "Epoch: 60/100 | step: 62/422 | loss: 0.3451145589351654\n",
      "Epoch: 60/100 | step: 63/422 | loss: 0.3858289122581482\n",
      "Epoch: 60/100 | step: 64/422 | loss: 0.5466207265853882\n",
      "Epoch: 60/100 | step: 65/422 | loss: 0.28138649463653564\n",
      "Epoch: 60/100 | step: 66/422 | loss: 0.4163278043270111\n",
      "Epoch: 60/100 | step: 67/422 | loss: 0.3276345729827881\n",
      "Epoch: 60/100 | step: 68/422 | loss: 0.41174623370170593\n",
      "Epoch: 60/100 | step: 69/422 | loss: 0.28317156434059143\n",
      "Epoch: 60/100 | step: 70/422 | loss: 0.1757611781358719\n",
      "Epoch: 60/100 | step: 71/422 | loss: 0.49111074209213257\n",
      "Epoch: 60/100 | step: 72/422 | loss: 0.3800579905509949\n",
      "Epoch: 60/100 | step: 73/422 | loss: 0.3558979332447052\n",
      "Epoch: 60/100 | step: 74/422 | loss: 0.5435968041419983\n",
      "Epoch: 60/100 | step: 75/422 | loss: 0.2632642984390259\n",
      "Epoch: 60/100 | step: 76/422 | loss: 0.3855222165584564\n",
      "Epoch: 60/100 | step: 77/422 | loss: 0.18051959574222565\n",
      "Epoch: 60/100 | step: 78/422 | loss: 0.4464699327945709\n",
      "Epoch: 60/100 | step: 79/422 | loss: 0.35688531398773193\n",
      "Epoch: 60/100 | step: 80/422 | loss: 0.422852098941803\n",
      "Epoch: 60/100 | step: 81/422 | loss: 0.5941609740257263\n",
      "Epoch: 60/100 | step: 82/422 | loss: 0.1547136902809143\n",
      "Epoch: 60/100 | step: 83/422 | loss: 0.24822744727134705\n",
      "Epoch: 60/100 | step: 84/422 | loss: 0.45759260654449463\n",
      "Epoch: 60/100 | step: 85/422 | loss: 0.5929002165794373\n",
      "Epoch: 60/100 | step: 86/422 | loss: 0.18244676291942596\n",
      "Epoch: 60/100 | step: 87/422 | loss: 0.30286964774131775\n",
      "Epoch: 60/100 | step: 88/422 | loss: 0.31937456130981445\n",
      "Epoch: 60/100 | step: 89/422 | loss: 0.29437655210494995\n",
      "Epoch: 60/100 | step: 90/422 | loss: 0.36076977849006653\n",
      "Epoch: 60/100 | step: 91/422 | loss: 0.3457086682319641\n",
      "Epoch: 60/100 | step: 92/422 | loss: 0.4890618324279785\n",
      "Epoch: 60/100 | step: 93/422 | loss: 0.3908642530441284\n",
      "Epoch: 60/100 | step: 94/422 | loss: 0.4452745020389557\n",
      "Epoch: 60/100 | step: 95/422 | loss: 0.2762407958507538\n",
      "Epoch: 60/100 | step: 96/422 | loss: 0.6439077854156494\n",
      "Epoch: 60/100 | step: 97/422 | loss: 0.2719629406929016\n",
      "Epoch: 60/100 | step: 98/422 | loss: 0.41558051109313965\n",
      "Epoch: 60/100 | step: 99/422 | loss: 0.15676794946193695\n",
      "Epoch: 60/100 | step: 100/422 | loss: 0.2724805474281311\n",
      "Epoch: 60/100 | step: 101/422 | loss: 0.38327622413635254\n",
      "Epoch: 60/100 | step: 102/422 | loss: 0.38030487298965454\n",
      "Epoch: 60/100 | step: 103/422 | loss: 0.3787253201007843\n",
      "Epoch: 60/100 | step: 104/422 | loss: 0.161491259932518\n",
      "Epoch: 60/100 | step: 105/422 | loss: 0.2852453589439392\n",
      "Epoch: 60/100 | step: 106/422 | loss: 0.33078646659851074\n",
      "Epoch: 60/100 | step: 107/422 | loss: 0.2654109299182892\n",
      "Epoch: 60/100 | step: 108/422 | loss: 0.41436243057250977\n",
      "Epoch: 60/100 | step: 109/422 | loss: 0.44726186990737915\n",
      "Epoch: 60/100 | step: 110/422 | loss: 0.37601780891418457\n",
      "Epoch: 60/100 | step: 111/422 | loss: 0.5790699124336243\n",
      "Epoch: 60/100 | step: 112/422 | loss: 0.4678938686847687\n",
      "Epoch: 60/100 | step: 113/422 | loss: 0.34001612663269043\n",
      "Epoch: 60/100 | step: 114/422 | loss: 0.34648042917251587\n",
      "Epoch: 60/100 | step: 115/422 | loss: 0.2237899750471115\n",
      "Epoch: 60/100 | step: 116/422 | loss: 0.6116607189178467\n",
      "Epoch: 60/100 | step: 117/422 | loss: 0.25784796476364136\n",
      "Epoch: 60/100 | step: 118/422 | loss: 0.2986937165260315\n",
      "Epoch: 60/100 | step: 119/422 | loss: 0.2548700273036957\n",
      "Epoch: 60/100 | step: 120/422 | loss: 0.3049895167350769\n",
      "Epoch: 60/100 | step: 121/422 | loss: 0.2871359884738922\n",
      "Epoch: 60/100 | step: 122/422 | loss: 0.43247389793395996\n",
      "Epoch: 60/100 | step: 123/422 | loss: 0.5984269380569458\n",
      "Epoch: 60/100 | step: 124/422 | loss: 0.27976924180984497\n",
      "Epoch: 60/100 | step: 125/422 | loss: 0.2208394855260849\n",
      "Epoch: 60/100 | step: 126/422 | loss: 0.3828730881214142\n",
      "Epoch: 60/100 | step: 127/422 | loss: 0.40961676836013794\n",
      "Epoch: 60/100 | step: 128/422 | loss: 0.18647798895835876\n",
      "Epoch: 60/100 | step: 129/422 | loss: 0.30904629826545715\n",
      "Epoch: 60/100 | step: 130/422 | loss: 0.47664129734039307\n",
      "Epoch: 60/100 | step: 131/422 | loss: 0.18766850233078003\n",
      "Epoch: 60/100 | step: 132/422 | loss: 0.3379524350166321\n",
      "Epoch: 60/100 | step: 133/422 | loss: 0.5568932890892029\n",
      "Epoch: 60/100 | step: 134/422 | loss: 0.2685050666332245\n",
      "Epoch: 60/100 | step: 135/422 | loss: 0.39579153060913086\n",
      "Epoch: 60/100 | step: 136/422 | loss: 0.34832122921943665\n",
      "Epoch: 60/100 | step: 137/422 | loss: 0.5550616383552551\n",
      "Epoch: 60/100 | step: 138/422 | loss: 0.29471084475517273\n",
      "Epoch: 60/100 | step: 139/422 | loss: 0.46630868315696716\n",
      "Epoch: 60/100 | step: 140/422 | loss: 0.27622881531715393\n",
      "Epoch: 60/100 | step: 141/422 | loss: 0.2618933320045471\n",
      "Epoch: 60/100 | step: 142/422 | loss: 0.2484254539012909\n",
      "Epoch: 60/100 | step: 143/422 | loss: 0.49527475237846375\n",
      "Epoch: 60/100 | step: 144/422 | loss: 0.3215157687664032\n",
      "Epoch: 60/100 | step: 145/422 | loss: 0.3767029345035553\n",
      "Epoch: 60/100 | step: 146/422 | loss: 0.6695298552513123\n",
      "Epoch: 60/100 | step: 147/422 | loss: 1.2134016752243042\n",
      "Epoch: 60/100 | step: 148/422 | loss: 0.32181116938591003\n",
      "Epoch: 60/100 | step: 149/422 | loss: 0.4910660982131958\n",
      "Epoch: 60/100 | step: 150/422 | loss: 0.20964589715003967\n",
      "Epoch: 60/100 | step: 151/422 | loss: 0.41657742857933044\n",
      "Epoch: 60/100 | step: 152/422 | loss: 0.4199839234352112\n",
      "Epoch: 60/100 | step: 153/422 | loss: 0.2045605331659317\n",
      "Epoch: 60/100 | step: 154/422 | loss: 0.37077003717422485\n",
      "Epoch: 60/100 | step: 155/422 | loss: 0.2771313190460205\n",
      "Epoch: 60/100 | step: 156/422 | loss: 0.43456709384918213\n",
      "Epoch: 60/100 | step: 157/422 | loss: 0.3775225877761841\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 61/100 | step: 1/422 | loss: 0.5048899054527283\n",
      "Epoch: 61/100 | step: 2/422 | loss: 0.4541630148887634\n",
      "Epoch: 61/100 | step: 3/422 | loss: 0.3446282148361206\n",
      "Epoch: 61/100 | step: 4/422 | loss: 0.29698723554611206\n",
      "Epoch: 61/100 | step: 5/422 | loss: 0.7318256497383118\n",
      "Epoch: 61/100 | step: 6/422 | loss: 0.3868327736854553\n",
      "Epoch: 61/100 | step: 7/422 | loss: 0.6575819849967957\n",
      "Epoch: 61/100 | step: 8/422 | loss: 0.7845072746276855\n",
      "Epoch: 61/100 | step: 9/422 | loss: 0.308889240026474\n",
      "Epoch: 61/100 | step: 10/422 | loss: 0.20189312100410461\n",
      "Epoch: 61/100 | step: 11/422 | loss: 0.776943564414978\n",
      "Epoch: 61/100 | step: 12/422 | loss: 0.38488778471946716\n",
      "Epoch: 61/100 | step: 13/422 | loss: 0.5759240388870239\n",
      "Epoch: 61/100 | step: 14/422 | loss: 0.8386262059211731\n",
      "Epoch: 61/100 | step: 15/422 | loss: 0.4292683005332947\n",
      "Epoch: 61/100 | step: 16/422 | loss: 0.21403466165065765\n",
      "Epoch: 61/100 | step: 17/422 | loss: 0.48064547777175903\n",
      "Epoch: 61/100 | step: 18/422 | loss: 0.442037433385849\n",
      "Epoch: 61/100 | step: 19/422 | loss: 0.41115739941596985\n",
      "Epoch: 61/100 | step: 20/422 | loss: 0.289610356092453\n",
      "Epoch: 61/100 | step: 21/422 | loss: 0.2181272804737091\n",
      "Epoch: 61/100 | step: 22/422 | loss: 0.2979200482368469\n",
      "Epoch: 61/100 | step: 23/422 | loss: 0.2063879370689392\n",
      "Epoch: 61/100 | step: 24/422 | loss: 0.2871924340724945\n",
      "Epoch: 61/100 | step: 25/422 | loss: 0.2139967530965805\n",
      "Epoch: 61/100 | step: 26/422 | loss: 0.3573428690433502\n",
      "Epoch: 61/100 | step: 27/422 | loss: 0.3727014660835266\n",
      "Epoch: 61/100 | step: 28/422 | loss: 0.497964084148407\n",
      "Epoch: 61/100 | step: 29/422 | loss: 0.3409188985824585\n",
      "Epoch: 61/100 | step: 30/422 | loss: 0.23707351088523865\n",
      "Epoch: 61/100 | step: 31/422 | loss: 0.35500726103782654\n",
      "Epoch: 61/100 | step: 32/422 | loss: 0.30504924058914185\n",
      "Epoch: 61/100 | step: 33/422 | loss: 0.21507221460342407\n",
      "Epoch: 61/100 | step: 34/422 | loss: 0.22866399586200714\n",
      "Epoch: 61/100 | step: 35/422 | loss: 0.5806195139884949\n",
      "Epoch: 61/100 | step: 36/422 | loss: 0.4317091405391693\n",
      "Epoch: 61/100 | step: 37/422 | loss: 0.3542112112045288\n",
      "Epoch: 61/100 | step: 38/422 | loss: 0.18642137944698334\n",
      "Epoch: 61/100 | step: 39/422 | loss: 0.303690642118454\n",
      "Epoch: 61/100 | step: 40/422 | loss: 0.400210440158844\n",
      "Epoch: 61/100 | step: 41/422 | loss: 0.1655408889055252\n",
      "Epoch: 61/100 | step: 42/422 | loss: 0.45272403955459595\n",
      "Epoch: 61/100 | step: 43/422 | loss: 0.24034303426742554\n",
      "Epoch: 61/100 | step: 44/422 | loss: 0.5461154580116272\n",
      "Epoch: 61/100 | step: 45/422 | loss: 0.5175774693489075\n",
      "Epoch: 61/100 | step: 46/422 | loss: 0.3275211751461029\n",
      "Epoch: 61/100 | step: 47/422 | loss: 0.18704457581043243\n",
      "Epoch: 61/100 | step: 48/422 | loss: 0.3354833126068115\n",
      "Epoch: 61/100 | step: 49/422 | loss: 0.19293437898159027\n",
      "Epoch: 61/100 | step: 50/422 | loss: 0.36157912015914917\n",
      "Epoch: 61/100 | step: 51/422 | loss: 0.2128802090883255\n",
      "Epoch: 61/100 | step: 52/422 | loss: 0.33811771869659424\n",
      "Epoch: 61/100 | step: 53/422 | loss: 0.2062278389930725\n",
      "Epoch: 61/100 | step: 54/422 | loss: 0.18479326367378235\n",
      "Epoch: 61/100 | step: 55/422 | loss: 0.34812232851982117\n",
      "Epoch: 61/100 | step: 56/422 | loss: 0.25786322355270386\n",
      "Epoch: 61/100 | step: 57/422 | loss: 0.42048874497413635\n",
      "Epoch: 61/100 | step: 58/422 | loss: 0.22662955522537231\n",
      "Epoch: 61/100 | step: 59/422 | loss: 0.16496172547340393\n",
      "Epoch: 61/100 | step: 60/422 | loss: 0.1914641559123993\n",
      "Epoch: 61/100 | step: 61/422 | loss: 0.3731098175048828\n",
      "Epoch: 61/100 | step: 62/422 | loss: 0.4374587833881378\n",
      "Epoch: 61/100 | step: 63/422 | loss: 0.22489698231220245\n",
      "Epoch: 61/100 | step: 64/422 | loss: 0.6645146608352661\n",
      "Epoch: 61/100 | step: 65/422 | loss: 0.5819041132926941\n",
      "Epoch: 61/100 | step: 66/422 | loss: 0.27345865964889526\n",
      "Epoch: 61/100 | step: 67/422 | loss: 0.48169004917144775\n",
      "Epoch: 61/100 | step: 68/422 | loss: 0.1740758717060089\n",
      "Epoch: 61/100 | step: 69/422 | loss: 0.48462188243865967\n",
      "Epoch: 61/100 | step: 70/422 | loss: 0.5252660512924194\n",
      "Epoch: 61/100 | step: 71/422 | loss: 0.31993982195854187\n",
      "Epoch: 61/100 | step: 72/422 | loss: 0.56102055311203\n",
      "Epoch: 61/100 | step: 73/422 | loss: 0.388515830039978\n",
      "Epoch: 61/100 | step: 74/422 | loss: 0.3796144425868988\n",
      "Epoch: 61/100 | step: 75/422 | loss: 0.23889929056167603\n",
      "Epoch: 61/100 | step: 76/422 | loss: 0.36921799182891846\n",
      "Epoch: 61/100 | step: 77/422 | loss: 0.28967612981796265\n",
      "Epoch: 61/100 | step: 78/422 | loss: 0.48289063572883606\n",
      "Epoch: 61/100 | step: 79/422 | loss: 0.516891598701477\n",
      "Epoch: 61/100 | step: 80/422 | loss: 0.26209756731987\n",
      "Epoch: 61/100 | step: 81/422 | loss: 0.40944746136665344\n",
      "Epoch: 61/100 | step: 82/422 | loss: 0.4681209623813629\n",
      "Epoch: 61/100 | step: 83/422 | loss: 0.21740056574344635\n",
      "Epoch: 61/100 | step: 84/422 | loss: 0.4236619472503662\n",
      "Epoch: 61/100 | step: 85/422 | loss: 0.3209768533706665\n",
      "Epoch: 61/100 | step: 86/422 | loss: 0.23282745480537415\n",
      "Epoch: 61/100 | step: 87/422 | loss: 0.4020283818244934\n",
      "Epoch: 61/100 | step: 88/422 | loss: 0.2458178997039795\n",
      "Epoch: 61/100 | step: 89/422 | loss: 0.32958370447158813\n",
      "Epoch: 61/100 | step: 90/422 | loss: 0.27527034282684326\n",
      "Epoch: 61/100 | step: 91/422 | loss: 0.42377394437789917\n",
      "Epoch: 61/100 | step: 92/422 | loss: 0.4599362313747406\n",
      "Epoch: 61/100 | step: 93/422 | loss: 0.4154273271560669\n",
      "Epoch: 61/100 | step: 94/422 | loss: 0.3078252077102661\n",
      "Epoch: 61/100 | step: 95/422 | loss: 0.281654953956604\n",
      "Epoch: 61/100 | step: 96/422 | loss: 0.3624565899372101\n",
      "Epoch: 61/100 | step: 97/422 | loss: 0.3247281610965729\n",
      "Epoch: 61/100 | step: 98/422 | loss: 0.7819652557373047\n",
      "Epoch: 61/100 | step: 99/422 | loss: 0.30130305886268616\n",
      "Epoch: 61/100 | step: 100/422 | loss: 0.18253614008426666\n",
      "Epoch: 61/100 | step: 101/422 | loss: 0.17834393680095673\n",
      "Epoch: 61/100 | step: 102/422 | loss: 0.43955332040786743\n",
      "Epoch: 61/100 | step: 103/422 | loss: 0.4653165340423584\n",
      "Epoch: 61/100 | step: 104/422 | loss: 0.3948884606361389\n",
      "Epoch: 61/100 | step: 105/422 | loss: 0.25850710272789\n",
      "Epoch: 61/100 | step: 106/422 | loss: 0.5087138414382935\n",
      "Epoch: 61/100 | step: 107/422 | loss: 0.5458371043205261\n",
      "Epoch: 61/100 | step: 108/422 | loss: 0.3353484570980072\n",
      "Epoch: 61/100 | step: 109/422 | loss: 0.31156477332115173\n",
      "Epoch: 61/100 | step: 110/422 | loss: 0.49404674768447876\n",
      "Epoch: 61/100 | step: 111/422 | loss: 0.4666113257408142\n",
      "Epoch: 61/100 | step: 112/422 | loss: 0.5061607956886292\n",
      "Epoch: 61/100 | step: 113/422 | loss: 0.40415871143341064\n",
      "Epoch: 61/100 | step: 114/422 | loss: 0.30518120527267456\n",
      "Epoch: 61/100 | step: 115/422 | loss: 0.4233973026275635\n",
      "Epoch: 61/100 | step: 116/422 | loss: 0.7272303700447083\n",
      "Epoch: 61/100 | step: 117/422 | loss: 0.3256361782550812\n",
      "Epoch: 61/100 | step: 118/422 | loss: 0.16941772401332855\n",
      "Epoch: 61/100 | step: 119/422 | loss: 0.2646418511867523\n",
      "Epoch: 61/100 | step: 120/422 | loss: 0.4243488609790802\n",
      "Epoch: 61/100 | step: 121/422 | loss: 0.29975754022598267\n",
      "Epoch: 61/100 | step: 122/422 | loss: 0.5024003982543945\n",
      "Epoch: 61/100 | step: 123/422 | loss: 0.3730660378932953\n",
      "Epoch: 61/100 | step: 124/422 | loss: 0.279442697763443\n",
      "Epoch: 61/100 | step: 125/422 | loss: 0.29425889253616333\n",
      "Epoch: 61/100 | step: 126/422 | loss: 0.5850721597671509\n",
      "Epoch: 61/100 | step: 127/422 | loss: 0.27380791306495667\n",
      "Epoch: 61/100 | step: 128/422 | loss: 0.3620964586734772\n",
      "Epoch: 61/100 | step: 129/422 | loss: 0.4947563707828522\n",
      "Epoch: 61/100 | step: 130/422 | loss: 0.2567847967147827\n",
      "Epoch: 61/100 | step: 131/422 | loss: 0.27326369285583496\n",
      "Epoch: 61/100 | step: 132/422 | loss: 0.5210607647895813\n",
      "Epoch: 61/100 | step: 133/422 | loss: 0.35871580243110657\n",
      "Epoch: 61/100 | step: 134/422 | loss: 0.31001806259155273\n",
      "Epoch: 61/100 | step: 135/422 | loss: 0.4846358597278595\n",
      "Epoch: 61/100 | step: 136/422 | loss: 0.1668892502784729\n",
      "Epoch: 61/100 | step: 137/422 | loss: 0.3543263375759125\n",
      "Epoch: 61/100 | step: 138/422 | loss: 0.1292981505393982\n",
      "Epoch: 61/100 | step: 139/422 | loss: 0.31205126643180847\n",
      "Epoch: 61/100 | step: 140/422 | loss: 0.31232431530952454\n",
      "Epoch: 61/100 | step: 141/422 | loss: 0.26609891653060913\n",
      "Epoch: 61/100 | step: 142/422 | loss: 0.3852923810482025\n",
      "Epoch: 61/100 | step: 143/422 | loss: 0.5018306374549866\n",
      "Epoch: 61/100 | step: 144/422 | loss: 0.42705225944519043\n",
      "Epoch: 61/100 | step: 145/422 | loss: 0.4471779465675354\n",
      "Epoch: 61/100 | step: 146/422 | loss: 0.5015490651130676\n",
      "Epoch: 61/100 | step: 147/422 | loss: 0.6490041017532349\n",
      "Epoch: 61/100 | step: 148/422 | loss: 0.4218789041042328\n",
      "Epoch: 61/100 | step: 149/422 | loss: 0.6698359251022339\n",
      "Epoch: 61/100 | step: 150/422 | loss: 0.25989004969596863\n",
      "Epoch: 61/100 | step: 151/422 | loss: 0.4004421830177307\n",
      "Epoch: 61/100 | step: 152/422 | loss: 0.26964691281318665\n",
      "Epoch: 61/100 | step: 153/422 | loss: 0.6743745803833008\n",
      "Epoch: 61/100 | step: 154/422 | loss: 0.3328745365142822\n",
      "Epoch: 61/100 | step: 155/422 | loss: 0.39038336277008057\n",
      "Epoch: 61/100 | step: 156/422 | loss: 0.8710041642189026\n",
      "Epoch: 61/100 | step: 157/422 | loss: 0.372610867023468\n",
      "Epoch: 61/100 | step: 158/422 | loss: 0.2009541392326355\n",
      "Epoch: 61/100 | step: 159/422 | loss: 0.3388245403766632\n",
      "Epoch: 61/100 | step: 160/422 | loss: 0.42545950412750244\n",
      "Epoch: 61/100 | step: 161/422 | loss: 0.34695345163345337\n",
      "Epoch: 61/100 | step: 162/422 | loss: 0.8399770259857178\n",
      "Epoch: 61/100 | step: 163/422 | loss: 0.4169461131095886\n",
      "Epoch: 61/100 | step: 164/422 | loss: 0.5367605090141296\n",
      "Epoch: 61/100 | step: 165/422 | loss: 0.3905373215675354\n",
      "Epoch: 61/100 | step: 166/422 | loss: 0.35853520035743713\n",
      "Epoch: 61/100 | step: 167/422 | loss: 0.6905421614646912\n",
      "Epoch: 61/100 | step: 168/422 | loss: 0.25519537925720215\n",
      "Epoch: 61/100 | step: 169/422 | loss: 0.2945484220981598\n",
      "Epoch: 61/100 | step: 170/422 | loss: 0.2881130874156952\n",
      "Epoch: 61/100 | step: 171/422 | loss: 0.4467814564704895\n",
      "Epoch: 61/100 | step: 172/422 | loss: 0.18994778394699097\n",
      "Epoch: 61/100 | step: 173/422 | loss: 0.17785143852233887\n",
      "Epoch: 61/100 | step: 174/422 | loss: 0.6467454433441162\n",
      "Epoch: 61/100 | step: 175/422 | loss: 0.3762388825416565\n",
      "Epoch: 61/100 | step: 176/422 | loss: 0.21030789613723755\n",
      "Epoch: 61/100 | step: 177/422 | loss: 0.11448892205953598\n",
      "Epoch: 61/100 | step: 178/422 | loss: 0.17704448103904724\n",
      "Epoch: 61/100 | step: 179/422 | loss: 0.286310613155365\n",
      "Epoch: 61/100 | step: 180/422 | loss: 0.2957168221473694\n",
      "Epoch: 61/100 | step: 181/422 | loss: 0.25689345598220825\n",
      "Epoch: 61/100 | step: 182/422 | loss: 0.37541091442108154\n",
      "Epoch: 61/100 | step: 183/422 | loss: 0.39486417174339294\n",
      "Epoch: 61/100 | step: 184/422 | loss: 0.3602575361728668\n",
      "Epoch: 61/100 | step: 185/422 | loss: 0.18815822899341583\n",
      "Epoch: 61/100 | step: 186/422 | loss: 0.18846707046031952\n",
      "Epoch: 61/100 | step: 187/422 | loss: 0.6764733195304871\n",
      "Epoch: 61/100 | step: 188/422 | loss: 0.5068475008010864\n",
      "Epoch: 61/100 | step: 189/422 | loss: 0.23424190282821655\n",
      "Epoch: 61/100 | step: 190/422 | loss: 0.328498512506485\n",
      "Epoch: 61/100 | step: 191/422 | loss: 0.59228515625\n",
      "Epoch: 61/100 | step: 192/422 | loss: 0.11740903556346893\n",
      "Epoch: 61/100 | step: 193/422 | loss: 0.24953867495059967\n",
      "Epoch: 61/100 | step: 194/422 | loss: 0.6185179948806763\n",
      "Epoch: 61/100 | step: 195/422 | loss: 0.426384836435318\n",
      "Epoch: 61/100 | step: 196/422 | loss: 0.40877434611320496\n",
      "Epoch: 61/100 | step: 197/422 | loss: 0.27111199498176575\n",
      "Epoch: 61/100 | step: 198/422 | loss: 0.42432335019111633\n",
      "Epoch: 61/100 | step: 199/422 | loss: 0.22216796875\n",
      "Epoch: 61/100 | step: 200/422 | loss: 0.2887019217014313\n",
      "Epoch: 61/100 | step: 201/422 | loss: 0.4927368760108948\n",
      "Epoch: 61/100 | step: 202/422 | loss: 0.42185062170028687\n",
      "Epoch: 61/100 | step: 203/422 | loss: 0.4208996593952179\n",
      "Epoch: 61/100 | step: 204/422 | loss: 0.344816118478775\n",
      "Epoch: 61/100 | step: 205/422 | loss: 0.249676913022995\n",
      "Epoch: 61/100 | step: 206/422 | loss: 0.7538259029388428\n",
      "Epoch: 61/100 | step: 207/422 | loss: 0.4946692883968353\n",
      "Epoch: 61/100 | step: 208/422 | loss: 0.2620563805103302\n",
      "Epoch: 61/100 | step: 209/422 | loss: 0.6097269058227539\n",
      "Epoch: 61/100 | step: 210/422 | loss: 0.9630342125892639\n",
      "Epoch: 61/100 | step: 211/422 | loss: 0.5909257531166077\n",
      "Epoch: 61/100 | step: 212/422 | loss: 0.27683961391448975\n",
      "Epoch: 61/100 | step: 213/422 | loss: 0.24623501300811768\n",
      "Epoch: 61/100 | step: 214/422 | loss: 0.4186074733734131\n",
      "Epoch: 61/100 | step: 215/422 | loss: 1.0691999197006226\n",
      "Epoch: 61/100 | step: 216/422 | loss: 0.3655751049518585\n",
      "Epoch: 61/100 | step: 217/422 | loss: 0.40846848487854004\n",
      "Epoch: 61/100 | step: 218/422 | loss: 0.6247081756591797\n",
      "Epoch: 61/100 | step: 219/422 | loss: 0.32605621218681335\n",
      "Epoch: 61/100 | step: 220/422 | loss: 0.3388228714466095\n",
      "Epoch: 61/100 | step: 221/422 | loss: 0.30720701813697815\n",
      "Epoch: 61/100 | step: 222/422 | loss: 0.4208795726299286\n",
      "Epoch: 61/100 | step: 223/422 | loss: 0.19974136352539062\n",
      "Epoch: 61/100 | step: 224/422 | loss: 0.21109436452388763\n",
      "Epoch: 61/100 | step: 225/422 | loss: 0.22320130467414856\n",
      "Epoch: 61/100 | step: 226/422 | loss: 0.2807851731777191\n",
      "Epoch: 61/100 | step: 227/422 | loss: 0.305441290140152\n",
      "Epoch: 61/100 | step: 228/422 | loss: 0.3871191740036011\n",
      "Epoch: 61/100 | step: 229/422 | loss: 0.3249149024486542\n",
      "Epoch: 61/100 | step: 230/422 | loss: 0.23979049921035767\n",
      "Epoch: 61/100 | step: 231/422 | loss: 0.30339890718460083\n",
      "Epoch: 61/100 | step: 232/422 | loss: 0.45592620968818665\n",
      "Epoch: 61/100 | step: 233/422 | loss: 0.3278287351131439\n",
      "Epoch: 61/100 | step: 234/422 | loss: 0.3970530331134796\n",
      "Epoch: 61/100 | step: 235/422 | loss: 0.18089659512043\n",
      "Epoch: 61/100 | step: 236/422 | loss: 0.23292286694049835\n",
      "Epoch: 61/100 | step: 237/422 | loss: 0.25914642214775085\n",
      "Epoch: 61/100 | step: 238/422 | loss: 0.38041752576828003\n",
      "Epoch: 61/100 | step: 239/422 | loss: 0.26323768496513367\n",
      "Epoch: 61/100 | step: 240/422 | loss: 0.2229565978050232\n",
      "Epoch: 61/100 | step: 241/422 | loss: 0.27945783734321594\n",
      "Epoch: 61/100 | step: 242/422 | loss: 0.18845275044441223\n",
      "Epoch: 61/100 | step: 243/422 | loss: 0.15420836210250854\n",
      "Epoch: 61/100 | step: 244/422 | loss: 0.21547111868858337\n",
      "Epoch: 61/100 | step: 245/422 | loss: 0.48308637738227844\n",
      "Epoch: 61/100 | step: 246/422 | loss: 0.3992334306240082\n",
      "Epoch: 61/100 | step: 247/422 | loss: 0.4248351454734802\n",
      "Epoch: 61/100 | step: 248/422 | loss: 0.20143215358257294\n",
      "Epoch: 61/100 | step: 249/422 | loss: 0.4030075967311859\n",
      "Epoch: 61/100 | step: 250/422 | loss: 0.25793084502220154\n",
      "Epoch: 61/100 | step: 251/422 | loss: 0.2660486102104187\n",
      "Epoch: 61/100 | step: 252/422 | loss: 0.3157083988189697\n",
      "Epoch: 61/100 | step: 253/422 | loss: 0.2677992582321167\n",
      "Epoch: 61/100 | step: 254/422 | loss: 0.2565131187438965\n",
      "Epoch: 61/100 | step: 255/422 | loss: 0.26372575759887695\n",
      "Epoch: 61/100 | step: 256/422 | loss: 0.28508269786834717\n",
      "Epoch: 61/100 | step: 257/422 | loss: 0.4001922905445099\n",
      "Epoch: 61/100 | step: 258/422 | loss: 0.25185441970825195\n",
      "Epoch: 61/100 | step: 259/422 | loss: 0.43234869837760925\n",
      "Epoch: 61/100 | step: 260/422 | loss: 0.6032268404960632\n",
      "Epoch: 61/100 | step: 261/422 | loss: 0.4232059717178345\n",
      "Epoch: 61/100 | step: 262/422 | loss: 0.36460572481155396\n",
      "Epoch: 61/100 | step: 263/422 | loss: 0.37512028217315674\n",
      "Epoch: 61/100 | step: 264/422 | loss: 0.6591680645942688\n",
      "Epoch: 61/100 | step: 265/422 | loss: 0.2733895778656006\n",
      "Epoch: 61/100 | step: 266/422 | loss: 0.44581353664398193\n",
      "Epoch: 61/100 | step: 267/422 | loss: 0.4875510632991791\n",
      "Epoch: 61/100 | step: 268/422 | loss: 0.23811377584934235\n",
      "Epoch: 61/100 | step: 269/422 | loss: 0.2728482782840729\n",
      "Epoch: 61/100 | step: 270/422 | loss: 0.33818408846855164\n",
      "Epoch: 61/100 | step: 271/422 | loss: 0.37662777304649353\n",
      "Epoch: 61/100 | step: 272/422 | loss: 0.2826806306838989\n",
      "Epoch: 61/100 | step: 273/422 | loss: 0.37379300594329834\n",
      "Epoch: 61/100 | step: 274/422 | loss: 0.3550156056880951\n",
      "Epoch: 61/100 | step: 275/422 | loss: 0.2541523873806\n",
      "Epoch: 61/100 | step: 276/422 | loss: 0.5880547165870667\n",
      "Epoch: 61/100 | step: 277/422 | loss: 0.30306512117385864\n",
      "Epoch: 61/100 | step: 278/422 | loss: 0.2085723727941513\n",
      "Epoch: 61/100 | step: 279/422 | loss: 0.23992206156253815\n",
      "Epoch: 61/100 | step: 280/422 | loss: 0.21328847110271454\n",
      "Epoch: 61/100 | step: 281/422 | loss: 0.4460875988006592\n",
      "Epoch: 61/100 | step: 282/422 | loss: 0.2443356066942215\n",
      "Epoch: 61/100 | step: 283/422 | loss: 0.1954067498445511\n",
      "Epoch: 61/100 | step: 284/422 | loss: 0.36527562141418457\n",
      "Epoch: 61/100 | step: 285/422 | loss: 0.4095457196235657\n",
      "Epoch: 61/100 | step: 286/422 | loss: 0.3842308223247528\n",
      "Epoch: 61/100 | step: 287/422 | loss: 0.39086243510246277\n",
      "Epoch: 61/100 | step: 288/422 | loss: 0.259682297706604\n",
      "Epoch: 61/100 | step: 289/422 | loss: 0.18598254024982452\n",
      "Epoch: 61/100 | step: 290/422 | loss: 0.48249948024749756\n",
      "Epoch: 61/100 | step: 291/422 | loss: 0.09550253301858902\n",
      "Epoch: 61/100 | step: 292/422 | loss: 0.40106624364852905\n",
      "Epoch: 61/100 | step: 293/422 | loss: 0.36168408393859863\n",
      "Epoch: 61/100 | step: 294/422 | loss: 0.42791157960891724\n",
      "Epoch: 61/100 | step: 295/422 | loss: 0.4716130793094635\n",
      "Epoch: 61/100 | step: 296/422 | loss: 0.4348047375679016\n",
      "Epoch: 61/100 | step: 297/422 | loss: 0.2046578973531723\n",
      "Epoch: 61/100 | step: 298/422 | loss: 0.5893357992172241\n",
      "Epoch: 61/100 | step: 299/422 | loss: 0.2796920835971832\n",
      "Epoch: 61/100 | step: 300/422 | loss: 0.6193907260894775\n",
      "Epoch: 61/100 | step: 301/422 | loss: 0.28585970401763916\n",
      "Epoch: 61/100 | step: 302/422 | loss: 0.39323192834854126\n",
      "Epoch: 61/100 | step: 303/422 | loss: 0.43996405601501465\n",
      "Epoch: 61/100 | step: 304/422 | loss: 0.2605171799659729\n",
      "Epoch: 61/100 | step: 305/422 | loss: 0.27060195803642273\n",
      "Epoch: 61/100 | step: 306/422 | loss: 0.39300695061683655\n",
      "Epoch: 61/100 | step: 307/422 | loss: 0.24635325372219086\n",
      "Epoch: 61/100 | step: 308/422 | loss: 0.2763933539390564\n",
      "Epoch: 61/100 | step: 309/422 | loss: 0.3330425024032593\n",
      "Epoch: 61/100 | step: 310/422 | loss: 0.424344927072525\n",
      "Epoch: 61/100 | step: 311/422 | loss: 0.6605859994888306\n",
      "Epoch: 61/100 | step: 312/422 | loss: 0.5682544112205505\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 62/100 | step: 1/422 | loss: 0.8197879791259766\n",
      "Epoch: 62/100 | step: 2/422 | loss: 0.17049746215343475\n",
      "Epoch: 62/100 | step: 3/422 | loss: 0.864810049533844\n",
      "Epoch: 62/100 | step: 4/422 | loss: 0.3593055009841919\n",
      "Epoch: 62/100 | step: 5/422 | loss: 0.3347698450088501\n",
      "Epoch: 62/100 | step: 6/422 | loss: 0.3169650733470917\n",
      "Epoch: 62/100 | step: 7/422 | loss: 0.36701729893684387\n",
      "Epoch: 62/100 | step: 8/422 | loss: 0.27922528982162476\n",
      "Epoch: 62/100 | step: 9/422 | loss: 0.3167228102684021\n",
      "Epoch: 62/100 | step: 10/422 | loss: 0.22772707045078278\n",
      "Epoch: 62/100 | step: 11/422 | loss: 0.18456006050109863\n",
      "Epoch: 62/100 | step: 12/422 | loss: 0.18713350594043732\n",
      "Epoch: 62/100 | step: 13/422 | loss: 0.13340209424495697\n",
      "Epoch: 62/100 | step: 14/422 | loss: 0.1640462875366211\n",
      "Epoch: 62/100 | step: 15/422 | loss: 0.34382057189941406\n",
      "Epoch: 62/100 | step: 16/422 | loss: 0.11721440404653549\n",
      "Epoch: 62/100 | step: 17/422 | loss: 0.11499162018299103\n",
      "Epoch: 62/100 | step: 18/422 | loss: 0.2926567792892456\n",
      "Epoch: 62/100 | step: 19/422 | loss: 0.27237918972969055\n",
      "Epoch: 62/100 | step: 20/422 | loss: 0.3524812161922455\n",
      "Epoch: 62/100 | step: 21/422 | loss: 0.19890695810317993\n",
      "Epoch: 62/100 | step: 22/422 | loss: 0.21078041195869446\n",
      "Epoch: 62/100 | step: 23/422 | loss: 0.30625489354133606\n",
      "Epoch: 62/100 | step: 24/422 | loss: 0.4472109377384186\n",
      "Epoch: 62/100 | step: 25/422 | loss: 0.3583846390247345\n",
      "Epoch: 62/100 | step: 26/422 | loss: 0.24720017611980438\n",
      "Epoch: 62/100 | step: 27/422 | loss: 0.23151956498622894\n",
      "Epoch: 62/100 | step: 28/422 | loss: 0.298221230506897\n",
      "Epoch: 62/100 | step: 29/422 | loss: 0.2254479080438614\n",
      "Epoch: 62/100 | step: 30/422 | loss: 0.15123970806598663\n",
      "Epoch: 62/100 | step: 31/422 | loss: 0.18417058885097504\n",
      "Epoch: 62/100 | step: 32/422 | loss: 0.2342269867658615\n",
      "Epoch: 62/100 | step: 33/422 | loss: 0.1681009978055954\n",
      "Epoch: 62/100 | step: 34/422 | loss: 0.2993384301662445\n",
      "Epoch: 62/100 | step: 35/422 | loss: 0.17941692471504211\n",
      "Epoch: 62/100 | step: 36/422 | loss: 0.29490503668785095\n",
      "Epoch: 62/100 | step: 37/422 | loss: 0.2699050307273865\n",
      "Epoch: 62/100 | step: 38/422 | loss: 0.23861372470855713\n",
      "Epoch: 62/100 | step: 39/422 | loss: 0.23576383292675018\n",
      "Epoch: 62/100 | step: 40/422 | loss: 0.38232994079589844\n",
      "Epoch: 62/100 | step: 41/422 | loss: 0.35135334730148315\n",
      "Epoch: 62/100 | step: 42/422 | loss: 0.13717931509017944\n",
      "Epoch: 62/100 | step: 43/422 | loss: 0.35156482458114624\n",
      "Epoch: 62/100 | step: 44/422 | loss: 0.17218391597270966\n",
      "Epoch: 62/100 | step: 45/422 | loss: 0.24541181325912476\n",
      "Epoch: 62/100 | step: 46/422 | loss: 0.19528554379940033\n",
      "Epoch: 62/100 | step: 47/422 | loss: 0.2879757881164551\n",
      "Epoch: 62/100 | step: 48/422 | loss: 0.17046579718589783\n",
      "Epoch: 62/100 | step: 49/422 | loss: 0.21002881228923798\n",
      "Epoch: 62/100 | step: 50/422 | loss: 0.1520681381225586\n",
      "Epoch: 62/100 | step: 51/422 | loss: 0.18378125131130219\n",
      "Epoch: 62/100 | step: 52/422 | loss: 0.2023279368877411\n",
      "Epoch: 62/100 | step: 53/422 | loss: 0.19546003639698029\n",
      "Epoch: 62/100 | step: 54/422 | loss: 0.1209881529211998\n",
      "Epoch: 62/100 | step: 55/422 | loss: 0.21689988672733307\n",
      "Epoch: 62/100 | step: 56/422 | loss: 0.1983112394809723\n",
      "Epoch: 62/100 | step: 57/422 | loss: 0.11764497309923172\n",
      "Epoch: 62/100 | step: 58/422 | loss: 0.19874967634677887\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 63/100 | step: 1/422 | loss: 0.5267120599746704\n",
      "Epoch: 63/100 | step: 2/422 | loss: 0.23160557448863983\n",
      "Epoch: 63/100 | step: 3/422 | loss: 0.15293702483177185\n",
      "Epoch: 63/100 | step: 4/422 | loss: 0.17443621158599854\n",
      "Epoch: 63/100 | step: 5/422 | loss: 0.12182921171188354\n",
      "Epoch: 63/100 | step: 6/422 | loss: 0.16502341628074646\n",
      "Epoch: 63/100 | step: 7/422 | loss: 0.18987925350666046\n",
      "Epoch: 63/100 | step: 8/422 | loss: 0.18350861966609955\n",
      "Epoch: 63/100 | step: 9/422 | loss: 0.5048387050628662\n",
      "Epoch: 63/100 | step: 10/422 | loss: 0.23694483935832977\n",
      "Epoch: 63/100 | step: 11/422 | loss: 0.14689220488071442\n",
      "Epoch: 63/100 | step: 12/422 | loss: 0.2380400002002716\n",
      "Epoch: 63/100 | step: 13/422 | loss: 0.1522250771522522\n",
      "Epoch: 63/100 | step: 14/422 | loss: 0.19297131896018982\n",
      "Epoch: 63/100 | step: 15/422 | loss: 0.1699296534061432\n",
      "Epoch: 63/100 | step: 16/422 | loss: 0.2853240370750427\n",
      "Epoch: 63/100 | step: 17/422 | loss: 0.1475009173154831\n",
      "Epoch: 63/100 | step: 18/422 | loss: 0.30518007278442383\n",
      "Epoch: 63/100 | step: 19/422 | loss: 0.08832534402608871\n",
      "Epoch: 63/100 | step: 20/422 | loss: 0.3711487352848053\n",
      "Epoch: 63/100 | step: 21/422 | loss: 0.31019920110702515\n",
      "Epoch: 63/100 | step: 22/422 | loss: 0.1637924462556839\n",
      "Epoch: 63/100 | step: 23/422 | loss: 0.24033677577972412\n",
      "Epoch: 63/100 | step: 24/422 | loss: 0.20444463193416595\n",
      "Epoch: 63/100 | step: 25/422 | loss: 0.23619556427001953\n",
      "Epoch: 63/100 | step: 26/422 | loss: 0.39068663120269775\n",
      "Epoch: 63/100 | step: 27/422 | loss: 0.09958603233098984\n",
      "Epoch: 63/100 | step: 28/422 | loss: 0.4855358898639679\n",
      "Epoch: 63/100 | step: 29/422 | loss: 0.29044678807258606\n",
      "Epoch: 63/100 | step: 30/422 | loss: 0.43636053800582886\n",
      "Epoch: 63/100 | step: 31/422 | loss: 0.6889417171478271\n",
      "Epoch: 63/100 | step: 32/422 | loss: 0.7633986473083496\n",
      "Epoch: 63/100 | step: 33/422 | loss: 0.34585386514663696\n",
      "Epoch: 63/100 | step: 34/422 | loss: 0.27885469794273376\n",
      "Epoch: 63/100 | step: 35/422 | loss: 0.552611231803894\n",
      "Epoch: 63/100 | step: 36/422 | loss: 0.10975365340709686\n",
      "Epoch: 63/100 | step: 37/422 | loss: 0.6212433576583862\n",
      "Epoch: 63/100 | step: 38/422 | loss: 0.2443130612373352\n",
      "Epoch: 63/100 | step: 39/422 | loss: 0.25951242446899414\n",
      "Epoch: 63/100 | step: 40/422 | loss: 0.3286258280277252\n",
      "Epoch: 63/100 | step: 41/422 | loss: 0.17426276206970215\n",
      "Epoch: 63/100 | step: 42/422 | loss: 0.19072239100933075\n",
      "Epoch: 63/100 | step: 43/422 | loss: 0.196041539311409\n",
      "Epoch: 63/100 | step: 44/422 | loss: 0.1462598592042923\n",
      "Epoch: 63/100 | step: 45/422 | loss: 0.23136937618255615\n",
      "Epoch: 63/100 | step: 46/422 | loss: 0.23968906700611115\n",
      "Epoch: 63/100 | step: 47/422 | loss: 0.11065974086523056\n",
      "Epoch: 63/100 | step: 48/422 | loss: 0.2691956162452698\n",
      "Epoch: 63/100 | step: 49/422 | loss: 0.2000633031129837\n",
      "Epoch: 63/100 | step: 50/422 | loss: 0.2726985812187195\n",
      "Epoch: 63/100 | step: 51/422 | loss: 0.35822466015815735\n",
      "Epoch: 63/100 | step: 52/422 | loss: 0.4677474796772003\n",
      "Epoch: 63/100 | step: 53/422 | loss: 0.1803775131702423\n",
      "Epoch: 63/100 | step: 54/422 | loss: 0.1992172747850418\n",
      "Epoch: 63/100 | step: 55/422 | loss: 0.10165023058652878\n",
      "Epoch: 63/100 | step: 56/422 | loss: 0.2745172083377838\n",
      "Epoch: 63/100 | step: 57/422 | loss: 0.43562957644462585\n",
      "Epoch: 63/100 | step: 58/422 | loss: 0.19331464171409607\n",
      "Epoch: 63/100 | step: 59/422 | loss: 0.1655096560716629\n",
      "Epoch: 63/100 | step: 60/422 | loss: 0.28920993208885193\n",
      "Epoch: 63/100 | step: 61/422 | loss: 0.21895329654216766\n",
      "Epoch: 63/100 | step: 62/422 | loss: 0.2820482850074768\n",
      "Epoch: 63/100 | step: 63/422 | loss: 0.3067609369754791\n",
      "Epoch: 63/100 | step: 64/422 | loss: 0.33001354336738586\n",
      "Epoch: 63/100 | step: 65/422 | loss: 0.39565885066986084\n",
      "Epoch: 63/100 | step: 66/422 | loss: 0.4428248405456543\n",
      "Epoch: 63/100 | step: 67/422 | loss: 0.6888242959976196\n",
      "Epoch: 63/100 | step: 68/422 | loss: 0.5519949793815613\n",
      "Epoch: 63/100 | step: 69/422 | loss: 0.5315147638320923\n",
      "Epoch: 63/100 | step: 70/422 | loss: 0.5834574699401855\n",
      "Epoch: 63/100 | step: 71/422 | loss: 0.4220739006996155\n",
      "Epoch: 63/100 | step: 72/422 | loss: 0.3122338652610779\n",
      "Epoch: 63/100 | step: 73/422 | loss: 0.2873559892177582\n",
      "Epoch: 63/100 | step: 74/422 | loss: 0.11230681091547012\n",
      "Epoch: 63/100 | step: 75/422 | loss: 0.12163137644529343\n",
      "Epoch: 63/100 | step: 76/422 | loss: 0.3124442398548126\n",
      "Epoch: 63/100 | step: 77/422 | loss: 0.370185911655426\n",
      "Epoch: 63/100 | step: 78/422 | loss: 0.30293408036231995\n",
      "Epoch: 63/100 | step: 79/422 | loss: 0.14756925404071808\n",
      "Epoch: 63/100 | step: 80/422 | loss: 0.1634887456893921\n",
      "Epoch: 63/100 | step: 81/422 | loss: 0.17347334325313568\n",
      "Epoch: 63/100 | step: 82/422 | loss: 0.21502624452114105\n",
      "Epoch: 63/100 | step: 83/422 | loss: 0.14777076244354248\n",
      "Epoch: 63/100 | step: 84/422 | loss: 0.10973195731639862\n",
      "Epoch: 63/100 | step: 85/422 | loss: 0.34421950578689575\n",
      "Epoch: 63/100 | step: 86/422 | loss: 0.39542922377586365\n",
      "Epoch: 63/100 | step: 87/422 | loss: 0.28100869059562683\n",
      "Epoch: 63/100 | step: 88/422 | loss: 0.11390712112188339\n",
      "Epoch: 63/100 | step: 89/422 | loss: 0.14164873957633972\n",
      "Epoch: 63/100 | step: 90/422 | loss: 0.19949232041835785\n",
      "Epoch: 63/100 | step: 91/422 | loss: 0.19482752680778503\n",
      "Epoch: 63/100 | step: 92/422 | loss: 0.16686876118183136\n",
      "Epoch: 63/100 | step: 93/422 | loss: 0.36571741104125977\n",
      "Epoch: 63/100 | step: 94/422 | loss: 0.2572111189365387\n",
      "Epoch: 63/100 | step: 95/422 | loss: 0.3219704329967499\n",
      "Epoch: 63/100 | step: 96/422 | loss: 0.17578916251659393\n",
      "Epoch: 63/100 | step: 97/422 | loss: 0.10628014802932739\n",
      "Epoch: 63/100 | step: 98/422 | loss: 0.26547807455062866\n",
      "Epoch: 63/100 | step: 99/422 | loss: 0.2168380618095398\n",
      "Epoch: 63/100 | step: 100/422 | loss: 0.27018752694129944\n",
      "Epoch: 63/100 | step: 101/422 | loss: 0.43328166007995605\n",
      "Epoch: 63/100 | step: 102/422 | loss: 0.2359011173248291\n",
      "Epoch: 63/100 | step: 103/422 | loss: 0.5415700674057007\n",
      "Epoch: 63/100 | step: 104/422 | loss: 0.4061919152736664\n",
      "Epoch: 63/100 | step: 105/422 | loss: 0.27703118324279785\n",
      "Epoch: 63/100 | step: 106/422 | loss: 0.5472791194915771\n",
      "Epoch: 63/100 | step: 107/422 | loss: 0.6174966096878052\n",
      "Epoch: 63/100 | step: 108/422 | loss: 0.19434501230716705\n",
      "Epoch: 63/100 | step: 109/422 | loss: 0.4849400520324707\n",
      "Epoch: 63/100 | step: 110/422 | loss: 0.36520227789878845\n",
      "Epoch: 63/100 | step: 111/422 | loss: 0.3787648379802704\n",
      "Epoch: 63/100 | step: 112/422 | loss: 0.4253919720649719\n",
      "Epoch: 63/100 | step: 113/422 | loss: 0.5803261399269104\n",
      "Epoch: 63/100 | step: 114/422 | loss: 0.7287918925285339\n",
      "Epoch: 63/100 | step: 115/422 | loss: 0.3261054456233978\n",
      "Epoch: 63/100 | step: 116/422 | loss: 0.34913870692253113\n",
      "Epoch: 63/100 | step: 117/422 | loss: 0.29983654618263245\n",
      "Epoch: 63/100 | step: 118/422 | loss: 0.33568352460861206\n",
      "Epoch: 63/100 | step: 119/422 | loss: 0.2728855013847351\n",
      "Epoch: 63/100 | step: 120/422 | loss: 0.549409031867981\n",
      "Epoch: 63/100 | step: 121/422 | loss: 0.327693909406662\n",
      "Epoch: 63/100 | step: 122/422 | loss: 0.7983654141426086\n",
      "Epoch: 63/100 | step: 123/422 | loss: 0.7637667655944824\n",
      "Epoch: 63/100 | step: 124/422 | loss: 0.46150684356689453\n",
      "Epoch: 63/100 | step: 125/422 | loss: 0.42491549253463745\n",
      "Epoch: 63/100 | step: 126/422 | loss: 0.2886958122253418\n",
      "Epoch: 63/100 | step: 127/422 | loss: 0.45777225494384766\n",
      "Epoch: 63/100 | step: 128/422 | loss: 0.3957384526729584\n",
      "Epoch: 63/100 | step: 129/422 | loss: 0.4649267792701721\n",
      "Epoch: 63/100 | step: 130/422 | loss: 0.12821005284786224\n",
      "Epoch: 63/100 | step: 131/422 | loss: 0.3499932587146759\n",
      "Epoch: 63/100 | step: 132/422 | loss: 0.21696831285953522\n",
      "Epoch: 63/100 | step: 133/422 | loss: 0.2261185348033905\n",
      "Epoch: 63/100 | step: 134/422 | loss: 0.068000927567482\n",
      "Epoch: 63/100 | step: 135/422 | loss: 0.0510411374270916\n",
      "Epoch: 63/100 | step: 136/422 | loss: 0.13745658099651337\n",
      "Epoch: 63/100 | step: 137/422 | loss: 0.21201291680335999\n",
      "Epoch: 63/100 | step: 138/422 | loss: 0.18710094690322876\n",
      "Epoch: 63/100 | step: 139/422 | loss: 0.38960376381874084\n",
      "Epoch: 63/100 | step: 140/422 | loss: 0.3710324764251709\n",
      "Epoch: 63/100 | step: 141/422 | loss: 0.3893767297267914\n",
      "Epoch: 63/100 | step: 142/422 | loss: 0.2268637716770172\n",
      "Epoch: 63/100 | step: 143/422 | loss: 0.36017951369285583\n",
      "Epoch: 63/100 | step: 144/422 | loss: 0.7081702351570129\n",
      "Epoch: 63/100 | step: 145/422 | loss: 0.2056952267885208\n",
      "Epoch: 63/100 | step: 146/422 | loss: 0.3060964047908783\n",
      "Epoch: 63/100 | step: 147/422 | loss: 0.2727676331996918\n",
      "Epoch: 63/100 | step: 148/422 | loss: 0.11308466643095016\n",
      "Epoch: 63/100 | step: 149/422 | loss: 0.24180451035499573\n",
      "Epoch: 63/100 | step: 150/422 | loss: 0.41474029421806335\n",
      "Epoch: 63/100 | step: 151/422 | loss: 0.22118330001831055\n",
      "Epoch: 63/100 | step: 152/422 | loss: 0.7041635513305664\n",
      "Epoch: 63/100 | step: 153/422 | loss: 0.46096137166023254\n",
      "Epoch: 63/100 | step: 154/422 | loss: 0.24080444872379303\n",
      "Epoch: 63/100 | step: 155/422 | loss: 0.49562227725982666\n",
      "Epoch: 63/100 | step: 156/422 | loss: 0.227410227060318\n",
      "Epoch: 63/100 | step: 157/422 | loss: 0.364969402551651\n",
      "Epoch: 63/100 | step: 158/422 | loss: 0.30870211124420166\n",
      "Epoch: 63/100 | step: 159/422 | loss: 0.19220465421676636\n",
      "Epoch: 63/100 | step: 160/422 | loss: 0.215401291847229\n",
      "Epoch: 63/100 | step: 161/422 | loss: 0.2530161142349243\n",
      "Epoch: 63/100 | step: 162/422 | loss: 0.420011043548584\n",
      "Epoch: 63/100 | step: 163/422 | loss: 0.2967773377895355\n",
      "Epoch: 63/100 | step: 164/422 | loss: 0.33036741614341736\n",
      "Epoch: 63/100 | step: 165/422 | loss: 0.3709189295768738\n",
      "Epoch: 63/100 | step: 166/422 | loss: 0.4727189540863037\n",
      "Epoch: 63/100 | step: 167/422 | loss: 0.26796823740005493\n",
      "Epoch: 63/100 | step: 168/422 | loss: 0.15115389227867126\n",
      "Epoch: 63/100 | step: 169/422 | loss: 0.31868264079093933\n",
      "Epoch: 63/100 | step: 170/422 | loss: 0.23690447211265564\n",
      "Epoch: 63/100 | step: 171/422 | loss: 0.2248132973909378\n",
      "Epoch: 63/100 | step: 172/422 | loss: 0.09075026214122772\n",
      "Epoch: 63/100 | step: 173/422 | loss: 0.36046504974365234\n",
      "Epoch: 63/100 | step: 174/422 | loss: 0.23152421414852142\n",
      "Epoch: 63/100 | step: 175/422 | loss: 0.4238381087779999\n",
      "Epoch: 63/100 | step: 176/422 | loss: 0.6291363835334778\n",
      "Epoch: 63/100 | step: 177/422 | loss: 0.2219279706478119\n",
      "Epoch: 63/100 | step: 178/422 | loss: 0.17598634958267212\n",
      "Epoch: 63/100 | step: 179/422 | loss: 0.17981672286987305\n",
      "Epoch: 63/100 | step: 180/422 | loss: 0.3864798843860626\n",
      "Epoch: 63/100 | step: 181/422 | loss: 0.6016563177108765\n",
      "Epoch: 63/100 | step: 182/422 | loss: 0.2904764711856842\n",
      "Epoch: 63/100 | step: 183/422 | loss: 0.327399343252182\n",
      "Epoch: 63/100 | step: 184/422 | loss: 0.2967107892036438\n",
      "Epoch: 63/100 | step: 185/422 | loss: 0.24420323967933655\n",
      "Epoch: 63/100 | step: 186/422 | loss: 0.3048766553401947\n",
      "Epoch: 63/100 | step: 187/422 | loss: 0.35444289445877075\n",
      "Epoch: 63/100 | step: 188/422 | loss: 0.4556750953197479\n",
      "Epoch: 63/100 | step: 189/422 | loss: 0.337432861328125\n",
      "Epoch: 63/100 | step: 190/422 | loss: 0.34175631403923035\n",
      "Epoch: 63/100 | step: 191/422 | loss: 0.27329733967781067\n",
      "Epoch: 63/100 | step: 192/422 | loss: 0.33955925703048706\n",
      "Epoch: 63/100 | step: 193/422 | loss: 0.21019621193408966\n",
      "Epoch: 63/100 | step: 194/422 | loss: 0.30425965785980225\n",
      "Epoch: 63/100 | step: 195/422 | loss: 0.4830135405063629\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 64/100 | step: 1/422 | loss: 0.39712613821029663\n",
      "Epoch: 64/100 | step: 2/422 | loss: 0.24872927367687225\n",
      "Epoch: 64/100 | step: 3/422 | loss: 0.18217115104198456\n",
      "Epoch: 64/100 | step: 4/422 | loss: 0.18730707466602325\n",
      "Epoch: 64/100 | step: 5/422 | loss: 0.39928579330444336\n",
      "Epoch: 64/100 | step: 6/422 | loss: 0.0806487649679184\n",
      "Epoch: 64/100 | step: 7/422 | loss: 0.18957799673080444\n",
      "Epoch: 64/100 | step: 8/422 | loss: 0.20044022798538208\n",
      "Epoch: 64/100 | step: 9/422 | loss: 0.16645458340644836\n",
      "Epoch: 64/100 | step: 10/422 | loss: 0.2500028610229492\n",
      "Epoch: 64/100 | step: 11/422 | loss: 0.16452546417713165\n",
      "Epoch: 64/100 | step: 12/422 | loss: 0.29476410150527954\n",
      "Epoch: 64/100 | step: 13/422 | loss: 0.3072168231010437\n",
      "Epoch: 64/100 | step: 14/422 | loss: 0.2618792653083801\n",
      "Epoch: 64/100 | step: 15/422 | loss: 0.2323552817106247\n",
      "Epoch: 64/100 | step: 16/422 | loss: 0.12878413498401642\n",
      "Epoch: 64/100 | step: 17/422 | loss: 0.2272733449935913\n",
      "Epoch: 64/100 | step: 18/422 | loss: 0.17325018346309662\n",
      "Epoch: 64/100 | step: 19/422 | loss: 0.22169846296310425\n",
      "Epoch: 64/100 | step: 20/422 | loss: 0.07323195040225983\n",
      "Epoch: 64/100 | step: 21/422 | loss: 0.1648455262184143\n",
      "Epoch: 64/100 | step: 22/422 | loss: 0.1292659342288971\n",
      "Epoch: 64/100 | step: 23/422 | loss: 0.4121888279914856\n",
      "Epoch: 64/100 | step: 24/422 | loss: 0.676658034324646\n",
      "Epoch: 64/100 | step: 25/422 | loss: 0.9284918904304504\n",
      "Epoch: 64/100 | step: 26/422 | loss: 0.1536753624677658\n",
      "Epoch: 64/100 | step: 27/422 | loss: 0.16328051686286926\n",
      "Epoch: 64/100 | step: 28/422 | loss: 0.3036344349384308\n",
      "Epoch: 64/100 | step: 29/422 | loss: 0.397458016872406\n",
      "Epoch: 64/100 | step: 30/422 | loss: 0.20746158063411713\n",
      "Epoch: 64/100 | step: 31/422 | loss: 0.4680665135383606\n",
      "Epoch: 64/100 | step: 32/422 | loss: 0.2559822201728821\n",
      "Epoch: 64/100 | step: 33/422 | loss: 0.3082953989505768\n",
      "Epoch: 64/100 | step: 34/422 | loss: 0.2358963042497635\n",
      "Epoch: 64/100 | step: 35/422 | loss: 0.14944753050804138\n",
      "Epoch: 64/100 | step: 36/422 | loss: 0.14970584213733673\n",
      "Epoch: 64/100 | step: 37/422 | loss: 0.17724238336086273\n",
      "Epoch: 64/100 | step: 38/422 | loss: 0.07039995491504669\n",
      "Epoch: 64/100 | step: 39/422 | loss: 0.10706634819507599\n",
      "Epoch: 64/100 | step: 40/422 | loss: 0.21416860818862915\n",
      "Epoch: 64/100 | step: 41/422 | loss: 0.3032895028591156\n",
      "Epoch: 64/100 | step: 42/422 | loss: 0.5197165608406067\n",
      "Epoch: 64/100 | step: 43/422 | loss: 0.6405395269393921\n",
      "Epoch: 64/100 | step: 44/422 | loss: 0.36626553535461426\n",
      "Epoch: 64/100 | step: 45/422 | loss: 0.4257656931877136\n",
      "Epoch: 64/100 | step: 46/422 | loss: 0.2928210496902466\n",
      "Epoch: 64/100 | step: 47/422 | loss: 0.23495230078697205\n",
      "Epoch: 64/100 | step: 48/422 | loss: 0.194098100066185\n",
      "Epoch: 64/100 | step: 49/422 | loss: 0.36981824040412903\n",
      "Epoch: 64/100 | step: 50/422 | loss: 0.2972135543823242\n",
      "Epoch: 64/100 | step: 51/422 | loss: 0.17893417179584503\n",
      "Epoch: 64/100 | step: 52/422 | loss: 0.1265181303024292\n",
      "Epoch: 64/100 | step: 53/422 | loss: 0.40711745619773865\n",
      "Epoch: 64/100 | step: 54/422 | loss: 0.1746995747089386\n",
      "Epoch: 64/100 | step: 55/422 | loss: 0.4534180760383606\n",
      "Epoch: 64/100 | step: 56/422 | loss: 0.1573532372713089\n",
      "Epoch: 64/100 | step: 57/422 | loss: 0.3568267226219177\n",
      "Epoch: 64/100 | step: 58/422 | loss: 0.662128210067749\n",
      "Epoch: 64/100 | step: 59/422 | loss: 0.5279279947280884\n",
      "Epoch: 64/100 | step: 60/422 | loss: 0.36849725246429443\n",
      "Epoch: 64/100 | step: 61/422 | loss: 0.2368481159210205\n",
      "Epoch: 64/100 | step: 62/422 | loss: 0.431304007768631\n",
      "Epoch: 64/100 | step: 63/422 | loss: 0.19372807443141937\n",
      "Epoch: 64/100 | step: 64/422 | loss: 0.23928998410701752\n",
      "Epoch: 64/100 | step: 65/422 | loss: 0.28166162967681885\n",
      "Epoch: 64/100 | step: 66/422 | loss: 0.19019940495491028\n",
      "Epoch: 64/100 | step: 67/422 | loss: 0.25737303495407104\n",
      "Epoch: 64/100 | step: 68/422 | loss: 0.2214803248643875\n",
      "Epoch: 64/100 | step: 69/422 | loss: 0.44734254479408264\n",
      "Epoch: 64/100 | step: 70/422 | loss: 0.3844771385192871\n",
      "Epoch: 64/100 | step: 71/422 | loss: 0.23493550717830658\n",
      "Epoch: 64/100 | step: 72/422 | loss: 0.20612011849880219\n",
      "Epoch: 64/100 | step: 73/422 | loss: 0.29116836190223694\n",
      "Epoch: 64/100 | step: 74/422 | loss: 0.22393542528152466\n",
      "Epoch: 64/100 | step: 75/422 | loss: 0.08149499446153641\n",
      "Epoch: 64/100 | step: 76/422 | loss: 0.15093983709812164\n",
      "Epoch: 64/100 | step: 77/422 | loss: 0.1288822591304779\n",
      "Epoch: 64/100 | step: 78/422 | loss: 0.19501568377017975\n",
      "Epoch: 64/100 | step: 79/422 | loss: 0.1740894913673401\n",
      "Epoch: 64/100 | step: 80/422 | loss: 0.30864953994750977\n",
      "Epoch: 64/100 | step: 81/422 | loss: 0.15249677002429962\n",
      "Epoch: 64/100 | step: 82/422 | loss: 0.19943565130233765\n",
      "Epoch: 64/100 | step: 83/422 | loss: 0.10945623368024826\n",
      "Epoch: 64/100 | step: 84/422 | loss: 0.16723540425300598\n",
      "Epoch: 64/100 | step: 85/422 | loss: 0.14505934715270996\n",
      "Epoch: 64/100 | step: 86/422 | loss: 0.09605615586042404\n",
      "Epoch: 64/100 | step: 87/422 | loss: 0.15078061819076538\n",
      "Epoch: 64/100 | step: 88/422 | loss: 0.1743987798690796\n",
      "Epoch: 64/100 | step: 89/422 | loss: 0.09631812572479248\n",
      "Epoch: 64/100 | step: 90/422 | loss: 0.07488042861223221\n",
      "Epoch: 64/100 | step: 91/422 | loss: 0.2803560197353363\n",
      "Epoch: 64/100 | step: 92/422 | loss: 0.12793192267417908\n",
      "Epoch: 64/100 | step: 93/422 | loss: 0.22259411215782166\n",
      "Epoch: 64/100 | step: 94/422 | loss: 0.22414125502109528\n",
      "Epoch: 64/100 | step: 95/422 | loss: 0.15329885482788086\n",
      "Epoch: 64/100 | step: 96/422 | loss: 0.33128875494003296\n",
      "Epoch: 64/100 | step: 97/422 | loss: 0.15056376159191132\n",
      "Epoch: 64/100 | step: 98/422 | loss: 0.2593502104282379\n",
      "Epoch: 64/100 | step: 99/422 | loss: 0.1262979954481125\n",
      "Epoch: 64/100 | step: 100/422 | loss: 0.2791827321052551\n",
      "Epoch: 64/100 | step: 101/422 | loss: 0.15315444767475128\n",
      "Epoch: 64/100 | step: 102/422 | loss: 0.19525398313999176\n",
      "Epoch: 64/100 | step: 103/422 | loss: 0.21196946501731873\n",
      "Epoch: 64/100 | step: 104/422 | loss: 0.14089420437812805\n",
      "Epoch: 64/100 | step: 105/422 | loss: 0.19398851692676544\n",
      "Epoch: 64/100 | step: 106/422 | loss: 0.24576056003570557\n",
      "Epoch: 64/100 | step: 107/422 | loss: 0.47361865639686584\n",
      "Epoch: 64/100 | step: 108/422 | loss: 0.30364876985549927\n",
      "Epoch: 64/100 | step: 109/422 | loss: 0.6159838438034058\n",
      "Epoch: 64/100 | step: 110/422 | loss: 0.19488966464996338\n",
      "Epoch: 64/100 | step: 111/422 | loss: 0.4199214279651642\n",
      "Epoch: 64/100 | step: 112/422 | loss: 0.21869751811027527\n",
      "Epoch: 64/100 | step: 113/422 | loss: 0.2421678900718689\n",
      "Epoch: 64/100 | step: 114/422 | loss: 0.24048680067062378\n",
      "Epoch: 64/100 | step: 115/422 | loss: 0.24535416066646576\n",
      "Epoch: 64/100 | step: 116/422 | loss: 0.3236748278141022\n",
      "Epoch: 64/100 | step: 117/422 | loss: 0.36046072840690613\n",
      "Epoch: 64/100 | step: 118/422 | loss: 0.5128009915351868\n",
      "Epoch: 64/100 | step: 119/422 | loss: 0.13490642607212067\n",
      "Epoch: 64/100 | step: 120/422 | loss: 0.15526339411735535\n",
      "Epoch: 64/100 | step: 121/422 | loss: 0.26754823327064514\n",
      "Epoch: 64/100 | step: 122/422 | loss: 0.20862194895744324\n",
      "Epoch: 64/100 | step: 123/422 | loss: 0.37114787101745605\n",
      "Epoch: 64/100 | step: 124/422 | loss: 0.20033994317054749\n",
      "Epoch: 64/100 | step: 125/422 | loss: 0.518853485584259\n",
      "Epoch: 64/100 | step: 126/422 | loss: 0.2408265918493271\n",
      "Epoch: 64/100 | step: 127/422 | loss: 0.28912198543548584\n",
      "Epoch: 64/100 | step: 128/422 | loss: 0.2757294178009033\n",
      "Epoch: 64/100 | step: 129/422 | loss: 0.6439419984817505\n",
      "Epoch: 64/100 | step: 130/422 | loss: 0.6799607872962952\n",
      "Epoch: 64/100 | step: 131/422 | loss: 0.39487966895103455\n",
      "Epoch: 64/100 | step: 132/422 | loss: 0.25194960832595825\n",
      "Epoch: 64/100 | step: 133/422 | loss: 0.3547053933143616\n",
      "Epoch: 64/100 | step: 134/422 | loss: 0.3083065152168274\n",
      "Epoch: 64/100 | step: 135/422 | loss: 0.6223636269569397\n",
      "Epoch: 64/100 | step: 136/422 | loss: 0.24708829820156097\n",
      "Epoch: 64/100 | step: 137/422 | loss: 0.41299009323120117\n",
      "Epoch: 64/100 | step: 138/422 | loss: 0.3563482165336609\n",
      "Epoch: 64/100 | step: 139/422 | loss: 0.5058737993240356\n",
      "Epoch: 64/100 | step: 140/422 | loss: 0.317635715007782\n",
      "Epoch: 64/100 | step: 141/422 | loss: 0.27879074215888977\n",
      "Epoch: 64/100 | step: 142/422 | loss: 0.1473689079284668\n",
      "Epoch: 64/100 | step: 143/422 | loss: 0.5277995467185974\n",
      "Epoch: 64/100 | step: 144/422 | loss: 0.29857710003852844\n",
      "Epoch: 64/100 | step: 145/422 | loss: 0.15033882856369019\n",
      "Epoch: 64/100 | step: 146/422 | loss: 0.1896880865097046\n",
      "Epoch: 64/100 | step: 147/422 | loss: 0.29517510533332825\n",
      "Epoch: 64/100 | step: 148/422 | loss: 0.14758987724781036\n",
      "Epoch: 64/100 | step: 149/422 | loss: 0.13887611031532288\n",
      "Epoch: 64/100 | step: 150/422 | loss: 0.2535402774810791\n",
      "Epoch: 64/100 | step: 151/422 | loss: 0.29160571098327637\n",
      "Epoch: 64/100 | step: 152/422 | loss: 0.24380707740783691\n",
      "Epoch: 64/100 | step: 153/422 | loss: 0.2725372612476349\n",
      "Epoch: 64/100 | step: 154/422 | loss: 0.24265539646148682\n",
      "Epoch: 64/100 | step: 155/422 | loss: 0.30011752247810364\n",
      "Epoch: 64/100 | step: 156/422 | loss: 0.3138216435909271\n",
      "Epoch: 64/100 | step: 157/422 | loss: 0.5304347276687622\n",
      "Epoch: 64/100 | step: 158/422 | loss: 0.35181158781051636\n",
      "Epoch: 64/100 | step: 159/422 | loss: 0.09545762091875076\n",
      "Epoch: 64/100 | step: 160/422 | loss: 0.16733217239379883\n",
      "Epoch: 64/100 | step: 161/422 | loss: 0.1682831346988678\n",
      "Epoch: 64/100 | step: 162/422 | loss: 0.14960676431655884\n",
      "Epoch: 64/100 | step: 163/422 | loss: 0.21261200308799744\n",
      "Epoch: 64/100 | step: 164/422 | loss: 0.3271273076534271\n",
      "Epoch: 64/100 | step: 165/422 | loss: 0.20260535180568695\n",
      "Epoch: 64/100 | step: 166/422 | loss: 0.24086707830429077\n",
      "Epoch: 64/100 | step: 167/422 | loss: 0.21668963134288788\n",
      "Epoch: 64/100 | step: 168/422 | loss: 0.07810068130493164\n",
      "Epoch: 64/100 | step: 169/422 | loss: 0.5179369449615479\n",
      "Epoch: 64/100 | step: 170/422 | loss: 0.1986437886953354\n",
      "Epoch: 64/100 | step: 171/422 | loss: 0.27395856380462646\n",
      "Epoch: 64/100 | step: 172/422 | loss: 0.31813672184944153\n",
      "Epoch: 64/100 | step: 173/422 | loss: 0.5881978273391724\n",
      "Epoch: 64/100 | step: 174/422 | loss: 0.46533191204071045\n",
      "Epoch: 64/100 | step: 175/422 | loss: 0.27648818492889404\n",
      "Epoch: 64/100 | step: 176/422 | loss: 0.18055272102355957\n",
      "Epoch: 64/100 | step: 177/422 | loss: 0.43111544847488403\n",
      "Epoch: 64/100 | step: 178/422 | loss: 0.17133989930152893\n",
      "Epoch: 64/100 | step: 179/422 | loss: 0.219418004155159\n",
      "Epoch: 64/100 | step: 180/422 | loss: 0.2070709764957428\n",
      "Epoch: 64/100 | step: 181/422 | loss: 0.23414015769958496\n",
      "Epoch: 64/100 | step: 182/422 | loss: 0.3810862898826599\n",
      "Epoch: 64/100 | step: 183/422 | loss: 0.3856821656227112\n",
      "Epoch: 64/100 | step: 184/422 | loss: 0.5570347309112549\n",
      "Epoch: 64/100 | step: 185/422 | loss: 0.48397356271743774\n",
      "Epoch: 64/100 | step: 186/422 | loss: 0.3325851261615753\n",
      "Epoch: 64/100 | step: 187/422 | loss: 0.2886274755001068\n",
      "Epoch: 64/100 | step: 188/422 | loss: 0.14774681627750397\n",
      "Epoch: 64/100 | step: 189/422 | loss: 0.45391809940338135\n",
      "Epoch: 64/100 | step: 190/422 | loss: 0.16795408725738525\n",
      "Epoch: 64/100 | step: 191/422 | loss: 0.1505424678325653\n",
      "Epoch: 64/100 | step: 192/422 | loss: 0.2902066111564636\n",
      "Epoch: 64/100 | step: 193/422 | loss: 0.14009937644004822\n",
      "Epoch: 64/100 | step: 194/422 | loss: 0.5420126914978027\n",
      "Epoch: 64/100 | step: 195/422 | loss: 0.09803513437509537\n",
      "Epoch: 64/100 | step: 196/422 | loss: 0.3043377697467804\n",
      "Epoch: 64/100 | step: 197/422 | loss: 0.25120726227760315\n",
      "Epoch: 64/100 | step: 198/422 | loss: 0.45433032512664795\n",
      "Epoch: 64/100 | step: 199/422 | loss: 0.47042199969291687\n",
      "Epoch: 64/100 | step: 200/422 | loss: 0.19632810354232788\n",
      "Epoch: 64/100 | step: 201/422 | loss: 0.12908288836479187\n",
      "Epoch: 64/100 | step: 202/422 | loss: 0.31409838795661926\n",
      "Epoch: 64/100 | step: 203/422 | loss: 0.343808114528656\n",
      "Epoch: 64/100 | step: 204/422 | loss: 0.2227441966533661\n",
      "Epoch: 64/100 | step: 205/422 | loss: 0.3737289607524872\n",
      "Epoch: 64/100 | step: 206/422 | loss: 0.20607562363147736\n",
      "Epoch: 64/100 | step: 207/422 | loss: 0.1719459444284439\n",
      "Epoch: 64/100 | step: 208/422 | loss: 0.31495770812034607\n",
      "Epoch: 64/100 | step: 209/422 | loss: 0.5745323896408081\n",
      "Epoch: 64/100 | step: 210/422 | loss: 0.18467861413955688\n",
      "Epoch: 64/100 | step: 211/422 | loss: 0.27581876516342163\n",
      "Epoch: 64/100 | step: 212/422 | loss: 0.3096136748790741\n",
      "Epoch: 64/100 | step: 213/422 | loss: 0.19777770340442657\n",
      "Epoch: 64/100 | step: 214/422 | loss: 0.32033252716064453\n",
      "Epoch: 64/100 | step: 215/422 | loss: 0.34632033109664917\n",
      "Epoch: 64/100 | step: 216/422 | loss: 0.11391013115644455\n",
      "Epoch: 64/100 | step: 217/422 | loss: 0.24443332850933075\n",
      "Epoch: 64/100 | step: 218/422 | loss: 0.1641443520784378\n",
      "Epoch: 64/100 | step: 219/422 | loss: 0.178272545337677\n",
      "Epoch: 64/100 | step: 220/422 | loss: 0.10540945082902908\n",
      "Epoch: 64/100 | step: 221/422 | loss: 0.1307152509689331\n",
      "Epoch: 64/100 | step: 222/422 | loss: 0.32298222184181213\n",
      "Epoch: 64/100 | step: 223/422 | loss: 0.45323002338409424\n",
      "Epoch: 64/100 | step: 224/422 | loss: 0.2537808418273926\n",
      "Epoch: 64/100 | step: 225/422 | loss: 0.18093155324459076\n",
      "Epoch: 64/100 | step: 226/422 | loss: 0.2158251702785492\n",
      "Epoch: 64/100 | step: 227/422 | loss: 0.17664781212806702\n",
      "Epoch: 64/100 | step: 228/422 | loss: 0.3790731132030487\n",
      "Epoch: 64/100 | step: 229/422 | loss: 0.10625796765089035\n",
      "Epoch: 64/100 | step: 230/422 | loss: 0.28582048416137695\n",
      "Epoch: 64/100 | step: 231/422 | loss: 0.11870617419481277\n",
      "Epoch: 64/100 | step: 232/422 | loss: 0.32216861844062805\n",
      "Epoch: 64/100 | step: 233/422 | loss: 0.16315320134162903\n",
      "Epoch: 64/100 | step: 234/422 | loss: 0.17148081958293915\n",
      "Epoch: 64/100 | step: 235/422 | loss: 0.17820480465888977\n",
      "Epoch: 64/100 | step: 236/422 | loss: 0.1805393397808075\n",
      "Epoch: 64/100 | step: 237/422 | loss: 0.3650170564651489\n",
      "Epoch: 64/100 | step: 238/422 | loss: 0.25909969210624695\n",
      "Epoch: 64/100 | step: 239/422 | loss: 0.12026549875736237\n",
      "Epoch: 64/100 | step: 240/422 | loss: 0.4622218906879425\n",
      "Epoch: 64/100 | step: 241/422 | loss: 0.5407781004905701\n",
      "Epoch: 64/100 | step: 242/422 | loss: 0.21304304897785187\n",
      "Epoch: 64/100 | step: 243/422 | loss: 0.44827455282211304\n",
      "Epoch: 64/100 | step: 244/422 | loss: 0.32852739095687866\n",
      "Epoch: 64/100 | step: 245/422 | loss: 0.1348366141319275\n",
      "Epoch: 64/100 | step: 246/422 | loss: 0.35753336548805237\n",
      "Epoch: 64/100 | step: 247/422 | loss: 0.3579079806804657\n",
      "Epoch: 64/100 | step: 248/422 | loss: 0.26956576108932495\n",
      "Epoch: 64/100 | step: 249/422 | loss: 0.42493492364883423\n",
      "Epoch: 64/100 | step: 250/422 | loss: 0.3385315239429474\n",
      "Epoch: 64/100 | step: 251/422 | loss: 0.19213876128196716\n",
      "Epoch: 64/100 | step: 252/422 | loss: 0.45484867691993713\n",
      "Epoch: 64/100 | step: 253/422 | loss: 0.33010023832321167\n",
      "Epoch: 64/100 | step: 254/422 | loss: 0.21267299354076385\n",
      "Epoch: 64/100 | step: 255/422 | loss: 0.5483468770980835\n",
      "Epoch: 64/100 | step: 256/422 | loss: 0.5260982513427734\n",
      "Epoch: 64/100 | step: 257/422 | loss: 0.22033660113811493\n",
      "Epoch: 64/100 | step: 258/422 | loss: 0.2766754627227783\n",
      "Epoch: 64/100 | step: 259/422 | loss: 0.12085624784231186\n",
      "Epoch: 64/100 | step: 260/422 | loss: 0.16221405565738678\n",
      "Epoch: 64/100 | step: 261/422 | loss: 0.11201079189777374\n",
      "Epoch: 64/100 | step: 262/422 | loss: 0.45225822925567627\n",
      "Epoch: 64/100 | step: 263/422 | loss: 0.26508602499961853\n",
      "Epoch: 64/100 | step: 264/422 | loss: 0.42797011137008667\n",
      "Epoch: 64/100 | step: 265/422 | loss: 0.09728962182998657\n",
      "Epoch: 64/100 | step: 266/422 | loss: 0.37635427713394165\n",
      "Epoch: 64/100 | step: 267/422 | loss: 0.15761156380176544\n",
      "Epoch: 64/100 | step: 268/422 | loss: 0.2717781662940979\n",
      "Epoch: 64/100 | step: 269/422 | loss: 0.19869311153888702\n",
      "Epoch: 64/100 | step: 270/422 | loss: 0.214457705616951\n",
      "Epoch: 64/100 | step: 271/422 | loss: 0.18068860471248627\n",
      "Epoch: 64/100 | step: 272/422 | loss: 0.30153632164001465\n",
      "Epoch: 64/100 | step: 273/422 | loss: 0.3909571170806885\n",
      "Epoch: 64/100 | step: 274/422 | loss: 0.3719536066055298\n",
      "Epoch: 64/100 | step: 275/422 | loss: 0.1870187520980835\n",
      "Epoch: 64/100 | step: 276/422 | loss: 0.5434760451316833\n",
      "Epoch: 64/100 | step: 277/422 | loss: 0.41050633788108826\n",
      "Epoch: 64/100 | step: 278/422 | loss: 0.4164671003818512\n",
      "Epoch: 64/100 | step: 279/422 | loss: 0.20219732820987701\n",
      "Epoch: 64/100 | step: 280/422 | loss: 0.25636616349220276\n",
      "Epoch: 64/100 | step: 281/422 | loss: 0.28793999552726746\n",
      "Epoch: 64/100 | step: 282/422 | loss: 0.24877730011940002\n",
      "Epoch: 64/100 | step: 283/422 | loss: 0.07155068218708038\n",
      "Epoch: 64/100 | step: 284/422 | loss: 0.142263263463974\n",
      "Epoch: 64/100 | step: 285/422 | loss: 0.17657561600208282\n",
      "Epoch: 64/100 | step: 286/422 | loss: 0.27177196741104126\n",
      "Epoch: 64/100 | step: 287/422 | loss: 0.42249348759651184\n",
      "Epoch: 64/100 | step: 288/422 | loss: 0.40870168805122375\n",
      "Epoch: 64/100 | step: 289/422 | loss: 0.3177972137928009\n",
      "Epoch: 64/100 | step: 290/422 | loss: 0.23406913876533508\n",
      "Epoch: 64/100 | step: 291/422 | loss: 0.42333242297172546\n",
      "Epoch: 64/100 | step: 292/422 | loss: 0.2146245837211609\n",
      "Epoch: 64/100 | step: 293/422 | loss: 0.09989520162343979\n",
      "Epoch: 64/100 | step: 294/422 | loss: 0.348394513130188\n",
      "Epoch: 64/100 | step: 295/422 | loss: 0.09969887137413025\n",
      "Epoch: 64/100 | step: 296/422 | loss: 0.17486917972564697\n",
      "Epoch: 64/100 | step: 297/422 | loss: 0.3953109085559845\n",
      "Epoch: 64/100 | step: 298/422 | loss: 0.41620856523513794\n",
      "Epoch: 64/100 | step: 299/422 | loss: 0.2506332993507385\n",
      "Epoch: 64/100 | step: 300/422 | loss: 0.17882254719734192\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 65/100 | step: 1/422 | loss: 0.13162356615066528\n",
      "Epoch: 65/100 | step: 2/422 | loss: 0.20669488608837128\n",
      "Epoch: 65/100 | step: 3/422 | loss: 0.1044122576713562\n",
      "Epoch: 65/100 | step: 4/422 | loss: 0.39213231205940247\n",
      "Epoch: 65/100 | step: 5/422 | loss: 0.18246759474277496\n",
      "Epoch: 65/100 | step: 6/422 | loss: 0.6424367427825928\n",
      "Epoch: 65/100 | step: 7/422 | loss: 0.3199063241481781\n",
      "Epoch: 65/100 | step: 8/422 | loss: 0.09264972060918808\n",
      "Epoch: 65/100 | step: 9/422 | loss: 0.18328100442886353\n",
      "Epoch: 65/100 | step: 10/422 | loss: 0.22035080194473267\n",
      "Epoch: 65/100 | step: 11/422 | loss: 0.15497291088104248\n",
      "Epoch: 65/100 | step: 12/422 | loss: 0.1536444127559662\n",
      "Epoch: 65/100 | step: 13/422 | loss: 0.05903187766671181\n",
      "Epoch: 65/100 | step: 14/422 | loss: 0.09582123905420303\n",
      "Epoch: 65/100 | step: 15/422 | loss: 0.14028768241405487\n",
      "Epoch: 65/100 | step: 16/422 | loss: 0.23522740602493286\n",
      "Epoch: 65/100 | step: 17/422 | loss: 0.1464925855398178\n",
      "Epoch: 65/100 | step: 18/422 | loss: 0.23688945174217224\n",
      "Epoch: 65/100 | step: 19/422 | loss: 0.18742816150188446\n",
      "Epoch: 65/100 | step: 20/422 | loss: 0.6324318051338196\n",
      "Epoch: 65/100 | step: 21/422 | loss: 0.14527572691440582\n",
      "Epoch: 65/100 | step: 22/422 | loss: 0.19025909900665283\n",
      "Epoch: 65/100 | step: 23/422 | loss: 0.12880462408065796\n",
      "Epoch: 65/100 | step: 24/422 | loss: 0.1743108183145523\n",
      "Epoch: 65/100 | step: 25/422 | loss: 0.42881038784980774\n",
      "Epoch: 65/100 | step: 26/422 | loss: 0.20882251858711243\n",
      "Epoch: 65/100 | step: 27/422 | loss: 0.16371020674705505\n",
      "Epoch: 65/100 | step: 28/422 | loss: 0.1421974003314972\n",
      "Epoch: 65/100 | step: 29/422 | loss: 0.09772483259439468\n",
      "Epoch: 65/100 | step: 30/422 | loss: 0.12967312335968018\n",
      "Epoch: 65/100 | step: 31/422 | loss: 0.2552449703216553\n",
      "Epoch: 65/100 | step: 32/422 | loss: 0.2837385833263397\n",
      "Epoch: 65/100 | step: 33/422 | loss: 0.33959752321243286\n",
      "Epoch: 65/100 | step: 34/422 | loss: 0.1530759036540985\n",
      "Epoch: 65/100 | step: 35/422 | loss: 0.11842930316925049\n",
      "Epoch: 65/100 | step: 36/422 | loss: 0.18157237768173218\n",
      "Epoch: 65/100 | step: 37/422 | loss: 0.07040852308273315\n",
      "Epoch: 65/100 | step: 38/422 | loss: 0.09109076857566833\n",
      "Epoch: 65/100 | step: 39/422 | loss: 0.09624290466308594\n",
      "Epoch: 65/100 | step: 40/422 | loss: 0.1701318770647049\n",
      "Epoch: 65/100 | step: 41/422 | loss: 0.2670619785785675\n",
      "Epoch: 65/100 | step: 42/422 | loss: 0.0983486920595169\n",
      "Epoch: 65/100 | step: 43/422 | loss: 0.1589493602514267\n",
      "Epoch: 65/100 | step: 44/422 | loss: 0.17198911309242249\n",
      "Epoch: 65/100 | step: 45/422 | loss: 0.17874394357204437\n",
      "Epoch: 65/100 | step: 46/422 | loss: 0.19507814943790436\n",
      "Epoch: 65/100 | step: 47/422 | loss: 0.3160102665424347\n",
      "Epoch: 65/100 | step: 48/422 | loss: 0.6274397969245911\n",
      "Epoch: 65/100 | step: 49/422 | loss: 0.2693853974342346\n",
      "Epoch: 65/100 | step: 50/422 | loss: 0.13418620824813843\n",
      "Epoch: 65/100 | step: 51/422 | loss: 0.1446973830461502\n",
      "Epoch: 65/100 | step: 52/422 | loss: 0.2893418073654175\n",
      "Epoch: 65/100 | step: 53/422 | loss: 0.3300403952598572\n",
      "Epoch: 65/100 | step: 54/422 | loss: 0.19336345791816711\n",
      "Epoch: 65/100 | step: 55/422 | loss: 0.7947766780853271\n",
      "Epoch: 65/100 | step: 56/422 | loss: 0.27325648069381714\n",
      "Epoch: 65/100 | step: 57/422 | loss: 0.41357651352882385\n",
      "Epoch: 65/100 | step: 58/422 | loss: 0.5185752511024475\n",
      "Epoch: 65/100 | step: 59/422 | loss: 0.4810740053653717\n",
      "Epoch: 65/100 | step: 60/422 | loss: 0.21846643090248108\n",
      "Epoch: 65/100 | step: 61/422 | loss: 0.3868916928768158\n",
      "Epoch: 65/100 | step: 62/422 | loss: 0.2408202439546585\n",
      "Epoch: 65/100 | step: 63/422 | loss: 0.21038419008255005\n",
      "Epoch: 65/100 | step: 64/422 | loss: 0.16448742151260376\n",
      "Epoch: 65/100 | step: 65/422 | loss: 0.7640652656555176\n",
      "Epoch: 65/100 | step: 66/422 | loss: 0.559800386428833\n",
      "Epoch: 65/100 | step: 67/422 | loss: 0.25005945563316345\n",
      "Epoch: 65/100 | step: 68/422 | loss: 0.19042186439037323\n",
      "Epoch: 65/100 | step: 69/422 | loss: 0.2536048889160156\n",
      "Epoch: 65/100 | step: 70/422 | loss: 0.15750889480113983\n",
      "Epoch: 65/100 | step: 71/422 | loss: 0.14835049211978912\n",
      "Epoch: 65/100 | step: 72/422 | loss: 0.40559321641921997\n",
      "Epoch: 65/100 | step: 73/422 | loss: 0.12908431887626648\n",
      "Epoch: 65/100 | step: 74/422 | loss: 0.1743904948234558\n",
      "Epoch: 65/100 | step: 75/422 | loss: 0.5476807355880737\n",
      "Epoch: 65/100 | step: 76/422 | loss: 0.12428899854421616\n",
      "Epoch: 65/100 | step: 77/422 | loss: 0.19673864543437958\n",
      "Epoch: 65/100 | step: 78/422 | loss: 0.2727268636226654\n",
      "Epoch: 65/100 | step: 79/422 | loss: 0.2897193133831024\n",
      "Epoch: 65/100 | step: 80/422 | loss: 0.312846839427948\n",
      "Epoch: 65/100 | step: 81/422 | loss: 0.538279116153717\n",
      "Epoch: 65/100 | step: 82/422 | loss: 0.08772004395723343\n",
      "Epoch: 65/100 | step: 83/422 | loss: 0.24109628796577454\n",
      "Epoch: 65/100 | step: 84/422 | loss: 0.26408129930496216\n",
      "Epoch: 65/100 | step: 85/422 | loss: 0.41262683272361755\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 66/100 | step: 1/422 | loss: 0.1630416363477707\n",
      "Epoch: 66/100 | step: 2/422 | loss: 0.17102780938148499\n",
      "Epoch: 66/100 | step: 3/422 | loss: 0.17422570288181305\n",
      "Epoch: 66/100 | step: 4/422 | loss: 0.13906778395175934\n",
      "Epoch: 66/100 | step: 5/422 | loss: 0.3801882863044739\n",
      "Epoch: 66/100 | step: 6/422 | loss: 0.14980703592300415\n",
      "Epoch: 66/100 | step: 7/422 | loss: 0.10143446922302246\n",
      "Epoch: 66/100 | step: 8/422 | loss: 0.14750222861766815\n",
      "Epoch: 66/100 | step: 9/422 | loss: 0.158748060464859\n",
      "Epoch: 66/100 | step: 10/422 | loss: 0.12117676436901093\n",
      "Epoch: 66/100 | step: 11/422 | loss: 0.15122708678245544\n",
      "Epoch: 66/100 | step: 12/422 | loss: 0.061807386577129364\n",
      "Epoch: 66/100 | step: 13/422 | loss: 0.12779179215431213\n",
      "Epoch: 66/100 | step: 14/422 | loss: 0.18680323660373688\n",
      "Epoch: 66/100 | step: 15/422 | loss: 0.23032169044017792\n",
      "Epoch: 66/100 | step: 16/422 | loss: 0.19572368264198303\n",
      "Epoch: 66/100 | step: 17/422 | loss: 0.5778514742851257\n",
      "Epoch: 66/100 | step: 18/422 | loss: 0.45556771755218506\n",
      "Epoch: 66/100 | step: 19/422 | loss: 0.28055521845817566\n",
      "Epoch: 66/100 | step: 20/422 | loss: 0.09567879885435104\n",
      "Epoch: 66/100 | step: 21/422 | loss: 0.14947940409183502\n",
      "Epoch: 66/100 | step: 22/422 | loss: 0.2375454306602478\n",
      "Epoch: 66/100 | step: 23/422 | loss: 0.23723438382148743\n",
      "Epoch: 66/100 | step: 24/422 | loss: 0.31479504704475403\n",
      "Epoch: 66/100 | step: 25/422 | loss: 0.2265377640724182\n",
      "Epoch: 66/100 | step: 26/422 | loss: 0.10403494536876678\n",
      "Epoch: 66/100 | step: 27/422 | loss: 0.11614436656236649\n",
      "Epoch: 66/100 | step: 28/422 | loss: 0.2805693745613098\n",
      "Epoch: 66/100 | step: 29/422 | loss: 0.2647740840911865\n",
      "Epoch: 66/100 | step: 30/422 | loss: 0.27603963017463684\n",
      "Epoch: 66/100 | step: 31/422 | loss: 0.25134944915771484\n",
      "Epoch: 66/100 | step: 32/422 | loss: 0.1624816656112671\n",
      "Epoch: 66/100 | step: 33/422 | loss: 0.10179175436496735\n",
      "Epoch: 66/100 | step: 34/422 | loss: 0.2803378105163574\n",
      "Epoch: 66/100 | step: 35/422 | loss: 0.21010033786296844\n",
      "Epoch: 66/100 | step: 36/422 | loss: 0.27782750129699707\n",
      "Epoch: 66/100 | step: 37/422 | loss: 0.1896071881055832\n",
      "Epoch: 66/100 | step: 38/422 | loss: 0.3181336522102356\n",
      "Epoch: 66/100 | step: 39/422 | loss: 0.07417004555463791\n",
      "Epoch: 66/100 | step: 40/422 | loss: 0.24038048088550568\n",
      "Epoch: 66/100 | step: 41/422 | loss: 0.47545021772384644\n",
      "Epoch: 66/100 | step: 42/422 | loss: 0.19586603343486786\n",
      "Epoch: 66/100 | step: 43/422 | loss: 0.42994898557662964\n",
      "Epoch: 66/100 | step: 44/422 | loss: 0.2673317790031433\n",
      "Epoch: 66/100 | step: 45/422 | loss: 0.20018088817596436\n",
      "Epoch: 66/100 | step: 46/422 | loss: 0.14702199399471283\n",
      "Epoch: 66/100 | step: 47/422 | loss: 0.09467528760433197\n",
      "Epoch: 66/100 | step: 48/422 | loss: 0.23404452204704285\n",
      "Epoch: 66/100 | step: 49/422 | loss: 0.2343616634607315\n",
      "Epoch: 66/100 | step: 50/422 | loss: 0.0726313441991806\n",
      "Epoch: 66/100 | step: 51/422 | loss: 0.22330190241336823\n",
      "Epoch: 66/100 | step: 52/422 | loss: 0.15325631201267242\n",
      "Epoch: 66/100 | step: 53/422 | loss: 0.11385498940944672\n",
      "Epoch: 66/100 | step: 54/422 | loss: 0.16863510012626648\n",
      "Epoch: 66/100 | step: 55/422 | loss: 0.09043467044830322\n",
      "Epoch: 66/100 | step: 56/422 | loss: 0.1429644078016281\n",
      "Epoch: 66/100 | step: 57/422 | loss: 0.0973142758011818\n",
      "Epoch: 66/100 | step: 58/422 | loss: 0.16604314744472504\n",
      "Epoch: 66/100 | step: 59/422 | loss: 0.4708619713783264\n",
      "Epoch: 66/100 | step: 60/422 | loss: 0.1177191436290741\n",
      "Epoch: 66/100 | step: 61/422 | loss: 0.2009008377790451\n",
      "Epoch: 66/100 | step: 62/422 | loss: 0.1642633080482483\n",
      "Epoch: 66/100 | step: 63/422 | loss: 0.2505244016647339\n",
      "Epoch: 66/100 | step: 64/422 | loss: 0.14551879465579987\n",
      "Epoch: 66/100 | step: 65/422 | loss: 0.18846160173416138\n",
      "Epoch: 66/100 | step: 66/422 | loss: 0.10103735327720642\n",
      "Epoch: 66/100 | step: 67/422 | loss: 0.07465813308954239\n",
      "Epoch: 66/100 | step: 68/422 | loss: 0.12373141199350357\n",
      "Epoch: 66/100 | step: 69/422 | loss: 0.1763080507516861\n",
      "Epoch: 66/100 | step: 70/422 | loss: 0.10691668093204498\n",
      "Epoch: 66/100 | step: 71/422 | loss: 0.2876746654510498\n",
      "Epoch: 66/100 | step: 72/422 | loss: 0.08647160977125168\n",
      "Epoch: 66/100 | step: 73/422 | loss: 0.27445080876350403\n",
      "Epoch: 66/100 | step: 74/422 | loss: 0.2719174325466156\n",
      "Epoch: 66/100 | step: 75/422 | loss: 0.1250414103269577\n",
      "Epoch: 66/100 | step: 76/422 | loss: 0.14822441339492798\n",
      "Epoch: 66/100 | step: 77/422 | loss: 0.2777242362499237\n",
      "Epoch: 66/100 | step: 78/422 | loss: 0.26262784004211426\n",
      "Epoch: 66/100 | step: 79/422 | loss: 0.24896177649497986\n",
      "Epoch: 66/100 | step: 80/422 | loss: 0.1992252767086029\n",
      "Epoch: 66/100 | step: 81/422 | loss: 0.14173810184001923\n",
      "Epoch: 66/100 | step: 82/422 | loss: 0.15527351200580597\n",
      "Epoch: 66/100 | step: 83/422 | loss: 0.13485665619373322\n",
      "Epoch: 66/100 | step: 84/422 | loss: 0.1399127095937729\n",
      "Epoch: 66/100 | step: 85/422 | loss: 0.19757665693759918\n",
      "Epoch: 66/100 | step: 86/422 | loss: 0.26994505524635315\n",
      "Epoch: 66/100 | step: 87/422 | loss: 0.09953518211841583\n",
      "Epoch: 66/100 | step: 88/422 | loss: 0.17650321125984192\n",
      "Epoch: 66/100 | step: 89/422 | loss: 0.173508420586586\n",
      "Epoch: 66/100 | step: 90/422 | loss: 0.1598973423242569\n",
      "Epoch: 66/100 | step: 91/422 | loss: 0.06498111039400101\n",
      "Epoch: 66/100 | step: 92/422 | loss: 0.0909026712179184\n",
      "Epoch: 66/100 | step: 93/422 | loss: 0.11351101845502853\n",
      "Epoch: 66/100 | step: 94/422 | loss: 0.28590700030326843\n",
      "Epoch: 66/100 | step: 95/422 | loss: 0.08386553078889847\n",
      "Epoch: 66/100 | step: 96/422 | loss: 0.1793665736913681\n",
      "Epoch: 66/100 | step: 97/422 | loss: 0.24823692440986633\n",
      "Epoch: 66/100 | step: 98/422 | loss: 0.15099301934242249\n",
      "Epoch: 66/100 | step: 99/422 | loss: 0.10348371416330338\n",
      "Epoch: 66/100 | step: 100/422 | loss: 0.20789951086044312\n",
      "Epoch: 66/100 | step: 101/422 | loss: 0.17429208755493164\n",
      "Epoch: 66/100 | step: 102/422 | loss: 0.3188762068748474\n",
      "Epoch: 66/100 | step: 103/422 | loss: 0.4204818904399872\n",
      "Epoch: 66/100 | step: 104/422 | loss: 0.09015727043151855\n",
      "Epoch: 66/100 | step: 105/422 | loss: 0.3428669273853302\n",
      "Epoch: 66/100 | step: 106/422 | loss: 0.17852240800857544\n",
      "Epoch: 66/100 | step: 107/422 | loss: 0.1308048814535141\n",
      "Epoch: 66/100 | step: 108/422 | loss: 0.15020133554935455\n",
      "Epoch: 66/100 | step: 109/422 | loss: 0.1331028789281845\n",
      "Epoch: 66/100 | step: 110/422 | loss: 0.3527851402759552\n",
      "Epoch: 66/100 | step: 111/422 | loss: 0.4942130744457245\n",
      "Epoch: 66/100 | step: 112/422 | loss: 0.2055549919605255\n",
      "Epoch: 66/100 | step: 113/422 | loss: 0.1836117058992386\n",
      "Epoch: 66/100 | step: 114/422 | loss: 0.5143495202064514\n",
      "Epoch: 66/100 | step: 115/422 | loss: 0.27712196111679077\n",
      "Epoch: 66/100 | step: 116/422 | loss: 0.26802897453308105\n",
      "Epoch: 66/100 | step: 117/422 | loss: 0.25342586636543274\n",
      "Epoch: 66/100 | step: 118/422 | loss: 0.15175028145313263\n",
      "Epoch: 66/100 | step: 119/422 | loss: 0.09464488178491592\n",
      "Epoch: 66/100 | step: 120/422 | loss: 0.21096870303153992\n",
      "Epoch: 66/100 | step: 121/422 | loss: 0.49440255761146545\n",
      "Epoch: 66/100 | step: 122/422 | loss: 0.24631810188293457\n",
      "Epoch: 66/100 | step: 123/422 | loss: 0.12541088461875916\n",
      "Epoch: 66/100 | step: 124/422 | loss: 0.15781140327453613\n",
      "Epoch: 66/100 | step: 125/422 | loss: 0.3967417776584625\n",
      "Epoch: 66/100 | step: 126/422 | loss: 0.4091370105743408\n",
      "Epoch: 66/100 | step: 127/422 | loss: 0.42306846380233765\n",
      "Epoch: 66/100 | step: 128/422 | loss: 0.412647545337677\n",
      "Epoch: 66/100 | step: 129/422 | loss: 0.2679979205131531\n",
      "Epoch: 66/100 | step: 130/422 | loss: 0.23372413218021393\n",
      "Epoch: 66/100 | step: 131/422 | loss: 0.19035221636295319\n",
      "Epoch: 66/100 | step: 132/422 | loss: 0.23342080414295197\n",
      "Epoch: 66/100 | step: 133/422 | loss: 0.12385815382003784\n",
      "Epoch: 66/100 | step: 134/422 | loss: 0.29879552125930786\n",
      "Epoch: 66/100 | step: 135/422 | loss: 0.08761320263147354\n",
      "Epoch: 66/100 | step: 136/422 | loss: 0.16530939936637878\n",
      "Epoch: 66/100 | step: 137/422 | loss: 0.06254153698682785\n",
      "Epoch: 66/100 | step: 138/422 | loss: 0.15744340419769287\n",
      "Epoch: 66/100 | step: 139/422 | loss: 0.2878419756889343\n",
      "Epoch: 66/100 | step: 140/422 | loss: 0.08726826310157776\n",
      "Epoch: 66/100 | step: 141/422 | loss: 0.23848924040794373\n",
      "Epoch: 66/100 | step: 142/422 | loss: 0.4517744481563568\n",
      "Epoch: 66/100 | step: 143/422 | loss: 0.20603671669960022\n",
      "Epoch: 66/100 | step: 144/422 | loss: 0.36978626251220703\n",
      "Epoch: 66/100 | step: 145/422 | loss: 0.2385149449110031\n",
      "Epoch: 66/100 | step: 146/422 | loss: 0.15054987370967865\n",
      "Epoch: 66/100 | step: 147/422 | loss: 0.18134713172912598\n",
      "Epoch: 66/100 | step: 148/422 | loss: 0.06496664881706238\n",
      "Epoch: 66/100 | step: 149/422 | loss: 0.244825541973114\n",
      "Epoch: 66/100 | step: 150/422 | loss: 0.483215868473053\n",
      "Epoch: 66/100 | step: 151/422 | loss: 0.13526789844036102\n",
      "Epoch: 66/100 | step: 152/422 | loss: 0.25426989793777466\n",
      "Epoch: 66/100 | step: 153/422 | loss: 0.11681092530488968\n",
      "Epoch: 66/100 | step: 154/422 | loss: 0.11759305745363235\n",
      "Epoch: 66/100 | step: 155/422 | loss: 0.14479009807109833\n",
      "Epoch: 66/100 | step: 156/422 | loss: 0.12700797617435455\n",
      "Epoch: 66/100 | step: 157/422 | loss: 0.3622313141822815\n",
      "Epoch: 66/100 | step: 158/422 | loss: 0.2708803713321686\n",
      "Epoch: 66/100 | step: 159/422 | loss: 0.0930478647351265\n",
      "Epoch: 66/100 | step: 160/422 | loss: 0.3728276193141937\n",
      "Epoch: 66/100 | step: 161/422 | loss: 0.1543661206960678\n",
      "Epoch: 66/100 | step: 162/422 | loss: 0.1654457002878189\n",
      "Epoch: 66/100 | step: 163/422 | loss: 0.3350987136363983\n",
      "Epoch: 66/100 | step: 164/422 | loss: 0.1732638031244278\n",
      "Epoch: 66/100 | step: 165/422 | loss: 0.09133810549974442\n",
      "Epoch: 66/100 | step: 166/422 | loss: 0.1619451642036438\n",
      "Epoch: 66/100 | step: 167/422 | loss: 0.16987085342407227\n",
      "Epoch: 66/100 | step: 168/422 | loss: 0.2683808505535126\n",
      "Epoch: 66/100 | step: 169/422 | loss: 0.27919498085975647\n",
      "Epoch: 66/100 | step: 170/422 | loss: 0.3064877986907959\n",
      "Epoch: 66/100 | step: 171/422 | loss: 0.2512500584125519\n",
      "Epoch: 66/100 | step: 172/422 | loss: 0.2796710133552551\n",
      "Epoch: 66/100 | step: 173/422 | loss: 0.07528840005397797\n",
      "Epoch: 66/100 | step: 174/422 | loss: 0.1652238667011261\n",
      "Epoch: 66/100 | step: 175/422 | loss: 0.19239073991775513\n",
      "Epoch: 66/100 | step: 176/422 | loss: 0.2751046419143677\n",
      "Epoch: 66/100 | step: 177/422 | loss: 0.09588658809661865\n",
      "Epoch: 66/100 | step: 178/422 | loss: 0.09160751849412918\n",
      "Epoch: 66/100 | step: 179/422 | loss: 0.14055204391479492\n",
      "Epoch: 66/100 | step: 180/422 | loss: 0.12610271573066711\n",
      "Epoch: 66/100 | step: 181/422 | loss: 0.3094862401485443\n",
      "Epoch: 66/100 | step: 182/422 | loss: 0.16758085787296295\n",
      "Epoch: 66/100 | step: 183/422 | loss: 0.290457546710968\n",
      "Epoch: 66/100 | step: 184/422 | loss: 0.3353581428527832\n",
      "Epoch: 66/100 | step: 185/422 | loss: 0.08162055164575577\n",
      "Epoch: 66/100 | step: 186/422 | loss: 0.1552407145500183\n",
      "Epoch: 66/100 | step: 187/422 | loss: 0.3844926357269287\n",
      "Epoch: 66/100 | step: 188/422 | loss: 0.10264445096254349\n",
      "Epoch: 66/100 | step: 189/422 | loss: 0.21476925909519196\n",
      "Epoch: 66/100 | step: 190/422 | loss: 0.28191474080085754\n",
      "Epoch: 66/100 | step: 191/422 | loss: 0.2340177744626999\n",
      "Epoch: 66/100 | step: 192/422 | loss: 0.07529963552951813\n",
      "Epoch: 66/100 | step: 193/422 | loss: 0.10360056161880493\n",
      "Epoch: 66/100 | step: 194/422 | loss: 0.3998803496360779\n",
      "Epoch: 66/100 | step: 195/422 | loss: 0.3836883306503296\n",
      "Epoch: 66/100 | step: 196/422 | loss: 0.13036306202411652\n",
      "Epoch: 66/100 | step: 197/422 | loss: 0.12178590148687363\n",
      "Epoch: 66/100 | step: 198/422 | loss: 0.1605548858642578\n",
      "Epoch: 66/100 | step: 199/422 | loss: 0.14484944939613342\n",
      "Epoch: 66/100 | step: 200/422 | loss: 0.1631568819284439\n",
      "Epoch: 66/100 | step: 201/422 | loss: 0.2570250332355499\n",
      "Epoch: 66/100 | step: 202/422 | loss: 0.33158403635025024\n",
      "Epoch: 66/100 | step: 203/422 | loss: 0.6444813013076782\n",
      "Epoch: 66/100 | step: 204/422 | loss: 0.7100355625152588\n",
      "Epoch: 66/100 | step: 205/422 | loss: 0.6294589042663574\n",
      "Epoch: 66/100 | step: 206/422 | loss: 1.0993015766143799\n",
      "Epoch: 66/100 | step: 207/422 | loss: 0.30852580070495605\n",
      "Epoch: 66/100 | step: 208/422 | loss: 0.4167953133583069\n",
      "Epoch: 66/100 | step: 209/422 | loss: 0.20292307436466217\n",
      "Epoch: 66/100 | step: 210/422 | loss: 0.241531103849411\n",
      "Epoch: 66/100 | step: 211/422 | loss: 0.29551002383232117\n",
      "Epoch: 66/100 | step: 212/422 | loss: 0.2901266813278198\n",
      "Epoch: 66/100 | step: 213/422 | loss: 0.2486693412065506\n",
      "Epoch: 66/100 | step: 214/422 | loss: 0.258281946182251\n",
      "Epoch: 66/100 | step: 215/422 | loss: 0.36498457193374634\n",
      "Epoch: 66/100 | step: 216/422 | loss: 0.11715966463088989\n",
      "Epoch: 66/100 | step: 217/422 | loss: 0.2692582905292511\n",
      "Epoch: 66/100 | step: 218/422 | loss: 0.47061586380004883\n",
      "Epoch: 66/100 | step: 219/422 | loss: 0.10506095737218857\n",
      "Epoch: 66/100 | step: 220/422 | loss: 0.34178921580314636\n",
      "Epoch: 66/100 | step: 221/422 | loss: 0.1965850442647934\n",
      "Epoch: 66/100 | step: 222/422 | loss: 0.09008269011974335\n",
      "Epoch: 66/100 | step: 223/422 | loss: 0.10728804767131805\n",
      "Epoch: 66/100 | step: 224/422 | loss: 0.21843372285366058\n",
      "Epoch: 66/100 | step: 225/422 | loss: 0.16964475810527802\n",
      "Epoch: 66/100 | step: 226/422 | loss: 0.439030259847641\n",
      "Epoch: 66/100 | step: 227/422 | loss: 0.08044509589672089\n",
      "Epoch: 66/100 | step: 228/422 | loss: 0.38335949182510376\n",
      "Epoch: 66/100 | step: 229/422 | loss: 0.2685087323188782\n",
      "Epoch: 66/100 | step: 230/422 | loss: 0.17564420402050018\n",
      "Epoch: 66/100 | step: 231/422 | loss: 0.6293891668319702\n",
      "Epoch: 66/100 | step: 232/422 | loss: 0.33000195026397705\n",
      "Epoch: 66/100 | step: 233/422 | loss: 0.15245690941810608\n",
      "Epoch: 66/100 | step: 234/422 | loss: 0.24126145243644714\n",
      "Epoch: 66/100 | step: 235/422 | loss: 0.18422171473503113\n",
      "Epoch: 66/100 | step: 236/422 | loss: 0.12635143101215363\n",
      "Epoch: 66/100 | step: 237/422 | loss: 0.17171157896518707\n",
      "Epoch: 66/100 | step: 238/422 | loss: 0.2777968645095825\n",
      "Epoch: 66/100 | step: 239/422 | loss: 0.12996414303779602\n",
      "Epoch: 66/100 | step: 240/422 | loss: 0.17238900065422058\n",
      "Epoch: 66/100 | step: 241/422 | loss: 0.40319594740867615\n",
      "Epoch: 66/100 | step: 242/422 | loss: 0.4308505058288574\n",
      "Epoch: 66/100 | step: 243/422 | loss: 0.12871672213077545\n",
      "Epoch: 66/100 | step: 244/422 | loss: 0.1256108283996582\n",
      "Epoch: 66/100 | step: 245/422 | loss: 0.5757983922958374\n",
      "Epoch: 66/100 | step: 246/422 | loss: 0.397691011428833\n",
      "Epoch: 66/100 | step: 247/422 | loss: 0.3185015022754669\n",
      "Epoch: 66/100 | step: 248/422 | loss: 0.1090623065829277\n",
      "Epoch: 66/100 | step: 249/422 | loss: 0.22102083265781403\n",
      "Epoch: 66/100 | step: 250/422 | loss: 0.2736908793449402\n",
      "Epoch: 66/100 | step: 251/422 | loss: 0.09408406168222427\n",
      "Epoch: 66/100 | step: 252/422 | loss: 0.12070047855377197\n",
      "Epoch: 66/100 | step: 253/422 | loss: 0.20809917151927948\n",
      "Epoch: 66/100 | step: 254/422 | loss: 0.3400111198425293\n",
      "Epoch: 66/100 | step: 255/422 | loss: 0.17724472284317017\n",
      "Epoch: 66/100 | step: 256/422 | loss: 0.5484186410903931\n",
      "Epoch: 66/100 | step: 257/422 | loss: 0.4356621205806732\n",
      "Epoch: 66/100 | step: 258/422 | loss: 0.1808125078678131\n",
      "Epoch: 66/100 | step: 259/422 | loss: 0.35661154985427856\n",
      "Epoch: 66/100 | step: 260/422 | loss: 0.09474664181470871\n",
      "Epoch: 66/100 | step: 261/422 | loss: 0.10850833356380463\n",
      "Epoch: 66/100 | step: 262/422 | loss: 0.10120484232902527\n",
      "Epoch: 66/100 | step: 263/422 | loss: 0.33810505270957947\n",
      "Epoch: 66/100 | step: 264/422 | loss: 0.3346317708492279\n",
      "Epoch: 66/100 | step: 265/422 | loss: 0.30584651231765747\n",
      "Epoch: 66/100 | step: 266/422 | loss: 0.2034144103527069\n",
      "Epoch: 66/100 | step: 267/422 | loss: 0.29552799463272095\n",
      "Epoch: 66/100 | step: 268/422 | loss: 0.2801683843135834\n",
      "Epoch: 66/100 | step: 269/422 | loss: 0.3642202615737915\n",
      "Epoch: 66/100 | step: 270/422 | loss: 0.14168058335781097\n",
      "Epoch: 66/100 | step: 271/422 | loss: 0.23894186317920685\n",
      "Epoch: 66/100 | step: 272/422 | loss: 0.137528657913208\n",
      "Epoch: 66/100 | step: 273/422 | loss: 0.22225338220596313\n",
      "Epoch: 66/100 | step: 274/422 | loss: 0.05261284112930298\n",
      "Epoch: 66/100 | step: 275/422 | loss: 0.12558108568191528\n",
      "Epoch: 66/100 | step: 276/422 | loss: 0.2655051648616791\n",
      "Epoch: 66/100 | step: 277/422 | loss: 0.12961244583129883\n",
      "Epoch: 66/100 | step: 278/422 | loss: 0.09826120734214783\n",
      "Epoch: 66/100 | step: 279/422 | loss: 0.3246135711669922\n",
      "Epoch: 66/100 | step: 280/422 | loss: 0.29163306951522827\n",
      "Epoch: 66/100 | step: 281/422 | loss: 0.22505980730056763\n",
      "Epoch: 66/100 | step: 282/422 | loss: 0.2670709192752838\n",
      "Epoch: 66/100 | step: 283/422 | loss: 0.18779313564300537\n",
      "Epoch: 66/100 | step: 284/422 | loss: 0.3077675700187683\n",
      "Epoch: 66/100 | step: 285/422 | loss: 0.3028438091278076\n",
      "Epoch: 66/100 | step: 286/422 | loss: 0.332481324672699\n",
      "Epoch: 66/100 | step: 287/422 | loss: 0.41829216480255127\n",
      "Epoch: 66/100 | step: 288/422 | loss: 0.1454259753227234\n",
      "Epoch: 66/100 | step: 289/422 | loss: 0.2716488242149353\n",
      "Epoch: 66/100 | step: 290/422 | loss: 0.3780517876148224\n",
      "Epoch: 66/100 | step: 291/422 | loss: 0.34769561886787415\n",
      "Epoch: 66/100 | step: 292/422 | loss: 0.6544979810714722\n",
      "Epoch: 66/100 | step: 293/422 | loss: 0.26409611105918884\n",
      "Epoch: 66/100 | step: 294/422 | loss: 0.10011772066354752\n",
      "Epoch: 66/100 | step: 295/422 | loss: 0.16793787479400635\n",
      "Epoch: 66/100 | step: 296/422 | loss: 0.26480597257614136\n",
      "Epoch: 66/100 | step: 297/422 | loss: 0.13104286789894104\n",
      "Epoch: 66/100 | step: 298/422 | loss: 0.34468895196914673\n",
      "Epoch: 66/100 | step: 299/422 | loss: 0.11926119774580002\n",
      "Epoch: 66/100 | step: 300/422 | loss: 0.16959108412265778\n",
      "Epoch: 66/100 | step: 301/422 | loss: 0.3535396158695221\n",
      "Epoch: 66/100 | step: 302/422 | loss: 0.1713007241487503\n",
      "Epoch: 66/100 | step: 303/422 | loss: 0.16058219969272614\n",
      "Epoch: 66/100 | step: 304/422 | loss: 0.1836445927619934\n",
      "Epoch: 66/100 | step: 305/422 | loss: 0.12857317924499512\n",
      "Epoch: 66/100 | step: 306/422 | loss: 0.09820016473531723\n",
      "Epoch: 66/100 | step: 307/422 | loss: 0.11734294146299362\n",
      "Epoch: 66/100 | step: 308/422 | loss: 0.15857835114002228\n",
      "Epoch: 66/100 | step: 309/422 | loss: 0.09617925435304642\n",
      "Epoch: 66/100 | step: 310/422 | loss: 0.3222297728061676\n",
      "Epoch: 66/100 | step: 311/422 | loss: 0.2747460901737213\n",
      "Epoch: 66/100 | step: 312/422 | loss: 0.21937504410743713\n",
      "Epoch: 66/100 | step: 313/422 | loss: 0.1825069934129715\n",
      "Epoch: 66/100 | step: 314/422 | loss: 0.23432497680187225\n",
      "Epoch: 66/100 | step: 315/422 | loss: 0.34264659881591797\n",
      "Epoch: 66/100 | step: 316/422 | loss: 0.12581296265125275\n",
      "Epoch: 66/100 | step: 317/422 | loss: 0.15027828514575958\n",
      "Epoch: 66/100 | step: 318/422 | loss: 0.33635351061820984\n",
      "Epoch: 66/100 | step: 319/422 | loss: 0.27580055594444275\n",
      "Epoch: 66/100 | step: 320/422 | loss: 0.349253386259079\n",
      "Epoch: 66/100 | step: 321/422 | loss: 0.05642876401543617\n",
      "Epoch: 66/100 | step: 322/422 | loss: 0.24028116464614868\n",
      "Epoch: 66/100 | step: 323/422 | loss: 0.2564443051815033\n",
      "Epoch: 66/100 | step: 324/422 | loss: 0.32139700651168823\n",
      "Epoch: 66/100 | step: 325/422 | loss: 0.14107638597488403\n",
      "Epoch: 66/100 | step: 326/422 | loss: 0.41474875807762146\n",
      "Epoch: 66/100 | step: 327/422 | loss: 0.28427502512931824\n",
      "Epoch: 66/100 | step: 328/422 | loss: 0.26256051659584045\n",
      "Epoch: 66/100 | step: 329/422 | loss: 0.11682184785604477\n",
      "Epoch: 66/100 | step: 330/422 | loss: 0.07625876367092133\n",
      "Epoch: 66/100 | step: 331/422 | loss: 0.23718103766441345\n",
      "Epoch: 66/100 | step: 332/422 | loss: 0.047868527472019196\n",
      "Epoch: 66/100 | step: 333/422 | loss: 0.19686299562454224\n",
      "Epoch: 66/100 | step: 334/422 | loss: 0.16075514256954193\n",
      "Epoch: 66/100 | step: 335/422 | loss: 0.3046478033065796\n",
      "Epoch: 66/100 | step: 336/422 | loss: 0.4402049779891968\n",
      "Epoch: 66/100 | step: 337/422 | loss: 0.439026415348053\n",
      "Epoch: 66/100 | step: 338/422 | loss: 0.5797284245491028\n",
      "Epoch: 66/100 | step: 339/422 | loss: 0.3803933262825012\n",
      "Epoch: 66/100 | step: 340/422 | loss: 0.3473655879497528\n",
      "Epoch: 66/100 | step: 341/422 | loss: 0.21835391223430634\n",
      "Epoch: 66/100 | step: 342/422 | loss: 0.26776570081710815\n",
      "Epoch: 66/100 | step: 343/422 | loss: 0.1375126838684082\n",
      "Epoch: 66/100 | step: 344/422 | loss: 0.2092992663383484\n",
      "Epoch: 66/100 | step: 345/422 | loss: 0.43162232637405396\n",
      "Epoch: 66/100 | step: 346/422 | loss: 0.23799659311771393\n",
      "Epoch: 66/100 | step: 347/422 | loss: 0.2676192820072174\n",
      "Epoch: 66/100 | step: 348/422 | loss: 0.30593499541282654\n",
      "Epoch: 66/100 | step: 349/422 | loss: 0.21781741082668304\n",
      "Epoch: 66/100 | step: 350/422 | loss: 0.11609075218439102\n",
      "Epoch: 66/100 | step: 351/422 | loss: 0.26234379410743713\n",
      "Epoch: 66/100 | step: 352/422 | loss: 0.08703911304473877\n",
      "Epoch: 66/100 | step: 353/422 | loss: 0.15176692605018616\n",
      "Epoch: 66/100 | step: 354/422 | loss: 0.21496954560279846\n",
      "Epoch: 66/100 | step: 355/422 | loss: 0.9110388159751892\n",
      "Epoch: 66/100 | step: 356/422 | loss: 0.35526394844055176\n",
      "Epoch: 66/100 | step: 357/422 | loss: 0.270485520362854\n",
      "Epoch: 66/100 | step: 358/422 | loss: 0.11978709697723389\n",
      "Epoch: 66/100 | step: 359/422 | loss: 0.24016332626342773\n",
      "Epoch: 66/100 | step: 360/422 | loss: 0.3681234121322632\n",
      "Epoch: 66/100 | step: 361/422 | loss: 0.15057843923568726\n",
      "Epoch: 66/100 | step: 362/422 | loss: 0.1537131816148758\n",
      "Epoch: 66/100 | step: 363/422 | loss: 0.1793963462114334\n",
      "Epoch: 66/100 | step: 364/422 | loss: 0.14058995246887207\n",
      "Epoch: 66/100 | step: 365/422 | loss: 0.19289281964302063\n",
      "Epoch: 66/100 | step: 366/422 | loss: 0.08536137640476227\n",
      "Epoch: 66/100 | step: 367/422 | loss: 0.1764552891254425\n",
      "Epoch: 66/100 | step: 368/422 | loss: 0.23492687940597534\n",
      "Epoch: 66/100 | step: 369/422 | loss: 0.13318771123886108\n",
      "Epoch: 66/100 | step: 370/422 | loss: 0.21421054005622864\n",
      "Epoch: 66/100 | step: 371/422 | loss: 0.4358108639717102\n",
      "Epoch: 66/100 | step: 372/422 | loss: 0.1802789717912674\n",
      "Epoch: 66/100 | step: 373/422 | loss: 0.2497270703315735\n",
      "Epoch: 66/100 | step: 374/422 | loss: 0.25235775113105774\n",
      "Epoch: 66/100 | step: 375/422 | loss: 0.04876849055290222\n",
      "Epoch: 66/100 | step: 376/422 | loss: 0.2008252739906311\n",
      "Epoch: 66/100 | step: 377/422 | loss: 0.29668307304382324\n",
      "Epoch: 66/100 | step: 378/422 | loss: 0.19814591109752655\n",
      "Epoch: 66/100 | step: 379/422 | loss: 0.24734406173229218\n",
      "Epoch: 66/100 | step: 380/422 | loss: 0.30655428767204285\n",
      "Epoch: 66/100 | step: 381/422 | loss: 0.04644855856895447\n",
      "Epoch: 66/100 | step: 382/422 | loss: 0.13004978001117706\n",
      "Epoch: 66/100 | step: 383/422 | loss: 0.2192302644252777\n",
      "Epoch: 66/100 | step: 384/422 | loss: 0.10570082813501358\n",
      "Epoch: 66/100 | step: 385/422 | loss: 0.15626260638237\n",
      "Epoch: 66/100 | step: 386/422 | loss: 0.27871111035346985\n",
      "Epoch: 66/100 | step: 387/422 | loss: 0.4504674971103668\n",
      "Epoch: 66/100 | step: 388/422 | loss: 0.10287393629550934\n",
      "Epoch: 66/100 | step: 389/422 | loss: 0.14124955236911774\n",
      "Epoch: 66/100 | step: 390/422 | loss: 0.23267850279808044\n",
      "Epoch: 66/100 | step: 391/422 | loss: 0.1837499439716339\n",
      "Epoch: 66/100 | step: 392/422 | loss: 0.07553794980049133\n",
      "Epoch: 66/100 | step: 393/422 | loss: 0.13208414614200592\n",
      "Epoch: 66/100 | step: 394/422 | loss: 0.16204263269901276\n",
      "Epoch: 66/100 | step: 395/422 | loss: 0.2956632375717163\n",
      "Epoch: 66/100 | step: 396/422 | loss: 0.2180350422859192\n",
      "Epoch: 66/100 | step: 397/422 | loss: 0.1701030731201172\n",
      "Epoch: 66/100 | step: 398/422 | loss: 0.14517825841903687\n",
      "Epoch: 66/100 | step: 399/422 | loss: 0.21281196177005768\n",
      "Epoch: 66/100 | step: 400/422 | loss: 0.23809675872325897\n",
      "Epoch: 66/100 | step: 401/422 | loss: 0.17265358567237854\n",
      "Epoch: 66/100 | step: 402/422 | loss: 0.1772077977657318\n",
      "Epoch: 66/100 | step: 403/422 | loss: 0.16964562237262726\n",
      "Epoch: 66/100 | step: 404/422 | loss: 0.2670622169971466\n",
      "Epoch: 66/100 | step: 405/422 | loss: 0.0795605257153511\n",
      "Epoch: 66/100 | step: 406/422 | loss: 0.18150992691516876\n",
      "Epoch: 66/100 | step: 407/422 | loss: 0.34150001406669617\n",
      "Epoch: 66/100 | step: 408/422 | loss: 0.21827355027198792\n",
      "Epoch: 66/100 | step: 409/422 | loss: 0.20395103096961975\n",
      "Epoch: 66/100 | step: 410/422 | loss: 0.2512926757335663\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 67/100 | step: 1/422 | loss: 0.09809216856956482\n",
      "Epoch: 67/100 | step: 2/422 | loss: 0.20691683888435364\n",
      "Epoch: 67/100 | step: 3/422 | loss: 0.24812933802604675\n",
      "Epoch: 67/100 | step: 4/422 | loss: 0.14349351823329926\n",
      "Epoch: 67/100 | step: 5/422 | loss: 0.09152023494243622\n",
      "Epoch: 67/100 | step: 6/422 | loss: 0.12541350722312927\n",
      "Epoch: 67/100 | step: 7/422 | loss: 0.06855043768882751\n",
      "Epoch: 67/100 | step: 8/422 | loss: 0.062040090560913086\n",
      "Epoch: 67/100 | step: 9/422 | loss: 0.07179322838783264\n",
      "Epoch: 67/100 | step: 10/422 | loss: 0.10847600549459457\n",
      "Epoch: 67/100 | step: 11/422 | loss: 0.30087730288505554\n",
      "Epoch: 67/100 | step: 12/422 | loss: 0.28662538528442383\n",
      "Epoch: 67/100 | step: 13/422 | loss: 0.30044299364089966\n",
      "Epoch: 67/100 | step: 14/422 | loss: 0.16107816994190216\n",
      "Epoch: 67/100 | step: 15/422 | loss: 0.12180737406015396\n",
      "Epoch: 67/100 | step: 16/422 | loss: 0.17679640650749207\n",
      "Epoch: 67/100 | step: 17/422 | loss: 0.2029944360256195\n",
      "Epoch: 67/100 | step: 18/422 | loss: 0.05453919991850853\n",
      "Epoch: 67/100 | step: 19/422 | loss: 0.07335259020328522\n",
      "Epoch: 67/100 | step: 20/422 | loss: 0.31198686361312866\n",
      "Epoch: 67/100 | step: 21/422 | loss: 0.21779796481132507\n",
      "Epoch: 67/100 | step: 22/422 | loss: 0.14207307994365692\n",
      "Epoch: 67/100 | step: 23/422 | loss: 0.28070494532585144\n",
      "Epoch: 67/100 | step: 24/422 | loss: 0.15675663948059082\n",
      "Epoch: 67/100 | step: 25/422 | loss: 0.12882287800312042\n",
      "Epoch: 67/100 | step: 26/422 | loss: 0.062010083347558975\n",
      "Epoch: 67/100 | step: 27/422 | loss: 0.09103044122457504\n",
      "Epoch: 67/100 | step: 28/422 | loss: 0.28417134284973145\n",
      "Epoch: 67/100 | step: 29/422 | loss: 0.1672426462173462\n",
      "Epoch: 67/100 | step: 30/422 | loss: 0.1568201631307602\n",
      "Epoch: 67/100 | step: 31/422 | loss: 0.13572226464748383\n",
      "Epoch: 67/100 | step: 32/422 | loss: 0.08158449083566666\n",
      "Epoch: 67/100 | step: 33/422 | loss: 0.07682863622903824\n",
      "Epoch: 67/100 | step: 34/422 | loss: 0.14410866796970367\n",
      "Epoch: 67/100 | step: 35/422 | loss: 0.12066016346216202\n",
      "Epoch: 67/100 | step: 36/422 | loss: 0.14085279405117035\n",
      "Epoch: 67/100 | step: 37/422 | loss: 0.11929220706224442\n",
      "Epoch: 67/100 | step: 38/422 | loss: 0.08057057112455368\n",
      "Epoch: 67/100 | step: 39/422 | loss: 0.06974726170301437\n",
      "Epoch: 67/100 | step: 40/422 | loss: 0.06664326786994934\n",
      "Epoch: 67/100 | step: 41/422 | loss: 0.1084580272436142\n",
      "Epoch: 67/100 | step: 42/422 | loss: 0.11279265582561493\n",
      "Epoch: 67/100 | step: 43/422 | loss: 0.1285354346036911\n",
      "Epoch: 67/100 | step: 44/422 | loss: 0.1315682977437973\n",
      "Epoch: 67/100 | step: 45/422 | loss: 0.04787258803844452\n",
      "Epoch: 67/100 | step: 46/422 | loss: 0.0777086690068245\n",
      "Epoch: 67/100 | step: 47/422 | loss: 0.1331811249256134\n",
      "Epoch: 67/100 | step: 48/422 | loss: 0.14856959879398346\n",
      "Epoch: 67/100 | step: 49/422 | loss: 0.21720892190933228\n",
      "Epoch: 67/100 | step: 50/422 | loss: 0.07474293559789658\n",
      "Epoch: 67/100 | step: 51/422 | loss: 0.26484233140945435\n",
      "Epoch: 67/100 | step: 52/422 | loss: 0.1882741004228592\n",
      "Epoch: 67/100 | step: 53/422 | loss: 0.05658220499753952\n",
      "Epoch: 67/100 | step: 54/422 | loss: 0.08687955141067505\n",
      "Epoch: 67/100 | step: 55/422 | loss: 0.08743655681610107\n",
      "Epoch: 67/100 | step: 56/422 | loss: 0.10350164026021957\n",
      "Epoch: 67/100 | step: 57/422 | loss: 0.12173836678266525\n",
      "Epoch: 67/100 | step: 58/422 | loss: 0.10163658112287521\n",
      "Epoch: 67/100 | step: 59/422 | loss: 0.1511402279138565\n",
      "Epoch: 67/100 | step: 60/422 | loss: 0.05623796954751015\n",
      "Epoch: 67/100 | step: 61/422 | loss: 0.3890274167060852\n",
      "Epoch: 67/100 | step: 62/422 | loss: 0.07251617312431335\n",
      "Epoch: 67/100 | step: 63/422 | loss: 0.03416558355093002\n",
      "Epoch: 67/100 | step: 64/422 | loss: 0.22876422107219696\n",
      "Epoch: 67/100 | step: 65/422 | loss: 0.07913670688867569\n",
      "Epoch: 67/100 | step: 66/422 | loss: 0.17270362377166748\n",
      "Epoch: 67/100 | step: 67/422 | loss: 0.14310359954833984\n",
      "Epoch: 67/100 | step: 68/422 | loss: 0.06900736689567566\n",
      "Epoch: 67/100 | step: 69/422 | loss: 0.08275154232978821\n",
      "Epoch: 67/100 | step: 70/422 | loss: 0.18975390493869781\n",
      "Epoch: 67/100 | step: 71/422 | loss: 0.3440496325492859\n",
      "Epoch: 67/100 | step: 72/422 | loss: 0.35379043221473694\n",
      "Epoch: 67/100 | step: 73/422 | loss: 0.6841409206390381\n",
      "Epoch: 67/100 | step: 74/422 | loss: 0.49384650588035583\n",
      "Epoch: 67/100 | step: 75/422 | loss: 0.30723750591278076\n",
      "Epoch: 67/100 | step: 76/422 | loss: 0.21266576647758484\n",
      "Epoch: 67/100 | step: 77/422 | loss: 0.15141285955905914\n",
      "Epoch: 67/100 | step: 78/422 | loss: 0.24786920845508575\n",
      "Epoch: 67/100 | step: 79/422 | loss: 0.3449714183807373\n",
      "Epoch: 67/100 | step: 80/422 | loss: 0.12187066674232483\n",
      "Epoch: 67/100 | step: 81/422 | loss: 0.23780356347560883\n",
      "Epoch: 67/100 | step: 82/422 | loss: 0.16090747714042664\n",
      "Epoch: 67/100 | step: 83/422 | loss: 0.06613872945308685\n",
      "Epoch: 67/100 | step: 84/422 | loss: 0.1553136110305786\n",
      "Epoch: 67/100 | step: 85/422 | loss: 0.043178312480449677\n",
      "Epoch: 67/100 | step: 86/422 | loss: 0.18107227981090546\n",
      "Epoch: 67/100 | step: 87/422 | loss: 0.12996144592761993\n",
      "Epoch: 67/100 | step: 88/422 | loss: 0.19961288571357727\n",
      "Epoch: 67/100 | step: 89/422 | loss: 0.20274659991264343\n",
      "Epoch: 67/100 | step: 90/422 | loss: 0.12252826243638992\n",
      "Epoch: 67/100 | step: 91/422 | loss: 0.14398224651813507\n",
      "Epoch: 67/100 | step: 92/422 | loss: 0.10615428537130356\n",
      "Epoch: 67/100 | step: 93/422 | loss: 0.062345996499061584\n",
      "Epoch: 67/100 | step: 94/422 | loss: 0.10016235709190369\n",
      "Epoch: 67/100 | step: 95/422 | loss: 0.0590682290494442\n",
      "Epoch: 67/100 | step: 96/422 | loss: 0.07528924196958542\n",
      "Epoch: 67/100 | step: 97/422 | loss: 0.14142285287380219\n",
      "Epoch: 67/100 | step: 98/422 | loss: 0.1167658194899559\n",
      "Epoch: 67/100 | step: 99/422 | loss: 0.17600885033607483\n",
      "Epoch: 67/100 | step: 100/422 | loss: 0.08015648275613785\n",
      "Epoch: 67/100 | step: 101/422 | loss: 0.4111640453338623\n",
      "Epoch: 67/100 | step: 102/422 | loss: 0.09386106580495834\n",
      "Epoch: 67/100 | step: 103/422 | loss: 0.130246102809906\n",
      "Epoch: 67/100 | step: 104/422 | loss: 0.22030016779899597\n",
      "Epoch: 67/100 | step: 105/422 | loss: 0.10983448475599289\n",
      "Epoch: 67/100 | step: 106/422 | loss: 0.3170306086540222\n",
      "Epoch: 67/100 | step: 107/422 | loss: 0.14278636872768402\n",
      "Epoch: 67/100 | step: 108/422 | loss: 0.20565731823444366\n",
      "Epoch: 67/100 | step: 109/422 | loss: 0.10451249778270721\n",
      "Epoch: 67/100 | step: 110/422 | loss: 0.24677999317646027\n",
      "Epoch: 67/100 | step: 111/422 | loss: 0.1409657895565033\n",
      "Epoch: 67/100 | step: 112/422 | loss: 0.07499910145998001\n",
      "Epoch: 67/100 | step: 113/422 | loss: 0.17700274288654327\n",
      "Epoch: 67/100 | step: 114/422 | loss: 0.12854215502738953\n",
      "Epoch: 67/100 | step: 115/422 | loss: 0.22660483419895172\n",
      "Epoch: 67/100 | step: 116/422 | loss: 0.23023809492588043\n",
      "Epoch: 67/100 | step: 117/422 | loss: 0.19346904754638672\n",
      "Epoch: 67/100 | step: 118/422 | loss: 0.15671013295650482\n",
      "Epoch: 67/100 | step: 119/422 | loss: 0.12935923039913177\n",
      "Epoch: 67/100 | step: 120/422 | loss: 0.27145329117774963\n",
      "Epoch: 67/100 | step: 121/422 | loss: 0.211103156208992\n",
      "Epoch: 67/100 | step: 122/422 | loss: 0.15662983059883118\n",
      "Epoch: 67/100 | step: 123/422 | loss: 0.12064728885889053\n",
      "Epoch: 67/100 | step: 124/422 | loss: 0.13741782307624817\n",
      "Epoch: 67/100 | step: 125/422 | loss: 0.15877920389175415\n",
      "Epoch: 67/100 | step: 126/422 | loss: 0.18915164470672607\n",
      "Epoch: 67/100 | step: 127/422 | loss: 0.19778317213058472\n",
      "Epoch: 67/100 | step: 128/422 | loss: 0.1464908868074417\n",
      "Epoch: 67/100 | step: 129/422 | loss: 0.14860957860946655\n",
      "Epoch: 67/100 | step: 130/422 | loss: 0.12176109105348587\n",
      "Epoch: 67/100 | step: 131/422 | loss: 0.20989210903644562\n",
      "Epoch: 67/100 | step: 132/422 | loss: 0.050913505256175995\n",
      "Epoch: 67/100 | step: 133/422 | loss: 0.1839206963777542\n",
      "Epoch: 67/100 | step: 134/422 | loss: 0.15577690303325653\n",
      "Epoch: 67/100 | step: 135/422 | loss: 0.23699268698692322\n",
      "Epoch: 67/100 | step: 136/422 | loss: 0.1063578724861145\n",
      "Epoch: 67/100 | step: 137/422 | loss: 0.22258219122886658\n",
      "Epoch: 67/100 | step: 138/422 | loss: 0.20527909696102142\n",
      "Epoch: 67/100 | step: 139/422 | loss: 0.07163878530263901\n",
      "Epoch: 67/100 | step: 140/422 | loss: 0.16786903142929077\n",
      "Epoch: 67/100 | step: 141/422 | loss: 0.06921786814928055\n",
      "Epoch: 67/100 | step: 142/422 | loss: 0.14301279187202454\n",
      "Epoch: 67/100 | step: 143/422 | loss: 0.1192593052983284\n",
      "Epoch: 67/100 | step: 144/422 | loss: 0.05103650689125061\n",
      "Epoch: 67/100 | step: 145/422 | loss: 0.0962766632437706\n",
      "Epoch: 67/100 | step: 146/422 | loss: 0.07444324344396591\n",
      "Epoch: 67/100 | step: 147/422 | loss: 0.07519657164812088\n",
      "Epoch: 67/100 | step: 148/422 | loss: 0.11837395280599594\n",
      "Epoch: 67/100 | step: 149/422 | loss: 0.06608780473470688\n",
      "Epoch: 67/100 | step: 150/422 | loss: 0.13037848472595215\n",
      "Epoch: 67/100 | step: 151/422 | loss: 0.05684249475598335\n",
      "Epoch: 67/100 | step: 152/422 | loss: 0.15187063813209534\n",
      "Epoch: 67/100 | step: 153/422 | loss: 0.11167150735855103\n",
      "Epoch: 67/100 | step: 154/422 | loss: 0.21894867718219757\n",
      "Epoch: 67/100 | step: 155/422 | loss: 0.22441624104976654\n",
      "Epoch: 67/100 | step: 156/422 | loss: 0.12901927530765533\n",
      "Epoch: 67/100 | step: 157/422 | loss: 0.1828491985797882\n",
      "Epoch: 67/100 | step: 158/422 | loss: 0.08595246076583862\n",
      "Epoch: 67/100 | step: 159/422 | loss: 0.11670596152544022\n",
      "Epoch: 67/100 | step: 160/422 | loss: 0.10073339194059372\n",
      "Epoch: 67/100 | step: 161/422 | loss: 0.10767810046672821\n",
      "Epoch: 67/100 | step: 162/422 | loss: 0.40342965722084045\n",
      "Epoch: 67/100 | step: 163/422 | loss: 0.15301361680030823\n",
      "Epoch: 67/100 | step: 164/422 | loss: 0.34401553869247437\n",
      "Epoch: 67/100 | step: 165/422 | loss: 0.3674893379211426\n",
      "Epoch: 67/100 | step: 166/422 | loss: 0.12671762704849243\n",
      "Epoch: 67/100 | step: 167/422 | loss: 0.15463830530643463\n",
      "Epoch: 67/100 | step: 168/422 | loss: 0.1104353591799736\n",
      "Epoch: 67/100 | step: 169/422 | loss: 0.19454947113990784\n",
      "Epoch: 67/100 | step: 170/422 | loss: 0.2314830720424652\n",
      "Epoch: 67/100 | step: 171/422 | loss: 0.09185891598463058\n",
      "Epoch: 67/100 | step: 172/422 | loss: 0.3263002932071686\n",
      "Epoch: 67/100 | step: 173/422 | loss: 0.21433590352535248\n",
      "Epoch: 67/100 | step: 174/422 | loss: 0.087576724588871\n",
      "Epoch: 67/100 | step: 175/422 | loss: 0.07937449216842651\n",
      "Epoch: 67/100 | step: 176/422 | loss: 0.14672115445137024\n",
      "Epoch: 67/100 | step: 177/422 | loss: 0.08634994179010391\n",
      "Epoch: 67/100 | step: 178/422 | loss: 0.14548370242118835\n",
      "Epoch: 67/100 | step: 179/422 | loss: 0.12466000020503998\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 68/100 | step: 1/422 | loss: 0.12124234437942505\n",
      "Epoch: 68/100 | step: 2/422 | loss: 0.1504705846309662\n",
      "Epoch: 68/100 | step: 3/422 | loss: 0.08363648504018784\n",
      "Epoch: 68/100 | step: 4/422 | loss: 0.23388072848320007\n",
      "Epoch: 68/100 | step: 5/422 | loss: 0.21111100912094116\n",
      "Epoch: 68/100 | step: 6/422 | loss: 0.1333339512348175\n",
      "Epoch: 68/100 | step: 7/422 | loss: 0.17088429629802704\n",
      "Epoch: 68/100 | step: 8/422 | loss: 0.2191755324602127\n",
      "Epoch: 68/100 | step: 9/422 | loss: 0.1265363097190857\n",
      "Epoch: 68/100 | step: 10/422 | loss: 0.22158010303974152\n",
      "Epoch: 68/100 | step: 11/422 | loss: 0.061088692396879196\n",
      "Epoch: 68/100 | step: 12/422 | loss: 0.0739840492606163\n",
      "Epoch: 68/100 | step: 13/422 | loss: 0.24683675169944763\n",
      "Epoch: 68/100 | step: 14/422 | loss: 0.07030541449785233\n",
      "Epoch: 68/100 | step: 15/422 | loss: 0.06881244480609894\n",
      "Epoch: 68/100 | step: 16/422 | loss: 0.06202501803636551\n",
      "Epoch: 68/100 | step: 17/422 | loss: 0.1468638926744461\n",
      "Epoch: 68/100 | step: 18/422 | loss: 0.24649713933467865\n",
      "Epoch: 68/100 | step: 19/422 | loss: 0.10105150192975998\n",
      "Epoch: 68/100 | step: 20/422 | loss: 0.08440675586462021\n",
      "Epoch: 68/100 | step: 21/422 | loss: 0.09616129845380783\n",
      "Epoch: 68/100 | step: 22/422 | loss: 0.04572255164384842\n",
      "Epoch: 68/100 | step: 23/422 | loss: 0.07435169070959091\n",
      "Epoch: 68/100 | step: 24/422 | loss: 0.09453419595956802\n",
      "Epoch: 68/100 | step: 25/422 | loss: 0.13362300395965576\n",
      "Epoch: 68/100 | step: 26/422 | loss: 0.16404902935028076\n",
      "Epoch: 68/100 | step: 27/422 | loss: 0.10637067258358002\n",
      "Epoch: 68/100 | step: 28/422 | loss: 0.09689781069755554\n",
      "Epoch: 68/100 | step: 29/422 | loss: 0.1937766820192337\n",
      "Epoch: 68/100 | step: 30/422 | loss: 0.17543074488639832\n",
      "Epoch: 68/100 | step: 31/422 | loss: 0.03516451641917229\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 69/100 | step: 1/422 | loss: 0.06072156876325607\n",
      "Epoch: 69/100 | step: 2/422 | loss: 0.028904559090733528\n",
      "Epoch: 69/100 | step: 3/422 | loss: 0.050768252462148666\n",
      "Epoch: 69/100 | step: 4/422 | loss: 0.09824252128601074\n",
      "Epoch: 69/100 | step: 5/422 | loss: 0.05271368846297264\n",
      "Epoch: 69/100 | step: 6/422 | loss: 0.1062106192111969\n",
      "Epoch: 69/100 | step: 7/422 | loss: 0.17900881171226501\n",
      "Epoch: 69/100 | step: 8/422 | loss: 0.09279022365808487\n",
      "Epoch: 69/100 | step: 9/422 | loss: 0.163792684674263\n",
      "Epoch: 69/100 | step: 10/422 | loss: 0.41671210527420044\n",
      "Epoch: 69/100 | step: 11/422 | loss: 0.10041623562574387\n",
      "Epoch: 69/100 | step: 12/422 | loss: 0.12151564657688141\n",
      "Epoch: 69/100 | step: 13/422 | loss: 0.06991838663816452\n",
      "Epoch: 69/100 | step: 14/422 | loss: 0.051368698477745056\n",
      "Epoch: 69/100 | step: 15/422 | loss: 0.09989757835865021\n",
      "Epoch: 69/100 | step: 16/422 | loss: 0.12548325955867767\n",
      "Epoch: 69/100 | step: 17/422 | loss: 0.17758406698703766\n",
      "Epoch: 69/100 | step: 18/422 | loss: 0.32897213101387024\n",
      "Epoch: 69/100 | step: 19/422 | loss: 0.2692093253135681\n",
      "Epoch: 69/100 | step: 20/422 | loss: 0.1424012929201126\n",
      "Epoch: 69/100 | step: 21/422 | loss: 0.19436228275299072\n",
      "Epoch: 69/100 | step: 22/422 | loss: 0.046617716550827026\n",
      "Epoch: 69/100 | step: 23/422 | loss: 0.5296531319618225\n",
      "Epoch: 69/100 | step: 24/422 | loss: 0.44701313972473145\n",
      "Epoch: 69/100 | step: 25/422 | loss: 0.14695674180984497\n",
      "Epoch: 69/100 | step: 26/422 | loss: 0.6273996233940125\n",
      "Epoch: 69/100 | step: 27/422 | loss: 0.18412090837955475\n",
      "Epoch: 69/100 | step: 28/422 | loss: 0.20817962288856506\n",
      "Epoch: 69/100 | step: 29/422 | loss: 0.1972188949584961\n",
      "Epoch: 69/100 | step: 30/422 | loss: 0.08611415326595306\n",
      "Epoch: 69/100 | step: 31/422 | loss: 0.1654053032398224\n",
      "Epoch: 69/100 | step: 32/422 | loss: 0.14279389381408691\n",
      "Epoch: 69/100 | step: 33/422 | loss: 0.044298555701971054\n",
      "Epoch: 69/100 | step: 34/422 | loss: 0.04852593317627907\n",
      "Epoch: 69/100 | step: 35/422 | loss: 0.45294949412345886\n",
      "Epoch: 69/100 | step: 36/422 | loss: 0.36702871322631836\n",
      "Epoch: 69/100 | step: 37/422 | loss: 0.1858752965927124\n",
      "Epoch: 69/100 | step: 38/422 | loss: 0.15763366222381592\n",
      "Epoch: 69/100 | step: 39/422 | loss: 0.25447604060173035\n",
      "Epoch: 69/100 | step: 40/422 | loss: 0.19142210483551025\n",
      "Epoch: 69/100 | step: 41/422 | loss: 0.10394221544265747\n",
      "Epoch: 69/100 | step: 42/422 | loss: 0.11900433152914047\n",
      "Epoch: 69/100 | step: 43/422 | loss: 0.08759323507547379\n",
      "Epoch: 69/100 | step: 44/422 | loss: 0.15470461547374725\n",
      "Epoch: 69/100 | step: 45/422 | loss: 0.10708194226026535\n",
      "Epoch: 69/100 | step: 46/422 | loss: 0.19820263981819153\n",
      "Epoch: 69/100 | step: 47/422 | loss: 0.1178295835852623\n",
      "Epoch: 69/100 | step: 48/422 | loss: 0.10734324902296066\n",
      "Epoch: 69/100 | step: 49/422 | loss: 0.242606058716774\n",
      "Epoch: 69/100 | step: 50/422 | loss: 0.08092676103115082\n",
      "Epoch: 69/100 | step: 51/422 | loss: 0.09344527125358582\n",
      "Epoch: 69/100 | step: 52/422 | loss: 0.14221911132335663\n",
      "Epoch: 69/100 | step: 53/422 | loss: 0.08697210997343063\n",
      "Epoch: 69/100 | step: 54/422 | loss: 0.09487587958574295\n",
      "Epoch: 69/100 | step: 55/422 | loss: 0.08008759468793869\n",
      "Epoch: 69/100 | step: 56/422 | loss: 0.07727713882923126\n",
      "Epoch: 69/100 | step: 57/422 | loss: 0.0505281537771225\n",
      "Epoch: 69/100 | step: 58/422 | loss: 0.09066681563854218\n",
      "Epoch: 69/100 | step: 59/422 | loss: 0.06948427855968475\n",
      "Epoch: 69/100 | step: 60/422 | loss: 0.08485301584005356\n",
      "Epoch: 69/100 | step: 61/422 | loss: 0.10336615890264511\n",
      "Epoch: 69/100 | step: 62/422 | loss: 0.056625235825777054\n",
      "Epoch: 69/100 | step: 63/422 | loss: 0.12364720553159714\n",
      "Epoch: 69/100 | step: 64/422 | loss: 0.12117324769496918\n",
      "Epoch: 69/100 | step: 65/422 | loss: 0.06435108929872513\n",
      "Epoch: 69/100 | step: 66/422 | loss: 0.0475529246032238\n",
      "Epoch: 69/100 | step: 67/422 | loss: 0.04249299317598343\n",
      "Epoch: 69/100 | step: 68/422 | loss: 0.03667569160461426\n",
      "Epoch: 69/100 | step: 69/422 | loss: 0.0491575263440609\n",
      "Epoch: 69/100 | step: 70/422 | loss: 0.05937739089131355\n",
      "Epoch: 69/100 | step: 71/422 | loss: 0.05131152644753456\n",
      "Epoch: 69/100 | step: 72/422 | loss: 0.135764479637146\n",
      "Epoch: 69/100 | step: 73/422 | loss: 0.10744680464267731\n",
      "Epoch: 69/100 | step: 74/422 | loss: 0.2562066912651062\n",
      "Epoch: 69/100 | step: 75/422 | loss: 0.34170863032341003\n",
      "Epoch: 69/100 | step: 76/422 | loss: 0.15892672538757324\n",
      "Epoch: 69/100 | step: 77/422 | loss: 0.07807926088571548\n",
      "Epoch: 69/100 | step: 78/422 | loss: 0.07714103907346725\n",
      "Epoch: 69/100 | step: 79/422 | loss: 0.09333314001560211\n",
      "Epoch: 69/100 | step: 80/422 | loss: 0.06859880685806274\n",
      "Epoch: 69/100 | step: 81/422 | loss: 0.1822342872619629\n",
      "Epoch: 69/100 | step: 82/422 | loss: 0.08111190795898438\n",
      "Epoch: 69/100 | step: 83/422 | loss: 0.1634484827518463\n",
      "Epoch: 69/100 | step: 84/422 | loss: 0.08589114993810654\n",
      "Epoch: 69/100 | step: 85/422 | loss: 0.06931031495332718\n",
      "Epoch: 69/100 | step: 86/422 | loss: 0.08183048665523529\n",
      "Epoch: 69/100 | step: 87/422 | loss: 0.03853078559041023\n",
      "Epoch: 69/100 | step: 88/422 | loss: 0.12228576093912125\n",
      "Epoch: 69/100 | step: 89/422 | loss: 0.06728729605674744\n",
      "Epoch: 69/100 | step: 90/422 | loss: 0.09204746037721634\n",
      "Epoch: 69/100 | step: 91/422 | loss: 0.09645893424749374\n",
      "Epoch: 69/100 | step: 92/422 | loss: 0.0407475084066391\n",
      "Epoch: 69/100 | step: 93/422 | loss: 0.056154001504182816\n",
      "Epoch: 69/100 | step: 94/422 | loss: 0.08156435191631317\n",
      "Epoch: 69/100 | step: 95/422 | loss: 0.08941589295864105\n",
      "Epoch: 69/100 | step: 96/422 | loss: 0.182814359664917\n",
      "Epoch: 69/100 | step: 97/422 | loss: 0.2111884206533432\n",
      "Epoch: 69/100 | step: 98/422 | loss: 0.03343303129076958\n",
      "Epoch: 69/100 | step: 99/422 | loss: 0.1003076359629631\n",
      "Epoch: 69/100 | step: 100/422 | loss: 0.08861945569515228\n",
      "Epoch: 69/100 | step: 101/422 | loss: 0.04752310737967491\n",
      "Epoch: 69/100 | step: 102/422 | loss: 0.09597349911928177\n",
      "Epoch: 69/100 | step: 103/422 | loss: 0.10736893862485886\n",
      "Epoch: 69/100 | step: 104/422 | loss: 0.09276695549488068\n",
      "Epoch: 69/100 | step: 105/422 | loss: 0.04688326269388199\n",
      "Epoch: 69/100 | step: 106/422 | loss: 0.06756902486085892\n",
      "Epoch: 69/100 | step: 107/422 | loss: 0.08323182910680771\n",
      "Epoch: 69/100 | step: 108/422 | loss: 0.10747049003839493\n",
      "Epoch: 69/100 | step: 109/422 | loss: 0.10197904706001282\n",
      "Epoch: 69/100 | step: 110/422 | loss: 0.20577363669872284\n",
      "Epoch: 69/100 | step: 111/422 | loss: 0.24281907081604004\n",
      "Epoch: 69/100 | step: 112/422 | loss: 0.1597844809293747\n",
      "Epoch: 69/100 | step: 113/422 | loss: 0.040897686034440994\n",
      "Epoch: 69/100 | step: 114/422 | loss: 0.12001411616802216\n",
      "Epoch: 69/100 | step: 115/422 | loss: 0.07937481254339218\n",
      "Epoch: 69/100 | step: 116/422 | loss: 0.11965056508779526\n",
      "Epoch: 69/100 | step: 117/422 | loss: 0.09650794416666031\n",
      "Epoch: 69/100 | step: 118/422 | loss: 0.32954177260398865\n",
      "Epoch: 69/100 | step: 119/422 | loss: 0.08305627107620239\n",
      "Epoch: 69/100 | step: 120/422 | loss: 0.10417423397302628\n",
      "Epoch: 69/100 | step: 121/422 | loss: 0.12297049164772034\n",
      "Epoch: 69/100 | step: 122/422 | loss: 0.09515523910522461\n",
      "Epoch: 69/100 | step: 123/422 | loss: 0.05910637602210045\n",
      "Epoch: 69/100 | step: 124/422 | loss: 0.0556490533053875\n",
      "Epoch: 69/100 | step: 125/422 | loss: 0.05119596794247627\n",
      "Epoch: 69/100 | step: 126/422 | loss: 0.21469101309776306\n",
      "Epoch: 69/100 | step: 127/422 | loss: 0.09080502390861511\n",
      "Epoch: 69/100 | step: 128/422 | loss: 0.09391289949417114\n",
      "Epoch: 69/100 | step: 129/422 | loss: 0.21167704463005066\n",
      "Epoch: 69/100 | step: 130/422 | loss: 0.24262350797653198\n",
      "Epoch: 69/100 | step: 131/422 | loss: 0.11510786414146423\n",
      "Epoch: 69/100 | step: 132/422 | loss: 0.0730409249663353\n",
      "Epoch: 69/100 | step: 133/422 | loss: 0.31108978390693665\n",
      "Epoch: 69/100 | step: 134/422 | loss: 0.37004658579826355\n",
      "Epoch: 69/100 | step: 135/422 | loss: 0.12744426727294922\n",
      "Epoch: 69/100 | step: 136/422 | loss: 0.2775905430316925\n",
      "Epoch: 69/100 | step: 137/422 | loss: 0.0473276823759079\n",
      "Epoch: 69/100 | step: 138/422 | loss: 0.26738181710243225\n",
      "Epoch: 69/100 | step: 139/422 | loss: 0.1363753080368042\n",
      "Epoch: 69/100 | step: 140/422 | loss: 0.24023182690143585\n",
      "Epoch: 69/100 | step: 141/422 | loss: 0.23391108214855194\n",
      "Epoch: 69/100 | step: 142/422 | loss: 0.11577196419239044\n",
      "Epoch: 69/100 | step: 143/422 | loss: 0.20339839160442352\n",
      "Epoch: 69/100 | step: 144/422 | loss: 0.1950567066669464\n",
      "Epoch: 69/100 | step: 145/422 | loss: 0.0754375085234642\n",
      "Epoch: 69/100 | step: 146/422 | loss: 0.1206706315279007\n",
      "Epoch: 69/100 | step: 147/422 | loss: 0.09963265806436539\n",
      "Epoch: 69/100 | step: 148/422 | loss: 0.05286320298910141\n",
      "Epoch: 69/100 | step: 149/422 | loss: 0.06116466969251633\n",
      "Epoch: 69/100 | step: 150/422 | loss: 0.05444486811757088\n",
      "Epoch: 69/100 | step: 151/422 | loss: 0.10352347791194916\n",
      "Epoch: 69/100 | step: 152/422 | loss: 0.171585351228714\n",
      "Epoch: 69/100 | step: 153/422 | loss: 0.2258118987083435\n",
      "Epoch: 69/100 | step: 154/422 | loss: 0.37907838821411133\n",
      "Epoch: 69/100 | step: 155/422 | loss: 0.6814755797386169\n",
      "Epoch: 69/100 | step: 156/422 | loss: 0.17314079403877258\n",
      "Epoch: 69/100 | step: 157/422 | loss: 0.2869284749031067\n",
      "Epoch: 69/100 | step: 158/422 | loss: 0.11473149806261063\n",
      "Epoch: 69/100 | step: 159/422 | loss: 0.10671474784612656\n",
      "Epoch: 69/100 | step: 160/422 | loss: 0.11936968564987183\n",
      "Epoch: 69/100 | step: 161/422 | loss: 0.2624613046646118\n",
      "Epoch: 69/100 | step: 162/422 | loss: 0.19096121191978455\n",
      "Epoch: 69/100 | step: 163/422 | loss: 0.2512252628803253\n",
      "Epoch: 69/100 | step: 164/422 | loss: 0.13836972415447235\n",
      "Epoch: 69/100 | step: 165/422 | loss: 0.09065548330545425\n",
      "Epoch: 69/100 | step: 166/422 | loss: 0.09475360065698624\n",
      "Epoch: 69/100 | step: 167/422 | loss: 0.17527782917022705\n",
      "Epoch: 69/100 | step: 168/422 | loss: 0.16099995374679565\n",
      "Epoch: 69/100 | step: 169/422 | loss: 0.09574958682060242\n",
      "Epoch: 69/100 | step: 170/422 | loss: 0.08276021480560303\n",
      "Epoch: 69/100 | step: 171/422 | loss: 0.06900952756404877\n",
      "Epoch: 69/100 | step: 172/422 | loss: 0.33189857006073\n",
      "Epoch: 69/100 | step: 173/422 | loss: 0.08529114723205566\n",
      "Epoch: 69/100 | step: 174/422 | loss: 0.0945504680275917\n",
      "Epoch: 69/100 | step: 175/422 | loss: 0.10429040342569351\n",
      "Epoch: 69/100 | step: 176/422 | loss: 0.17805920541286469\n",
      "Epoch: 69/100 | step: 177/422 | loss: 0.08491111546754837\n",
      "Epoch: 69/100 | step: 178/422 | loss: 0.39935633540153503\n",
      "Epoch: 69/100 | step: 179/422 | loss: 0.3133925497531891\n",
      "Epoch: 69/100 | step: 180/422 | loss: 0.1642170548439026\n",
      "Epoch: 69/100 | step: 181/422 | loss: 0.148709237575531\n",
      "Epoch: 69/100 | step: 182/422 | loss: 0.10608260333538055\n",
      "Epoch: 69/100 | step: 183/422 | loss: 0.19426792860031128\n",
      "Epoch: 69/100 | step: 184/422 | loss: 0.11210208386182785\n",
      "Epoch: 69/100 | step: 185/422 | loss: 0.13802795112133026\n",
      "Epoch: 69/100 | step: 186/422 | loss: 0.07055468112230301\n",
      "Epoch: 69/100 | step: 187/422 | loss: 0.11780314147472382\n",
      "Epoch: 69/100 | step: 188/422 | loss: 0.28804126381874084\n",
      "Epoch: 69/100 | step: 189/422 | loss: 0.19354745745658875\n",
      "Epoch: 69/100 | step: 190/422 | loss: 0.1649177074432373\n",
      "Epoch: 69/100 | step: 191/422 | loss: 0.22703610360622406\n",
      "Epoch: 69/100 | step: 192/422 | loss: 0.10234560817480087\n",
      "Epoch: 69/100 | step: 193/422 | loss: 0.07082784175872803\n",
      "Epoch: 69/100 | step: 194/422 | loss: 0.06234271079301834\n",
      "Epoch: 69/100 | step: 195/422 | loss: 0.14827029407024384\n",
      "Epoch: 69/100 | step: 196/422 | loss: 0.14222103357315063\n",
      "Epoch: 69/100 | step: 197/422 | loss: 0.04234326630830765\n",
      "Epoch: 69/100 | step: 198/422 | loss: 0.10236206650733948\n",
      "Epoch: 69/100 | step: 199/422 | loss: 0.16527296602725983\n",
      "Epoch: 69/100 | step: 200/422 | loss: 0.10686339437961578\n",
      "Epoch: 69/100 | step: 201/422 | loss: 0.1559213399887085\n",
      "Epoch: 69/100 | step: 202/422 | loss: 0.09495442360639572\n",
      "Epoch: 69/100 | step: 203/422 | loss: 0.09512804448604584\n",
      "Epoch: 69/100 | step: 204/422 | loss: 0.15503080189228058\n",
      "Epoch: 69/100 | step: 205/422 | loss: 0.20531095564365387\n",
      "Epoch: 69/100 | step: 206/422 | loss: 0.1092008501291275\n",
      "Epoch: 69/100 | step: 207/422 | loss: 0.0932103767991066\n",
      "Epoch: 69/100 | step: 208/422 | loss: 0.14129021763801575\n",
      "Epoch: 69/100 | step: 209/422 | loss: 0.04316986724734306\n",
      "Epoch: 69/100 | step: 210/422 | loss: 0.11698242276906967\n",
      "Epoch: 69/100 | step: 211/422 | loss: 0.09331539273262024\n",
      "Epoch: 69/100 | step: 212/422 | loss: 0.1311814785003662\n",
      "Epoch: 69/100 | step: 213/422 | loss: 0.17184039950370789\n",
      "Epoch: 69/100 | step: 214/422 | loss: 0.10091643035411835\n",
      "Epoch: 69/100 | step: 215/422 | loss: 0.13861824572086334\n",
      "Epoch: 69/100 | step: 216/422 | loss: 0.18116439878940582\n",
      "Epoch: 69/100 | step: 217/422 | loss: 0.26934146881103516\n",
      "Epoch: 69/100 | step: 218/422 | loss: 0.12477308511734009\n",
      "Epoch: 69/100 | step: 219/422 | loss: 0.10920844227075577\n",
      "Epoch: 69/100 | step: 220/422 | loss: 0.21870000660419464\n",
      "Epoch: 69/100 | step: 221/422 | loss: 0.22515897452831268\n",
      "Epoch: 69/100 | step: 222/422 | loss: 0.089951291680336\n",
      "Epoch: 69/100 | step: 223/422 | loss: 0.14207135140895844\n",
      "Epoch: 69/100 | step: 224/422 | loss: 0.20194241404533386\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 70/100 | step: 1/422 | loss: 0.2721779942512512\n",
      "Epoch: 70/100 | step: 2/422 | loss: 0.22160425782203674\n",
      "Epoch: 70/100 | step: 3/422 | loss: 0.11835559457540512\n",
      "Epoch: 70/100 | step: 4/422 | loss: 0.07928422838449478\n",
      "Epoch: 70/100 | step: 5/422 | loss: 0.0868837982416153\n",
      "Epoch: 70/100 | step: 6/422 | loss: 0.0734013095498085\n",
      "Epoch: 70/100 | step: 7/422 | loss: 0.07226547598838806\n",
      "Epoch: 70/100 | step: 8/422 | loss: 0.06038782745599747\n",
      "Epoch: 70/100 | step: 9/422 | loss: 0.06686707586050034\n",
      "Epoch: 70/100 | step: 10/422 | loss: 0.04982924088835716\n",
      "Epoch: 70/100 | step: 11/422 | loss: 0.04412971809506416\n",
      "Epoch: 70/100 | step: 12/422 | loss: 0.044283267110586166\n",
      "Epoch: 70/100 | step: 13/422 | loss: 0.07199622690677643\n",
      "Epoch: 70/100 | step: 14/422 | loss: 0.1264631450176239\n",
      "Epoch: 70/100 | step: 15/422 | loss: 0.0399046428501606\n",
      "Epoch: 70/100 | step: 16/422 | loss: 0.02898271009325981\n",
      "Epoch: 70/100 | step: 17/422 | loss: 0.21095964312553406\n",
      "Epoch: 70/100 | step: 18/422 | loss: 0.11377708613872528\n",
      "Epoch: 70/100 | step: 19/422 | loss: 0.037183113396167755\n",
      "Epoch: 70/100 | step: 20/422 | loss: 0.05200617387890816\n",
      "Epoch: 70/100 | step: 21/422 | loss: 0.06901367753744125\n",
      "Epoch: 70/100 | step: 22/422 | loss: 0.13476569950580597\n",
      "Epoch: 70/100 | step: 23/422 | loss: 0.12069226056337357\n",
      "Epoch: 70/100 | step: 24/422 | loss: 0.11288458108901978\n",
      "Epoch: 70/100 | step: 25/422 | loss: 0.08635716885328293\n",
      "Epoch: 70/100 | step: 26/422 | loss: 0.06888207048177719\n",
      "Epoch: 70/100 | step: 27/422 | loss: 0.05595548450946808\n",
      "Epoch: 70/100 | step: 28/422 | loss: 0.11890333890914917\n",
      "Epoch: 70/100 | step: 29/422 | loss: 0.1646794080734253\n",
      "Epoch: 70/100 | step: 30/422 | loss: 0.08315185457468033\n",
      "Epoch: 70/100 | step: 31/422 | loss: 0.1148923709988594\n",
      "Epoch: 70/100 | step: 32/422 | loss: 0.09011317044496536\n",
      "Epoch: 70/100 | step: 33/422 | loss: 0.03827393427491188\n",
      "Epoch: 70/100 | step: 34/422 | loss: 0.1035766452550888\n",
      "Epoch: 70/100 | step: 35/422 | loss: 0.10181102901697159\n",
      "Epoch: 70/100 | step: 36/422 | loss: 0.11736997216939926\n",
      "Epoch: 70/100 | step: 37/422 | loss: 0.046732738614082336\n",
      "Epoch: 70/100 | step: 38/422 | loss: 0.19333148002624512\n",
      "Epoch: 70/100 | step: 39/422 | loss: 0.132294699549675\n",
      "Epoch: 70/100 | step: 40/422 | loss: 0.14542683959007263\n",
      "Epoch: 70/100 | step: 41/422 | loss: 0.17571797966957092\n",
      "Epoch: 70/100 | step: 42/422 | loss: 0.512614369392395\n",
      "Epoch: 70/100 | step: 43/422 | loss: 0.08693956583738327\n",
      "Epoch: 70/100 | step: 44/422 | loss: 0.16832248866558075\n",
      "Epoch: 70/100 | step: 45/422 | loss: 0.1250436007976532\n",
      "Epoch: 70/100 | step: 46/422 | loss: 0.10674985498189926\n",
      "Epoch: 70/100 | step: 47/422 | loss: 0.174320787191391\n",
      "Epoch: 70/100 | step: 48/422 | loss: 0.08045733720064163\n",
      "Epoch: 70/100 | step: 49/422 | loss: 0.2290465235710144\n",
      "Epoch: 70/100 | step: 50/422 | loss: 0.13251230120658875\n",
      "Epoch: 70/100 | step: 51/422 | loss: 0.0725717768073082\n",
      "Epoch: 70/100 | step: 52/422 | loss: 0.0733899176120758\n",
      "Epoch: 70/100 | step: 53/422 | loss: 0.16740122437477112\n",
      "Epoch: 70/100 | step: 54/422 | loss: 0.14492595195770264\n",
      "Epoch: 70/100 | step: 55/422 | loss: 0.038136616349220276\n",
      "Epoch: 70/100 | step: 56/422 | loss: 0.07168149948120117\n",
      "Epoch: 70/100 | step: 57/422 | loss: 0.08266054093837738\n",
      "Epoch: 70/100 | step: 58/422 | loss: 0.06988916546106339\n",
      "Epoch: 70/100 | step: 59/422 | loss: 0.0730833187699318\n",
      "Epoch: 70/100 | step: 60/422 | loss: 0.09223076701164246\n",
      "Epoch: 70/100 | step: 61/422 | loss: 0.237808495759964\n",
      "Epoch: 70/100 | step: 62/422 | loss: 0.1268869936466217\n",
      "Epoch: 70/100 | step: 63/422 | loss: 0.10753034800291061\n",
      "Epoch: 70/100 | step: 64/422 | loss: 0.06709817051887512\n",
      "Epoch: 70/100 | step: 65/422 | loss: 0.024017831310629845\n",
      "Epoch: 70/100 | step: 66/422 | loss: 0.07126853615045547\n",
      "Epoch: 70/100 | step: 67/422 | loss: 0.11145955324172974\n",
      "Epoch: 70/100 | step: 68/422 | loss: 0.07499448955059052\n",
      "Epoch: 70/100 | step: 69/422 | loss: 0.09000606089830399\n",
      "Epoch: 70/100 | step: 70/422 | loss: 0.0884871557354927\n",
      "Epoch: 70/100 | step: 71/422 | loss: 0.08574531227350235\n",
      "Epoch: 70/100 | step: 72/422 | loss: 0.11742152273654938\n",
      "Epoch: 70/100 | step: 73/422 | loss: 0.08013042062520981\n",
      "Epoch: 70/100 | step: 74/422 | loss: 0.17271527647972107\n",
      "Epoch: 70/100 | step: 75/422 | loss: 0.06509432196617126\n",
      "Epoch: 70/100 | step: 76/422 | loss: 0.061566438525915146\n",
      "Epoch: 70/100 | step: 77/422 | loss: 0.14852237701416016\n",
      "Epoch: 70/100 | step: 78/422 | loss: 0.060020022094249725\n",
      "Epoch: 70/100 | step: 79/422 | loss: 0.12309129536151886\n",
      "Epoch: 70/100 | step: 80/422 | loss: 0.1524859368801117\n",
      "Epoch: 70/100 | step: 81/422 | loss: 0.16458198428153992\n",
      "Epoch: 70/100 | step: 82/422 | loss: 0.13951024413108826\n",
      "Epoch: 70/100 | step: 83/422 | loss: 0.09607437998056412\n",
      "Epoch: 70/100 | step: 84/422 | loss: 0.05659002065658569\n",
      "Epoch: 70/100 | step: 85/422 | loss: 0.05876009166240692\n",
      "Epoch: 70/100 | step: 86/422 | loss: 0.08490213751792908\n",
      "Epoch: 70/100 | step: 87/422 | loss: 0.07611875981092453\n",
      "Epoch: 70/100 | step: 88/422 | loss: 0.11991934478282928\n",
      "Epoch: 70/100 | step: 89/422 | loss: 0.1801302433013916\n",
      "Epoch: 70/100 | step: 90/422 | loss: 0.18563896417617798\n",
      "Epoch: 70/100 | step: 91/422 | loss: 0.08867748826742172\n",
      "Epoch: 70/100 | step: 92/422 | loss: 0.15909722447395325\n",
      "Epoch: 70/100 | step: 93/422 | loss: 0.0550009161233902\n",
      "Epoch: 70/100 | step: 94/422 | loss: 0.05145689845085144\n",
      "Epoch: 70/100 | step: 95/422 | loss: 0.16824780404567719\n",
      "Epoch: 70/100 | step: 96/422 | loss: 0.34560319781303406\n",
      "Epoch: 70/100 | step: 97/422 | loss: 0.11692149937152863\n",
      "Epoch: 70/100 | step: 98/422 | loss: 0.048466309905052185\n",
      "Epoch: 70/100 | step: 99/422 | loss: 0.39797231554985046\n",
      "Epoch: 70/100 | step: 100/422 | loss: 0.18380360305309296\n",
      "Epoch: 70/100 | step: 101/422 | loss: 0.1401747316122055\n",
      "Epoch: 70/100 | step: 102/422 | loss: 0.038308121263980865\n",
      "Epoch: 70/100 | step: 103/422 | loss: 0.07687006890773773\n",
      "Epoch: 70/100 | step: 104/422 | loss: 0.07574643194675446\n",
      "Epoch: 70/100 | step: 105/422 | loss: 0.12888874113559723\n",
      "Epoch: 70/100 | step: 106/422 | loss: 0.05858398973941803\n",
      "Epoch: 70/100 | step: 107/422 | loss: 0.13388262689113617\n",
      "Epoch: 70/100 | step: 108/422 | loss: 0.06680548191070557\n",
      "Epoch: 70/100 | step: 109/422 | loss: 0.04701602831482887\n",
      "Epoch: 70/100 | step: 110/422 | loss: 0.1378079354763031\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 71/100 | step: 1/422 | loss: 0.03150813281536102\n",
      "Epoch: 71/100 | step: 2/422 | loss: 0.10466895997524261\n",
      "Epoch: 71/100 | step: 3/422 | loss: 0.06553218513727188\n",
      "Epoch: 71/100 | step: 4/422 | loss: 0.06092038005590439\n",
      "Epoch: 71/100 | step: 5/422 | loss: 0.034950923174619675\n",
      "Epoch: 71/100 | step: 6/422 | loss: 0.0904192328453064\n",
      "Epoch: 71/100 | step: 7/422 | loss: 0.03914836421608925\n",
      "Epoch: 71/100 | step: 8/422 | loss: 0.09436371922492981\n",
      "Epoch: 71/100 | step: 9/422 | loss: 0.16213050484657288\n",
      "Epoch: 71/100 | step: 10/422 | loss: 0.07927603274583817\n",
      "Epoch: 71/100 | step: 11/422 | loss: 0.03887578472495079\n",
      "Epoch: 71/100 | step: 12/422 | loss: 0.04607143625617027\n",
      "Epoch: 71/100 | step: 13/422 | loss: 0.2431793063879013\n",
      "Epoch: 71/100 | step: 14/422 | loss: 0.07238490879535675\n",
      "Epoch: 71/100 | step: 15/422 | loss: 0.13114802539348602\n",
      "Epoch: 71/100 | step: 16/422 | loss: 0.06495247781276703\n",
      "Epoch: 71/100 | step: 17/422 | loss: 0.07190687954425812\n",
      "Epoch: 71/100 | step: 18/422 | loss: 0.03578564524650574\n",
      "Epoch: 71/100 | step: 19/422 | loss: 0.12285390496253967\n",
      "Epoch: 71/100 | step: 20/422 | loss: 0.0818217471241951\n",
      "Epoch: 71/100 | step: 21/422 | loss: 0.07992220669984818\n",
      "Epoch: 71/100 | step: 22/422 | loss: 0.11404982209205627\n",
      "Epoch: 71/100 | step: 23/422 | loss: 0.13016104698181152\n",
      "Epoch: 71/100 | step: 24/422 | loss: 0.12455559521913528\n",
      "Epoch: 71/100 | step: 25/422 | loss: 0.157981276512146\n",
      "Epoch: 71/100 | step: 26/422 | loss: 0.1064831092953682\n",
      "Epoch: 71/100 | step: 27/422 | loss: 0.09168161451816559\n",
      "Epoch: 71/100 | step: 28/422 | loss: 0.071241095662117\n",
      "Epoch: 71/100 | step: 29/422 | loss: 0.0949738472700119\n",
      "Epoch: 71/100 | step: 30/422 | loss: 0.04321612790226936\n",
      "Epoch: 71/100 | step: 31/422 | loss: 0.05425645038485527\n",
      "Epoch: 71/100 | step: 32/422 | loss: 0.08946634829044342\n",
      "Epoch: 71/100 | step: 33/422 | loss: 0.05404938757419586\n",
      "Epoch: 71/100 | step: 34/422 | loss: 0.057529266923666\n",
      "Epoch: 71/100 | step: 35/422 | loss: 0.0731063261628151\n",
      "Epoch: 71/100 | step: 36/422 | loss: 0.053605541586875916\n",
      "Epoch: 71/100 | step: 37/422 | loss: 0.1619681715965271\n",
      "Epoch: 71/100 | step: 38/422 | loss: 0.055698689073324203\n",
      "Epoch: 71/100 | step: 39/422 | loss: 0.04347587749361992\n",
      "Epoch: 71/100 | step: 40/422 | loss: 0.053261660039424896\n",
      "Epoch: 71/100 | step: 41/422 | loss: 0.12726126611232758\n",
      "Epoch: 71/100 | step: 42/422 | loss: 0.05177195370197296\n",
      "Epoch: 71/100 | step: 43/422 | loss: 0.03471696749329567\n",
      "Epoch: 71/100 | step: 44/422 | loss: 0.05075536668300629\n",
      "Epoch: 71/100 | step: 45/422 | loss: 0.05841676890850067\n",
      "Epoch: 71/100 | step: 46/422 | loss: 0.09329487383365631\n",
      "Epoch: 71/100 | step: 47/422 | loss: 0.1339019238948822\n",
      "Epoch: 71/100 | step: 48/422 | loss: 0.12279056757688522\n",
      "Epoch: 71/100 | step: 49/422 | loss: 0.04742639511823654\n",
      "Epoch: 71/100 | step: 50/422 | loss: 0.07665441930294037\n",
      "Epoch: 71/100 | step: 51/422 | loss: 0.02958272583782673\n",
      "Epoch: 71/100 | step: 52/422 | loss: 0.03615184873342514\n",
      "Epoch: 71/100 | step: 53/422 | loss: 0.12077534198760986\n",
      "Epoch: 71/100 | step: 54/422 | loss: 0.16909381747245789\n",
      "Epoch: 71/100 | step: 55/422 | loss: 0.1670910269021988\n",
      "Epoch: 71/100 | step: 56/422 | loss: 0.16381333768367767\n",
      "Epoch: 71/100 | step: 57/422 | loss: 0.24516424536705017\n",
      "Epoch: 71/100 | step: 58/422 | loss: 0.059572186321020126\n",
      "Epoch: 71/100 | step: 59/422 | loss: 0.1711472123861313\n",
      "Epoch: 71/100 | step: 60/422 | loss: 0.38121435046195984\n",
      "Epoch: 71/100 | step: 61/422 | loss: 0.3251023292541504\n",
      "Epoch: 71/100 | step: 62/422 | loss: 0.1791277974843979\n",
      "Epoch: 71/100 | step: 63/422 | loss: 0.04807515814900398\n",
      "Epoch: 71/100 | step: 64/422 | loss: 0.08588755130767822\n",
      "Epoch: 71/100 | step: 65/422 | loss: 0.18370530009269714\n",
      "Epoch: 71/100 | step: 66/422 | loss: 0.03176306560635567\n",
      "Epoch: 71/100 | step: 67/422 | loss: 0.13243545591831207\n",
      "Epoch: 71/100 | step: 68/422 | loss: 0.05904180184006691\n",
      "Epoch: 71/100 | step: 69/422 | loss: 0.07975857704877853\n",
      "Epoch: 71/100 | step: 70/422 | loss: 0.27232471108436584\n",
      "Epoch: 71/100 | step: 71/422 | loss: 0.08059843629598618\n",
      "Epoch: 71/100 | step: 72/422 | loss: 0.0441921129822731\n",
      "Epoch: 71/100 | step: 73/422 | loss: 0.1113799661397934\n",
      "Epoch: 71/100 | step: 74/422 | loss: 0.085658498108387\n",
      "Epoch: 71/100 | step: 75/422 | loss: 0.07797911018133163\n",
      "Epoch: 71/100 | step: 76/422 | loss: 0.06693180650472641\n",
      "Epoch: 71/100 | step: 77/422 | loss: 0.08110552281141281\n",
      "Epoch: 71/100 | step: 78/422 | loss: 0.03645969182252884\n",
      "Epoch: 71/100 | step: 79/422 | loss: 0.06277503073215485\n",
      "Epoch: 71/100 | step: 80/422 | loss: 0.052055202424526215\n",
      "Epoch: 71/100 | step: 81/422 | loss: 0.0869608223438263\n",
      "Epoch: 71/100 | step: 82/422 | loss: 0.23736748099327087\n",
      "Epoch: 71/100 | step: 83/422 | loss: 0.28589746356010437\n",
      "Epoch: 71/100 | step: 84/422 | loss: 0.33792757987976074\n",
      "Epoch: 71/100 | step: 85/422 | loss: 0.0738077163696289\n",
      "Epoch: 71/100 | step: 86/422 | loss: 0.10134293884038925\n",
      "Epoch: 71/100 | step: 87/422 | loss: 0.05126338079571724\n",
      "Epoch: 71/100 | step: 88/422 | loss: 0.11891986429691315\n",
      "Epoch: 71/100 | step: 89/422 | loss: 0.11436634510755539\n",
      "Epoch: 71/100 | step: 90/422 | loss: 0.14851126074790955\n",
      "Epoch: 71/100 | step: 91/422 | loss: 0.1125849261879921\n",
      "Epoch: 71/100 | step: 92/422 | loss: 0.04046611115336418\n",
      "Epoch: 71/100 | step: 93/422 | loss: 0.20910760760307312\n",
      "Epoch: 71/100 | step: 94/422 | loss: 0.2997707426548004\n",
      "Epoch: 71/100 | step: 95/422 | loss: 0.4654434025287628\n",
      "Epoch: 71/100 | step: 96/422 | loss: 0.21161265671253204\n",
      "Epoch: 71/100 | step: 97/422 | loss: 0.2620764970779419\n",
      "Epoch: 71/100 | step: 98/422 | loss: 0.35069575905799866\n",
      "Epoch: 71/100 | step: 99/422 | loss: 0.20521578192710876\n",
      "Epoch: 71/100 | step: 100/422 | loss: 0.030219335108995438\n",
      "Epoch: 71/100 | step: 101/422 | loss: 0.08789825439453125\n",
      "Epoch: 71/100 | step: 102/422 | loss: 0.11448302865028381\n",
      "Epoch: 71/100 | step: 103/422 | loss: 0.1283903867006302\n",
      "Epoch: 71/100 | step: 104/422 | loss: 0.07004494220018387\n",
      "Epoch: 71/100 | step: 105/422 | loss: 0.07617121934890747\n",
      "Epoch: 71/100 | step: 106/422 | loss: 0.08004626631736755\n",
      "Epoch: 71/100 | step: 107/422 | loss: 0.19166450202465057\n",
      "Epoch: 71/100 | step: 108/422 | loss: 0.30846941471099854\n",
      "Epoch: 71/100 | step: 109/422 | loss: 0.19092483818531036\n",
      "Epoch: 71/100 | step: 110/422 | loss: 0.11887427419424057\n",
      "Epoch: 71/100 | step: 111/422 | loss: 0.09292113035917282\n",
      "Epoch: 71/100 | step: 112/422 | loss: 0.11033987998962402\n",
      "Epoch: 71/100 | step: 113/422 | loss: 0.10365321487188339\n",
      "Epoch: 71/100 | step: 114/422 | loss: 0.1413465142250061\n",
      "Epoch: 71/100 | step: 115/422 | loss: 0.24830636382102966\n",
      "Epoch: 71/100 | step: 116/422 | loss: 0.18009053170681\n",
      "Epoch: 71/100 | step: 117/422 | loss: 0.09593810141086578\n",
      "Epoch: 71/100 | step: 118/422 | loss: 0.2639124095439911\n",
      "Epoch: 71/100 | step: 119/422 | loss: 0.08040802925825119\n",
      "Epoch: 71/100 | step: 120/422 | loss: 0.1196807324886322\n",
      "Epoch: 71/100 | step: 121/422 | loss: 0.13956685364246368\n",
      "Epoch: 71/100 | step: 122/422 | loss: 0.16194048523902893\n",
      "Epoch: 71/100 | step: 123/422 | loss: 0.061813995242118835\n",
      "Epoch: 71/100 | step: 124/422 | loss: 0.07775602489709854\n",
      "Epoch: 71/100 | step: 125/422 | loss: 0.06959894299507141\n",
      "Epoch: 71/100 | step: 126/422 | loss: 0.04960816726088524\n",
      "Epoch: 71/100 | step: 127/422 | loss: 0.06620428711175919\n",
      "Epoch: 71/100 | step: 128/422 | loss: 0.05668792128562927\n",
      "Epoch: 71/100 | step: 129/422 | loss: 0.05419039726257324\n",
      "Epoch: 71/100 | step: 130/422 | loss: 0.23750850558280945\n",
      "Epoch: 71/100 | step: 131/422 | loss: 0.050724804401397705\n",
      "Epoch: 71/100 | step: 132/422 | loss: 0.04900198057293892\n",
      "Epoch: 71/100 | step: 133/422 | loss: 0.06970526278018951\n",
      "Epoch: 71/100 | step: 134/422 | loss: 0.28608235716819763\n",
      "Epoch: 71/100 | step: 135/422 | loss: 0.04798692464828491\n",
      "Epoch: 71/100 | step: 136/422 | loss: 0.037171293050050735\n",
      "Epoch: 71/100 | step: 137/422 | loss: 0.21568165719509125\n",
      "Epoch: 71/100 | step: 138/422 | loss: 0.14641231298446655\n",
      "Epoch: 71/100 | step: 139/422 | loss: 0.10247226059436798\n",
      "Epoch: 71/100 | step: 140/422 | loss: 0.034846626222133636\n",
      "Epoch: 71/100 | step: 141/422 | loss: 0.04616513475775719\n",
      "Epoch: 71/100 | step: 142/422 | loss: 0.1804105043411255\n",
      "Epoch: 71/100 | step: 143/422 | loss: 0.3594154417514801\n",
      "Epoch: 71/100 | step: 144/422 | loss: 0.3301039934158325\n",
      "Epoch: 71/100 | step: 145/422 | loss: 0.16145752370357513\n",
      "Epoch: 71/100 | step: 146/422 | loss: 0.3929077088832855\n",
      "Epoch: 71/100 | step: 147/422 | loss: 0.24116142094135284\n",
      "Epoch: 71/100 | step: 148/422 | loss: 0.22037792205810547\n",
      "Epoch: 71/100 | step: 149/422 | loss: 0.12071710079908371\n",
      "Epoch: 71/100 | step: 150/422 | loss: 0.08057598024606705\n",
      "Epoch: 71/100 | step: 151/422 | loss: 0.05478452146053314\n",
      "Epoch: 71/100 | step: 152/422 | loss: 0.2700897753238678\n",
      "Epoch: 71/100 | step: 153/422 | loss: 0.08799377083778381\n",
      "Epoch: 71/100 | step: 154/422 | loss: 0.07474774867296219\n",
      "Epoch: 71/100 | step: 155/422 | loss: 0.13975118100643158\n",
      "Epoch: 71/100 | step: 156/422 | loss: 0.09362947940826416\n",
      "Epoch: 71/100 | step: 157/422 | loss: 0.1082305908203125\n",
      "Epoch: 71/100 | step: 158/422 | loss: 0.1379799097776413\n",
      "Epoch: 71/100 | step: 159/422 | loss: 0.11546797305345535\n",
      "Epoch: 71/100 | step: 160/422 | loss: 0.17596621811389923\n",
      "Epoch: 71/100 | step: 161/422 | loss: 0.11341791599988937\n",
      "Epoch: 71/100 | step: 162/422 | loss: 0.18731774389743805\n",
      "Epoch: 71/100 | step: 163/422 | loss: 0.21285079419612885\n",
      "Epoch: 71/100 | step: 164/422 | loss: 0.11185382306575775\n",
      "Epoch: 71/100 | step: 165/422 | loss: 0.2603099048137665\n",
      "Epoch: 71/100 | step: 166/422 | loss: 0.2115476429462433\n",
      "Epoch: 71/100 | step: 167/422 | loss: 0.1753934770822525\n",
      "Epoch: 71/100 | step: 168/422 | loss: 0.18204379081726074\n",
      "Epoch: 71/100 | step: 169/422 | loss: 0.12976036965847015\n",
      "Epoch: 71/100 | step: 170/422 | loss: 0.4527090787887573\n",
      "Epoch: 71/100 | step: 171/422 | loss: 0.12141461670398712\n",
      "Epoch: 71/100 | step: 172/422 | loss: 0.08506587147712708\n",
      "Epoch: 71/100 | step: 173/422 | loss: 0.2527816593647003\n",
      "Epoch: 71/100 | step: 174/422 | loss: 0.17882958054542542\n",
      "Epoch: 71/100 | step: 175/422 | loss: 0.2994082272052765\n",
      "Epoch: 71/100 | step: 176/422 | loss: 0.6525094509124756\n",
      "Epoch: 71/100 | step: 177/422 | loss: 0.10951967537403107\n",
      "Epoch: 71/100 | step: 178/422 | loss: 0.18731249868869781\n",
      "Epoch: 71/100 | step: 179/422 | loss: 0.11542671173810959\n",
      "Epoch: 71/100 | step: 180/422 | loss: 0.17665745317935944\n",
      "Epoch: 71/100 | step: 181/422 | loss: 0.17830538749694824\n",
      "Epoch: 71/100 | step: 182/422 | loss: 0.2401890903711319\n",
      "Epoch: 71/100 | step: 183/422 | loss: 0.4022510051727295\n",
      "Epoch: 71/100 | step: 184/422 | loss: 0.3004961311817169\n",
      "Epoch: 71/100 | step: 185/422 | loss: 0.33404892683029175\n",
      "Epoch: 71/100 | step: 186/422 | loss: 0.42873135209083557\n",
      "Epoch: 71/100 | step: 187/422 | loss: 0.23806533217430115\n",
      "Epoch: 71/100 | step: 188/422 | loss: 0.21951551735401154\n",
      "Epoch: 71/100 | step: 189/422 | loss: 0.22919975221157074\n",
      "Epoch: 71/100 | step: 190/422 | loss: 0.1612972617149353\n",
      "Epoch: 71/100 | step: 191/422 | loss: 0.10007304698228836\n",
      "Epoch: 71/100 | step: 192/422 | loss: 0.1501978486776352\n",
      "Epoch: 71/100 | step: 193/422 | loss: 0.04170583188533783\n",
      "Epoch: 71/100 | step: 194/422 | loss: 0.055696867406368256\n",
      "Epoch: 71/100 | step: 195/422 | loss: 0.14554718136787415\n",
      "Epoch: 71/100 | step: 196/422 | loss: 0.04217592999339104\n",
      "Epoch: 71/100 | step: 197/422 | loss: 0.1462043970823288\n",
      "Epoch: 71/100 | step: 198/422 | loss: 0.12776117026805878\n",
      "Epoch: 71/100 | step: 199/422 | loss: 0.18738441169261932\n",
      "Epoch: 71/100 | step: 200/422 | loss: 0.5174170136451721\n",
      "Epoch: 71/100 | step: 201/422 | loss: 0.06492140889167786\n",
      "Epoch: 71/100 | step: 202/422 | loss: 0.26683011651039124\n",
      "Epoch: 71/100 | step: 203/422 | loss: 0.05169907212257385\n",
      "Epoch: 71/100 | step: 204/422 | loss: 0.1414521187543869\n",
      "Epoch: 71/100 | step: 205/422 | loss: 0.7063984870910645\n",
      "Epoch: 71/100 | step: 206/422 | loss: 0.6206997036933899\n",
      "Epoch: 71/100 | step: 207/422 | loss: 0.18632450699806213\n",
      "Epoch: 71/100 | step: 208/422 | loss: 0.28744494915008545\n",
      "Epoch: 71/100 | step: 209/422 | loss: 0.13968591392040253\n",
      "Epoch: 71/100 | step: 210/422 | loss: 0.4306681156158447\n",
      "Epoch: 71/100 | step: 211/422 | loss: 0.17678949236869812\n",
      "Epoch: 71/100 | step: 212/422 | loss: 0.2091878056526184\n",
      "Epoch: 71/100 | step: 213/422 | loss: 0.10486575961112976\n",
      "Epoch: 71/100 | step: 214/422 | loss: 0.10941874235868454\n",
      "Epoch: 71/100 | step: 215/422 | loss: 0.14742246270179749\n",
      "Epoch: 71/100 | step: 216/422 | loss: 0.2805405259132385\n",
      "Epoch: 71/100 | step: 217/422 | loss: 0.24662896990776062\n",
      "Epoch: 71/100 | step: 218/422 | loss: 0.2343970537185669\n",
      "Epoch: 71/100 | step: 219/422 | loss: 0.09418070316314697\n",
      "Epoch: 71/100 | step: 220/422 | loss: 0.12988688051700592\n",
      "Epoch: 71/100 | step: 221/422 | loss: 0.5020663738250732\n",
      "Epoch: 71/100 | step: 222/422 | loss: 0.31241893768310547\n",
      "Epoch: 71/100 | step: 223/422 | loss: 0.42930853366851807\n",
      "Epoch: 71/100 | step: 224/422 | loss: 0.2230994701385498\n",
      "Epoch: 71/100 | step: 225/422 | loss: 0.1923498660326004\n",
      "Epoch: 71/100 | step: 226/422 | loss: 0.24925069510936737\n",
      "Epoch: 71/100 | step: 227/422 | loss: 0.2847670912742615\n",
      "Epoch: 71/100 | step: 228/422 | loss: 0.2368604838848114\n",
      "Epoch: 71/100 | step: 229/422 | loss: 0.4807124733924866\n",
      "Epoch: 71/100 | step: 230/422 | loss: 0.25109878182411194\n",
      "Epoch: 71/100 | step: 231/422 | loss: 0.08373121172189713\n",
      "Epoch: 71/100 | step: 232/422 | loss: 0.27990055084228516\n",
      "Epoch: 71/100 | step: 233/422 | loss: 0.37323588132858276\n",
      "Epoch: 71/100 | step: 234/422 | loss: 0.24586421251296997\n",
      "Epoch: 71/100 | step: 235/422 | loss: 0.224061518907547\n",
      "Epoch: 71/100 | step: 236/422 | loss: 0.2829110622406006\n",
      "Epoch: 71/100 | step: 237/422 | loss: 0.11390653252601624\n",
      "Epoch: 71/100 | step: 238/422 | loss: 0.14607876539230347\n",
      "Epoch: 71/100 | step: 239/422 | loss: 0.07621967047452927\n",
      "Epoch: 71/100 | step: 240/422 | loss: 0.13283096253871918\n",
      "Epoch: 71/100 | step: 241/422 | loss: 0.06406276673078537\n",
      "Epoch: 71/100 | step: 242/422 | loss: 0.05836208537220955\n",
      "Epoch: 71/100 | step: 243/422 | loss: 0.08771562576293945\n",
      "Epoch: 71/100 | step: 244/422 | loss: 0.17783592641353607\n",
      "Epoch: 71/100 | step: 245/422 | loss: 0.1385415643453598\n",
      "Epoch: 71/100 | step: 246/422 | loss: 0.12041138857603073\n",
      "Epoch: 71/100 | step: 247/422 | loss: 0.08619871735572815\n",
      "Epoch: 71/100 | step: 248/422 | loss: 0.1507638543844223\n",
      "Epoch: 71/100 | step: 249/422 | loss: 0.25010982155799866\n",
      "Epoch: 71/100 | step: 250/422 | loss: 0.09540687501430511\n",
      "Epoch: 71/100 | step: 251/422 | loss: 0.10978250950574875\n",
      "Epoch: 71/100 | step: 252/422 | loss: 0.15414729714393616\n",
      "Epoch: 71/100 | step: 253/422 | loss: 0.11331404745578766\n",
      "Epoch: 71/100 | step: 254/422 | loss: 0.1081072986125946\n",
      "Epoch: 71/100 | step: 255/422 | loss: 0.08223029226064682\n",
      "Epoch: 71/100 | step: 256/422 | loss: 0.20255421102046967\n",
      "Epoch: 71/100 | step: 257/422 | loss: 0.3062915503978729\n",
      "Epoch: 71/100 | step: 258/422 | loss: 0.19654640555381775\n",
      "Epoch: 71/100 | step: 259/422 | loss: 0.15841016173362732\n",
      "Epoch: 71/100 | step: 260/422 | loss: 0.14479579031467438\n",
      "Epoch: 71/100 | step: 261/422 | loss: 0.08181947469711304\n",
      "Epoch: 71/100 | step: 262/422 | loss: 0.08170623332262039\n",
      "Epoch: 71/100 | step: 263/422 | loss: 0.1851392239332199\n",
      "Epoch: 71/100 | step: 264/422 | loss: 0.1093050092458725\n",
      "Epoch: 71/100 | step: 265/422 | loss: 0.6909388303756714\n",
      "Epoch: 71/100 | step: 266/422 | loss: 0.11101631820201874\n",
      "Epoch: 71/100 | step: 267/422 | loss: 0.25106358528137207\n",
      "Epoch: 71/100 | step: 268/422 | loss: 0.4130563735961914\n",
      "Epoch: 71/100 | step: 269/422 | loss: 0.30918872356414795\n",
      "Epoch: 71/100 | step: 270/422 | loss: 0.12033692002296448\n",
      "Epoch: 71/100 | step: 271/422 | loss: 0.11828028410673141\n",
      "Epoch: 71/100 | step: 272/422 | loss: 0.29212048649787903\n",
      "Epoch: 71/100 | step: 273/422 | loss: 0.1818752884864807\n",
      "Epoch: 71/100 | step: 274/422 | loss: 0.15897804498672485\n",
      "Epoch: 71/100 | step: 275/422 | loss: 0.10426745563745499\n",
      "Epoch: 71/100 | step: 276/422 | loss: 0.09351220726966858\n",
      "Epoch: 71/100 | step: 277/422 | loss: 0.1380579173564911\n",
      "Epoch: 71/100 | step: 278/422 | loss: 0.33938440680503845\n",
      "Epoch: 71/100 | step: 279/422 | loss: 0.05879664793610573\n",
      "Epoch: 71/100 | step: 280/422 | loss: 0.04568449407815933\n",
      "Epoch: 71/100 | step: 281/422 | loss: 0.13430137932300568\n",
      "Epoch: 71/100 | step: 282/422 | loss: 0.1109246239066124\n",
      "Epoch: 71/100 | step: 283/422 | loss: 0.08887840807437897\n",
      "Epoch: 71/100 | step: 284/422 | loss: 0.17884735763072968\n",
      "Epoch: 71/100 | step: 285/422 | loss: 0.25863829255104065\n",
      "Epoch: 71/100 | step: 286/422 | loss: 0.4550589323043823\n",
      "Epoch: 71/100 | step: 287/422 | loss: 0.23694579303264618\n",
      "Epoch: 71/100 | step: 288/422 | loss: 0.2865700125694275\n",
      "Epoch: 71/100 | step: 289/422 | loss: 0.30827245116233826\n",
      "Epoch: 71/100 | step: 290/422 | loss: 0.15881456434726715\n",
      "Epoch: 71/100 | step: 291/422 | loss: 0.33347150683403015\n",
      "Epoch: 71/100 | step: 292/422 | loss: 0.14636166393756866\n",
      "Epoch: 71/100 | step: 293/422 | loss: 0.09230148792266846\n",
      "Epoch: 71/100 | step: 294/422 | loss: 0.0800786167383194\n",
      "Epoch: 71/100 | step: 295/422 | loss: 0.44178998470306396\n",
      "Epoch: 71/100 | step: 296/422 | loss: 0.2608366310596466\n",
      "Epoch: 71/100 | step: 297/422 | loss: 0.120630644261837\n",
      "Epoch: 71/100 | step: 298/422 | loss: 0.08360717445611954\n",
      "Epoch: 71/100 | step: 299/422 | loss: 0.07757943868637085\n",
      "Epoch: 71/100 | step: 300/422 | loss: 0.07817146182060242\n",
      "Epoch: 71/100 | step: 301/422 | loss: 0.15162393450737\n",
      "Epoch: 71/100 | step: 302/422 | loss: 0.06901489943265915\n",
      "Epoch: 71/100 | step: 303/422 | loss: 0.08153035491704941\n",
      "Epoch: 71/100 | step: 304/422 | loss: 0.15208584070205688\n",
      "Epoch: 71/100 | step: 305/422 | loss: 0.21622128784656525\n",
      "Epoch: 71/100 | step: 306/422 | loss: 0.4193117022514343\n",
      "Epoch: 71/100 | step: 307/422 | loss: 0.1792205423116684\n",
      "Epoch: 71/100 | step: 308/422 | loss: 0.11885260790586472\n",
      "Epoch: 71/100 | step: 309/422 | loss: 0.1388186365365982\n",
      "Epoch: 71/100 | step: 310/422 | loss: 0.06786941736936569\n",
      "Epoch: 71/100 | step: 311/422 | loss: 0.08817785978317261\n",
      "Epoch: 71/100 | step: 312/422 | loss: 0.15700553357601166\n",
      "Epoch: 71/100 | step: 313/422 | loss: 0.12164925038814545\n",
      "Epoch: 71/100 | step: 314/422 | loss: 0.13244059681892395\n",
      "Epoch: 71/100 | step: 315/422 | loss: 0.06912700086832047\n",
      "Epoch: 71/100 | step: 316/422 | loss: 0.07663409411907196\n",
      "Epoch: 71/100 | step: 317/422 | loss: 0.3818185329437256\n",
      "Epoch: 71/100 | step: 318/422 | loss: 0.10638714581727982\n",
      "Epoch: 71/100 | step: 319/422 | loss: 0.07039818167686462\n",
      "Epoch: 71/100 | step: 320/422 | loss: 0.08401759713888168\n",
      "Epoch: 71/100 | step: 321/422 | loss: 0.16199563443660736\n",
      "Epoch: 71/100 | step: 322/422 | loss: 0.3160642981529236\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 72/100 | step: 1/422 | loss: 0.6831629276275635\n",
      "Epoch: 72/100 | step: 2/422 | loss: 0.6194601058959961\n",
      "Epoch: 72/100 | step: 3/422 | loss: 0.08451751619577408\n",
      "Epoch: 72/100 | step: 4/422 | loss: 0.2874395549297333\n",
      "Epoch: 72/100 | step: 5/422 | loss: 0.12295078486204147\n",
      "Epoch: 72/100 | step: 6/422 | loss: 0.14069829881191254\n",
      "Epoch: 72/100 | step: 7/422 | loss: 0.21702510118484497\n",
      "Epoch: 72/100 | step: 8/422 | loss: 0.10880806297063828\n",
      "Epoch: 72/100 | step: 9/422 | loss: 0.12147203832864761\n",
      "Epoch: 72/100 | step: 10/422 | loss: 0.261349081993103\n",
      "Epoch: 72/100 | step: 11/422 | loss: 0.10288742929697037\n",
      "Epoch: 72/100 | step: 12/422 | loss: 0.11816948652267456\n",
      "Epoch: 72/100 | step: 13/422 | loss: 0.08917266130447388\n",
      "Epoch: 72/100 | step: 14/422 | loss: 0.09149463474750519\n",
      "Epoch: 72/100 | step: 15/422 | loss: 0.03969024866819382\n",
      "Epoch: 72/100 | step: 16/422 | loss: 0.05122290179133415\n",
      "Epoch: 72/100 | step: 17/422 | loss: 0.06381276249885559\n",
      "Epoch: 72/100 | step: 18/422 | loss: 0.05874140188097954\n",
      "Epoch: 72/100 | step: 19/422 | loss: 0.0704675167798996\n",
      "Epoch: 72/100 | step: 20/422 | loss: 0.058061979711055756\n",
      "Epoch: 72/100 | step: 21/422 | loss: 0.16169723868370056\n",
      "Epoch: 72/100 | step: 22/422 | loss: 0.09525667876005173\n",
      "Epoch: 72/100 | step: 23/422 | loss: 0.08584021776914597\n",
      "Epoch: 72/100 | step: 24/422 | loss: 0.2591399550437927\n",
      "Epoch: 72/100 | step: 25/422 | loss: 0.06649520993232727\n",
      "Epoch: 72/100 | step: 26/422 | loss: 0.11450982838869095\n",
      "Epoch: 72/100 | step: 27/422 | loss: 0.06291510164737701\n",
      "Epoch: 72/100 | step: 28/422 | loss: 0.1348346769809723\n",
      "Epoch: 72/100 | step: 29/422 | loss: 0.21623988449573517\n",
      "Epoch: 72/100 | step: 30/422 | loss: 0.20607711374759674\n",
      "Epoch: 72/100 | step: 31/422 | loss: 0.15719346702098846\n",
      "Epoch: 72/100 | step: 32/422 | loss: 0.04676242545247078\n",
      "Epoch: 72/100 | step: 33/422 | loss: 0.0758528783917427\n",
      "Epoch: 72/100 | step: 34/422 | loss: 0.0549788698554039\n",
      "Epoch: 72/100 | step: 35/422 | loss: 0.10503502190113068\n",
      "Epoch: 72/100 | step: 36/422 | loss: 0.35657039284706116\n",
      "Epoch: 72/100 | step: 37/422 | loss: 0.19263698160648346\n",
      "Epoch: 72/100 | step: 38/422 | loss: 0.14931800961494446\n",
      "Epoch: 72/100 | step: 39/422 | loss: 0.22685633599758148\n",
      "Epoch: 72/100 | step: 40/422 | loss: 0.042331892997026443\n",
      "Epoch: 72/100 | step: 41/422 | loss: 0.199575275182724\n",
      "Epoch: 72/100 | step: 42/422 | loss: 0.04706139862537384\n",
      "Epoch: 72/100 | step: 43/422 | loss: 0.33800604939460754\n",
      "Epoch: 72/100 | step: 44/422 | loss: 0.07934476435184479\n",
      "Epoch: 72/100 | step: 45/422 | loss: 0.04438280686736107\n",
      "Epoch: 72/100 | step: 46/422 | loss: 0.061392296105623245\n",
      "Epoch: 72/100 | step: 47/422 | loss: 0.0682649090886116\n",
      "Epoch: 72/100 | step: 48/422 | loss: 0.07075034826993942\n",
      "Epoch: 72/100 | step: 49/422 | loss: 0.06186108663678169\n",
      "Epoch: 72/100 | step: 50/422 | loss: 0.05604129657149315\n",
      "Epoch: 72/100 | step: 51/422 | loss: 0.0650741308927536\n",
      "Epoch: 72/100 | step: 52/422 | loss: 0.07958704978227615\n",
      "Epoch: 72/100 | step: 53/422 | loss: 0.1386604905128479\n",
      "Epoch: 72/100 | step: 54/422 | loss: 0.37365788221359253\n",
      "Epoch: 72/100 | step: 55/422 | loss: 0.09978793561458588\n",
      "Epoch: 72/100 | step: 56/422 | loss: 0.40117886662483215\n",
      "Epoch: 72/100 | step: 57/422 | loss: 0.09061072021722794\n",
      "Epoch: 72/100 | step: 58/422 | loss: 0.7223744988441467\n",
      "Epoch: 72/100 | step: 59/422 | loss: 0.09337440878152847\n",
      "Epoch: 72/100 | step: 60/422 | loss: 0.17739622294902802\n",
      "Epoch: 72/100 | step: 61/422 | loss: 0.27347680926322937\n",
      "Epoch: 72/100 | step: 62/422 | loss: 0.11584082990884781\n",
      "Epoch: 72/100 | step: 63/422 | loss: 0.104742132127285\n",
      "Epoch: 72/100 | step: 64/422 | loss: 0.074635811150074\n",
      "Epoch: 72/100 | step: 65/422 | loss: 0.06848277896642685\n",
      "Epoch: 72/100 | step: 66/422 | loss: 0.0531635545194149\n",
      "Epoch: 72/100 | step: 67/422 | loss: 0.1809132993221283\n",
      "Epoch: 72/100 | step: 68/422 | loss: 0.08616506308317184\n",
      "Epoch: 72/100 | step: 69/422 | loss: 0.15810807049274445\n",
      "Epoch: 72/100 | step: 70/422 | loss: 0.058598749339580536\n",
      "Epoch: 72/100 | step: 71/422 | loss: 0.11283691227436066\n",
      "Epoch: 72/100 | step: 72/422 | loss: 0.030870921909809113\n",
      "Epoch: 72/100 | step: 73/422 | loss: 0.07126493752002716\n",
      "Epoch: 72/100 | step: 74/422 | loss: 0.09605275094509125\n",
      "Epoch: 72/100 | step: 75/422 | loss: 0.07137423753738403\n",
      "Epoch: 72/100 | step: 76/422 | loss: 0.09121192246675491\n",
      "Epoch: 72/100 | step: 77/422 | loss: 0.06975701451301575\n",
      "Epoch: 72/100 | step: 78/422 | loss: 0.13928423821926117\n",
      "Epoch: 72/100 | step: 79/422 | loss: 0.08395460247993469\n",
      "Epoch: 72/100 | step: 80/422 | loss: 0.06127893179655075\n",
      "Epoch: 72/100 | step: 81/422 | loss: 0.03802558034658432\n",
      "Epoch: 72/100 | step: 82/422 | loss: 0.11144645512104034\n",
      "Epoch: 72/100 | step: 83/422 | loss: 0.04782726243138313\n",
      "Epoch: 72/100 | step: 84/422 | loss: 0.08003837615251541\n",
      "Epoch: 72/100 | step: 85/422 | loss: 0.11371412873268127\n",
      "Epoch: 72/100 | step: 86/422 | loss: 0.06875308603048325\n",
      "Epoch: 72/100 | step: 87/422 | loss: 0.07181306928396225\n",
      "Epoch: 72/100 | step: 88/422 | loss: 0.06540112942457199\n",
      "Epoch: 72/100 | step: 89/422 | loss: 0.03370169550180435\n",
      "Epoch: 72/100 | step: 90/422 | loss: 0.037793658673763275\n",
      "Epoch: 72/100 | step: 91/422 | loss: 0.07859191298484802\n",
      "Epoch: 72/100 | step: 92/422 | loss: 0.06728976964950562\n",
      "Epoch: 72/100 | step: 93/422 | loss: 0.08590206503868103\n",
      "Epoch: 72/100 | step: 94/422 | loss: 0.05707654729485512\n",
      "Epoch: 72/100 | step: 95/422 | loss: 0.04875511676073074\n",
      "Epoch: 72/100 | step: 96/422 | loss: 0.04538048431277275\n",
      "Epoch: 72/100 | step: 97/422 | loss: 0.05122986435890198\n",
      "Epoch: 72/100 | step: 98/422 | loss: 0.04126034304499626\n",
      "Epoch: 72/100 | step: 99/422 | loss: 0.06659475713968277\n",
      "Epoch: 72/100 | step: 100/422 | loss: 0.047025762498378754\n",
      "Epoch: 72/100 | step: 101/422 | loss: 0.041675351560115814\n",
      "Epoch: 72/100 | step: 102/422 | loss: 0.0757220908999443\n",
      "Epoch: 72/100 | step: 103/422 | loss: 0.04291119426488876\n",
      "Epoch: 72/100 | step: 104/422 | loss: 0.041196681559085846\n",
      "Epoch: 72/100 | step: 105/422 | loss: 0.13442087173461914\n",
      "Epoch: 72/100 | step: 106/422 | loss: 0.06639786064624786\n",
      "Epoch: 72/100 | step: 107/422 | loss: 0.35029515624046326\n",
      "Epoch: 72/100 | step: 108/422 | loss: 0.15332713723182678\n",
      "Epoch: 72/100 | step: 109/422 | loss: 0.24419912695884705\n",
      "Epoch: 72/100 | step: 110/422 | loss: 0.1620519459247589\n",
      "Epoch: 72/100 | step: 111/422 | loss: 0.12247557193040848\n",
      "Epoch: 72/100 | step: 112/422 | loss: 0.593205988407135\n",
      "Epoch: 72/100 | step: 113/422 | loss: 0.11288601160049438\n",
      "Epoch: 72/100 | step: 114/422 | loss: 0.029269183054566383\n",
      "Epoch: 72/100 | step: 115/422 | loss: 0.10894414782524109\n",
      "Epoch: 72/100 | step: 116/422 | loss: 0.04884835332632065\n",
      "Epoch: 72/100 | step: 117/422 | loss: 0.0815436914563179\n",
      "Epoch: 72/100 | step: 118/422 | loss: 0.19550985097885132\n",
      "Epoch: 72/100 | step: 119/422 | loss: 0.06111796200275421\n",
      "Epoch: 72/100 | step: 120/422 | loss: 0.15331605076789856\n",
      "Epoch: 72/100 | step: 121/422 | loss: 0.15167279541492462\n",
      "Epoch: 72/100 | step: 122/422 | loss: 0.03422790765762329\n",
      "Epoch: 72/100 | step: 123/422 | loss: 0.10010676085948944\n",
      "Epoch: 72/100 | step: 124/422 | loss: 0.24476787447929382\n",
      "Epoch: 72/100 | step: 125/422 | loss: 0.15070071816444397\n",
      "Epoch: 72/100 | step: 126/422 | loss: 0.12350782006978989\n",
      "Epoch: 72/100 | step: 127/422 | loss: 0.5257357358932495\n",
      "Epoch: 72/100 | step: 128/422 | loss: 0.4609029293060303\n",
      "Epoch: 72/100 | step: 129/422 | loss: 0.19829970598220825\n",
      "Epoch: 72/100 | step: 130/422 | loss: 0.20143312215805054\n",
      "Epoch: 72/100 | step: 131/422 | loss: 0.21902738511562347\n",
      "Epoch: 72/100 | step: 132/422 | loss: 0.163010373711586\n",
      "Epoch: 72/100 | step: 133/422 | loss: 0.05390220880508423\n",
      "Epoch: 72/100 | step: 134/422 | loss: 0.05146879702806473\n",
      "Epoch: 72/100 | step: 135/422 | loss: 0.04291693493723869\n",
      "Epoch: 72/100 | step: 136/422 | loss: 0.0773148238658905\n",
      "Epoch: 72/100 | step: 137/422 | loss: 0.12074663490056992\n",
      "Epoch: 72/100 | step: 138/422 | loss: 0.12574359774589539\n",
      "Epoch: 72/100 | step: 139/422 | loss: 0.0856853798031807\n",
      "Epoch: 72/100 | step: 140/422 | loss: 0.11323677003383636\n",
      "Epoch: 72/100 | step: 141/422 | loss: 0.18301384150981903\n",
      "Epoch: 72/100 | step: 142/422 | loss: 0.13856402039527893\n",
      "Epoch: 72/100 | step: 143/422 | loss: 0.29392388463020325\n",
      "Epoch: 72/100 | step: 144/422 | loss: 0.3411198556423187\n",
      "Epoch: 72/100 | step: 145/422 | loss: 0.10914014279842377\n",
      "Epoch: 72/100 | step: 146/422 | loss: 0.19206887483596802\n",
      "Epoch: 72/100 | step: 147/422 | loss: 0.1581822633743286\n",
      "Epoch: 72/100 | step: 148/422 | loss: 0.18686015903949738\n",
      "Epoch: 72/100 | step: 149/422 | loss: 0.3753378987312317\n",
      "Epoch: 72/100 | step: 150/422 | loss: 0.17583103477954865\n",
      "Epoch: 72/100 | step: 151/422 | loss: 0.17192909121513367\n",
      "Epoch: 72/100 | step: 152/422 | loss: 0.11040171980857849\n",
      "Epoch: 72/100 | step: 153/422 | loss: 0.08594106882810593\n",
      "Epoch: 72/100 | step: 154/422 | loss: 0.1276434361934662\n",
      "Epoch: 72/100 | step: 155/422 | loss: 0.39281052350997925\n",
      "Epoch: 72/100 | step: 156/422 | loss: 0.15349127352237701\n",
      "Epoch: 72/100 | step: 157/422 | loss: 0.26891028881073\n",
      "Epoch: 72/100 | step: 158/422 | loss: 0.04630402848124504\n",
      "Epoch: 72/100 | step: 159/422 | loss: 0.051855020225048065\n",
      "Epoch: 72/100 | step: 160/422 | loss: 0.17236332595348358\n",
      "Epoch: 72/100 | step: 161/422 | loss: 0.2221095710992813\n",
      "Epoch: 72/100 | step: 162/422 | loss: 0.20303699374198914\n",
      "Epoch: 72/100 | step: 163/422 | loss: 0.20812417566776276\n",
      "Epoch: 72/100 | step: 164/422 | loss: 0.08172672241926193\n",
      "Epoch: 72/100 | step: 165/422 | loss: 0.08010264486074448\n",
      "Epoch: 72/100 | step: 166/422 | loss: 0.10278256982564926\n",
      "Epoch: 72/100 | step: 167/422 | loss: 0.05209241434931755\n",
      "Epoch: 72/100 | step: 168/422 | loss: 0.0813032016158104\n",
      "Epoch: 72/100 | step: 169/422 | loss: 0.07868001610040665\n",
      "Epoch: 72/100 | step: 170/422 | loss: 0.06642915308475494\n",
      "Epoch: 72/100 | step: 171/422 | loss: 0.03115578182041645\n",
      "Epoch: 72/100 | step: 172/422 | loss: 0.12577083706855774\n",
      "Epoch: 72/100 | step: 173/422 | loss: 0.16734659671783447\n",
      "Epoch: 72/100 | step: 174/422 | loss: 0.1355169713497162\n",
      "Epoch: 72/100 | step: 175/422 | loss: 0.1094500869512558\n",
      "Epoch: 72/100 | step: 176/422 | loss: 0.0574609637260437\n",
      "Epoch: 72/100 | step: 177/422 | loss: 0.07657387107610703\n",
      "Epoch: 72/100 | step: 178/422 | loss: 0.04676671326160431\n",
      "Epoch: 72/100 | step: 179/422 | loss: 0.040472064167261124\n",
      "Epoch: 72/100 | step: 180/422 | loss: 0.02611210197210312\n",
      "Epoch: 72/100 | step: 181/422 | loss: 0.04502763971686363\n",
      "Epoch: 72/100 | step: 182/422 | loss: 0.3508528470993042\n",
      "Epoch: 72/100 | step: 183/422 | loss: 0.2700659930706024\n",
      "Epoch: 72/100 | step: 184/422 | loss: 0.037983238697052\n",
      "Epoch: 72/100 | step: 185/422 | loss: 0.067244753241539\n",
      "Epoch: 72/100 | step: 186/422 | loss: 0.03889761120080948\n",
      "Epoch: 72/100 | step: 187/422 | loss: 0.1375032216310501\n",
      "Epoch: 72/100 | step: 188/422 | loss: 0.07989047467708588\n",
      "Epoch: 72/100 | step: 189/422 | loss: 0.18134568631649017\n",
      "Epoch: 72/100 | step: 190/422 | loss: 0.2012483924627304\n",
      "Epoch: 72/100 | step: 191/422 | loss: 0.5153881311416626\n",
      "Epoch: 72/100 | step: 192/422 | loss: 0.08806513249874115\n",
      "Epoch: 72/100 | step: 193/422 | loss: 0.22917316854000092\n",
      "Epoch: 72/100 | step: 194/422 | loss: 0.173981174826622\n",
      "Epoch: 72/100 | step: 195/422 | loss: 0.16056783497333527\n",
      "Epoch: 72/100 | step: 196/422 | loss: 0.13265188038349152\n",
      "Epoch: 72/100 | step: 197/422 | loss: 0.12052436918020248\n",
      "Epoch: 72/100 | step: 198/422 | loss: 0.09600593149662018\n",
      "Epoch: 72/100 | step: 199/422 | loss: 0.10465966910123825\n",
      "Epoch: 72/100 | step: 200/422 | loss: 0.0891934260725975\n",
      "Epoch: 72/100 | step: 201/422 | loss: 0.07550652325153351\n",
      "Epoch: 72/100 | step: 202/422 | loss: 0.051980145275592804\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 73/100 | step: 1/422 | loss: 0.04640859365463257\n",
      "Epoch: 73/100 | step: 2/422 | loss: 0.06205230578780174\n",
      "Epoch: 73/100 | step: 3/422 | loss: 0.09305132925510406\n",
      "Epoch: 73/100 | step: 4/422 | loss: 0.0325542688369751\n",
      "Epoch: 73/100 | step: 5/422 | loss: 0.06694469600915909\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 74/100 | step: 1/422 | loss: 0.040897440165281296\n",
      "Epoch: 74/100 | step: 2/422 | loss: 0.060990966856479645\n",
      "Epoch: 74/100 | step: 3/422 | loss: 0.04751737788319588\n",
      "Epoch: 74/100 | step: 4/422 | loss: 0.044396694749593735\n",
      "Epoch: 74/100 | step: 5/422 | loss: 0.050263334065675735\n",
      "Epoch: 74/100 | step: 6/422 | loss: 0.10656095296144485\n",
      "Epoch: 74/100 | step: 7/422 | loss: 0.035868462175130844\n",
      "Epoch: 74/100 | step: 8/422 | loss: 0.0740935206413269\n",
      "Epoch: 74/100 | step: 9/422 | loss: 0.053181909024715424\n",
      "Epoch: 74/100 | step: 10/422 | loss: 0.07387277483940125\n",
      "Epoch: 74/100 | step: 11/422 | loss: 0.02362685278058052\n",
      "Epoch: 74/100 | step: 12/422 | loss: 0.061348192393779755\n",
      "Epoch: 74/100 | step: 13/422 | loss: 0.03372425585985184\n",
      "Epoch: 74/100 | step: 14/422 | loss: 0.06847530603408813\n",
      "Epoch: 74/100 | step: 15/422 | loss: 0.04822992905974388\n",
      "Epoch: 74/100 | step: 16/422 | loss: 0.08451772481203079\n",
      "Epoch: 74/100 | step: 17/422 | loss: 0.042293399572372437\n",
      "Epoch: 74/100 | step: 18/422 | loss: 0.02307288348674774\n",
      "Epoch: 74/100 | step: 19/422 | loss: 0.05083991214632988\n",
      "Epoch: 74/100 | step: 20/422 | loss: 0.03477088734507561\n",
      "Epoch: 74/100 | step: 21/422 | loss: 0.07765776664018631\n",
      "Epoch: 74/100 | step: 22/422 | loss: 0.11038129031658173\n",
      "Epoch: 74/100 | step: 23/422 | loss: 0.04388711228966713\n",
      "Epoch: 74/100 | step: 24/422 | loss: 0.07116030156612396\n",
      "Epoch: 74/100 | step: 25/422 | loss: 0.019901033490896225\n",
      "Epoch: 74/100 | step: 26/422 | loss: 0.04464901611208916\n",
      "Epoch: 74/100 | step: 27/422 | loss: 0.03184165433049202\n",
      "Epoch: 74/100 | step: 28/422 | loss: 0.06433147192001343\n",
      "Epoch: 74/100 | step: 29/422 | loss: 0.03739497810602188\n",
      "Epoch: 74/100 | step: 30/422 | loss: 0.05660730600357056\n",
      "Epoch: 74/100 | step: 31/422 | loss: 0.04380118101835251\n",
      "Epoch: 74/100 | step: 32/422 | loss: 0.09131870418787003\n",
      "Epoch: 74/100 | step: 33/422 | loss: 0.04266192018985748\n",
      "Epoch: 74/100 | step: 34/422 | loss: 0.0571785569190979\n",
      "Epoch: 74/100 | step: 35/422 | loss: 0.04042012616991997\n",
      "Epoch: 74/100 | step: 36/422 | loss: 0.049807824194431305\n",
      "Epoch: 74/100 | step: 37/422 | loss: 0.06926432251930237\n",
      "Epoch: 74/100 | step: 38/422 | loss: 0.042264968156814575\n",
      "Epoch: 74/100 | step: 39/422 | loss: 0.03217746317386627\n",
      "Epoch: 74/100 | step: 40/422 | loss: 0.028527757152915\n",
      "Epoch: 74/100 | step: 41/422 | loss: 0.14528799057006836\n",
      "Epoch: 74/100 | step: 42/422 | loss: 0.03776488080620766\n",
      "Epoch: 74/100 | step: 43/422 | loss: 0.055453769862651825\n",
      "Epoch: 74/100 | step: 44/422 | loss: 0.026337305083870888\n",
      "Epoch: 74/100 | step: 45/422 | loss: 0.19093412160873413\n",
      "Epoch: 74/100 | step: 46/422 | loss: 0.06821412593126297\n",
      "Epoch: 74/100 | step: 47/422 | loss: 0.06866932660341263\n",
      "Epoch: 74/100 | step: 48/422 | loss: 0.025269079953432083\n",
      "Epoch: 74/100 | step: 49/422 | loss: 0.034965481609106064\n",
      "Epoch: 74/100 | step: 50/422 | loss: 0.09830513596534729\n",
      "Epoch: 74/100 | step: 51/422 | loss: 0.06622012704610825\n",
      "Epoch: 74/100 | step: 52/422 | loss: 0.04284258931875229\n",
      "Epoch: 74/100 | step: 53/422 | loss: 0.02741221711039543\n",
      "Epoch: 74/100 | step: 54/422 | loss: 0.06935714930295944\n",
      "Epoch: 74/100 | step: 55/422 | loss: 0.05763617157936096\n",
      "Epoch: 74/100 | step: 56/422 | loss: 0.051735952496528625\n",
      "Epoch: 74/100 | step: 57/422 | loss: 0.0844760611653328\n",
      "Epoch: 74/100 | step: 58/422 | loss: 0.12213484197854996\n",
      "Epoch: 74/100 | step: 59/422 | loss: 0.07009439170360565\n",
      "Epoch: 74/100 | step: 60/422 | loss: 0.06155377998948097\n",
      "Epoch: 74/100 | step: 61/422 | loss: 0.13648006319999695\n",
      "Epoch: 74/100 | step: 62/422 | loss: 0.10209332406520844\n",
      "Epoch: 74/100 | step: 63/422 | loss: 0.09751613438129425\n",
      "Epoch: 74/100 | step: 64/422 | loss: 0.1113344058394432\n",
      "Epoch: 74/100 | step: 65/422 | loss: 0.08072572201490402\n",
      "Epoch: 74/100 | step: 66/422 | loss: 0.07271382957696915\n",
      "Epoch: 74/100 | step: 67/422 | loss: 0.2079039365053177\n",
      "Epoch: 74/100 | step: 68/422 | loss: 0.10174144804477692\n",
      "Epoch: 74/100 | step: 69/422 | loss: 0.2633722722530365\n",
      "Epoch: 74/100 | step: 70/422 | loss: 0.05334153398871422\n",
      "Epoch: 74/100 | step: 71/422 | loss: 0.3373420536518097\n",
      "Epoch: 74/100 | step: 72/422 | loss: 0.4113273620605469\n",
      "Epoch: 74/100 | step: 73/422 | loss: 0.17286759614944458\n",
      "Epoch: 74/100 | step: 74/422 | loss: 0.47602540254592896\n",
      "Epoch: 74/100 | step: 75/422 | loss: 0.09459470957517624\n",
      "Epoch: 74/100 | step: 76/422 | loss: 0.6542438864707947\n",
      "Epoch: 74/100 | step: 77/422 | loss: 0.16597652435302734\n",
      "Epoch: 74/100 | step: 78/422 | loss: 0.674326479434967\n",
      "Epoch: 74/100 | step: 79/422 | loss: 0.09644120186567307\n",
      "Epoch: 74/100 | step: 80/422 | loss: 0.20562590658664703\n",
      "Epoch: 74/100 | step: 81/422 | loss: 0.22703780233860016\n",
      "Epoch: 74/100 | step: 82/422 | loss: 0.06438424438238144\n",
      "Epoch: 74/100 | step: 83/422 | loss: 0.06661280989646912\n",
      "Epoch: 74/100 | step: 84/422 | loss: 0.321869820356369\n",
      "Epoch: 74/100 | step: 85/422 | loss: 0.25743353366851807\n",
      "Epoch: 74/100 | step: 86/422 | loss: 0.13006845116615295\n",
      "Epoch: 74/100 | step: 87/422 | loss: 0.07392700761556625\n",
      "Epoch: 74/100 | step: 88/422 | loss: 0.07125303894281387\n",
      "Epoch: 74/100 | step: 89/422 | loss: 0.03932787477970123\n",
      "Epoch: 74/100 | step: 90/422 | loss: 0.31894442439079285\n",
      "Epoch: 74/100 | step: 91/422 | loss: 0.028668716549873352\n",
      "Epoch: 74/100 | step: 92/422 | loss: 0.10485021024942398\n",
      "Epoch: 74/100 | step: 93/422 | loss: 0.05858738720417023\n",
      "Epoch: 74/100 | step: 94/422 | loss: 0.12051533907651901\n",
      "Epoch: 74/100 | step: 95/422 | loss: 0.06961386650800705\n",
      "Epoch: 74/100 | step: 96/422 | loss: 0.05801922082901001\n",
      "Epoch: 74/100 | step: 97/422 | loss: 0.1689988374710083\n",
      "Epoch: 74/100 | step: 98/422 | loss: 0.10208594799041748\n",
      "Epoch: 74/100 | step: 99/422 | loss: 0.08842340856790543\n",
      "Epoch: 74/100 | step: 100/422 | loss: 0.11741769313812256\n",
      "Epoch: 74/100 | step: 101/422 | loss: 0.04070074111223221\n",
      "Epoch: 74/100 | step: 102/422 | loss: 0.19957338273525238\n",
      "Epoch: 74/100 | step: 103/422 | loss: 0.0869794487953186\n",
      "Epoch: 74/100 | step: 104/422 | loss: 0.21892496943473816\n",
      "Epoch: 74/100 | step: 105/422 | loss: 0.057716004550457\n",
      "Epoch: 74/100 | step: 106/422 | loss: 0.16137303411960602\n",
      "Epoch: 74/100 | step: 107/422 | loss: 0.049769315868616104\n",
      "Epoch: 74/100 | step: 108/422 | loss: 0.13733655214309692\n",
      "Epoch: 74/100 | step: 109/422 | loss: 0.12136441469192505\n",
      "Epoch: 74/100 | step: 110/422 | loss: 0.040138501673936844\n",
      "Epoch: 74/100 | step: 111/422 | loss: 0.04157606139779091\n",
      "Epoch: 74/100 | step: 112/422 | loss: 0.08031170070171356\n",
      "Epoch: 74/100 | step: 113/422 | loss: 0.066790871322155\n",
      "Epoch: 74/100 | step: 114/422 | loss: 0.12179764360189438\n",
      "Epoch: 74/100 | step: 115/422 | loss: 0.055711884051561356\n",
      "Epoch: 74/100 | step: 116/422 | loss: 0.0276214387267828\n",
      "Epoch: 74/100 | step: 117/422 | loss: 0.0314507931470871\n",
      "Epoch: 74/100 | step: 118/422 | loss: 0.03524221479892731\n",
      "Epoch: 74/100 | step: 119/422 | loss: 0.042693234980106354\n",
      "Epoch: 74/100 | step: 120/422 | loss: 0.07999114692211151\n",
      "Epoch: 74/100 | step: 121/422 | loss: 0.08573760837316513\n",
      "Epoch: 74/100 | step: 122/422 | loss: 0.15223060548305511\n",
      "Epoch: 74/100 | step: 123/422 | loss: 0.07515490800142288\n",
      "Epoch: 74/100 | step: 124/422 | loss: 0.047269150614738464\n",
      "Epoch: 74/100 | step: 125/422 | loss: 0.10872351378202438\n",
      "Epoch: 74/100 | step: 126/422 | loss: 0.13720069825649261\n",
      "Epoch: 74/100 | step: 127/422 | loss: 0.10437342524528503\n",
      "Epoch: 74/100 | step: 128/422 | loss: 0.05478497967123985\n",
      "Epoch: 74/100 | step: 129/422 | loss: 0.03552422299981117\n",
      "Epoch: 74/100 | step: 130/422 | loss: 0.032780785113573074\n",
      "Epoch: 74/100 | step: 131/422 | loss: 0.03533388674259186\n",
      "Epoch: 74/100 | step: 132/422 | loss: 0.028962397947907448\n",
      "Epoch: 74/100 | step: 133/422 | loss: 0.03340098261833191\n",
      "Epoch: 74/100 | step: 134/422 | loss: 0.06704045832157135\n",
      "Epoch: 74/100 | step: 135/422 | loss: 0.068752221763134\n",
      "Epoch: 74/100 | step: 136/422 | loss: 0.07963495701551437\n",
      "Epoch: 74/100 | step: 137/422 | loss: 0.04591292515397072\n",
      "Epoch: 74/100 | step: 138/422 | loss: 0.04249792546033859\n",
      "Epoch: 74/100 | step: 139/422 | loss: 0.13576605916023254\n",
      "Epoch: 74/100 | step: 140/422 | loss: 0.047436270862817764\n",
      "Epoch: 74/100 | step: 141/422 | loss: 0.2848585844039917\n",
      "Epoch: 74/100 | step: 142/422 | loss: 0.10797832161188126\n",
      "Epoch: 74/100 | step: 143/422 | loss: 0.02970670349895954\n",
      "Epoch: 74/100 | step: 144/422 | loss: 0.05237944424152374\n",
      "Epoch: 74/100 | step: 145/422 | loss: 0.09222277253866196\n",
      "Epoch: 74/100 | step: 146/422 | loss: 0.050377290695905685\n",
      "Epoch: 74/100 | step: 147/422 | loss: 0.10562385618686676\n",
      "Epoch: 74/100 | step: 148/422 | loss: 0.055594343692064285\n",
      "Epoch: 74/100 | step: 149/422 | loss: 0.02985803224146366\n",
      "Epoch: 74/100 | step: 150/422 | loss: 0.09314732998609543\n",
      "Epoch: 74/100 | step: 151/422 | loss: 0.12663453817367554\n",
      "Epoch: 74/100 | step: 152/422 | loss: 0.03405293449759483\n",
      "Epoch: 74/100 | step: 153/422 | loss: 0.1581798791885376\n",
      "Epoch: 74/100 | step: 154/422 | loss: 0.17455869913101196\n",
      "Epoch: 74/100 | step: 155/422 | loss: 0.06809426844120026\n",
      "Epoch: 74/100 | step: 156/422 | loss: 0.14553165435791016\n",
      "Epoch: 74/100 | step: 157/422 | loss: 0.12714707851409912\n",
      "Epoch: 74/100 | step: 158/422 | loss: 0.039615996181964874\n",
      "Epoch: 74/100 | step: 159/422 | loss: 0.0778193473815918\n",
      "Epoch: 74/100 | step: 160/422 | loss: 0.07252883166074753\n",
      "Epoch: 74/100 | step: 161/422 | loss: 0.026964489370584488\n",
      "Epoch: 74/100 | step: 162/422 | loss: 0.17520859837532043\n",
      "Epoch: 74/100 | step: 163/422 | loss: 0.18389806151390076\n",
      "Epoch: 74/100 | step: 164/422 | loss: 0.1663445234298706\n",
      "Epoch: 74/100 | step: 165/422 | loss: 0.049678727984428406\n",
      "Epoch: 74/100 | step: 166/422 | loss: 0.04238951578736305\n",
      "Epoch: 74/100 | step: 167/422 | loss: 0.039263173937797546\n",
      "Epoch: 74/100 | step: 168/422 | loss: 0.12159238010644913\n",
      "Epoch: 74/100 | step: 169/422 | loss: 0.03756767138838768\n",
      "Epoch: 74/100 | step: 170/422 | loss: 0.023969599977135658\n",
      "Epoch: 74/100 | step: 171/422 | loss: 0.019664905965328217\n",
      "Epoch: 74/100 | step: 172/422 | loss: 0.05422688275575638\n",
      "Epoch: 74/100 | step: 173/422 | loss: 0.05001490190625191\n",
      "Epoch: 74/100 | step: 174/422 | loss: 0.024829095229506493\n",
      "Epoch: 74/100 | step: 175/422 | loss: 0.11553867161273956\n",
      "Epoch: 74/100 | step: 176/422 | loss: 0.13785217702388763\n",
      "Epoch: 74/100 | step: 177/422 | loss: 0.139098659157753\n",
      "Epoch: 74/100 | step: 178/422 | loss: 0.15659649670124054\n",
      "Epoch: 74/100 | step: 179/422 | loss: 0.13842666149139404\n",
      "Epoch: 74/100 | step: 180/422 | loss: 0.07606703042984009\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 75/100 | step: 1/422 | loss: 0.272324800491333\n",
      "Epoch: 75/100 | step: 2/422 | loss: 0.13693033158779144\n",
      "Epoch: 75/100 | step: 3/422 | loss: 0.05514940246939659\n",
      "Epoch: 75/100 | step: 4/422 | loss: 0.14724518358707428\n",
      "Epoch: 75/100 | step: 5/422 | loss: 0.6358423829078674\n",
      "Epoch: 75/100 | step: 6/422 | loss: 0.2044028341770172\n",
      "Epoch: 75/100 | step: 7/422 | loss: 0.24669645726680756\n",
      "Epoch: 75/100 | step: 8/422 | loss: 0.0889584869146347\n",
      "Epoch: 75/100 | step: 9/422 | loss: 0.06680259108543396\n",
      "Epoch: 75/100 | step: 10/422 | loss: 0.09434542804956436\n",
      "Epoch: 75/100 | step: 11/422 | loss: 0.11865241825580597\n",
      "Epoch: 75/100 | step: 12/422 | loss: 0.033479105681180954\n",
      "Epoch: 75/100 | step: 13/422 | loss: 0.11498110741376877\n",
      "Epoch: 75/100 | step: 14/422 | loss: 0.07245158404111862\n",
      "Epoch: 75/100 | step: 15/422 | loss: 0.13551589846611023\n",
      "Epoch: 75/100 | step: 16/422 | loss: 0.08159928768873215\n",
      "Epoch: 75/100 | step: 17/422 | loss: 0.08034216612577438\n",
      "Epoch: 75/100 | step: 18/422 | loss: 0.13423214852809906\n",
      "Epoch: 75/100 | step: 19/422 | loss: 0.07839498668909073\n",
      "Epoch: 75/100 | step: 20/422 | loss: 0.044791676104068756\n",
      "Epoch: 75/100 | step: 21/422 | loss: 0.10748358815908432\n",
      "Epoch: 75/100 | step: 22/422 | loss: 0.03795742243528366\n",
      "Epoch: 75/100 | step: 23/422 | loss: 0.118580661714077\n",
      "Epoch: 75/100 | step: 24/422 | loss: 0.1146804615855217\n",
      "Epoch: 75/100 | step: 25/422 | loss: 0.04684605076909065\n",
      "Epoch: 75/100 | step: 26/422 | loss: 0.20244592428207397\n",
      "Epoch: 75/100 | step: 27/422 | loss: 0.25601115822792053\n",
      "Epoch: 75/100 | step: 28/422 | loss: 0.04817984253168106\n",
      "Epoch: 75/100 | step: 29/422 | loss: 0.06061014533042908\n",
      "Epoch: 75/100 | step: 30/422 | loss: 0.13970963656902313\n",
      "Epoch: 75/100 | step: 31/422 | loss: 0.05670373514294624\n",
      "Epoch: 75/100 | step: 32/422 | loss: 0.27666768431663513\n",
      "Epoch: 75/100 | step: 33/422 | loss: 0.4123501777648926\n",
      "Epoch: 75/100 | step: 34/422 | loss: 0.40070098638534546\n",
      "Epoch: 75/100 | step: 35/422 | loss: 0.07880871742963791\n",
      "Epoch: 75/100 | step: 36/422 | loss: 0.15762655436992645\n",
      "Epoch: 75/100 | step: 37/422 | loss: 0.066213458776474\n",
      "Epoch: 75/100 | step: 38/422 | loss: 0.045188333839178085\n",
      "Epoch: 75/100 | step: 39/422 | loss: 0.08844676613807678\n",
      "Epoch: 75/100 | step: 40/422 | loss: 0.11552080512046814\n",
      "Epoch: 75/100 | step: 41/422 | loss: 0.03955278918147087\n",
      "Epoch: 75/100 | step: 42/422 | loss: 0.05372690409421921\n",
      "Epoch: 75/100 | step: 43/422 | loss: 0.07040216773748398\n",
      "Epoch: 75/100 | step: 44/422 | loss: 0.09106820076704025\n",
      "Epoch: 75/100 | step: 45/422 | loss: 0.08073395490646362\n",
      "Epoch: 75/100 | step: 46/422 | loss: 0.07485516369342804\n",
      "Epoch: 75/100 | step: 47/422 | loss: 0.08569006621837616\n",
      "Epoch: 75/100 | step: 48/422 | loss: 0.0396774560213089\n",
      "Epoch: 75/100 | step: 49/422 | loss: 0.03361990302801132\n",
      "Epoch: 75/100 | step: 50/422 | loss: 0.11601271480321884\n",
      "Epoch: 75/100 | step: 51/422 | loss: 0.06630163639783859\n",
      "Epoch: 75/100 | step: 52/422 | loss: 0.1118864193558693\n",
      "Epoch: 75/100 | step: 53/422 | loss: 0.07894237339496613\n",
      "Epoch: 75/100 | step: 54/422 | loss: 0.032486848533153534\n",
      "Epoch: 75/100 | step: 55/422 | loss: 0.06699876487255096\n",
      "Epoch: 75/100 | step: 56/422 | loss: 0.03456435352563858\n",
      "Epoch: 75/100 | step: 57/422 | loss: 0.23737944662570953\n",
      "Epoch: 75/100 | step: 58/422 | loss: 0.09842005372047424\n",
      "Epoch: 75/100 | step: 59/422 | loss: 0.2082986980676651\n",
      "Epoch: 75/100 | step: 60/422 | loss: 0.28438082337379456\n",
      "Epoch: 75/100 | step: 61/422 | loss: 0.05933331698179245\n",
      "Epoch: 75/100 | step: 62/422 | loss: 0.041493650525808334\n",
      "Epoch: 75/100 | step: 63/422 | loss: 0.17861205339431763\n",
      "Epoch: 75/100 | step: 64/422 | loss: 0.216542586684227\n",
      "Epoch: 75/100 | step: 65/422 | loss: 0.08255218714475632\n",
      "Epoch: 75/100 | step: 66/422 | loss: 0.1377442479133606\n",
      "Epoch: 75/100 | step: 67/422 | loss: 0.039416126906871796\n",
      "Epoch: 75/100 | step: 68/422 | loss: 0.09286436438560486\n",
      "Epoch: 75/100 | step: 69/422 | loss: 0.03520862013101578\n",
      "Epoch: 75/100 | step: 70/422 | loss: 0.04875611141324043\n",
      "Epoch: 75/100 | step: 71/422 | loss: 0.0525674931704998\n",
      "Epoch: 75/100 | step: 72/422 | loss: 0.11894412338733673\n",
      "Epoch: 75/100 | step: 73/422 | loss: 0.04564926028251648\n",
      "Epoch: 75/100 | step: 74/422 | loss: 0.09548277407884598\n",
      "Epoch: 75/100 | step: 75/422 | loss: 0.058649927377700806\n",
      "Epoch: 75/100 | step: 76/422 | loss: 0.055189769715070724\n",
      "Epoch: 75/100 | step: 77/422 | loss: 0.06008097901940346\n",
      "Epoch: 75/100 | step: 78/422 | loss: 0.08748828619718552\n",
      "Epoch: 75/100 | step: 79/422 | loss: 0.07961719483137131\n",
      "Epoch: 75/100 | step: 80/422 | loss: 0.054181549698114395\n",
      "Epoch: 75/100 | step: 81/422 | loss: 0.06498326361179352\n",
      "Epoch: 75/100 | step: 82/422 | loss: 0.11475154012441635\n",
      "Epoch: 75/100 | step: 83/422 | loss: 0.10219188779592514\n",
      "Epoch: 75/100 | step: 84/422 | loss: 0.04979143291711807\n",
      "Epoch: 75/100 | step: 85/422 | loss: 0.041004687547683716\n",
      "Epoch: 75/100 | step: 86/422 | loss: 0.14559917151927948\n",
      "Epoch: 75/100 | step: 87/422 | loss: 0.29691365361213684\n",
      "Epoch: 75/100 | step: 88/422 | loss: 0.19041629135608673\n",
      "Epoch: 75/100 | step: 89/422 | loss: 0.12908920645713806\n",
      "Epoch: 75/100 | step: 90/422 | loss: 0.3467927575111389\n",
      "Epoch: 75/100 | step: 91/422 | loss: 0.04388348385691643\n",
      "Epoch: 75/100 | step: 92/422 | loss: 0.0678449273109436\n",
      "Epoch: 75/100 | step: 93/422 | loss: 0.056089214980602264\n",
      "Epoch: 75/100 | step: 94/422 | loss: 0.14262697100639343\n",
      "Epoch: 75/100 | step: 95/422 | loss: 0.09185413271188736\n",
      "Epoch: 75/100 | step: 96/422 | loss: 0.1693333387374878\n",
      "Epoch: 75/100 | step: 97/422 | loss: 0.12256684899330139\n",
      "Epoch: 75/100 | step: 98/422 | loss: 0.16283413767814636\n",
      "Epoch: 75/100 | step: 99/422 | loss: 0.06214239075779915\n",
      "Epoch: 75/100 | step: 100/422 | loss: 0.04258176311850548\n",
      "Epoch: 75/100 | step: 101/422 | loss: 0.05225999653339386\n",
      "Epoch: 75/100 | step: 102/422 | loss: 0.06337351351976395\n",
      "Epoch: 75/100 | step: 103/422 | loss: 0.05179079622030258\n",
      "Epoch: 75/100 | step: 104/422 | loss: 0.023924551904201508\n",
      "Epoch: 75/100 | step: 105/422 | loss: 0.12469037622213364\n",
      "Epoch: 75/100 | step: 106/422 | loss: 0.037314485758543015\n",
      "Epoch: 75/100 | step: 107/422 | loss: 0.037646349519491196\n",
      "Epoch: 75/100 | step: 108/422 | loss: 0.09834093600511551\n",
      "Epoch: 75/100 | step: 109/422 | loss: 0.039333708584308624\n",
      "Epoch: 75/100 | step: 110/422 | loss: 0.0340951532125473\n",
      "Epoch: 75/100 | step: 111/422 | loss: 0.03376998379826546\n",
      "Epoch: 75/100 | step: 112/422 | loss: 0.11752798408269882\n",
      "Epoch: 75/100 | step: 113/422 | loss: 0.09916435182094574\n",
      "Epoch: 75/100 | step: 114/422 | loss: 0.06715413182973862\n",
      "Epoch: 75/100 | step: 115/422 | loss: 0.07889145612716675\n",
      "Epoch: 75/100 | step: 116/422 | loss: 0.04893455281853676\n",
      "Epoch: 75/100 | step: 117/422 | loss: 0.0694025307893753\n",
      "Epoch: 75/100 | step: 118/422 | loss: 0.06538821756839752\n",
      "Epoch: 75/100 | step: 119/422 | loss: 0.01956814154982567\n",
      "Epoch: 75/100 | step: 120/422 | loss: 0.03797077387571335\n",
      "Epoch: 75/100 | step: 121/422 | loss: 0.057535987347364426\n",
      "Epoch: 75/100 | step: 122/422 | loss: 0.03100086934864521\n",
      "Epoch: 75/100 | step: 123/422 | loss: 0.03557034209370613\n",
      "Epoch: 75/100 | step: 124/422 | loss: 0.12998563051223755\n",
      "Epoch: 75/100 | step: 125/422 | loss: 0.027214160189032555\n",
      "Epoch: 75/100 | step: 126/422 | loss: 0.03806028515100479\n",
      "Epoch: 75/100 | step: 127/422 | loss: 0.056480225175619125\n",
      "Epoch: 75/100 | step: 128/422 | loss: 0.12061803787946701\n",
      "Epoch: 75/100 | step: 129/422 | loss: 0.06728602200746536\n",
      "Epoch: 75/100 | step: 130/422 | loss: 0.08249441534280777\n",
      "Epoch: 75/100 | step: 131/422 | loss: 0.09723035991191864\n",
      "Epoch: 75/100 | step: 132/422 | loss: 0.024646125733852386\n",
      "Epoch: 75/100 | step: 133/422 | loss: 0.027829594910144806\n",
      "Epoch: 75/100 | step: 134/422 | loss: 0.025952370837330818\n",
      "Epoch: 75/100 | step: 135/422 | loss: 0.04193646088242531\n",
      "Epoch: 75/100 | step: 136/422 | loss: 0.07170337438583374\n",
      "Epoch: 75/100 | step: 137/422 | loss: 0.0466550849378109\n",
      "Epoch: 75/100 | step: 138/422 | loss: 0.0686584934592247\n",
      "Epoch: 75/100 | step: 139/422 | loss: 0.045092251151800156\n",
      "Epoch: 75/100 | step: 140/422 | loss: 0.039644937962293625\n",
      "Epoch: 75/100 | step: 141/422 | loss: 0.08564946800470352\n",
      "Epoch: 75/100 | step: 142/422 | loss: 0.048909079283475876\n",
      "Epoch: 75/100 | step: 143/422 | loss: 0.06548198312520981\n",
      "Epoch: 75/100 | step: 144/422 | loss: 0.03858063742518425\n",
      "Epoch: 75/100 | step: 145/422 | loss: 0.01826358400285244\n",
      "Epoch: 75/100 | step: 146/422 | loss: 0.028286121785640717\n",
      "Epoch: 75/100 | step: 147/422 | loss: 0.046002358198165894\n",
      "Epoch: 75/100 | step: 148/422 | loss: 0.03122471272945404\n",
      "Epoch: 75/100 | step: 149/422 | loss: 0.08972290903329849\n",
      "Epoch: 75/100 | step: 150/422 | loss: 0.060381077229976654\n",
      "Epoch: 75/100 | step: 151/422 | loss: 0.11124233901500702\n",
      "Epoch: 75/100 | step: 152/422 | loss: 0.08562864363193512\n",
      "Epoch: 75/100 | step: 153/422 | loss: 0.18908487260341644\n",
      "Epoch: 75/100 | step: 154/422 | loss: 0.07255569100379944\n",
      "Epoch: 75/100 | step: 155/422 | loss: 0.05197620391845703\n",
      "Epoch: 75/100 | step: 156/422 | loss: 0.07622232288122177\n",
      "Epoch: 75/100 | step: 157/422 | loss: 0.10759514570236206\n",
      "Epoch: 75/100 | step: 158/422 | loss: 0.03354976698756218\n",
      "Epoch: 75/100 | step: 159/422 | loss: 0.10550929605960846\n",
      "Epoch: 75/100 | step: 160/422 | loss: 0.03526447340846062\n",
      "Epoch: 75/100 | step: 161/422 | loss: 0.04090424254536629\n",
      "Epoch: 75/100 | step: 162/422 | loss: 0.03897397965192795\n",
      "Epoch: 75/100 | step: 163/422 | loss: 0.106327585875988\n",
      "Epoch: 75/100 | step: 164/422 | loss: 0.06118914112448692\n",
      "Epoch: 75/100 | step: 165/422 | loss: 0.038038220256567\n",
      "Epoch: 75/100 | step: 166/422 | loss: 0.05610673874616623\n",
      "Epoch: 75/100 | step: 167/422 | loss: 0.07395906001329422\n",
      "Epoch: 75/100 | step: 168/422 | loss: 0.09638480842113495\n",
      "Epoch: 75/100 | step: 169/422 | loss: 0.0741533488035202\n",
      "Epoch: 75/100 | step: 170/422 | loss: 0.04443424567580223\n",
      "Epoch: 75/100 | step: 171/422 | loss: 0.05827207490801811\n",
      "Epoch: 75/100 | step: 172/422 | loss: 0.08121901005506516\n",
      "Epoch: 75/100 | step: 173/422 | loss: 0.11698020994663239\n",
      "Epoch: 75/100 | step: 174/422 | loss: 0.035330869257450104\n",
      "Epoch: 75/100 | step: 175/422 | loss: 0.02878269925713539\n",
      "Epoch: 75/100 | step: 176/422 | loss: 0.04782968387007713\n",
      "Epoch: 75/100 | step: 177/422 | loss: 0.06203499436378479\n",
      "Epoch: 75/100 | step: 178/422 | loss: 0.02531949058175087\n",
      "Epoch: 75/100 | step: 179/422 | loss: 0.047241393476724625\n",
      "Epoch: 75/100 | step: 180/422 | loss: 0.06536746770143509\n",
      "Epoch: 75/100 | step: 181/422 | loss: 0.03060353733599186\n",
      "Epoch: 75/100 | step: 182/422 | loss: 0.027144011110067368\n",
      "Epoch: 75/100 | step: 183/422 | loss: 0.0242374949157238\n",
      "Epoch: 75/100 | step: 184/422 | loss: 0.061139825731515884\n",
      "Epoch: 75/100 | step: 185/422 | loss: 0.030117787420749664\n",
      "Epoch: 75/100 | step: 186/422 | loss: 0.053065575659275055\n",
      "Epoch: 75/100 | step: 187/422 | loss: 0.05670087784528732\n",
      "Epoch: 75/100 | step: 188/422 | loss: 0.07992299646139145\n",
      "Epoch: 75/100 | step: 189/422 | loss: 0.0632501021027565\n",
      "Epoch: 75/100 | step: 190/422 | loss: 0.05331747606396675\n",
      "Epoch: 75/100 | step: 191/422 | loss: 0.04429520294070244\n",
      "Epoch: 75/100 | step: 192/422 | loss: 0.1467476487159729\n",
      "Epoch: 75/100 | step: 193/422 | loss: 0.21641181409358978\n",
      "Epoch: 75/100 | step: 194/422 | loss: 0.07117725163698196\n",
      "Epoch: 75/100 | step: 195/422 | loss: 0.09623685479164124\n",
      "Epoch: 75/100 | step: 196/422 | loss: 0.1799396276473999\n",
      "Epoch: 75/100 | step: 197/422 | loss: 0.16604581475257874\n",
      "Epoch: 75/100 | step: 198/422 | loss: 0.1262717843055725\n",
      "Epoch: 75/100 | step: 199/422 | loss: 0.06271594017744064\n",
      "Epoch: 75/100 | step: 200/422 | loss: 0.03998160362243652\n",
      "Epoch: 75/100 | step: 201/422 | loss: 0.1129741370677948\n",
      "Epoch: 75/100 | step: 202/422 | loss: 0.04810629412531853\n",
      "Epoch: 75/100 | step: 203/422 | loss: 0.023079784587025642\n",
      "Epoch: 75/100 | step: 204/422 | loss: 0.16636618971824646\n",
      "Epoch: 75/100 | step: 205/422 | loss: 0.19202999770641327\n",
      "Epoch: 75/100 | step: 206/422 | loss: 0.062007635831832886\n",
      "Epoch: 75/100 | step: 207/422 | loss: 0.05549565330147743\n",
      "Epoch: 75/100 | step: 208/422 | loss: 0.04192012548446655\n",
      "Epoch: 75/100 | step: 209/422 | loss: 0.04518961161375046\n",
      "Epoch: 75/100 | step: 210/422 | loss: 0.040479522198438644\n",
      "Epoch: 75/100 | step: 211/422 | loss: 0.03298834711313248\n",
      "Epoch: 75/100 | step: 212/422 | loss: 0.09689514338970184\n",
      "Epoch: 75/100 | step: 213/422 | loss: 0.03767800331115723\n",
      "Epoch: 75/100 | step: 214/422 | loss: 0.1001843512058258\n",
      "Epoch: 75/100 | step: 215/422 | loss: 0.03960495442152023\n",
      "Epoch: 75/100 | step: 216/422 | loss: 0.026733724400401115\n",
      "Epoch: 75/100 | step: 217/422 | loss: 0.10393568873405457\n",
      "Epoch: 75/100 | step: 218/422 | loss: 0.02676277793943882\n",
      "Epoch: 75/100 | step: 219/422 | loss: 0.02918795309960842\n",
      "Epoch: 75/100 | step: 220/422 | loss: 0.17304052412509918\n",
      "Epoch: 75/100 | step: 221/422 | loss: 0.05090615153312683\n",
      "Epoch: 75/100 | step: 222/422 | loss: 0.08305874466896057\n",
      "Epoch: 75/100 | step: 223/422 | loss: 0.06681019812822342\n",
      "Epoch: 75/100 | step: 224/422 | loss: 0.06284865736961365\n",
      "Epoch: 75/100 | step: 225/422 | loss: 0.13470473885536194\n",
      "Epoch: 75/100 | step: 226/422 | loss: 0.04495561867952347\n",
      "Epoch: 75/100 | step: 227/422 | loss: 0.05659118667244911\n",
      "Epoch: 75/100 | step: 228/422 | loss: 0.08256489783525467\n",
      "Epoch: 75/100 | step: 229/422 | loss: 0.04264907166361809\n",
      "Epoch: 75/100 | step: 230/422 | loss: 0.02800423465669155\n",
      "Epoch: 75/100 | step: 231/422 | loss: 0.056794196367263794\n",
      "Epoch: 75/100 | step: 232/422 | loss: 0.05629153177142143\n",
      "Epoch: 75/100 | step: 233/422 | loss: 0.0490528903901577\n",
      "Epoch: 75/100 | step: 234/422 | loss: 0.03412232547998428\n",
      "Epoch: 75/100 | step: 235/422 | loss: 0.029377607628703117\n",
      "Epoch: 75/100 | step: 236/422 | loss: 0.1344921737909317\n",
      "Epoch: 75/100 | step: 237/422 | loss: 0.03043876402080059\n",
      "Epoch: 75/100 | step: 238/422 | loss: 0.08274446427822113\n",
      "Epoch: 75/100 | step: 239/422 | loss: 0.09814904630184174\n",
      "Epoch: 75/100 | step: 240/422 | loss: 0.05042054504156113\n",
      "Epoch: 75/100 | step: 241/422 | loss: 0.046431899070739746\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 76/100 | step: 1/422 | loss: 0.0667477399110794\n",
      "Epoch: 76/100 | step: 2/422 | loss: 0.06333261728286743\n",
      "Epoch: 76/100 | step: 3/422 | loss: 0.10634032636880875\n",
      "Epoch: 76/100 | step: 4/422 | loss: 0.06066649779677391\n",
      "Epoch: 76/100 | step: 5/422 | loss: 0.11309732496738434\n",
      "Epoch: 76/100 | step: 6/422 | loss: 0.17799654603004456\n",
      "Epoch: 76/100 | step: 7/422 | loss: 0.220420703291893\n",
      "Epoch: 76/100 | step: 8/422 | loss: 0.15784503519535065\n",
      "Epoch: 76/100 | step: 9/422 | loss: 0.1540805548429489\n",
      "Epoch: 76/100 | step: 10/422 | loss: 0.07729217410087585\n",
      "Epoch: 76/100 | step: 11/422 | loss: 0.03178146108984947\n",
      "Epoch: 76/100 | step: 12/422 | loss: 0.039652612060308456\n",
      "Epoch: 76/100 | step: 13/422 | loss: 0.06052115932106972\n",
      "Epoch: 76/100 | step: 14/422 | loss: 0.047613244503736496\n",
      "Epoch: 76/100 | step: 15/422 | loss: 0.04260501638054848\n",
      "Epoch: 76/100 | step: 16/422 | loss: 0.049174126237630844\n",
      "Epoch: 76/100 | step: 17/422 | loss: 0.031108204275369644\n",
      "Epoch: 76/100 | step: 18/422 | loss: 0.07421991974115372\n",
      "Epoch: 76/100 | step: 19/422 | loss: 0.027980761602520943\n",
      "Epoch: 76/100 | step: 20/422 | loss: 0.03673885017633438\n",
      "Epoch: 76/100 | step: 21/422 | loss: 0.046582095324993134\n",
      "Epoch: 76/100 | step: 22/422 | loss: 0.02457277476787567\n",
      "Epoch: 76/100 | step: 23/422 | loss: 0.029409853741526604\n",
      "Epoch: 76/100 | step: 24/422 | loss: 0.064145028591156\n",
      "Epoch: 76/100 | step: 25/422 | loss: 0.03959028795361519\n",
      "Epoch: 76/100 | step: 26/422 | loss: 0.05963807553052902\n",
      "Epoch: 76/100 | step: 27/422 | loss: 0.029645761474967003\n",
      "Epoch: 76/100 | step: 28/422 | loss: 0.038484081625938416\n",
      "Epoch: 76/100 | step: 29/422 | loss: 0.16022224724292755\n",
      "Epoch: 76/100 | step: 30/422 | loss: 0.0958775132894516\n",
      "Epoch: 76/100 | step: 31/422 | loss: 0.03918428719043732\n",
      "Epoch: 76/100 | step: 32/422 | loss: 0.04189419373869896\n",
      "Epoch: 76/100 | step: 33/422 | loss: 0.049397535622119904\n",
      "Epoch: 76/100 | step: 34/422 | loss: 0.036082785576581955\n",
      "Epoch: 76/100 | step: 35/422 | loss: 0.053402334451675415\n",
      "Epoch: 76/100 | step: 36/422 | loss: 0.2759591341018677\n",
      "Epoch: 76/100 | step: 37/422 | loss: 0.048508286476135254\n",
      "Epoch: 76/100 | step: 38/422 | loss: 0.10238196700811386\n",
      "Epoch: 76/100 | step: 39/422 | loss: 0.07507544755935669\n",
      "Epoch: 76/100 | step: 40/422 | loss: 0.04413461685180664\n",
      "Epoch: 76/100 | step: 41/422 | loss: 0.1040399819612503\n",
      "Epoch: 76/100 | step: 42/422 | loss: 0.06181652098894119\n",
      "Epoch: 76/100 | step: 43/422 | loss: 0.08414159715175629\n",
      "Epoch: 76/100 | step: 44/422 | loss: 0.03250264748930931\n",
      "Epoch: 76/100 | step: 45/422 | loss: 0.028555210679769516\n",
      "Epoch: 76/100 | step: 46/422 | loss: 0.21108844876289368\n",
      "Epoch: 76/100 | step: 47/422 | loss: 0.05190744996070862\n",
      "Epoch: 76/100 | step: 48/422 | loss: 0.022583570331335068\n",
      "Epoch: 76/100 | step: 49/422 | loss: 0.0327799916267395\n",
      "Epoch: 76/100 | step: 50/422 | loss: 0.0774838924407959\n",
      "Epoch: 76/100 | step: 51/422 | loss: 0.11334757506847382\n",
      "Epoch: 76/100 | step: 52/422 | loss: 0.055380165576934814\n",
      "Epoch: 76/100 | step: 53/422 | loss: 0.043037865310907364\n",
      "Epoch: 76/100 | step: 54/422 | loss: 0.06001009792089462\n",
      "Epoch: 76/100 | step: 55/422 | loss: 0.07777924835681915\n",
      "Epoch: 76/100 | step: 56/422 | loss: 0.05102427676320076\n",
      "Epoch: 76/100 | step: 57/422 | loss: 0.04169745370745659\n",
      "Epoch: 76/100 | step: 58/422 | loss: 0.042377371340990067\n",
      "Epoch: 76/100 | step: 59/422 | loss: 0.13296006619930267\n",
      "Epoch: 76/100 | step: 60/422 | loss: 0.04860107600688934\n",
      "Epoch: 76/100 | step: 61/422 | loss: 0.016644960269331932\n",
      "Epoch: 76/100 | step: 62/422 | loss: 0.03538130968809128\n",
      "Epoch: 76/100 | step: 63/422 | loss: 0.03214672580361366\n",
      "Epoch: 76/100 | step: 64/422 | loss: 0.035300593823194504\n",
      "Epoch: 76/100 | step: 65/422 | loss: 0.04890025034546852\n",
      "Epoch: 76/100 | step: 66/422 | loss: 0.05944325029850006\n",
      "Epoch: 76/100 | step: 67/422 | loss: 0.14664226770401\n",
      "Epoch: 76/100 | step: 68/422 | loss: 0.16800343990325928\n",
      "Epoch: 76/100 | step: 69/422 | loss: 0.04038896784186363\n",
      "Epoch: 76/100 | step: 70/422 | loss: 0.10646256059408188\n",
      "Epoch: 76/100 | step: 71/422 | loss: 0.04026544466614723\n",
      "Epoch: 76/100 | step: 72/422 | loss: 0.08374480903148651\n",
      "Epoch: 76/100 | step: 73/422 | loss: 0.06962849199771881\n",
      "Epoch: 76/100 | step: 74/422 | loss: 0.028796205297112465\n",
      "Epoch: 76/100 | step: 75/422 | loss: 0.04850654676556587\n",
      "Epoch: 76/100 | step: 76/422 | loss: 0.03114628791809082\n",
      "Epoch: 76/100 | step: 77/422 | loss: 0.06190461292862892\n",
      "Epoch: 76/100 | step: 78/422 | loss: 0.05409961938858032\n",
      "Epoch: 76/100 | step: 79/422 | loss: 0.06516195833683014\n",
      "Epoch: 76/100 | step: 80/422 | loss: 0.02752024494111538\n",
      "Epoch: 76/100 | step: 81/422 | loss: 0.022482287138700485\n",
      "Epoch: 76/100 | step: 82/422 | loss: 0.04054088145494461\n",
      "Epoch: 76/100 | step: 83/422 | loss: 0.09160861372947693\n",
      "Epoch: 76/100 | step: 84/422 | loss: 0.077251136302948\n",
      "Epoch: 76/100 | step: 85/422 | loss: 0.07019292563199997\n",
      "Epoch: 76/100 | step: 86/422 | loss: 0.034345488995313644\n",
      "Epoch: 76/100 | step: 87/422 | loss: 0.07302336394786835\n",
      "Epoch: 76/100 | step: 88/422 | loss: 0.08569075912237167\n",
      "Epoch: 76/100 | step: 89/422 | loss: 0.04852568730711937\n",
      "Epoch: 76/100 | step: 90/422 | loss: 0.028262827545404434\n",
      "Epoch: 76/100 | step: 91/422 | loss: 0.24955080449581146\n",
      "Epoch: 76/100 | step: 92/422 | loss: 0.6164708733558655\n",
      "Epoch: 76/100 | step: 93/422 | loss: 0.38492992520332336\n",
      "Epoch: 76/100 | step: 94/422 | loss: 0.5248775482177734\n",
      "Epoch: 76/100 | step: 95/422 | loss: 0.04700882360339165\n",
      "Epoch: 76/100 | step: 96/422 | loss: 0.10817216336727142\n",
      "Epoch: 76/100 | step: 97/422 | loss: 0.340034157037735\n",
      "Epoch: 76/100 | step: 98/422 | loss: 0.023835062980651855\n",
      "Epoch: 76/100 | step: 99/422 | loss: 0.016883278265595436\n",
      "Epoch: 76/100 | step: 100/422 | loss: 0.5095915794372559\n",
      "Epoch: 76/100 | step: 101/422 | loss: 0.027347106486558914\n",
      "Epoch: 76/100 | step: 102/422 | loss: 0.040738146752119064\n",
      "Epoch: 76/100 | step: 103/422 | loss: 0.06940550357103348\n",
      "Epoch: 76/100 | step: 104/422 | loss: 0.07194645702838898\n",
      "Epoch: 76/100 | step: 105/422 | loss: 0.05092041566967964\n",
      "Epoch: 76/100 | step: 106/422 | loss: 0.019085334613919258\n",
      "Epoch: 76/100 | step: 107/422 | loss: 0.06369315087795258\n",
      "Epoch: 76/100 | step: 108/422 | loss: 0.04043899476528168\n",
      "Epoch: 76/100 | step: 109/422 | loss: 0.03014197386801243\n",
      "Epoch: 76/100 | step: 110/422 | loss: 0.023670148104429245\n",
      "Epoch: 76/100 | step: 111/422 | loss: 0.04390355199575424\n",
      "Epoch: 76/100 | step: 112/422 | loss: 0.45462921261787415\n",
      "Epoch: 76/100 | step: 113/422 | loss: 0.10525623708963394\n",
      "Epoch: 76/100 | step: 114/422 | loss: 0.0388876236975193\n",
      "Epoch: 76/100 | step: 115/422 | loss: 0.07573557645082474\n",
      "Epoch: 76/100 | step: 116/422 | loss: 0.08161415904760361\n",
      "Epoch: 76/100 | step: 117/422 | loss: 0.041051317006349564\n",
      "Epoch: 76/100 | step: 118/422 | loss: 0.03326220065355301\n",
      "Epoch: 76/100 | step: 119/422 | loss: 0.20425455272197723\n",
      "Epoch: 76/100 | step: 120/422 | loss: 0.0818413570523262\n",
      "Epoch: 76/100 | step: 121/422 | loss: 0.09270729869604111\n",
      "Epoch: 76/100 | step: 122/422 | loss: 0.13942120969295502\n",
      "Epoch: 76/100 | step: 123/422 | loss: 0.03210495412349701\n",
      "Epoch: 76/100 | step: 124/422 | loss: 0.020902013406157494\n",
      "Epoch: 76/100 | step: 125/422 | loss: 0.049354780465364456\n",
      "Epoch: 76/100 | step: 126/422 | loss: 0.05666331574320793\n",
      "Epoch: 76/100 | step: 127/422 | loss: 0.036568690091371536\n",
      "Epoch: 76/100 | step: 128/422 | loss: 0.02686544880270958\n",
      "Epoch: 76/100 | step: 129/422 | loss: 0.025429435074329376\n",
      "Epoch: 76/100 | step: 130/422 | loss: 0.04467201605439186\n",
      "Epoch: 76/100 | step: 131/422 | loss: 0.026994358748197556\n",
      "Epoch: 76/100 | step: 132/422 | loss: 0.07507967948913574\n",
      "Epoch: 76/100 | step: 133/422 | loss: 0.10139823704957962\n",
      "Epoch: 76/100 | step: 134/422 | loss: 0.12389718741178513\n",
      "Epoch: 76/100 | step: 135/422 | loss: 0.08712062239646912\n",
      "Epoch: 76/100 | step: 136/422 | loss: 0.1643754243850708\n",
      "Epoch: 76/100 | step: 137/422 | loss: 0.20842044055461884\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 77/100 | step: 1/422 | loss: 0.2893228232860565\n",
      "Epoch: 77/100 | step: 2/422 | loss: 0.11726032942533493\n",
      "Epoch: 77/100 | step: 3/422 | loss: 0.02555163763463497\n",
      "Epoch: 77/100 | step: 4/422 | loss: 0.08804191648960114\n",
      "Epoch: 77/100 | step: 5/422 | loss: 0.04772666096687317\n",
      "Epoch: 77/100 | step: 6/422 | loss: 0.2886536121368408\n",
      "Epoch: 77/100 | step: 7/422 | loss: 0.04636611416935921\n",
      "Epoch: 77/100 | step: 8/422 | loss: 0.10684286803007126\n",
      "Epoch: 77/100 | step: 9/422 | loss: 0.046447575092315674\n",
      "Epoch: 77/100 | step: 10/422 | loss: 0.039210714399814606\n",
      "Epoch: 77/100 | step: 11/422 | loss: 0.032217495143413544\n",
      "Epoch: 77/100 | step: 12/422 | loss: 0.07864564657211304\n",
      "Epoch: 77/100 | step: 13/422 | loss: 0.031381674110889435\n",
      "Epoch: 77/100 | step: 14/422 | loss: 0.07480673491954803\n",
      "Epoch: 77/100 | step: 15/422 | loss: 0.06611783057451248\n",
      "Epoch: 77/100 | step: 16/422 | loss: 0.06021447107195854\n",
      "Epoch: 77/100 | step: 17/422 | loss: 0.04904366657137871\n",
      "Epoch: 77/100 | step: 18/422 | loss: 0.2458314299583435\n",
      "Epoch: 77/100 | step: 19/422 | loss: 0.2675243616104126\n",
      "Epoch: 77/100 | step: 20/422 | loss: 0.03053726628422737\n",
      "Epoch: 77/100 | step: 21/422 | loss: 0.03934090957045555\n",
      "Epoch: 77/100 | step: 22/422 | loss: 0.026675553992390633\n",
      "Epoch: 77/100 | step: 23/422 | loss: 0.0345190092921257\n",
      "Epoch: 77/100 | step: 24/422 | loss: 0.027222702279686928\n",
      "Epoch: 77/100 | step: 25/422 | loss: 0.053235117346048355\n",
      "Epoch: 77/100 | step: 26/422 | loss: 0.035844478756189346\n",
      "Epoch: 77/100 | step: 27/422 | loss: 0.039103224873542786\n",
      "Epoch: 77/100 | step: 28/422 | loss: 0.027097703889012337\n",
      "Epoch: 77/100 | step: 29/422 | loss: 0.0392400361597538\n",
      "Epoch: 77/100 | step: 30/422 | loss: 0.061329856514930725\n",
      "Epoch: 77/100 | step: 31/422 | loss: 0.03653201088309288\n",
      "Epoch: 77/100 | step: 32/422 | loss: 0.032139603048563004\n",
      "Epoch: 77/100 | step: 33/422 | loss: 0.023997720330953598\n",
      "Epoch: 77/100 | step: 34/422 | loss: 0.050028275698423386\n",
      "Epoch: 77/100 | step: 35/422 | loss: 0.056224849075078964\n",
      "Epoch: 77/100 | step: 36/422 | loss: 0.03313682600855827\n",
      "Epoch: 77/100 | step: 37/422 | loss: 0.02573757991194725\n",
      "Epoch: 77/100 | step: 38/422 | loss: 0.10340961068868637\n",
      "Epoch: 77/100 | step: 39/422 | loss: 0.06823862344026566\n",
      "Epoch: 77/100 | step: 40/422 | loss: 0.04384654015302658\n",
      "Epoch: 77/100 | step: 41/422 | loss: 0.018192585557699203\n",
      "Epoch: 77/100 | step: 42/422 | loss: 0.03397276997566223\n",
      "Epoch: 77/100 | step: 43/422 | loss: 0.047428689897060394\n",
      "Epoch: 77/100 | step: 44/422 | loss: 0.08306502550840378\n",
      "Epoch: 77/100 | step: 45/422 | loss: 0.03176445886492729\n",
      "Epoch: 77/100 | step: 46/422 | loss: 0.06570152938365936\n",
      "Epoch: 77/100 | step: 47/422 | loss: 0.04537617415189743\n",
      "Epoch: 77/100 | step: 48/422 | loss: 0.08793572336435318\n",
      "Epoch: 77/100 | step: 49/422 | loss: 0.0278020016849041\n",
      "Epoch: 77/100 | step: 50/422 | loss: 0.03306935727596283\n",
      "Epoch: 77/100 | step: 51/422 | loss: 0.02933526411652565\n",
      "Epoch: 77/100 | step: 52/422 | loss: 0.024105804041028023\n",
      "Epoch: 77/100 | step: 53/422 | loss: 0.04445425420999527\n",
      "Epoch: 77/100 | step: 54/422 | loss: 0.10848015546798706\n",
      "Epoch: 77/100 | step: 55/422 | loss: 0.12853288650512695\n",
      "Epoch: 77/100 | step: 56/422 | loss: 0.03885149210691452\n",
      "Epoch: 77/100 | step: 57/422 | loss: 0.04164305329322815\n",
      "Epoch: 77/100 | step: 58/422 | loss: 0.035889171063899994\n",
      "Epoch: 77/100 | step: 59/422 | loss: 0.06578165292739868\n",
      "Epoch: 77/100 | step: 60/422 | loss: 0.027740318328142166\n",
      "Epoch: 77/100 | step: 61/422 | loss: 0.09276887029409409\n",
      "Epoch: 77/100 | step: 62/422 | loss: 0.0338120199739933\n",
      "Epoch: 77/100 | step: 63/422 | loss: 0.1556105613708496\n",
      "Epoch: 77/100 | step: 64/422 | loss: 0.24120235443115234\n",
      "Epoch: 77/100 | step: 65/422 | loss: 0.25843021273612976\n",
      "Epoch: 77/100 | step: 66/422 | loss: 0.04826721176505089\n",
      "Epoch: 77/100 | step: 67/422 | loss: 0.08234412223100662\n",
      "Epoch: 77/100 | step: 68/422 | loss: 0.04911581054329872\n",
      "Epoch: 77/100 | step: 69/422 | loss: 0.04763419181108475\n",
      "Epoch: 77/100 | step: 70/422 | loss: 0.10417348146438599\n",
      "Epoch: 77/100 | step: 71/422 | loss: 0.02545085921883583\n",
      "Epoch: 77/100 | step: 72/422 | loss: 0.042175643146038055\n",
      "Epoch: 77/100 | step: 73/422 | loss: 0.12965989112854004\n",
      "Epoch: 77/100 | step: 74/422 | loss: 0.29690101742744446\n",
      "Epoch: 77/100 | step: 75/422 | loss: 0.331744521856308\n",
      "Epoch: 77/100 | step: 76/422 | loss: 0.25485286116600037\n",
      "Epoch: 77/100 | step: 77/422 | loss: 0.1092747375369072\n",
      "Epoch: 77/100 | step: 78/422 | loss: 0.06926441192626953\n",
      "Epoch: 77/100 | step: 79/422 | loss: 0.044734805822372437\n",
      "Epoch: 77/100 | step: 80/422 | loss: 0.035490091890096664\n",
      "Epoch: 77/100 | step: 81/422 | loss: 0.058568522334098816\n",
      "Epoch: 77/100 | step: 82/422 | loss: 0.02148633636534214\n",
      "Epoch: 77/100 | step: 83/422 | loss: 0.07425955682992935\n",
      "Epoch: 77/100 | step: 84/422 | loss: 0.04202421009540558\n",
      "Epoch: 77/100 | step: 85/422 | loss: 0.028239551931619644\n",
      "Epoch: 77/100 | step: 86/422 | loss: 0.04518941789865494\n",
      "Epoch: 77/100 | step: 87/422 | loss: 0.03616924583911896\n",
      "Epoch: 77/100 | step: 88/422 | loss: 0.09437642991542816\n",
      "Epoch: 77/100 | step: 89/422 | loss: 0.09959572553634644\n",
      "Epoch: 77/100 | step: 90/422 | loss: 0.19968447089195251\n",
      "Epoch: 77/100 | step: 91/422 | loss: 0.10268296301364899\n",
      "Epoch: 77/100 | step: 92/422 | loss: 0.060195192694664\n",
      "Epoch: 77/100 | step: 93/422 | loss: 0.10265261679887772\n",
      "Epoch: 77/100 | step: 94/422 | loss: 0.028917593881487846\n",
      "Epoch: 77/100 | step: 95/422 | loss: 0.039740681648254395\n",
      "Epoch: 77/100 | step: 96/422 | loss: 0.03305555507540703\n",
      "Epoch: 77/100 | step: 97/422 | loss: 0.029636899009346962\n",
      "Epoch: 77/100 | step: 98/422 | loss: 0.05527152493596077\n",
      "Epoch: 77/100 | step: 99/422 | loss: 0.029102327302098274\n",
      "Epoch: 77/100 | step: 100/422 | loss: 0.05209391564130783\n",
      "Epoch: 77/100 | step: 101/422 | loss: 0.023688727989792824\n",
      "Epoch: 77/100 | step: 102/422 | loss: 0.025292720645666122\n",
      "Epoch: 77/100 | step: 103/422 | loss: 0.04737663269042969\n",
      "Epoch: 77/100 | step: 104/422 | loss: 0.06664155423641205\n",
      "Epoch: 77/100 | step: 105/422 | loss: 0.04016238451004028\n",
      "Epoch: 77/100 | step: 106/422 | loss: 0.06207290664315224\n",
      "Epoch: 77/100 | step: 107/422 | loss: 0.23253336548805237\n",
      "Epoch: 77/100 | step: 108/422 | loss: 0.04076644405722618\n",
      "Epoch: 77/100 | step: 109/422 | loss: 0.08843893557786942\n",
      "Epoch: 77/100 | step: 110/422 | loss: 0.08861403167247772\n",
      "Epoch: 77/100 | step: 111/422 | loss: 0.040904298424720764\n",
      "Epoch: 77/100 | step: 112/422 | loss: 0.04021309316158295\n",
      "Epoch: 77/100 | step: 113/422 | loss: 0.02201092801988125\n",
      "Epoch: 77/100 | step: 114/422 | loss: 0.027424689382314682\n",
      "Epoch: 77/100 | step: 115/422 | loss: 0.0459737591445446\n",
      "Epoch: 77/100 | step: 116/422 | loss: 0.0529937818646431\n",
      "Epoch: 77/100 | step: 117/422 | loss: 0.06757432222366333\n",
      "Epoch: 77/100 | step: 118/422 | loss: 0.04870114475488663\n",
      "Epoch: 77/100 | step: 119/422 | loss: 0.024787159636616707\n",
      "Epoch: 77/100 | step: 120/422 | loss: 0.024628473445773125\n",
      "Epoch: 77/100 | step: 121/422 | loss: 0.03572474792599678\n",
      "Epoch: 77/100 | step: 122/422 | loss: 0.08015139400959015\n",
      "Epoch: 77/100 | step: 123/422 | loss: 0.06129134073853493\n",
      "Epoch: 77/100 | step: 124/422 | loss: 0.03413871303200722\n",
      "Epoch: 77/100 | step: 125/422 | loss: 0.048044219613075256\n",
      "Epoch: 77/100 | step: 126/422 | loss: 0.0812445729970932\n",
      "Epoch: 77/100 | step: 127/422 | loss: 0.08184602856636047\n",
      "Epoch: 77/100 | step: 128/422 | loss: 0.026657456532120705\n",
      "Epoch: 77/100 | step: 129/422 | loss: 0.10376730561256409\n",
      "Epoch: 77/100 | step: 130/422 | loss: 0.04191825911402702\n",
      "Epoch: 77/100 | step: 131/422 | loss: 0.02533622458577156\n",
      "Epoch: 77/100 | step: 132/422 | loss: 0.02720893733203411\n",
      "Epoch: 77/100 | step: 133/422 | loss: 0.0409976989030838\n",
      "Epoch: 77/100 | step: 134/422 | loss: 0.029288340359926224\n",
      "Epoch: 77/100 | step: 135/422 | loss: 0.0857880488038063\n",
      "Epoch: 77/100 | step: 136/422 | loss: 0.03496308997273445\n",
      "Epoch: 77/100 | step: 137/422 | loss: 0.09202113002538681\n",
      "Epoch: 77/100 | step: 138/422 | loss: 0.018331337720155716\n",
      "Epoch: 77/100 | step: 139/422 | loss: 0.15840192139148712\n",
      "Epoch: 77/100 | step: 140/422 | loss: 0.10216657817363739\n",
      "Epoch: 77/100 | step: 141/422 | loss: 0.03777837008237839\n",
      "Epoch: 77/100 | step: 142/422 | loss: 0.08520093560218811\n",
      "Epoch: 77/100 | step: 143/422 | loss: 0.042356543242931366\n",
      "Epoch: 77/100 | step: 144/422 | loss: 0.035648271441459656\n",
      "Epoch: 77/100 | step: 145/422 | loss: 0.040868692100048065\n",
      "Epoch: 77/100 | step: 146/422 | loss: 0.06811003386974335\n",
      "Epoch: 77/100 | step: 147/422 | loss: 0.07334721088409424\n",
      "Epoch: 77/100 | step: 148/422 | loss: 0.04481714591383934\n",
      "Epoch: 77/100 | step: 149/422 | loss: 0.08779052644968033\n",
      "Epoch: 77/100 | step: 150/422 | loss: 0.23274004459381104\n",
      "Epoch: 77/100 | step: 151/422 | loss: 0.4133831262588501\n",
      "Epoch: 77/100 | step: 152/422 | loss: 0.2931458055973053\n",
      "Epoch: 77/100 | step: 153/422 | loss: 0.3383607864379883\n",
      "Epoch: 77/100 | step: 154/422 | loss: 0.40414243936538696\n",
      "Epoch: 77/100 | step: 155/422 | loss: 0.4941089451313019\n",
      "Epoch: 77/100 | step: 156/422 | loss: 0.03307681903243065\n",
      "Epoch: 77/100 | step: 157/422 | loss: 0.07440654933452606\n",
      "Epoch: 77/100 | step: 158/422 | loss: 0.12845942378044128\n",
      "Epoch: 77/100 | step: 159/422 | loss: 0.14648832380771637\n",
      "Epoch: 77/100 | step: 160/422 | loss: 0.09815719723701477\n",
      "Epoch: 77/100 | step: 161/422 | loss: 0.35830017924308777\n",
      "Epoch: 77/100 | step: 162/422 | loss: 0.04016810655593872\n",
      "Epoch: 77/100 | step: 163/422 | loss: 0.13027837872505188\n",
      "Epoch: 77/100 | step: 164/422 | loss: 0.07935383915901184\n",
      "Epoch: 77/100 | step: 165/422 | loss: 0.06623317301273346\n",
      "Epoch: 77/100 | step: 166/422 | loss: 0.10412026196718216\n",
      "Epoch: 77/100 | step: 167/422 | loss: 0.1250809133052826\n",
      "Epoch: 77/100 | step: 168/422 | loss: 0.14264249801635742\n",
      "Epoch: 77/100 | step: 169/422 | loss: 0.06272847205400467\n",
      "Epoch: 77/100 | step: 170/422 | loss: 0.28495633602142334\n",
      "Epoch: 77/100 | step: 171/422 | loss: 0.053773391991853714\n",
      "Epoch: 77/100 | step: 172/422 | loss: 0.05003074184060097\n",
      "Epoch: 77/100 | step: 173/422 | loss: 0.05623893439769745\n",
      "Epoch: 77/100 | step: 174/422 | loss: 0.13840246200561523\n",
      "Epoch: 77/100 | step: 175/422 | loss: 0.06141038239002228\n",
      "Epoch: 77/100 | step: 176/422 | loss: 0.14599324762821198\n",
      "Epoch: 77/100 | step: 177/422 | loss: 0.050948746502399445\n",
      "Epoch: 77/100 | step: 178/422 | loss: 0.06401051580905914\n",
      "Epoch: 77/100 | step: 179/422 | loss: 0.04906633123755455\n",
      "Epoch: 77/100 | step: 180/422 | loss: 0.11556995660066605\n",
      "Epoch: 77/100 | step: 181/422 | loss: 0.057369571179151535\n",
      "Epoch: 77/100 | step: 182/422 | loss: 0.050325602293014526\n",
      "Epoch: 77/100 | step: 183/422 | loss: 0.07486485689878464\n",
      "Epoch: 77/100 | step: 184/422 | loss: 0.11102201044559479\n",
      "Epoch: 77/100 | step: 185/422 | loss: 0.08609922230243683\n",
      "Epoch: 77/100 | step: 186/422 | loss: 0.051123179495334625\n",
      "Epoch: 77/100 | step: 187/422 | loss: 0.048437733203172684\n",
      "Epoch: 77/100 | step: 188/422 | loss: 0.05264100432395935\n",
      "Epoch: 77/100 | step: 189/422 | loss: 0.0942964106798172\n",
      "Epoch: 77/100 | step: 190/422 | loss: 0.030258748680353165\n",
      "Epoch: 77/100 | step: 191/422 | loss: 0.034070879220962524\n",
      "Epoch: 77/100 | step: 192/422 | loss: 0.10973020642995834\n",
      "Epoch: 77/100 | step: 193/422 | loss: 0.028346441686153412\n",
      "Epoch: 77/100 | step: 194/422 | loss: 0.06705529242753983\n",
      "Epoch: 77/100 | step: 195/422 | loss: 0.02752518095076084\n",
      "Epoch: 77/100 | step: 196/422 | loss: 0.051403820514678955\n",
      "Epoch: 77/100 | step: 197/422 | loss: 0.021174324676394463\n",
      "Epoch: 77/100 | step: 198/422 | loss: 0.028227632865309715\n",
      "Epoch: 77/100 | step: 199/422 | loss: 0.036515891551971436\n",
      "Epoch: 77/100 | step: 200/422 | loss: 0.026209259405732155\n",
      "Epoch: 77/100 | step: 201/422 | loss: 0.1570037454366684\n",
      "Epoch: 77/100 | step: 202/422 | loss: 0.048306070268154144\n",
      "Epoch: 77/100 | step: 203/422 | loss: 0.12004981189966202\n",
      "Epoch: 77/100 | step: 204/422 | loss: 0.04236452281475067\n",
      "Epoch: 77/100 | step: 205/422 | loss: 0.20345842838287354\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 78/100 | step: 1/422 | loss: 0.07240905612707138\n",
      "Epoch: 78/100 | step: 2/422 | loss: 0.03748422861099243\n",
      "Epoch: 78/100 | step: 3/422 | loss: 0.138163760304451\n",
      "Epoch: 78/100 | step: 4/422 | loss: 0.07607513666152954\n",
      "Epoch: 78/100 | step: 5/422 | loss: 0.15134203433990479\n",
      "Epoch: 78/100 | step: 6/422 | loss: 0.05679324269294739\n",
      "Epoch: 78/100 | step: 7/422 | loss: 0.027720361948013306\n",
      "Epoch: 78/100 | step: 8/422 | loss: 0.0939139649271965\n",
      "Epoch: 78/100 | step: 9/422 | loss: 0.026424948126077652\n",
      "Epoch: 78/100 | step: 10/422 | loss: 0.01925045996904373\n",
      "Epoch: 78/100 | step: 11/422 | loss: 0.02349916286766529\n",
      "Epoch: 78/100 | step: 12/422 | loss: 0.03150460496544838\n",
      "Epoch: 78/100 | step: 13/422 | loss: 0.020890304818749428\n",
      "Epoch: 78/100 | step: 14/422 | loss: 0.020322682335972786\n",
      "Epoch: 78/100 | step: 15/422 | loss: 0.03462379053235054\n",
      "Epoch: 78/100 | step: 16/422 | loss: 0.02802460454404354\n",
      "Epoch: 78/100 | step: 17/422 | loss: 0.026034370064735413\n",
      "Epoch: 78/100 | step: 18/422 | loss: 0.043104104697704315\n",
      "Epoch: 78/100 | step: 19/422 | loss: 0.0351395420730114\n",
      "Epoch: 78/100 | step: 20/422 | loss: 0.023856839165091515\n",
      "Epoch: 78/100 | step: 21/422 | loss: 0.017165128141641617\n",
      "Epoch: 78/100 | step: 22/422 | loss: 0.05355774238705635\n",
      "Epoch: 78/100 | step: 23/422 | loss: 0.015660656616091728\n",
      "Epoch: 78/100 | step: 24/422 | loss: 0.020241253077983856\n",
      "Epoch: 78/100 | step: 25/422 | loss: 0.06522006541490555\n",
      "Epoch: 78/100 | step: 26/422 | loss: 0.10395212471485138\n",
      "Epoch: 78/100 | step: 27/422 | loss: 0.13529285788536072\n",
      "Epoch: 78/100 | step: 28/422 | loss: 0.13486412167549133\n",
      "Epoch: 78/100 | step: 29/422 | loss: 0.08809632062911987\n",
      "Epoch: 78/100 | step: 30/422 | loss: 0.07270824909210205\n",
      "Epoch: 78/100 | step: 31/422 | loss: 0.2481006383895874\n",
      "Epoch: 78/100 | step: 32/422 | loss: 0.1260319948196411\n",
      "Epoch: 78/100 | step: 33/422 | loss: 0.03565097972750664\n",
      "Epoch: 78/100 | step: 34/422 | loss: 0.05169467628002167\n",
      "Epoch: 78/100 | step: 35/422 | loss: 0.036056820303201675\n",
      "Epoch: 78/100 | step: 36/422 | loss: 0.11509840190410614\n",
      "Epoch: 78/100 | step: 37/422 | loss: 0.11892994493246078\n",
      "Epoch: 78/100 | step: 38/422 | loss: 0.13758674263954163\n",
      "Epoch: 78/100 | step: 39/422 | loss: 0.06618333607912064\n",
      "Epoch: 78/100 | step: 40/422 | loss: 0.18294887244701385\n",
      "Epoch: 78/100 | step: 41/422 | loss: 0.1684720814228058\n",
      "Epoch: 78/100 | step: 42/422 | loss: 0.10395374894142151\n",
      "Epoch: 78/100 | step: 43/422 | loss: 0.05281544104218483\n",
      "Epoch: 78/100 | step: 44/422 | loss: 0.09013873338699341\n",
      "Epoch: 78/100 | step: 45/422 | loss: 0.033770106732845306\n",
      "Epoch: 78/100 | step: 46/422 | loss: 0.058296628296375275\n",
      "Epoch: 78/100 | step: 47/422 | loss: 0.03422990441322327\n",
      "Epoch: 78/100 | step: 48/422 | loss: 0.034057460725307465\n",
      "Epoch: 78/100 | step: 49/422 | loss: 0.04038972407579422\n",
      "Epoch: 78/100 | step: 50/422 | loss: 0.03729359060525894\n",
      "Epoch: 78/100 | step: 51/422 | loss: 0.13728414475917816\n",
      "Epoch: 78/100 | step: 52/422 | loss: 0.05727970227599144\n",
      "Epoch: 78/100 | step: 53/422 | loss: 0.04037468135356903\n",
      "Epoch: 78/100 | step: 54/422 | loss: 0.020538602024316788\n",
      "Epoch: 78/100 | step: 55/422 | loss: 0.043429259210824966\n",
      "Epoch: 78/100 | step: 56/422 | loss: 0.033455878496170044\n",
      "Epoch: 78/100 | step: 57/422 | loss: 0.01898501254618168\n",
      "Epoch: 78/100 | step: 58/422 | loss: 0.054195284843444824\n",
      "Epoch: 78/100 | step: 59/422 | loss: 0.038587260991334915\n",
      "Epoch: 78/100 | step: 60/422 | loss: 0.11314631253480911\n",
      "Epoch: 78/100 | step: 61/422 | loss: 0.044193949550390244\n",
      "Epoch: 78/100 | step: 62/422 | loss: 0.02190687693655491\n",
      "Epoch: 78/100 | step: 63/422 | loss: 0.012956497259438038\n",
      "Epoch: 78/100 | step: 64/422 | loss: 0.047987841069698334\n",
      "Epoch: 78/100 | step: 65/422 | loss: 0.3067091405391693\n",
      "Epoch: 78/100 | step: 66/422 | loss: 0.20697037875652313\n",
      "Epoch: 78/100 | step: 67/422 | loss: 0.23640169203281403\n",
      "Epoch: 78/100 | step: 68/422 | loss: 0.044023383408784866\n",
      "Epoch: 78/100 | step: 69/422 | loss: 0.023318395018577576\n",
      "Epoch: 78/100 | step: 70/422 | loss: 0.2914215326309204\n",
      "Epoch: 78/100 | step: 71/422 | loss: 0.11961787194013596\n",
      "Epoch: 78/100 | step: 72/422 | loss: 0.028462650254368782\n",
      "Epoch: 78/100 | step: 73/422 | loss: 0.035624440759420395\n",
      "Epoch: 78/100 | step: 74/422 | loss: 0.1661851555109024\n",
      "Epoch: 78/100 | step: 75/422 | loss: 0.13872118294239044\n",
      "Epoch: 78/100 | step: 76/422 | loss: 0.19960150122642517\n",
      "Epoch: 78/100 | step: 77/422 | loss: 0.040322087705135345\n",
      "Epoch: 78/100 | step: 78/422 | loss: 0.0896519348025322\n",
      "Epoch: 78/100 | step: 79/422 | loss: 0.03840065747499466\n",
      "Epoch: 78/100 | step: 80/422 | loss: 0.02374895103275776\n",
      "Epoch: 78/100 | step: 81/422 | loss: 0.0726155936717987\n",
      "Epoch: 78/100 | step: 82/422 | loss: 0.33805909752845764\n",
      "Epoch: 78/100 | step: 83/422 | loss: 0.15702146291732788\n",
      "Epoch: 78/100 | step: 84/422 | loss: 0.03395061194896698\n",
      "Epoch: 78/100 | step: 85/422 | loss: 0.07305607944726944\n",
      "Epoch: 78/100 | step: 86/422 | loss: 0.03317058086395264\n",
      "Epoch: 78/100 | step: 87/422 | loss: 0.062278471887111664\n",
      "Epoch: 78/100 | step: 88/422 | loss: 0.054607804864645004\n",
      "Epoch: 78/100 | step: 89/422 | loss: 0.03710503876209259\n",
      "Epoch: 78/100 | step: 90/422 | loss: 0.14143462479114532\n",
      "Epoch: 78/100 | step: 91/422 | loss: 0.0466025210916996\n",
      "Epoch: 78/100 | step: 92/422 | loss: 0.09353197365999222\n",
      "Epoch: 78/100 | step: 93/422 | loss: 0.13731127977371216\n",
      "Epoch: 78/100 | step: 94/422 | loss: 0.03898758441209793\n",
      "Epoch: 78/100 | step: 95/422 | loss: 0.08424200862646103\n",
      "Epoch: 78/100 | step: 96/422 | loss: 0.047208771109580994\n",
      "Epoch: 78/100 | step: 97/422 | loss: 0.03717136010527611\n",
      "Epoch: 78/100 | step: 98/422 | loss: 0.03051689825952053\n",
      "Epoch: 78/100 | step: 99/422 | loss: 0.05234893783926964\n",
      "Epoch: 78/100 | step: 100/422 | loss: 0.0835922434926033\n",
      "Epoch: 78/100 | step: 101/422 | loss: 0.1069122925400734\n",
      "Epoch: 78/100 | step: 102/422 | loss: 0.040531061589717865\n",
      "Epoch: 78/100 | step: 103/422 | loss: 0.06381293386220932\n",
      "Epoch: 78/100 | step: 104/422 | loss: 0.036887358874082565\n",
      "Epoch: 78/100 | step: 105/422 | loss: 0.06206388771533966\n",
      "Epoch: 78/100 | step: 106/422 | loss: 0.1463329941034317\n",
      "Epoch: 78/100 | step: 107/422 | loss: 0.1170523390173912\n",
      "Epoch: 78/100 | step: 108/422 | loss: 0.024364586919546127\n",
      "Epoch: 78/100 | step: 109/422 | loss: 0.0727861151099205\n",
      "Epoch: 78/100 | step: 110/422 | loss: 0.10922113060951233\n",
      "Epoch: 78/100 | step: 111/422 | loss: 0.04144154489040375\n",
      "Epoch: 78/100 | step: 112/422 | loss: 0.050010811537504196\n",
      "Epoch: 78/100 | step: 113/422 | loss: 0.09267636388540268\n",
      "Epoch: 78/100 | step: 114/422 | loss: 0.037478573620319366\n",
      "Epoch: 78/100 | step: 115/422 | loss: 0.02787616103887558\n",
      "Epoch: 78/100 | step: 116/422 | loss: 0.1525004506111145\n",
      "Epoch: 78/100 | step: 117/422 | loss: 0.13012629747390747\n",
      "Epoch: 78/100 | step: 118/422 | loss: 0.04680665582418442\n",
      "Epoch: 78/100 | step: 119/422 | loss: 0.024353567510843277\n",
      "Epoch: 78/100 | step: 120/422 | loss: 0.21996963024139404\n",
      "Epoch: 78/100 | step: 121/422 | loss: 0.05488476902246475\n",
      "Epoch: 78/100 | step: 122/422 | loss: 0.03646610677242279\n",
      "Epoch: 78/100 | step: 123/422 | loss: 0.05759945139288902\n",
      "Epoch: 78/100 | step: 124/422 | loss: 0.03365558758378029\n",
      "Epoch: 78/100 | step: 125/422 | loss: 0.03290896490216255\n",
      "Epoch: 78/100 | step: 126/422 | loss: 0.035598672926425934\n",
      "Epoch: 78/100 | step: 127/422 | loss: 0.031595148146152496\n",
      "Epoch: 78/100 | step: 128/422 | loss: 0.16881243884563446\n",
      "Epoch: 78/100 | step: 129/422 | loss: 0.023852618411183357\n",
      "Epoch: 78/100 | step: 130/422 | loss: 0.06305040419101715\n",
      "Epoch: 78/100 | step: 131/422 | loss: 0.05352877452969551\n",
      "Epoch: 78/100 | step: 132/422 | loss: 0.07471399009227753\n",
      "Epoch: 78/100 | step: 133/422 | loss: 0.29803338646888733\n",
      "Epoch: 78/100 | step: 134/422 | loss: 0.23685480654239655\n",
      "Epoch: 78/100 | step: 135/422 | loss: 0.21696847677230835\n",
      "Epoch: 78/100 | step: 136/422 | loss: 0.15690788626670837\n",
      "Epoch: 78/100 | step: 137/422 | loss: 0.06872519850730896\n",
      "Epoch: 78/100 | step: 138/422 | loss: 0.1963956356048584\n",
      "Epoch: 78/100 | step: 139/422 | loss: 0.13492508232593536\n",
      "Epoch: 78/100 | step: 140/422 | loss: 0.16792385280132294\n",
      "Epoch: 78/100 | step: 141/422 | loss: 0.11193973571062088\n",
      "Epoch: 78/100 | step: 142/422 | loss: 0.17434966564178467\n",
      "Epoch: 78/100 | step: 143/422 | loss: 0.03455180674791336\n",
      "Epoch: 78/100 | step: 144/422 | loss: 0.04788398742675781\n",
      "Epoch: 78/100 | step: 145/422 | loss: 0.04579520970582962\n",
      "Epoch: 78/100 | step: 146/422 | loss: 0.09689916670322418\n",
      "Epoch: 78/100 | step: 147/422 | loss: 0.05892466753721237\n",
      "Epoch: 78/100 | step: 148/422 | loss: 0.2890300452709198\n",
      "Epoch: 78/100 | step: 149/422 | loss: 0.0893770083785057\n",
      "Epoch: 78/100 | step: 150/422 | loss: 0.12251433730125427\n",
      "Epoch: 78/100 | step: 151/422 | loss: 0.061187390238046646\n",
      "Epoch: 78/100 | step: 152/422 | loss: 0.06851959228515625\n",
      "Epoch: 78/100 | step: 153/422 | loss: 0.028313031420111656\n",
      "Epoch: 78/100 | step: 154/422 | loss: 0.09289240092039108\n",
      "Epoch: 78/100 | step: 155/422 | loss: 0.027323944494128227\n",
      "Epoch: 78/100 | step: 156/422 | loss: 0.03722269460558891\n",
      "Epoch: 78/100 | step: 157/422 | loss: 0.0735335648059845\n",
      "Epoch: 78/100 | step: 158/422 | loss: 0.0361594632267952\n",
      "Epoch: 78/100 | step: 159/422 | loss: 0.035768695175647736\n",
      "Epoch: 78/100 | step: 160/422 | loss: 0.030038760975003242\n",
      "Epoch: 78/100 | step: 161/422 | loss: 0.035188984125852585\n",
      "Epoch: 78/100 | step: 162/422 | loss: 0.03614429011940956\n",
      "Epoch: 78/100 | step: 163/422 | loss: 0.03582172840833664\n",
      "Epoch: 78/100 | step: 164/422 | loss: 0.2478787750005722\n",
      "Epoch: 78/100 | step: 165/422 | loss: 0.03739382326602936\n",
      "Epoch: 78/100 | step: 166/422 | loss: 0.03164207562804222\n",
      "Epoch: 78/100 | step: 167/422 | loss: 0.0249503031373024\n",
      "Epoch: 78/100 | step: 168/422 | loss: 0.035992417484521866\n",
      "Epoch: 78/100 | step: 169/422 | loss: 0.08392920345067978\n",
      "Epoch: 78/100 | step: 170/422 | loss: 0.2154230773448944\n",
      "Epoch: 78/100 | step: 171/422 | loss: 0.10658591985702515\n",
      "Epoch: 78/100 | step: 172/422 | loss: 0.08959297090768814\n",
      "Epoch: 78/100 | step: 173/422 | loss: 0.11848916113376617\n",
      "Epoch: 78/100 | step: 174/422 | loss: 0.02689611352980137\n",
      "Epoch: 78/100 | step: 175/422 | loss: 0.05960826575756073\n",
      "Epoch: 78/100 | step: 176/422 | loss: 0.05801183357834816\n",
      "Epoch: 78/100 | step: 177/422 | loss: 0.049856700003147125\n",
      "Epoch: 78/100 | step: 178/422 | loss: 0.02899647131562233\n",
      "Epoch: 78/100 | step: 179/422 | loss: 0.044190503656864166\n",
      "Epoch: 78/100 | step: 180/422 | loss: 0.029942583292722702\n",
      "Epoch: 78/100 | step: 181/422 | loss: 0.0363537073135376\n",
      "Epoch: 78/100 | step: 182/422 | loss: 0.06910331547260284\n",
      "Epoch: 78/100 | step: 183/422 | loss: 0.04957485944032669\n",
      "Epoch: 78/100 | step: 184/422 | loss: 0.04475221037864685\n",
      "Epoch: 78/100 | step: 185/422 | loss: 0.14759987592697144\n",
      "Epoch: 78/100 | step: 186/422 | loss: 0.06698320806026459\n",
      "Epoch: 78/100 | step: 187/422 | loss: 0.051499754190444946\n",
      "Epoch: 78/100 | step: 188/422 | loss: 0.13772742450237274\n",
      "Epoch: 78/100 | step: 189/422 | loss: 0.033291760832071304\n",
      "Epoch: 78/100 | step: 190/422 | loss: 0.03148658573627472\n",
      "Epoch: 78/100 | step: 191/422 | loss: 0.04468117281794548\n",
      "Epoch: 78/100 | step: 192/422 | loss: 0.06269616633653641\n",
      "Epoch: 78/100 | step: 193/422 | loss: 0.098750539124012\n",
      "Epoch: 78/100 | step: 194/422 | loss: 0.13330218195915222\n",
      "Epoch: 78/100 | step: 195/422 | loss: 0.16915260255336761\n",
      "Epoch: 78/100 | step: 196/422 | loss: 0.11252081394195557\n",
      "Epoch: 78/100 | step: 197/422 | loss: 0.19971325993537903\n",
      "Epoch: 78/100 | step: 198/422 | loss: 0.15886478126049042\n",
      "Epoch: 78/100 | step: 199/422 | loss: 0.33670660853385925\n",
      "Epoch: 78/100 | step: 200/422 | loss: 0.16031834483146667\n",
      "Epoch: 78/100 | step: 201/422 | loss: 0.058430016040802\n",
      "Epoch: 78/100 | step: 202/422 | loss: 0.390333890914917\n",
      "Epoch: 78/100 | step: 203/422 | loss: 1.0345748662948608\n",
      "Epoch: 78/100 | step: 204/422 | loss: 0.3493398427963257\n",
      "Epoch: 78/100 | step: 205/422 | loss: 0.646018922328949\n",
      "Epoch: 78/100 | step: 206/422 | loss: 0.41971340775489807\n",
      "Epoch: 78/100 | step: 207/422 | loss: 0.6353592872619629\n",
      "Epoch: 78/100 | step: 208/422 | loss: 0.2266048938035965\n",
      "Epoch: 78/100 | step: 209/422 | loss: 0.19930437207221985\n",
      "Epoch: 78/100 | step: 210/422 | loss: 0.08527591079473495\n",
      "Epoch: 78/100 | step: 211/422 | loss: 0.47200003266334534\n",
      "Epoch: 78/100 | step: 212/422 | loss: 0.5147658586502075\n",
      "Epoch: 78/100 | step: 213/422 | loss: 0.07410439848899841\n",
      "Epoch: 78/100 | step: 214/422 | loss: 0.039190709590911865\n",
      "Epoch: 78/100 | step: 215/422 | loss: 0.07871260493993759\n",
      "Epoch: 78/100 | step: 216/422 | loss: 0.0338263064622879\n",
      "Epoch: 78/100 | step: 217/422 | loss: 0.06429526209831238\n",
      "Epoch: 78/100 | step: 218/422 | loss: 0.1653968095779419\n",
      "Epoch: 78/100 | step: 219/422 | loss: 0.18453718721866608\n",
      "Epoch: 78/100 | step: 220/422 | loss: 0.052069585770368576\n",
      "Epoch: 78/100 | step: 221/422 | loss: 0.03679559752345085\n",
      "Epoch: 78/100 | step: 222/422 | loss: 0.13757678866386414\n",
      "Epoch: 78/100 | step: 223/422 | loss: 0.18641041219234467\n",
      "Epoch: 78/100 | step: 224/422 | loss: 0.23784956336021423\n",
      "Epoch: 78/100 | step: 225/422 | loss: 0.11441722512245178\n",
      "Epoch: 78/100 | step: 226/422 | loss: 0.039472248405218124\n",
      "Epoch: 78/100 | step: 227/422 | loss: 0.12134344130754471\n",
      "Epoch: 78/100 | step: 228/422 | loss: 0.024729378521442413\n",
      "Epoch: 78/100 | step: 229/422 | loss: 0.05246793106198311\n",
      "Epoch: 78/100 | step: 230/422 | loss: 0.039515331387519836\n",
      "Epoch: 78/100 | step: 231/422 | loss: 0.03363802284002304\n",
      "Epoch: 78/100 | step: 232/422 | loss: 0.14410075545310974\n",
      "Epoch: 78/100 | step: 233/422 | loss: 0.07089884579181671\n",
      "Epoch: 78/100 | step: 234/422 | loss: 0.059472355991601944\n",
      "Epoch: 78/100 | step: 235/422 | loss: 0.1090625748038292\n",
      "Epoch: 78/100 | step: 236/422 | loss: 0.0681534931063652\n",
      "Epoch: 78/100 | step: 237/422 | loss: 0.09514763951301575\n",
      "Epoch: 78/100 | step: 238/422 | loss: 0.04753369092941284\n",
      "Epoch: 78/100 | step: 239/422 | loss: 0.05273488909006119\n",
      "Epoch: 78/100 | step: 240/422 | loss: 0.06771482527256012\n",
      "Epoch: 78/100 | step: 241/422 | loss: 0.1727941632270813\n",
      "Epoch: 78/100 | step: 242/422 | loss: 0.020329298451542854\n",
      "Epoch: 78/100 | step: 243/422 | loss: 0.07929640263319016\n",
      "Epoch: 78/100 | step: 244/422 | loss: 0.090458445250988\n",
      "Epoch: 78/100 | step: 245/422 | loss: 0.021175285801291466\n",
      "Epoch: 78/100 | step: 246/422 | loss: 0.11085613071918488\n",
      "Epoch: 78/100 | step: 247/422 | loss: 0.01866535097360611\n",
      "Epoch: 78/100 | step: 248/422 | loss: 0.13222211599349976\n",
      "Epoch: 78/100 | step: 249/422 | loss: 0.23801784217357635\n",
      "Epoch: 78/100 | step: 250/422 | loss: 0.047167737036943436\n",
      "Epoch: 78/100 | step: 251/422 | loss: 0.03610572963953018\n",
      "Epoch: 78/100 | step: 252/422 | loss: 0.04529087617993355\n",
      "Epoch: 78/100 | step: 253/422 | loss: 0.0679033026099205\n",
      "Epoch: 78/100 | step: 254/422 | loss: 0.13813799619674683\n",
      "Epoch: 78/100 | step: 255/422 | loss: 0.02819857932627201\n",
      "Epoch: 78/100 | step: 256/422 | loss: 0.09538441151380539\n",
      "Epoch: 78/100 | step: 257/422 | loss: 0.05192304775118828\n",
      "Epoch: 78/100 | step: 258/422 | loss: 0.055288709700107574\n",
      "Epoch: 78/100 | step: 259/422 | loss: 0.03383594751358032\n",
      "Epoch: 78/100 | step: 260/422 | loss: 0.04753166437149048\n",
      "Epoch: 78/100 | step: 261/422 | loss: 0.04211852699518204\n",
      "Epoch: 78/100 | step: 262/422 | loss: 0.010707450099289417\n",
      "Epoch: 78/100 | step: 263/422 | loss: 0.04672078415751457\n",
      "Epoch: 78/100 | step: 264/422 | loss: 0.030780352652072906\n",
      "Epoch: 78/100 | step: 265/422 | loss: 0.040575966238975525\n",
      "Epoch: 78/100 | step: 266/422 | loss: 0.09231695532798767\n",
      "Epoch: 78/100 | step: 267/422 | loss: 0.017665013670921326\n",
      "Epoch: 78/100 | step: 268/422 | loss: 0.03168527036905289\n",
      "Epoch: 78/100 | step: 269/422 | loss: 0.031723879277706146\n",
      "Epoch: 78/100 | step: 270/422 | loss: 0.04849018156528473\n",
      "Epoch: 78/100 | step: 271/422 | loss: 0.028987161815166473\n",
      "Epoch: 78/100 | step: 272/422 | loss: 0.03292238712310791\n",
      "Epoch: 78/100 | step: 273/422 | loss: 0.03678972274065018\n",
      "Epoch: 78/100 | step: 274/422 | loss: 0.019976772367954254\n",
      "Epoch: 78/100 | step: 275/422 | loss: 0.030678851529955864\n",
      "Epoch: 78/100 | step: 276/422 | loss: 0.02820420078933239\n",
      "Epoch: 78/100 | step: 277/422 | loss: 0.07852858304977417\n",
      "Epoch: 78/100 | step: 278/422 | loss: 0.02364545315504074\n",
      "Epoch: 78/100 | step: 279/422 | loss: 0.01972566731274128\n",
      "Epoch: 78/100 | step: 280/422 | loss: 0.020552203059196472\n",
      "Epoch: 78/100 | step: 281/422 | loss: 0.03022276610136032\n",
      "Epoch: 78/100 | step: 282/422 | loss: 0.1018265038728714\n",
      "Epoch: 78/100 | step: 283/422 | loss: 0.07961565256118774\n",
      "Epoch: 78/100 | step: 284/422 | loss: 0.05554725602269173\n",
      "Epoch: 78/100 | step: 285/422 | loss: 0.03788686543703079\n",
      "Epoch: 78/100 | step: 286/422 | loss: 0.032095763832330704\n",
      "Epoch: 78/100 | step: 287/422 | loss: 0.031861722469329834\n",
      "Epoch: 78/100 | step: 288/422 | loss: 0.025977615267038345\n",
      "Epoch: 78/100 | step: 289/422 | loss: 0.11297398805618286\n",
      "Epoch: 78/100 | step: 290/422 | loss: 0.1020357757806778\n",
      "Epoch: 78/100 | step: 291/422 | loss: 0.02885999158024788\n",
      "Epoch: 78/100 | step: 292/422 | loss: 0.034710679203271866\n",
      "Epoch: 78/100 | step: 293/422 | loss: 0.20174731314182281\n",
      "Epoch: 78/100 | step: 294/422 | loss: 0.13584592938423157\n",
      "Epoch: 78/100 | step: 295/422 | loss: 0.0320519395172596\n",
      "Epoch: 78/100 | step: 296/422 | loss: 0.054419200867414474\n",
      "Epoch: 78/100 | step: 297/422 | loss: 0.04094184935092926\n",
      "Epoch: 78/100 | step: 298/422 | loss: 0.07290250808000565\n",
      "Epoch: 78/100 | step: 299/422 | loss: 0.08330325782299042\n",
      "Epoch: 78/100 | step: 300/422 | loss: 0.09790629148483276\n",
      "Epoch: 78/100 | step: 301/422 | loss: 0.05866893753409386\n",
      "Epoch: 78/100 | step: 302/422 | loss: 0.06517953425645828\n",
      "Epoch: 78/100 | step: 303/422 | loss: 0.05827287584543228\n",
      "Epoch: 78/100 | step: 304/422 | loss: 0.09590867161750793\n",
      "Epoch: 78/100 | step: 305/422 | loss: 0.13790170848369598\n",
      "Epoch: 78/100 | step: 306/422 | loss: 0.22298994660377502\n",
      "Epoch: 78/100 | step: 307/422 | loss: 0.04564348980784416\n",
      "Epoch: 78/100 | step: 308/422 | loss: 0.05587213486433029\n",
      "Epoch: 78/100 | step: 309/422 | loss: 0.04670663923025131\n",
      "Epoch: 78/100 | step: 310/422 | loss: 0.08748825639486313\n",
      "Epoch: 78/100 | step: 311/422 | loss: 0.04785637930035591\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 79/100 | step: 1/422 | loss: 0.044820815324783325\n",
      "Epoch: 79/100 | step: 2/422 | loss: 0.03530782088637352\n",
      "Epoch: 79/100 | step: 3/422 | loss: 0.12637346982955933\n",
      "Epoch: 79/100 | step: 4/422 | loss: 0.028763802722096443\n",
      "Epoch: 79/100 | step: 5/422 | loss: 0.02495909482240677\n",
      "Epoch: 79/100 | step: 6/422 | loss: 0.036914050579071045\n",
      "Epoch: 79/100 | step: 7/422 | loss: 0.03477451577782631\n",
      "Epoch: 79/100 | step: 8/422 | loss: 0.028286030516028404\n",
      "Epoch: 79/100 | step: 9/422 | loss: 0.0302842129021883\n",
      "Epoch: 79/100 | step: 10/422 | loss: 0.031440336257219315\n",
      "Epoch: 79/100 | step: 11/422 | loss: 0.01760610193014145\n",
      "Epoch: 79/100 | step: 12/422 | loss: 0.023948704823851585\n",
      "Epoch: 79/100 | step: 13/422 | loss: 0.02417740225791931\n",
      "Epoch: 79/100 | step: 14/422 | loss: 0.051611002534627914\n",
      "Epoch: 79/100 | step: 15/422 | loss: 0.033000558614730835\n",
      "Epoch: 79/100 | step: 16/422 | loss: 0.02844545617699623\n",
      "Epoch: 79/100 | step: 17/422 | loss: 0.013360298238694668\n",
      "Epoch: 79/100 | step: 18/422 | loss: 0.05193976312875748\n",
      "Epoch: 79/100 | step: 19/422 | loss: 0.04955219477415085\n",
      "Epoch: 79/100 | step: 20/422 | loss: 0.014780900441110134\n",
      "Epoch: 79/100 | step: 21/422 | loss: 0.02927088737487793\n",
      "Epoch: 79/100 | step: 22/422 | loss: 0.026889599859714508\n",
      "Epoch: 79/100 | step: 23/422 | loss: 0.10652997344732285\n",
      "Epoch: 79/100 | step: 24/422 | loss: 0.06976761668920517\n",
      "Epoch: 79/100 | step: 25/422 | loss: 0.02371654473245144\n",
      "Epoch: 79/100 | step: 26/422 | loss: 0.05504267290234566\n",
      "Epoch: 79/100 | step: 27/422 | loss: 0.023245785385370255\n",
      "Epoch: 79/100 | step: 28/422 | loss: 0.04567404091358185\n",
      "Epoch: 79/100 | step: 29/422 | loss: 0.031990136951208115\n",
      "Epoch: 79/100 | step: 30/422 | loss: 0.011866122484207153\n",
      "Epoch: 79/100 | step: 31/422 | loss: 0.03759942576289177\n",
      "Epoch: 79/100 | step: 32/422 | loss: 0.06527924537658691\n",
      "Epoch: 79/100 | step: 33/422 | loss: 0.01811315305531025\n",
      "Epoch: 79/100 | step: 34/422 | loss: 0.05104083567857742\n",
      "Epoch: 79/100 | step: 35/422 | loss: 0.08545639365911484\n",
      "Epoch: 79/100 | step: 36/422 | loss: 0.04696284234523773\n",
      "Epoch: 79/100 | step: 37/422 | loss: 0.04446297138929367\n",
      "Epoch: 79/100 | step: 38/422 | loss: 0.02224813401699066\n",
      "Epoch: 79/100 | step: 39/422 | loss: 0.019687004387378693\n",
      "Epoch: 79/100 | step: 40/422 | loss: 0.04153518006205559\n",
      "Epoch: 79/100 | step: 41/422 | loss: 0.013809913769364357\n",
      "Epoch: 79/100 | step: 42/422 | loss: 0.033575452864170074\n",
      "Epoch: 79/100 | step: 43/422 | loss: 0.025686513632535934\n",
      "Epoch: 79/100 | step: 44/422 | loss: 0.016835307702422142\n",
      "Epoch: 79/100 | step: 45/422 | loss: 0.022638371214270592\n",
      "Epoch: 79/100 | step: 46/422 | loss: 0.01562763750553131\n",
      "Epoch: 79/100 | step: 47/422 | loss: 0.07696334272623062\n",
      "Epoch: 79/100 | step: 48/422 | loss: 0.04202418401837349\n",
      "Epoch: 79/100 | step: 49/422 | loss: 0.06842034310102463\n",
      "Epoch: 79/100 | step: 50/422 | loss: 0.0575447753071785\n",
      "Epoch: 79/100 | step: 51/422 | loss: 0.10840365290641785\n",
      "Epoch: 79/100 | step: 52/422 | loss: 0.07610559463500977\n",
      "Epoch: 79/100 | step: 53/422 | loss: 0.031901534646749496\n",
      "Epoch: 79/100 | step: 54/422 | loss: 0.0229992363601923\n",
      "Epoch: 79/100 | step: 55/422 | loss: 0.030949542298913002\n",
      "Epoch: 79/100 | step: 56/422 | loss: 0.04021279513835907\n",
      "Epoch: 79/100 | step: 57/422 | loss: 0.04142278432846069\n",
      "Epoch: 79/100 | step: 58/422 | loss: 0.015570229850709438\n",
      "Epoch: 79/100 | step: 59/422 | loss: 0.057922255247831345\n",
      "Epoch: 79/100 | step: 60/422 | loss: 0.045327626168727875\n",
      "Epoch: 79/100 | step: 61/422 | loss: 0.03035103902220726\n",
      "Epoch: 79/100 | step: 62/422 | loss: 0.08482086658477783\n",
      "Epoch: 79/100 | step: 63/422 | loss: 0.13218356668949127\n",
      "Epoch: 79/100 | step: 64/422 | loss: 0.031180590391159058\n",
      "Epoch: 79/100 | step: 65/422 | loss: 0.1029866561293602\n",
      "Epoch: 79/100 | step: 66/422 | loss: 0.046341560781002045\n",
      "Epoch: 79/100 | step: 67/422 | loss: 0.10384828597307205\n",
      "Epoch: 79/100 | step: 68/422 | loss: 0.07038440555334091\n",
      "Epoch: 79/100 | step: 69/422 | loss: 0.023266566917300224\n",
      "Epoch: 79/100 | step: 70/422 | loss: 0.03293865919113159\n",
      "Epoch: 79/100 | step: 71/422 | loss: 0.1028822660446167\n",
      "Epoch: 79/100 | step: 72/422 | loss: 0.04461343586444855\n",
      "Epoch: 79/100 | step: 73/422 | loss: 0.04192386567592621\n",
      "Epoch: 79/100 | step: 74/422 | loss: 0.025528082624077797\n",
      "Epoch: 79/100 | step: 75/422 | loss: 0.022353217005729675\n",
      "Epoch: 79/100 | step: 76/422 | loss: 0.04050039127469063\n",
      "Epoch: 79/100 | step: 77/422 | loss: 0.02972002699971199\n",
      "Epoch: 79/100 | step: 78/422 | loss: 0.04266706109046936\n",
      "Epoch: 79/100 | step: 79/422 | loss: 0.020526358857750893\n",
      "Epoch: 79/100 | step: 80/422 | loss: 0.02154967002570629\n",
      "Epoch: 79/100 | step: 81/422 | loss: 0.026087254285812378\n",
      "Epoch: 79/100 | step: 82/422 | loss: 0.038170527666807175\n",
      "Epoch: 79/100 | step: 83/422 | loss: 0.0161802526563406\n",
      "Epoch: 79/100 | step: 84/422 | loss: 0.031816884875297546\n",
      "Epoch: 79/100 | step: 85/422 | loss: 0.021097712218761444\n",
      "Epoch: 79/100 | step: 86/422 | loss: 0.017798425629734993\n",
      "Epoch: 79/100 | step: 87/422 | loss: 0.06709707528352737\n",
      "Epoch: 79/100 | step: 88/422 | loss: 0.03721969574689865\n",
      "Epoch: 79/100 | step: 89/422 | loss: 0.020933862775564194\n",
      "Epoch: 79/100 | step: 90/422 | loss: 0.03831866756081581\n",
      "Epoch: 79/100 | step: 91/422 | loss: 0.056484468281269073\n",
      "Epoch: 79/100 | step: 92/422 | loss: 0.029743434861302376\n",
      "Epoch: 79/100 | step: 93/422 | loss: 0.01646706834435463\n",
      "Epoch: 79/100 | step: 94/422 | loss: 0.01742863468825817\n",
      "Epoch: 79/100 | step: 95/422 | loss: 0.03690977394580841\n",
      "Epoch: 79/100 | step: 96/422 | loss: 0.059164922684431076\n",
      "Epoch: 79/100 | step: 97/422 | loss: 0.019390039145946503\n",
      "Epoch: 79/100 | step: 98/422 | loss: 0.020105596631765366\n",
      "Epoch: 79/100 | step: 99/422 | loss: 0.026016054674983025\n",
      "Epoch: 79/100 | step: 100/422 | loss: 0.03511617332696915\n",
      "Epoch: 79/100 | step: 101/422 | loss: 0.020740646868944168\n",
      "Epoch: 79/100 | step: 102/422 | loss: 0.04288829490542412\n",
      "Epoch: 79/100 | step: 103/422 | loss: 0.012144815176725388\n",
      "Epoch: 79/100 | step: 104/422 | loss: 0.036156121641397476\n",
      "Epoch: 79/100 | step: 105/422 | loss: 0.014471535570919514\n",
      "Epoch: 79/100 | step: 106/422 | loss: 0.044190190732479095\n",
      "Epoch: 79/100 | step: 107/422 | loss: 0.0622415617108345\n",
      "Epoch: 79/100 | step: 108/422 | loss: 0.018859511241316795\n",
      "Epoch: 79/100 | step: 109/422 | loss: 0.03372542932629585\n",
      "Epoch: 79/100 | step: 110/422 | loss: 0.015941621735692024\n",
      "Epoch: 79/100 | step: 111/422 | loss: 0.03214436396956444\n",
      "Epoch: 79/100 | step: 112/422 | loss: 0.018407953903079033\n",
      "Epoch: 79/100 | step: 113/422 | loss: 0.017819521948695183\n",
      "Epoch: 79/100 | step: 114/422 | loss: 0.017467480152845383\n",
      "Epoch: 79/100 | step: 115/422 | loss: 0.07685747742652893\n",
      "Epoch: 79/100 | step: 116/422 | loss: 0.10486176609992981\n",
      "Epoch: 79/100 | step: 117/422 | loss: 0.03295532241463661\n",
      "Epoch: 79/100 | step: 118/422 | loss: 0.01891043782234192\n",
      "Epoch: 79/100 | step: 119/422 | loss: 0.048716265708208084\n",
      "Epoch: 79/100 | step: 120/422 | loss: 0.09707373380661011\n",
      "Epoch: 79/100 | step: 121/422 | loss: 0.18680259585380554\n",
      "Epoch: 79/100 | step: 122/422 | loss: 0.3834279775619507\n",
      "Epoch: 79/100 | step: 123/422 | loss: 0.03819817677140236\n",
      "Epoch: 79/100 | step: 124/422 | loss: 0.0376734733581543\n",
      "Epoch: 79/100 | step: 125/422 | loss: 0.07272765040397644\n",
      "Epoch: 79/100 | step: 126/422 | loss: 0.12832781672477722\n",
      "Epoch: 79/100 | step: 127/422 | loss: 0.021310955286026\n",
      "Epoch: 79/100 | step: 128/422 | loss: 0.051559001207351685\n",
      "Epoch: 79/100 | step: 129/422 | loss: 0.03753932937979698\n",
      "Epoch: 79/100 | step: 130/422 | loss: 0.019882965832948685\n",
      "Epoch: 79/100 | step: 131/422 | loss: 0.04111592844128609\n",
      "Epoch: 79/100 | step: 132/422 | loss: 0.036648742854595184\n",
      "Epoch: 79/100 | step: 133/422 | loss: 0.022977016866207123\n",
      "Epoch: 79/100 | step: 134/422 | loss: 0.03723173588514328\n",
      "Epoch: 79/100 | step: 135/422 | loss: 0.02807927317917347\n",
      "Epoch: 79/100 | step: 136/422 | loss: 0.015590553171932697\n",
      "Epoch: 79/100 | step: 137/422 | loss: 0.03054206259548664\n",
      "Epoch: 79/100 | step: 138/422 | loss: 0.020196236670017242\n",
      "Epoch: 79/100 | step: 139/422 | loss: 0.03101014532148838\n",
      "Epoch: 79/100 | step: 140/422 | loss: 0.07165998220443726\n",
      "Epoch: 79/100 | step: 141/422 | loss: 0.050602756440639496\n",
      "Epoch: 79/100 | step: 142/422 | loss: 0.04809204861521721\n",
      "Epoch: 79/100 | step: 143/422 | loss: 0.02187657356262207\n",
      "Epoch: 79/100 | step: 144/422 | loss: 0.022255845367908478\n",
      "Epoch: 79/100 | step: 145/422 | loss: 0.08601220697164536\n",
      "Epoch: 79/100 | step: 146/422 | loss: 0.015532390214502811\n",
      "Epoch: 79/100 | step: 147/422 | loss: 0.07210202515125275\n",
      "Epoch: 79/100 | step: 148/422 | loss: 0.03197815269231796\n",
      "Epoch: 79/100 | step: 149/422 | loss: 0.056875456124544144\n",
      "Epoch: 79/100 | step: 150/422 | loss: 0.02091383934020996\n",
      "Epoch: 79/100 | step: 151/422 | loss: 0.03327581286430359\n",
      "Epoch: 79/100 | step: 152/422 | loss: 0.013916488736867905\n",
      "Epoch: 79/100 | step: 153/422 | loss: 0.02133721485733986\n",
      "Epoch: 79/100 | step: 154/422 | loss: 0.021805090829730034\n",
      "Epoch: 79/100 | step: 155/422 | loss: 0.02720867283642292\n",
      "Epoch: 79/100 | step: 156/422 | loss: 0.020664256066083908\n",
      "Epoch: 79/100 | step: 157/422 | loss: 0.047873709350824356\n",
      "Epoch: 79/100 | step: 158/422 | loss: 0.03098440170288086\n",
      "Epoch: 79/100 | step: 159/422 | loss: 0.044062137603759766\n",
      "Epoch: 79/100 | step: 160/422 | loss: 0.062442418187856674\n",
      "Epoch: 79/100 | step: 161/422 | loss: 0.06500979512929916\n",
      "Epoch: 79/100 | step: 162/422 | loss: 0.062001366168260574\n",
      "Epoch: 79/100 | step: 163/422 | loss: 0.020507197827100754\n",
      "Epoch: 79/100 | step: 164/422 | loss: 0.051770083606243134\n",
      "Epoch: 79/100 | step: 165/422 | loss: 0.023408718407154083\n",
      "Epoch: 79/100 | step: 166/422 | loss: 0.04363719001412392\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 80/100 | step: 1/422 | loss: 0.06695575267076492\n",
      "Epoch: 80/100 | step: 2/422 | loss: 0.037220317870378494\n",
      "Epoch: 80/100 | step: 3/422 | loss: 0.02639494463801384\n",
      "Epoch: 80/100 | step: 4/422 | loss: 0.0292959101498127\n",
      "Epoch: 80/100 | step: 5/422 | loss: 0.031138677150011063\n",
      "Epoch: 80/100 | step: 6/422 | loss: 0.029901359230279922\n",
      "Epoch: 80/100 | step: 7/422 | loss: 0.016797494143247604\n",
      "Epoch: 80/100 | step: 8/422 | loss: 0.026847975328564644\n",
      "Epoch: 80/100 | step: 9/422 | loss: 0.06784842908382416\n",
      "Epoch: 80/100 | step: 10/422 | loss: 0.25412845611572266\n",
      "Epoch: 80/100 | step: 11/422 | loss: 0.015022446401417255\n",
      "Epoch: 80/100 | step: 12/422 | loss: 0.09110645949840546\n",
      "Epoch: 80/100 | step: 13/422 | loss: 0.054764799773693085\n",
      "Epoch: 80/100 | step: 14/422 | loss: 0.08346785604953766\n",
      "Epoch: 80/100 | step: 15/422 | loss: 0.18893446028232574\n",
      "Epoch: 80/100 | step: 16/422 | loss: 0.03571205958724022\n",
      "Epoch: 80/100 | step: 17/422 | loss: 0.10164930671453476\n",
      "Epoch: 80/100 | step: 18/422 | loss: 0.05353313311934471\n",
      "Epoch: 80/100 | step: 19/422 | loss: 0.027120234444737434\n",
      "Epoch: 80/100 | step: 20/422 | loss: 0.04318494349718094\n",
      "Epoch: 80/100 | step: 21/422 | loss: 0.014596784487366676\n",
      "Epoch: 80/100 | step: 22/422 | loss: 0.040903326123952866\n",
      "Epoch: 80/100 | step: 23/422 | loss: 0.035856131464242935\n",
      "Epoch: 80/100 | step: 24/422 | loss: 0.018001005053520203\n",
      "Epoch: 80/100 | step: 25/422 | loss: 0.09970907121896744\n",
      "Epoch: 80/100 | step: 26/422 | loss: 0.10760032385587692\n",
      "Epoch: 80/100 | step: 27/422 | loss: 0.04005153477191925\n",
      "Epoch: 80/100 | step: 28/422 | loss: 0.018525777384638786\n",
      "Epoch: 80/100 | step: 29/422 | loss: 0.029016466811299324\n",
      "Epoch: 80/100 | step: 30/422 | loss: 0.10587957501411438\n",
      "Epoch: 80/100 | step: 31/422 | loss: 0.05738982558250427\n",
      "Epoch: 80/100 | step: 32/422 | loss: 0.025926167145371437\n",
      "Epoch: 80/100 | step: 33/422 | loss: 0.02022649347782135\n",
      "Epoch: 80/100 | step: 34/422 | loss: 0.0513312928378582\n",
      "Epoch: 80/100 | step: 35/422 | loss: 0.03230196237564087\n",
      "Epoch: 80/100 | step: 36/422 | loss: 0.04192540422081947\n",
      "Epoch: 80/100 | step: 37/422 | loss: 0.020253363996744156\n",
      "Epoch: 80/100 | step: 38/422 | loss: 0.03432450816035271\n",
      "Epoch: 80/100 | step: 39/422 | loss: 0.042963963001966476\n",
      "Epoch: 80/100 | step: 40/422 | loss: 0.01637004129588604\n",
      "Epoch: 80/100 | step: 41/422 | loss: 0.02996625006198883\n",
      "Epoch: 80/100 | step: 42/422 | loss: 0.032697878777980804\n",
      "Epoch: 80/100 | step: 43/422 | loss: 0.01794884167611599\n",
      "Epoch: 80/100 | step: 44/422 | loss: 0.034550316631793976\n",
      "Epoch: 80/100 | step: 45/422 | loss: 0.015757771208882332\n",
      "Epoch: 80/100 | step: 46/422 | loss: 0.012294890359044075\n",
      "Epoch: 80/100 | step: 47/422 | loss: 0.029836488887667656\n",
      "Epoch: 80/100 | step: 48/422 | loss: 0.019453534856438637\n",
      "Epoch: 80/100 | step: 49/422 | loss: 0.0198031235486269\n",
      "Epoch: 80/100 | step: 50/422 | loss: 0.010905943810939789\n",
      "Epoch: 80/100 | step: 51/422 | loss: 0.026192376390099525\n",
      "Epoch: 80/100 | step: 52/422 | loss: 0.027340296655893326\n",
      "Epoch: 80/100 | step: 53/422 | loss: 0.019929643720388412\n",
      "Epoch: 80/100 | step: 54/422 | loss: 0.0551651194691658\n",
      "Epoch: 80/100 | step: 55/422 | loss: 0.02038746513426304\n",
      "Epoch: 80/100 | step: 56/422 | loss: 0.028783144429326057\n",
      "Epoch: 80/100 | step: 57/422 | loss: 0.02761409990489483\n",
      "Epoch: 80/100 | step: 58/422 | loss: 0.015170741826295853\n",
      "Epoch: 80/100 | step: 59/422 | loss: 0.00868259184062481\n",
      "Epoch: 80/100 | step: 60/422 | loss: 0.032414548099040985\n",
      "Epoch: 80/100 | step: 61/422 | loss: 0.02112846076488495\n",
      "Epoch: 80/100 | step: 62/422 | loss: 0.03531992807984352\n",
      "Epoch: 80/100 | step: 63/422 | loss: 0.025065990164875984\n",
      "Epoch: 80/100 | step: 64/422 | loss: 0.03542379289865494\n",
      "Epoch: 80/100 | step: 65/422 | loss: 0.02148682437837124\n",
      "Epoch: 80/100 | step: 66/422 | loss: 0.027850840240716934\n",
      "Epoch: 80/100 | step: 67/422 | loss: 0.0232619009912014\n",
      "Epoch: 80/100 | step: 68/422 | loss: 0.028639115393161774\n",
      "Epoch: 80/100 | step: 69/422 | loss: 0.023556718602776527\n",
      "Epoch: 80/100 | step: 70/422 | loss: 0.02373257465660572\n",
      "Epoch: 80/100 | step: 71/422 | loss: 0.02465766668319702\n",
      "Epoch: 80/100 | step: 72/422 | loss: 0.019747966900467873\n",
      "Epoch: 80/100 | step: 73/422 | loss: 0.021129602566361427\n",
      "Epoch: 80/100 | step: 74/422 | loss: 0.013460514135658741\n",
      "Epoch: 80/100 | step: 75/422 | loss: 0.01442226767539978\n",
      "Epoch: 80/100 | step: 76/422 | loss: 0.022166935727000237\n",
      "Epoch: 80/100 | step: 77/422 | loss: 0.024845747277140617\n",
      "Epoch: 80/100 | step: 78/422 | loss: 0.03096754290163517\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 81/100 | step: 1/422 | loss: 0.019730519503355026\n",
      "Epoch: 81/100 | step: 2/422 | loss: 0.022806750610470772\n",
      "Epoch: 81/100 | step: 3/422 | loss: 0.01445005927234888\n",
      "Epoch: 81/100 | step: 4/422 | loss: 0.02402500994503498\n",
      "Epoch: 81/100 | step: 5/422 | loss: 0.04118518531322479\n",
      "Epoch: 81/100 | step: 6/422 | loss: 0.030834831297397614\n",
      "Epoch: 81/100 | step: 7/422 | loss: 0.025287678465247154\n",
      "Epoch: 81/100 | step: 8/422 | loss: 0.035873375833034515\n",
      "Epoch: 81/100 | step: 9/422 | loss: 0.011363859288394451\n",
      "Epoch: 81/100 | step: 10/422 | loss: 0.0353652723133564\n",
      "Epoch: 81/100 | step: 11/422 | loss: 0.04640654847025871\n",
      "Epoch: 81/100 | step: 12/422 | loss: 0.02293943427503109\n",
      "Epoch: 81/100 | step: 13/422 | loss: 0.03852960094809532\n",
      "Epoch: 81/100 | step: 14/422 | loss: 0.05046100914478302\n",
      "Epoch: 81/100 | step: 15/422 | loss: 0.022401658818125725\n",
      "Epoch: 81/100 | step: 16/422 | loss: 0.04027007147669792\n",
      "Epoch: 81/100 | step: 17/422 | loss: 0.06413807719945908\n",
      "Epoch: 81/100 | step: 18/422 | loss: 0.055214956402778625\n",
      "Epoch: 81/100 | step: 19/422 | loss: 0.05538328364491463\n",
      "Epoch: 81/100 | step: 20/422 | loss: 0.019487956538796425\n",
      "Epoch: 81/100 | step: 21/422 | loss: 0.012383569963276386\n",
      "Epoch: 81/100 | step: 22/422 | loss: 0.02157198078930378\n",
      "Epoch: 81/100 | step: 23/422 | loss: 0.013051575981080532\n",
      "Epoch: 81/100 | step: 24/422 | loss: 0.09266695380210876\n",
      "Epoch: 81/100 | step: 25/422 | loss: 0.02878991700708866\n",
      "Epoch: 81/100 | step: 26/422 | loss: 0.027968382462859154\n",
      "Epoch: 81/100 | step: 27/422 | loss: 0.023995986208319664\n",
      "Epoch: 81/100 | step: 28/422 | loss: 0.04747353494167328\n",
      "Epoch: 81/100 | step: 29/422 | loss: 0.03156275674700737\n",
      "Epoch: 81/100 | step: 30/422 | loss: 0.022284716367721558\n",
      "Epoch: 81/100 | step: 31/422 | loss: 0.1439637690782547\n",
      "Epoch: 81/100 | step: 32/422 | loss: 0.16417281329631805\n",
      "Epoch: 81/100 | step: 33/422 | loss: 0.09017545729875565\n",
      "Epoch: 81/100 | step: 34/422 | loss: 0.10134667158126831\n",
      "Epoch: 81/100 | step: 35/422 | loss: 0.05498361960053444\n",
      "Epoch: 81/100 | step: 36/422 | loss: 0.022174008190631866\n",
      "Epoch: 81/100 | step: 37/422 | loss: 0.024841342121362686\n",
      "Epoch: 81/100 | step: 38/422 | loss: 0.03204328566789627\n",
      "Epoch: 81/100 | step: 39/422 | loss: 0.09032543003559113\n",
      "Epoch: 81/100 | step: 40/422 | loss: 0.021971138194203377\n",
      "Epoch: 81/100 | step: 41/422 | loss: 0.08813265711069107\n",
      "Epoch: 81/100 | step: 42/422 | loss: 0.008930307812988758\n",
      "Epoch: 81/100 | step: 43/422 | loss: 0.031165407970547676\n",
      "Epoch: 81/100 | step: 44/422 | loss: 0.03567605838179588\n",
      "Epoch: 81/100 | step: 45/422 | loss: 0.01806613616645336\n",
      "Epoch: 81/100 | step: 46/422 | loss: 0.1340409815311432\n",
      "Epoch: 81/100 | step: 47/422 | loss: 0.030854998156428337\n",
      "Epoch: 81/100 | step: 48/422 | loss: 0.020278818905353546\n",
      "Epoch: 81/100 | step: 49/422 | loss: 0.02528209239244461\n",
      "Epoch: 81/100 | step: 50/422 | loss: 0.031197331845760345\n",
      "Epoch: 81/100 | step: 51/422 | loss: 0.019893040880560875\n",
      "Epoch: 81/100 | step: 52/422 | loss: 0.018077101558446884\n",
      "Epoch: 81/100 | step: 53/422 | loss: 0.013637050986289978\n",
      "Epoch: 81/100 | step: 54/422 | loss: 0.029359478503465652\n",
      "Epoch: 81/100 | step: 55/422 | loss: 0.01488424837589264\n",
      "Epoch: 81/100 | step: 56/422 | loss: 0.024971645325422287\n",
      "Epoch: 81/100 | step: 57/422 | loss: 0.017115384340286255\n",
      "Epoch: 81/100 | step: 58/422 | loss: 0.010615793988108635\n",
      "Epoch: 81/100 | step: 59/422 | loss: 0.022176498547196388\n",
      "Epoch: 81/100 | step: 60/422 | loss: 0.019197333604097366\n",
      "Epoch: 81/100 | step: 61/422 | loss: 0.018077488988637924\n",
      "Epoch: 81/100 | step: 62/422 | loss: 0.010438510216772556\n",
      "Epoch: 81/100 | step: 63/422 | loss: 0.017712246626615524\n",
      "Epoch: 81/100 | step: 64/422 | loss: 0.020322032272815704\n",
      "Epoch: 81/100 | step: 65/422 | loss: 0.010047216899693012\n",
      "Epoch: 81/100 | step: 66/422 | loss: 0.016996823251247406\n",
      "Epoch: 81/100 | step: 67/422 | loss: 0.022770827636122704\n",
      "Epoch: 81/100 | step: 68/422 | loss: 0.04475617781281471\n",
      "Epoch: 81/100 | step: 69/422 | loss: 0.025158580392599106\n",
      "Epoch: 81/100 | step: 70/422 | loss: 0.019654374569654465\n",
      "Epoch: 81/100 | step: 71/422 | loss: 0.017038466408848763\n",
      "Epoch: 81/100 | step: 72/422 | loss: 0.028709836304187775\n",
      "Epoch: 81/100 | step: 73/422 | loss: 0.01179773360490799\n",
      "Epoch: 81/100 | step: 74/422 | loss: 0.011996225453913212\n",
      "Epoch: 81/100 | step: 75/422 | loss: 0.01561623252928257\n",
      "Epoch: 81/100 | step: 76/422 | loss: 0.02388196438550949\n",
      "Epoch: 81/100 | step: 77/422 | loss: 0.0242888405919075\n",
      "Epoch: 81/100 | step: 78/422 | loss: 0.04354328662157059\n",
      "Epoch: 81/100 | step: 79/422 | loss: 0.0810607522726059\n",
      "Epoch: 81/100 | step: 80/422 | loss: 0.016535155475139618\n",
      "Epoch: 81/100 | step: 81/422 | loss: 0.1383982002735138\n",
      "Epoch: 81/100 | step: 82/422 | loss: 0.07540737837553024\n",
      "Epoch: 81/100 | step: 83/422 | loss: 0.024503761902451515\n",
      "Epoch: 81/100 | step: 84/422 | loss: 0.02019404247403145\n",
      "Epoch: 81/100 | step: 85/422 | loss: 0.028911780565977097\n",
      "Epoch: 81/100 | step: 86/422 | loss: 0.01269599050283432\n",
      "Epoch: 81/100 | step: 87/422 | loss: 0.042358361184597015\n",
      "Epoch: 81/100 | step: 88/422 | loss: 0.05996684357523918\n",
      "Epoch: 81/100 | step: 89/422 | loss: 0.024103237316012383\n",
      "Epoch: 81/100 | step: 90/422 | loss: 0.011980701237916946\n",
      "Epoch: 81/100 | step: 91/422 | loss: 0.07576483488082886\n",
      "Epoch: 81/100 | step: 92/422 | loss: 0.04284026101231575\n",
      "Epoch: 81/100 | step: 93/422 | loss: 0.020620878785848618\n",
      "Epoch: 81/100 | step: 94/422 | loss: 0.055442120879888535\n",
      "Epoch: 81/100 | step: 95/422 | loss: 0.023629212751984596\n",
      "Epoch: 81/100 | step: 96/422 | loss: 0.02319139987230301\n",
      "Epoch: 81/100 | step: 97/422 | loss: 0.06573399901390076\n",
      "Epoch: 81/100 | step: 98/422 | loss: 0.02474397048354149\n",
      "Epoch: 81/100 | step: 99/422 | loss: 0.026463573798537254\n",
      "Epoch: 81/100 | step: 100/422 | loss: 0.03830897435545921\n",
      "Epoch: 81/100 | step: 101/422 | loss: 0.05625346675515175\n",
      "Epoch: 81/100 | step: 102/422 | loss: 0.020184509456157684\n",
      "Epoch: 81/100 | step: 103/422 | loss: 0.02459600381553173\n",
      "Epoch: 81/100 | step: 104/422 | loss: 0.015666576102375984\n",
      "Epoch: 81/100 | step: 105/422 | loss: 0.015323675237596035\n",
      "Epoch: 81/100 | step: 106/422 | loss: 0.015272701159119606\n",
      "Epoch: 81/100 | step: 107/422 | loss: 0.018583251163363457\n",
      "Epoch: 81/100 | step: 108/422 | loss: 0.018126966431736946\n",
      "Epoch: 81/100 | step: 109/422 | loss: 0.015013949014246464\n",
      "Epoch: 81/100 | step: 110/422 | loss: 0.01934746280312538\n",
      "Epoch: 81/100 | step: 111/422 | loss: 0.030974816530942917\n",
      "Epoch: 81/100 | step: 112/422 | loss: 0.01616647094488144\n",
      "Epoch: 81/100 | step: 113/422 | loss: 0.026246150955557823\n",
      "Epoch: 81/100 | step: 114/422 | loss: 0.02301807887852192\n",
      "Epoch: 81/100 | step: 115/422 | loss: 0.020795557647943497\n",
      "Epoch: 81/100 | step: 116/422 | loss: 0.03157643601298332\n",
      "Epoch: 81/100 | step: 117/422 | loss: 0.028032345697283745\n",
      "Epoch: 81/100 | step: 118/422 | loss: 0.013271169736981392\n",
      "Epoch: 81/100 | step: 119/422 | loss: 0.009928743354976177\n",
      "Epoch: 81/100 | step: 120/422 | loss: 0.02311627008020878\n",
      "Epoch: 81/100 | step: 121/422 | loss: 0.04661326855421066\n",
      "Epoch: 81/100 | step: 122/422 | loss: 0.018000541254878044\n",
      "Epoch: 81/100 | step: 123/422 | loss: 0.015537434257566929\n",
      "Epoch: 81/100 | step: 124/422 | loss: 0.02336709573864937\n",
      "Epoch: 81/100 | step: 125/422 | loss: 0.02582792565226555\n",
      "Epoch: 81/100 | step: 126/422 | loss: 0.027144934982061386\n",
      "Epoch: 81/100 | step: 127/422 | loss: 0.020888663828372955\n",
      "Epoch: 81/100 | step: 128/422 | loss: 0.021029388532042503\n",
      "Epoch: 81/100 | step: 129/422 | loss: 0.020178817212581635\n",
      "Epoch: 81/100 | step: 130/422 | loss: 0.020546788349747658\n",
      "Epoch: 81/100 | step: 131/422 | loss: 0.02338290400803089\n",
      "Epoch: 81/100 | step: 132/422 | loss: 0.014505327679216862\n",
      "Epoch: 81/100 | step: 133/422 | loss: 0.03879941627383232\n",
      "Epoch: 81/100 | step: 134/422 | loss: 0.026031944900751114\n",
      "Epoch: 81/100 | step: 135/422 | loss: 0.13844847679138184\n",
      "Epoch: 81/100 | step: 136/422 | loss: 0.03555533289909363\n",
      "Epoch: 81/100 | step: 137/422 | loss: 0.03440660238265991\n",
      "Epoch: 81/100 | step: 138/422 | loss: 0.09851429611444473\n",
      "Epoch: 81/100 | step: 139/422 | loss: 0.05447208508849144\n",
      "Epoch: 81/100 | step: 140/422 | loss: 0.06648199260234833\n",
      "Epoch: 81/100 | step: 141/422 | loss: 0.025168536230921745\n",
      "Epoch: 81/100 | step: 142/422 | loss: 0.05052146315574646\n",
      "Epoch: 81/100 | step: 143/422 | loss: 0.023629849776625633\n",
      "Epoch: 81/100 | step: 144/422 | loss: 0.03536039590835571\n",
      "Epoch: 81/100 | step: 145/422 | loss: 0.03714682534337044\n",
      "Epoch: 81/100 | step: 146/422 | loss: 0.055160652846097946\n",
      "Epoch: 81/100 | step: 147/422 | loss: 0.029130924493074417\n",
      "Epoch: 81/100 | step: 148/422 | loss: 0.017242014408111572\n",
      "Epoch: 81/100 | step: 149/422 | loss: 0.013609129935503006\n",
      "Epoch: 81/100 | step: 150/422 | loss: 0.03307558596134186\n",
      "Epoch: 81/100 | step: 151/422 | loss: 0.019666923210024834\n",
      "Epoch: 81/100 | step: 152/422 | loss: 0.03213449940085411\n",
      "Epoch: 81/100 | step: 153/422 | loss: 0.026337221264839172\n",
      "Epoch: 81/100 | step: 154/422 | loss: 0.02352994680404663\n",
      "Epoch: 81/100 | step: 155/422 | loss: 0.00740667013451457\n",
      "Epoch: 81/100 | step: 156/422 | loss: 0.018557937815785408\n",
      "Epoch: 81/100 | step: 157/422 | loss: 0.01358524989336729\n",
      "Epoch: 81/100 | step: 158/422 | loss: 0.017550570890307426\n",
      "Epoch: 81/100 | step: 159/422 | loss: 0.021678168326616287\n",
      "Epoch: 81/100 | step: 160/422 | loss: 0.03479962795972824\n",
      "Epoch: 81/100 | step: 161/422 | loss: 0.028786638751626015\n",
      "Epoch: 81/100 | step: 162/422 | loss: 0.014371010474860668\n",
      "Epoch: 81/100 | step: 163/422 | loss: 0.014621058478951454\n",
      "Epoch: 81/100 | step: 164/422 | loss: 0.015740519389510155\n",
      "Epoch: 81/100 | step: 165/422 | loss: 0.01733066514134407\n",
      "Epoch: 81/100 | step: 166/422 | loss: 0.10001016408205032\n",
      "Epoch: 81/100 | step: 167/422 | loss: 0.02986367791891098\n",
      "Epoch: 81/100 | step: 168/422 | loss: 0.3033512830734253\n",
      "Epoch: 81/100 | step: 169/422 | loss: 0.20704475045204163\n",
      "Epoch: 81/100 | step: 170/422 | loss: 0.10256513953208923\n",
      "Epoch: 81/100 | step: 171/422 | loss: 0.02089342288672924\n",
      "Epoch: 81/100 | step: 172/422 | loss: 0.033932849764823914\n",
      "Epoch: 81/100 | step: 173/422 | loss: 0.02821814827620983\n",
      "Epoch: 81/100 | step: 174/422 | loss: 0.09131714701652527\n",
      "Epoch: 81/100 | step: 175/422 | loss: 0.012147050350904465\n",
      "Epoch: 81/100 | step: 176/422 | loss: 0.015310049057006836\n",
      "Epoch: 81/100 | step: 177/422 | loss: 0.050270773470401764\n",
      "Epoch: 81/100 | step: 178/422 | loss: 0.03494013473391533\n",
      "Epoch: 81/100 | step: 179/422 | loss: 0.032834168523550034\n",
      "Epoch: 81/100 | step: 180/422 | loss: 0.06302093714475632\n",
      "Epoch: 81/100 | step: 181/422 | loss: 0.039501652121543884\n",
      "Epoch: 81/100 | step: 182/422 | loss: 0.019017111510038376\n",
      "Epoch: 81/100 | step: 183/422 | loss: 0.02097049355506897\n",
      "Epoch: 81/100 | step: 184/422 | loss: 0.02935446985065937\n",
      "Epoch: 81/100 | step: 185/422 | loss: 0.038471419364213943\n",
      "Epoch: 81/100 | step: 186/422 | loss: 0.03365306928753853\n",
      "Epoch: 81/100 | step: 187/422 | loss: 0.012659722939133644\n",
      "Epoch: 81/100 | step: 188/422 | loss: 0.03161889314651489\n",
      "Epoch: 81/100 | step: 189/422 | loss: 0.01751638762652874\n",
      "Epoch: 81/100 | step: 190/422 | loss: 0.12481853365898132\n",
      "Epoch: 81/100 | step: 191/422 | loss: 0.057028818875551224\n",
      "Epoch: 81/100 | step: 192/422 | loss: 0.019899507984519005\n",
      "Epoch: 81/100 | step: 193/422 | loss: 0.02675422839820385\n",
      "Epoch: 81/100 | step: 194/422 | loss: 0.019643161445856094\n",
      "Epoch: 81/100 | step: 195/422 | loss: 0.02127547562122345\n",
      "Epoch: 81/100 | step: 196/422 | loss: 0.05821118503808975\n",
      "Epoch: 81/100 | step: 197/422 | loss: 0.04152378812432289\n",
      "Epoch: 81/100 | step: 198/422 | loss: 0.09645302593708038\n",
      "Epoch: 81/100 | step: 199/422 | loss: 0.07170866429805756\n",
      "Epoch: 81/100 | step: 200/422 | loss: 0.05486880615353584\n",
      "Epoch: 81/100 | step: 201/422 | loss: 0.07104126363992691\n",
      "Epoch: 81/100 | step: 202/422 | loss: 0.18255195021629333\n",
      "Epoch: 81/100 | step: 203/422 | loss: 0.09296872466802597\n",
      "Epoch: 81/100 | step: 204/422 | loss: 0.06572296470403671\n",
      "Epoch: 81/100 | step: 205/422 | loss: 0.11599090695381165\n",
      "Epoch: 81/100 | step: 206/422 | loss: 0.10185610502958298\n",
      "Epoch: 81/100 | step: 207/422 | loss: 0.04093620181083679\n",
      "Epoch: 81/100 | step: 208/422 | loss: 0.050897661596536636\n",
      "Epoch: 81/100 | step: 209/422 | loss: 0.05682658404111862\n",
      "Epoch: 81/100 | step: 210/422 | loss: 0.04320664703845978\n",
      "Epoch: 81/100 | step: 211/422 | loss: 0.05572463572025299\n",
      "Epoch: 81/100 | step: 212/422 | loss: 0.04404950886964798\n",
      "Epoch: 81/100 | step: 213/422 | loss: 0.14267686009407043\n",
      "Epoch: 81/100 | step: 214/422 | loss: 0.05264853686094284\n",
      "Epoch: 81/100 | step: 215/422 | loss: 0.026973431929945946\n",
      "Epoch: 81/100 | step: 216/422 | loss: 0.03865708038210869\n",
      "Epoch: 81/100 | step: 217/422 | loss: 0.03571772202849388\n",
      "Epoch: 81/100 | step: 218/422 | loss: 0.03961120918393135\n",
      "Epoch: 81/100 | step: 219/422 | loss: 0.01676972024142742\n",
      "Epoch: 81/100 | step: 220/422 | loss: 0.032711584120988846\n",
      "Epoch: 81/100 | step: 221/422 | loss: 0.022212838754057884\n",
      "Epoch: 81/100 | step: 222/422 | loss: 0.0226832814514637\n",
      "Epoch: 81/100 | step: 223/422 | loss: 0.016101274639368057\n",
      "Epoch: 81/100 | step: 224/422 | loss: 0.024543628096580505\n",
      "Epoch: 81/100 | step: 225/422 | loss: 0.029049551114439964\n",
      "Epoch: 81/100 | step: 226/422 | loss: 0.021847475320100784\n",
      "Epoch: 81/100 | step: 227/422 | loss: 0.03143201768398285\n",
      "Epoch: 81/100 | step: 228/422 | loss: 0.018425526097416878\n",
      "Epoch: 81/100 | step: 229/422 | loss: 0.01563308574259281\n",
      "Epoch: 81/100 | step: 230/422 | loss: 0.032019831240177155\n",
      "Epoch: 81/100 | step: 231/422 | loss: 0.07505602389574051\n",
      "Epoch: 81/100 | step: 232/422 | loss: 0.06826116889715195\n",
      "Epoch: 81/100 | step: 233/422 | loss: 0.053886428475379944\n",
      "Epoch: 81/100 | step: 234/422 | loss: 0.11704961210489273\n",
      "Epoch: 81/100 | step: 235/422 | loss: 0.02852568030357361\n",
      "Epoch: 81/100 | step: 236/422 | loss: 0.017835140228271484\n",
      "Epoch: 81/100 | step: 237/422 | loss: 0.015639569610357285\n",
      "Epoch: 81/100 | step: 238/422 | loss: 0.038766711950302124\n",
      "Epoch: 81/100 | step: 239/422 | loss: 0.03737768158316612\n",
      "Epoch: 81/100 | step: 240/422 | loss: 0.04125287011265755\n",
      "Epoch: 81/100 | step: 241/422 | loss: 0.032928936183452606\n",
      "Epoch: 81/100 | step: 242/422 | loss: 0.04187043383717537\n",
      "Epoch: 81/100 | step: 243/422 | loss: 0.05324311554431915\n",
      "Epoch: 81/100 | step: 244/422 | loss: 0.014694682322442532\n",
      "Epoch: 81/100 | step: 245/422 | loss: 0.010512201115489006\n",
      "Epoch: 81/100 | step: 246/422 | loss: 0.020215000957250595\n",
      "Epoch: 81/100 | step: 247/422 | loss: 0.010351341217756271\n",
      "Epoch: 81/100 | step: 248/422 | loss: 0.024617495015263557\n",
      "Epoch: 81/100 | step: 249/422 | loss: 0.014229778200387955\n",
      "Epoch: 81/100 | step: 250/422 | loss: 0.0205491054803133\n",
      "Epoch: 81/100 | step: 251/422 | loss: 0.01275266706943512\n",
      "Epoch: 81/100 | step: 252/422 | loss: 0.014627981930971146\n",
      "Epoch: 81/100 | step: 253/422 | loss: 0.03550884500145912\n",
      "Epoch: 81/100 | step: 254/422 | loss: 0.02527516335248947\n",
      "Epoch: 81/100 | step: 255/422 | loss: 0.08413363248109818\n",
      "Epoch: 81/100 | step: 256/422 | loss: 0.019824255257844925\n",
      "Epoch: 81/100 | step: 257/422 | loss: 0.022745992988348007\n",
      "Epoch: 81/100 | step: 258/422 | loss: 0.023493148386478424\n",
      "Epoch: 81/100 | step: 259/422 | loss: 0.030657300725579262\n",
      "Epoch: 81/100 | step: 260/422 | loss: 0.04437077417969704\n",
      "Epoch: 81/100 | step: 261/422 | loss: 0.09218533337116241\n",
      "Epoch: 81/100 | step: 262/422 | loss: 0.04846009984612465\n",
      "Epoch: 81/100 | step: 263/422 | loss: 0.02281106263399124\n",
      "Epoch: 81/100 | step: 264/422 | loss: 0.055642176419496536\n",
      "Epoch: 81/100 | step: 265/422 | loss: 0.1272188127040863\n",
      "Epoch: 81/100 | step: 266/422 | loss: 0.027919868007302284\n",
      "Epoch: 81/100 | step: 267/422 | loss: 0.027471648529171944\n",
      "Epoch: 81/100 | step: 268/422 | loss: 0.016856076195836067\n",
      "Epoch: 81/100 | step: 269/422 | loss: 0.0274567361921072\n",
      "Epoch: 81/100 | step: 270/422 | loss: 0.0669480413198471\n",
      "Epoch: 81/100 | step: 271/422 | loss: 0.03157661855220795\n",
      "Epoch: 81/100 | step: 272/422 | loss: 0.012970762327313423\n",
      "Epoch: 81/100 | step: 273/422 | loss: 0.016332145780324936\n",
      "Epoch: 81/100 | step: 274/422 | loss: 0.03453585505485535\n",
      "Epoch: 81/100 | step: 275/422 | loss: 0.016352906823158264\n",
      "Epoch: 81/100 | step: 276/422 | loss: 0.07893380522727966\n",
      "Epoch: 81/100 | step: 277/422 | loss: 0.05125103518366814\n",
      "Epoch: 81/100 | step: 278/422 | loss: 0.03893666714429855\n",
      "Epoch: 81/100 | step: 279/422 | loss: 0.024974223226308823\n",
      "Epoch: 81/100 | step: 280/422 | loss: 0.03143646940588951\n",
      "Epoch: 81/100 | step: 281/422 | loss: 0.03656948357820511\n",
      "Epoch: 81/100 | step: 282/422 | loss: 0.016811231151223183\n",
      "Epoch: 81/100 | step: 283/422 | loss: 0.025195593014359474\n",
      "Epoch: 81/100 | step: 284/422 | loss: 0.044661421328783035\n",
      "Epoch: 81/100 | step: 285/422 | loss: 0.02660457417368889\n",
      "Epoch: 81/100 | step: 286/422 | loss: 0.016876133158802986\n",
      "Epoch: 81/100 | step: 287/422 | loss: 0.05954616889357567\n",
      "Epoch: 81/100 | step: 288/422 | loss: 0.04485570639371872\n",
      "Epoch: 81/100 | step: 289/422 | loss: 0.025370273739099503\n",
      "Epoch: 81/100 | step: 290/422 | loss: 0.023733817040920258\n",
      "Epoch: 81/100 | step: 291/422 | loss: 0.019428150728344917\n",
      "Epoch: 81/100 | step: 292/422 | loss: 0.02491459622979164\n",
      "Epoch: 81/100 | step: 293/422 | loss: 0.021110976114869118\n",
      "Epoch: 81/100 | step: 294/422 | loss: 0.031152691692113876\n",
      "Epoch: 81/100 | step: 295/422 | loss: 0.044012024998664856\n",
      "Epoch: 81/100 | step: 296/422 | loss: 0.039303965866565704\n",
      "Epoch: 81/100 | step: 297/422 | loss: 0.08160858601331711\n",
      "Epoch: 81/100 | step: 298/422 | loss: 0.03749983757734299\n",
      "Epoch: 81/100 | step: 299/422 | loss: 0.041784435510635376\n",
      "Epoch: 81/100 | step: 300/422 | loss: 0.0741528570652008\n",
      "Epoch: 81/100 | step: 301/422 | loss: 0.019410843029618263\n",
      "Epoch: 81/100 | step: 302/422 | loss: 0.0244147926568985\n",
      "Epoch: 81/100 | step: 303/422 | loss: 0.03339569643139839\n",
      "Epoch: 81/100 | step: 304/422 | loss: 0.02959943562746048\n",
      "Epoch: 81/100 | step: 305/422 | loss: 0.07923291623592377\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 82/100 | step: 1/422 | loss: 0.045434679836034775\n",
      "Epoch: 82/100 | step: 2/422 | loss: 0.021658988669514656\n",
      "Epoch: 82/100 | step: 3/422 | loss: 0.016141319647431374\n",
      "Epoch: 82/100 | step: 4/422 | loss: 0.010615450330078602\n",
      "Epoch: 82/100 | step: 5/422 | loss: 0.09243461489677429\n",
      "Epoch: 82/100 | step: 6/422 | loss: 0.02723812311887741\n",
      "Epoch: 82/100 | step: 7/422 | loss: 0.021837474778294563\n",
      "Epoch: 82/100 | step: 8/422 | loss: 0.08707987517118454\n",
      "Epoch: 82/100 | step: 9/422 | loss: 0.018108360469341278\n",
      "Epoch: 82/100 | step: 10/422 | loss: 0.014813579618930817\n",
      "Epoch: 82/100 | step: 11/422 | loss: 0.04704781994223595\n",
      "Epoch: 82/100 | step: 12/422 | loss: 0.009562117047607899\n",
      "Epoch: 82/100 | step: 13/422 | loss: 0.022792253643274307\n",
      "Epoch: 82/100 | step: 14/422 | loss: 0.01365178357809782\n",
      "Epoch: 82/100 | step: 15/422 | loss: 0.018084725365042686\n",
      "Epoch: 82/100 | step: 16/422 | loss: 0.010114941745996475\n",
      "Epoch: 82/100 | step: 17/422 | loss: 0.03323615714907646\n",
      "Epoch: 82/100 | step: 18/422 | loss: 0.02133648283779621\n",
      "Epoch: 82/100 | step: 19/422 | loss: 0.025052059441804886\n",
      "Epoch: 82/100 | step: 20/422 | loss: 0.010865562595427036\n",
      "Epoch: 82/100 | step: 21/422 | loss: 0.03588816151022911\n",
      "Epoch: 82/100 | step: 22/422 | loss: 0.008845699019730091\n",
      "Epoch: 82/100 | step: 23/422 | loss: 0.021119752898812294\n",
      "Epoch: 82/100 | step: 24/422 | loss: 0.009314559400081635\n",
      "Epoch: 82/100 | step: 25/422 | loss: 0.027220232412219048\n",
      "Epoch: 82/100 | step: 26/422 | loss: 0.016734834760427475\n",
      "Epoch: 82/100 | step: 27/422 | loss: 0.02971833385527134\n",
      "Epoch: 82/100 | step: 28/422 | loss: 0.019085094332695007\n",
      "Epoch: 82/100 | step: 29/422 | loss: 0.013596834614872932\n",
      "Epoch: 82/100 | step: 30/422 | loss: 0.0333138033747673\n",
      "Epoch: 82/100 | step: 31/422 | loss: 0.012101034633815289\n",
      "Epoch: 82/100 | step: 32/422 | loss: 0.024164143949747086\n",
      "Epoch: 82/100 | step: 33/422 | loss: 0.029873598366975784\n",
      "Epoch: 82/100 | step: 34/422 | loss: 0.019512800499796867\n",
      "Epoch: 82/100 | step: 35/422 | loss: 0.011408211663365364\n",
      "Epoch: 82/100 | step: 36/422 | loss: 0.017246920615434647\n",
      "Epoch: 82/100 | step: 37/422 | loss: 0.049600664526224136\n",
      "Epoch: 82/100 | step: 38/422 | loss: 0.028680281713604927\n",
      "Epoch: 82/100 | step: 39/422 | loss: 0.010718630626797676\n",
      "Epoch: 82/100 | step: 40/422 | loss: 0.015020915307104588\n",
      "Epoch: 82/100 | step: 41/422 | loss: 0.018425779417157173\n",
      "Epoch: 82/100 | step: 42/422 | loss: 0.01080066803842783\n",
      "Epoch: 82/100 | step: 43/422 | loss: 0.014300874434411526\n",
      "Epoch: 82/100 | step: 44/422 | loss: 0.021920498460531235\n",
      "Epoch: 82/100 | step: 45/422 | loss: 0.007870027795433998\n",
      "Epoch: 82/100 | step: 46/422 | loss: 0.017927274107933044\n",
      "Epoch: 82/100 | step: 47/422 | loss: 0.02086232416331768\n",
      "Epoch: 82/100 | step: 48/422 | loss: 0.020880939438939095\n",
      "Epoch: 82/100 | step: 49/422 | loss: 0.011262907646596432\n",
      "Epoch: 82/100 | step: 50/422 | loss: 0.02181585691869259\n",
      "Epoch: 82/100 | step: 51/422 | loss: 0.07038700580596924\n",
      "Epoch: 82/100 | step: 52/422 | loss: 0.014125525020062923\n",
      "Epoch: 82/100 | step: 53/422 | loss: 0.039736099541187286\n",
      "Epoch: 82/100 | step: 54/422 | loss: 0.03441343083977699\n",
      "Epoch: 82/100 | step: 55/422 | loss: 0.033689528703689575\n",
      "Epoch: 82/100 | step: 56/422 | loss: 0.03734089434146881\n",
      "Epoch: 82/100 | step: 57/422 | loss: 0.01706317439675331\n",
      "Epoch: 82/100 | step: 58/422 | loss: 0.02471114508807659\n",
      "Epoch: 82/100 | step: 59/422 | loss: 0.01010966394096613\n",
      "Epoch: 82/100 | step: 60/422 | loss: 0.019375644624233246\n",
      "Epoch: 82/100 | step: 61/422 | loss: 0.028948361054062843\n",
      "Epoch: 82/100 | step: 62/422 | loss: 0.016248518601059914\n",
      "Epoch: 82/100 | step: 63/422 | loss: 0.019899791106581688\n",
      "Epoch: 82/100 | step: 64/422 | loss: 0.04021758213639259\n",
      "Epoch: 82/100 | step: 65/422 | loss: 0.014824215322732925\n",
      "Epoch: 82/100 | step: 66/422 | loss: 0.012384993955492973\n",
      "Epoch: 82/100 | step: 67/422 | loss: 0.08208208531141281\n",
      "Epoch: 82/100 | step: 68/422 | loss: 0.05430047586560249\n",
      "Epoch: 82/100 | step: 69/422 | loss: 0.028085406869649887\n",
      "Epoch: 82/100 | step: 70/422 | loss: 0.0233798548579216\n",
      "Epoch: 82/100 | step: 71/422 | loss: 0.01536304410547018\n",
      "Epoch: 82/100 | step: 72/422 | loss: 0.014982817694544792\n",
      "Epoch: 82/100 | step: 73/422 | loss: 0.01681388169527054\n",
      "Epoch: 82/100 | step: 74/422 | loss: 0.039181772619485855\n",
      "Epoch: 82/100 | step: 75/422 | loss: 0.014530088752508163\n",
      "Epoch: 82/100 | step: 76/422 | loss: 0.047175876796245575\n",
      "Epoch: 82/100 | step: 77/422 | loss: 0.027984872460365295\n",
      "Epoch: 82/100 | step: 78/422 | loss: 0.07383016496896744\n",
      "Epoch: 82/100 | step: 79/422 | loss: 0.021283892914652824\n",
      "Epoch: 82/100 | step: 80/422 | loss: 0.02594326063990593\n",
      "Epoch: 82/100 | step: 81/422 | loss: 0.06537715345621109\n",
      "Epoch: 82/100 | step: 82/422 | loss: 0.01612977683544159\n",
      "Epoch: 82/100 | step: 83/422 | loss: 0.05428748205304146\n",
      "Epoch: 82/100 | step: 84/422 | loss: 0.024578573182225227\n",
      "Epoch: 82/100 | step: 85/422 | loss: 0.028298350051045418\n",
      "Epoch: 82/100 | step: 86/422 | loss: 0.053867366164922714\n",
      "Epoch: 82/100 | step: 87/422 | loss: 0.04798883572220802\n",
      "Epoch: 82/100 | step: 88/422 | loss: 0.025849224999547005\n",
      "Epoch: 82/100 | step: 89/422 | loss: 0.011305035091936588\n",
      "Epoch: 82/100 | step: 90/422 | loss: 0.027163952589035034\n",
      "Epoch: 82/100 | step: 91/422 | loss: 0.02437477931380272\n",
      "Epoch: 82/100 | step: 92/422 | loss: 0.009641274809837341\n",
      "Epoch: 82/100 | step: 93/422 | loss: 0.017808355391025543\n",
      "Epoch: 82/100 | step: 94/422 | loss: 0.19763688743114471\n",
      "Epoch: 82/100 | step: 95/422 | loss: 0.03279115632176399\n",
      "Epoch: 82/100 | step: 96/422 | loss: 0.030854852870106697\n",
      "Epoch: 82/100 | step: 97/422 | loss: 0.13064157962799072\n",
      "Epoch: 82/100 | step: 98/422 | loss: 0.20653140544891357\n",
      "Epoch: 82/100 | step: 99/422 | loss: 0.2919871211051941\n",
      "Epoch: 82/100 | step: 100/422 | loss: 0.09504708647727966\n",
      "Epoch: 82/100 | step: 101/422 | loss: 0.035600002855062485\n",
      "Epoch: 82/100 | step: 102/422 | loss: 0.09655338525772095\n",
      "Epoch: 82/100 | step: 103/422 | loss: 0.03413177281618118\n",
      "Epoch: 82/100 | step: 104/422 | loss: 0.03552400693297386\n",
      "Epoch: 82/100 | step: 105/422 | loss: 0.009867514483630657\n",
      "Epoch: 82/100 | step: 106/422 | loss: 0.03704091161489487\n",
      "Epoch: 82/100 | step: 107/422 | loss: 0.053209297358989716\n",
      "Epoch: 82/100 | step: 108/422 | loss: 0.08631536364555359\n",
      "Epoch: 82/100 | step: 109/422 | loss: 0.3565782904624939\n",
      "Epoch: 82/100 | step: 110/422 | loss: 0.21532979607582092\n",
      "Epoch: 82/100 | step: 111/422 | loss: 0.5011410713195801\n",
      "Epoch: 82/100 | step: 112/422 | loss: 0.022363804280757904\n",
      "Epoch: 82/100 | step: 113/422 | loss: 0.05510900542140007\n",
      "Epoch: 82/100 | step: 114/422 | loss: 0.017048146575689316\n",
      "Epoch: 82/100 | step: 115/422 | loss: 0.446050763130188\n",
      "Epoch: 82/100 | step: 116/422 | loss: 0.2933090031147003\n",
      "Epoch: 82/100 | step: 117/422 | loss: 0.1866181194782257\n",
      "Epoch: 82/100 | step: 118/422 | loss: 0.19414761662483215\n",
      "Epoch: 82/100 | step: 119/422 | loss: 0.1831027865409851\n",
      "Epoch: 82/100 | step: 120/422 | loss: 0.030193660408258438\n",
      "Epoch: 82/100 | step: 121/422 | loss: 0.06697436422109604\n",
      "Epoch: 82/100 | step: 122/422 | loss: 0.020471498370170593\n",
      "Epoch: 82/100 | step: 123/422 | loss: 0.04389144852757454\n",
      "Epoch: 82/100 | step: 124/422 | loss: 0.03073282539844513\n",
      "Epoch: 82/100 | step: 125/422 | loss: 0.018983827903866768\n",
      "Epoch: 82/100 | step: 126/422 | loss: 0.07494824379682541\n",
      "Epoch: 82/100 | step: 127/422 | loss: 0.019443131983280182\n",
      "Epoch: 82/100 | step: 128/422 | loss: 0.10801368951797485\n",
      "Epoch: 82/100 | step: 129/422 | loss: 0.042021457105875015\n",
      "Epoch: 82/100 | step: 130/422 | loss: 0.022331641986966133\n",
      "Epoch: 82/100 | step: 131/422 | loss: 0.016532547771930695\n",
      "Epoch: 82/100 | step: 132/422 | loss: 0.030033690854907036\n",
      "Epoch: 82/100 | step: 133/422 | loss: 0.016930630430579185\n",
      "Epoch: 82/100 | step: 134/422 | loss: 0.08446567505598068\n",
      "Epoch: 82/100 | step: 135/422 | loss: 0.025415144860744476\n",
      "Epoch: 82/100 | step: 136/422 | loss: 0.014642351306974888\n",
      "Epoch: 82/100 | step: 137/422 | loss: 0.0214887373149395\n",
      "Epoch: 82/100 | step: 138/422 | loss: 0.015439202077686787\n",
      "Epoch: 82/100 | step: 139/422 | loss: 0.012403551489114761\n",
      "Epoch: 82/100 | step: 140/422 | loss: 0.03653237596154213\n",
      "Epoch: 82/100 | step: 141/422 | loss: 0.010935897007584572\n",
      "Epoch: 82/100 | step: 142/422 | loss: 0.04004734382033348\n",
      "Epoch: 82/100 | step: 143/422 | loss: 0.02056836150586605\n",
      "Epoch: 82/100 | step: 144/422 | loss: 0.03473807871341705\n",
      "Epoch: 82/100 | step: 145/422 | loss: 0.008881169371306896\n",
      "Epoch: 82/100 | step: 146/422 | loss: 0.023720065131783485\n",
      "Epoch: 82/100 | step: 147/422 | loss: 0.022286811843514442\n",
      "Epoch: 82/100 | step: 148/422 | loss: 0.020791364833712578\n",
      "Epoch: 82/100 | step: 149/422 | loss: 0.0385892316699028\n",
      "Epoch: 82/100 | step: 150/422 | loss: 0.01773078925907612\n",
      "Epoch: 82/100 | step: 151/422 | loss: 0.013485790230333805\n",
      "Epoch: 82/100 | step: 152/422 | loss: 0.029291683807969093\n",
      "Epoch: 82/100 | step: 153/422 | loss: 0.0111242039129138\n",
      "Epoch: 82/100 | step: 154/422 | loss: 0.021343721076846123\n",
      "Epoch: 82/100 | step: 155/422 | loss: 0.014616717584431171\n",
      "Epoch: 82/100 | step: 156/422 | loss: 0.011905239894986153\n",
      "Epoch: 82/100 | step: 157/422 | loss: 0.017458001151680946\n",
      "Epoch: 82/100 | step: 158/422 | loss: 0.018198063597083092\n",
      "Epoch: 82/100 | step: 159/422 | loss: 0.037331290543079376\n",
      "Epoch: 82/100 | step: 160/422 | loss: 0.07989923655986786\n",
      "Epoch: 82/100 | step: 161/422 | loss: 0.025068694725632668\n",
      "Epoch: 82/100 | step: 162/422 | loss: 0.02377636730670929\n",
      "Epoch: 82/100 | step: 163/422 | loss: 0.023259839043021202\n",
      "Epoch: 82/100 | step: 164/422 | loss: 0.010236271657049656\n",
      "Epoch: 82/100 | step: 165/422 | loss: 0.027162382379174232\n",
      "Epoch: 82/100 | step: 166/422 | loss: 0.016872268170118332\n",
      "Epoch: 82/100 | step: 167/422 | loss: 0.01782468520104885\n",
      "Epoch: 82/100 | step: 168/422 | loss: 0.02015090547502041\n",
      "Epoch: 82/100 | step: 169/422 | loss: 0.022168921306729317\n",
      "Epoch: 82/100 | step: 170/422 | loss: 0.020887548103928566\n",
      "Epoch: 82/100 | step: 171/422 | loss: 0.012767020612955093\n",
      "Epoch: 82/100 | step: 172/422 | loss: 0.011818166822195053\n",
      "Epoch: 82/100 | step: 173/422 | loss: 0.027459891512989998\n",
      "Epoch: 82/100 | step: 174/422 | loss: 0.010025914758443832\n",
      "Epoch: 82/100 | step: 175/422 | loss: 0.013060488738119602\n",
      "Epoch: 82/100 | step: 176/422 | loss: 0.014059297740459442\n",
      "Epoch: 82/100 | step: 177/422 | loss: 0.035503216087818146\n",
      "Epoch: 82/100 | step: 178/422 | loss: 0.055258214473724365\n",
      "Epoch: 82/100 | step: 179/422 | loss: 0.020196115598082542\n",
      "Epoch: 82/100 | step: 180/422 | loss: 0.030264372006058693\n",
      "Epoch: 82/100 | step: 181/422 | loss: 0.03353007882833481\n",
      "Epoch: 82/100 | step: 182/422 | loss: 0.01523757353425026\n",
      "Epoch: 82/100 | step: 183/422 | loss: 0.014788832515478134\n",
      "Epoch: 82/100 | step: 184/422 | loss: 0.016604313626885414\n",
      "Epoch: 82/100 | step: 185/422 | loss: 0.026079872623085976\n",
      "Epoch: 82/100 | step: 186/422 | loss: 0.021070078015327454\n",
      "Epoch: 82/100 | step: 187/422 | loss: 0.06490962207317352\n",
      "Epoch: 82/100 | step: 188/422 | loss: 0.013172639533877373\n",
      "Epoch: 82/100 | step: 189/422 | loss: 0.010228507220745087\n",
      "Epoch: 82/100 | step: 190/422 | loss: 0.01890699937939644\n",
      "Epoch: 82/100 | step: 191/422 | loss: 0.007433445658534765\n",
      "Epoch: 82/100 | step: 192/422 | loss: 0.1954444795846939\n",
      "Epoch: 82/100 | step: 193/422 | loss: 0.1539400815963745\n",
      "Epoch: 82/100 | step: 194/422 | loss: 0.023047534748911858\n",
      "Epoch: 82/100 | step: 195/422 | loss: 0.02144327200949192\n",
      "Epoch: 82/100 | step: 196/422 | loss: 0.052433762699365616\n",
      "Epoch: 82/100 | step: 197/422 | loss: 0.028713637962937355\n",
      "Epoch: 82/100 | step: 198/422 | loss: 0.10542161762714386\n",
      "Epoch: 82/100 | step: 199/422 | loss: 0.02889978140592575\n",
      "Epoch: 82/100 | step: 200/422 | loss: 0.0415356308221817\n",
      "Epoch: 82/100 | step: 201/422 | loss: 0.02629430592060089\n",
      "Epoch: 82/100 | step: 202/422 | loss: 0.01433870755136013\n",
      "Epoch: 82/100 | step: 203/422 | loss: 0.030264612287282944\n",
      "Epoch: 82/100 | step: 204/422 | loss: 0.03545568138360977\n",
      "Epoch: 82/100 | step: 205/422 | loss: 0.01444289181381464\n",
      "Epoch: 82/100 | step: 206/422 | loss: 0.033683620393276215\n",
      "Epoch: 82/100 | step: 207/422 | loss: 0.02809697762131691\n",
      "Epoch: 82/100 | step: 208/422 | loss: 0.027925454080104828\n",
      "Epoch: 82/100 | step: 209/422 | loss: 0.049815833568573\n",
      "Epoch: 82/100 | step: 210/422 | loss: 0.013420799747109413\n",
      "Epoch: 82/100 | step: 211/422 | loss: 0.04175705090165138\n",
      "Epoch: 82/100 | step: 212/422 | loss: 0.03757509961724281\n",
      "Epoch: 82/100 | step: 213/422 | loss: 0.018374111503362656\n",
      "Epoch: 82/100 | step: 214/422 | loss: 0.020063050091266632\n",
      "Epoch: 82/100 | step: 215/422 | loss: 0.021033328026533127\n",
      "Epoch: 82/100 | step: 216/422 | loss: 0.011849155649542809\n",
      "Epoch: 82/100 | step: 217/422 | loss: 0.010879411362111568\n",
      "Epoch: 82/100 | step: 218/422 | loss: 0.054428327828645706\n",
      "Epoch: 82/100 | step: 219/422 | loss: 0.032749149948358536\n",
      "Epoch: 82/100 | step: 220/422 | loss: 0.023110678419470787\n",
      "Epoch: 82/100 | step: 221/422 | loss: 0.03789656236767769\n",
      "Epoch: 82/100 | step: 222/422 | loss: 0.029302656650543213\n",
      "Epoch: 82/100 | step: 223/422 | loss: 0.023802464827895164\n",
      "Epoch: 82/100 | step: 224/422 | loss: 0.021085385233163834\n",
      "Epoch: 82/100 | step: 225/422 | loss: 0.03107968159019947\n",
      "Epoch: 82/100 | step: 226/422 | loss: 0.03790430724620819\n",
      "Epoch: 82/100 | step: 227/422 | loss: 0.017230533063411713\n",
      "Epoch: 82/100 | step: 228/422 | loss: 0.016265276819467545\n",
      "Epoch: 82/100 | step: 229/422 | loss: 0.01418337319046259\n",
      "Epoch: 82/100 | step: 230/422 | loss: 0.060071274638175964\n",
      "Epoch: 82/100 | step: 231/422 | loss: 0.05422763526439667\n",
      "Epoch: 82/100 | step: 232/422 | loss: 0.07487549632787704\n",
      "Epoch: 82/100 | step: 233/422 | loss: 0.017711473628878593\n",
      "Epoch: 82/100 | step: 234/422 | loss: 0.02037489041686058\n",
      "Epoch: 82/100 | step: 235/422 | loss: 0.018455421552062035\n",
      "Epoch: 82/100 | step: 236/422 | loss: 0.010757768526673317\n",
      "Epoch: 82/100 | step: 237/422 | loss: 0.020587893202900887\n",
      "Epoch: 82/100 | step: 238/422 | loss: 0.010851786471903324\n",
      "Epoch: 82/100 | step: 239/422 | loss: 0.015010804869234562\n",
      "Epoch: 82/100 | step: 240/422 | loss: 0.015510961413383484\n",
      "Epoch: 82/100 | step: 241/422 | loss: 0.05061263591051102\n",
      "Epoch: 82/100 | step: 242/422 | loss: 0.020843464881181717\n",
      "Epoch: 82/100 | step: 243/422 | loss: 0.016512427479028702\n",
      "Epoch: 82/100 | step: 244/422 | loss: 0.023759568110108376\n",
      "Epoch: 82/100 | step: 245/422 | loss: 0.017686672508716583\n",
      "Epoch: 82/100 | step: 246/422 | loss: 0.015613460913300514\n",
      "Epoch: 82/100 | step: 247/422 | loss: 0.010601398535072803\n",
      "Epoch: 82/100 | step: 248/422 | loss: 0.09494985640048981\n",
      "Epoch: 82/100 | step: 249/422 | loss: 0.03716573119163513\n",
      "Epoch: 82/100 | step: 250/422 | loss: 0.07133127003908157\n",
      "Epoch: 82/100 | step: 251/422 | loss: 0.0253720935434103\n",
      "Epoch: 82/100 | step: 252/422 | loss: 0.14731402695178986\n",
      "Epoch: 82/100 | step: 253/422 | loss: 0.021064376458525658\n",
      "Epoch: 82/100 | step: 254/422 | loss: 0.12136687338352203\n",
      "Epoch: 82/100 | step: 255/422 | loss: 0.017911694943904877\n",
      "Epoch: 82/100 | step: 256/422 | loss: 0.03873870149254799\n",
      "Epoch: 82/100 | step: 257/422 | loss: 0.03893046826124191\n",
      "Epoch: 82/100 | step: 258/422 | loss: 0.02474762499332428\n",
      "Epoch: 82/100 | step: 259/422 | loss: 0.029201151803135872\n",
      "Epoch: 82/100 | step: 260/422 | loss: 0.03327673673629761\n",
      "Epoch: 82/100 | step: 261/422 | loss: 0.016451328992843628\n",
      "Epoch: 82/100 | step: 262/422 | loss: 0.03361627086997032\n",
      "Epoch: 82/100 | step: 263/422 | loss: 0.04628889262676239\n",
      "Epoch: 82/100 | step: 264/422 | loss: 0.067774198949337\n",
      "Epoch: 82/100 | step: 265/422 | loss: 0.030117932707071304\n",
      "Epoch: 82/100 | step: 266/422 | loss: 0.03759017214179039\n",
      "Epoch: 82/100 | step: 267/422 | loss: 0.02222776599228382\n",
      "Epoch: 82/100 | step: 268/422 | loss: 0.014499565586447716\n",
      "Epoch: 82/100 | step: 269/422 | loss: 0.015812603756785393\n",
      "Epoch: 82/100 | step: 270/422 | loss: 0.014829481020569801\n",
      "Epoch: 82/100 | step: 271/422 | loss: 0.010793400928378105\n",
      "Epoch: 82/100 | step: 272/422 | loss: 0.014975355006754398\n",
      "Epoch: 82/100 | step: 273/422 | loss: 0.019467240199446678\n",
      "Epoch: 82/100 | step: 274/422 | loss: 0.018188390880823135\n",
      "Epoch: 82/100 | step: 275/422 | loss: 0.034572478383779526\n",
      "Epoch: 82/100 | step: 276/422 | loss: 0.0202063899487257\n",
      "Epoch: 82/100 | step: 277/422 | loss: 0.0139922471717\n",
      "Epoch: 82/100 | step: 278/422 | loss: 0.01682453230023384\n",
      "Epoch: 82/100 | step: 279/422 | loss: 0.015915248543024063\n",
      "Epoch: 82/100 | step: 280/422 | loss: 0.033335160464048386\n",
      "Epoch: 82/100 | step: 281/422 | loss: 0.011848430149257183\n",
      "Epoch: 82/100 | step: 282/422 | loss: 0.026284724473953247\n",
      "Epoch: 82/100 | step: 283/422 | loss: 0.020357949659228325\n",
      "Epoch: 82/100 | step: 284/422 | loss: 0.011699163354933262\n",
      "Epoch: 82/100 | step: 285/422 | loss: 0.014242799952626228\n",
      "Epoch: 82/100 | step: 286/422 | loss: 0.01615796610713005\n",
      "Epoch: 82/100 | step: 287/422 | loss: 0.025812629610300064\n",
      "Epoch: 82/100 | step: 288/422 | loss: 0.02744382619857788\n",
      "Epoch: 82/100 | step: 289/422 | loss: 0.12177135795354843\n",
      "Epoch: 82/100 | step: 290/422 | loss: 0.05946949124336243\n",
      "Epoch: 82/100 | step: 291/422 | loss: 0.019891535863280296\n",
      "Epoch: 82/100 | step: 292/422 | loss: 0.03786341845989227\n",
      "Epoch: 82/100 | step: 293/422 | loss: 0.06628343462944031\n",
      "Epoch: 82/100 | step: 294/422 | loss: 0.009672071784734726\n",
      "Epoch: 82/100 | step: 295/422 | loss: 0.013643329031765461\n",
      "Epoch: 82/100 | step: 296/422 | loss: 0.01637980155646801\n",
      "Epoch: 82/100 | step: 297/422 | loss: 0.01635889522731304\n",
      "Epoch: 82/100 | step: 298/422 | loss: 0.0260876826941967\n",
      "Epoch: 82/100 | step: 299/422 | loss: 0.06191220134496689\n",
      "Epoch: 82/100 | step: 300/422 | loss: 0.02510625682771206\n",
      "Epoch: 82/100 | step: 301/422 | loss: 0.3121514618396759\n",
      "Epoch: 82/100 | step: 302/422 | loss: 0.09738199412822723\n",
      "Epoch: 82/100 | step: 303/422 | loss: 0.13008537888526917\n",
      "Epoch: 82/100 | step: 304/422 | loss: 0.013266545720398426\n",
      "Epoch: 82/100 | step: 305/422 | loss: 0.02607303485274315\n",
      "Epoch: 82/100 | step: 306/422 | loss: 0.0527123399078846\n",
      "Epoch: 82/100 | step: 307/422 | loss: 0.07472387701272964\n",
      "Epoch: 82/100 | step: 308/422 | loss: 0.019188564270734787\n",
      "Epoch: 82/100 | step: 309/422 | loss: 0.057508356869220734\n",
      "Epoch: 82/100 | step: 310/422 | loss: 0.04276055470108986\n",
      "Epoch: 82/100 | step: 311/422 | loss: 0.03362661227583885\n",
      "Epoch: 82/100 | step: 312/422 | loss: 0.03555206581950188\n",
      "Epoch: 82/100 | step: 313/422 | loss: 0.010521698743104935\n",
      "Epoch: 82/100 | step: 314/422 | loss: 0.01311672106385231\n",
      "Epoch: 82/100 | step: 315/422 | loss: 0.01038687489926815\n",
      "Epoch: 82/100 | step: 316/422 | loss: 0.040671829134225845\n",
      "Epoch: 82/100 | step: 317/422 | loss: 0.02968849055469036\n",
      "Epoch: 82/100 | step: 318/422 | loss: 0.01596320979297161\n",
      "Epoch: 82/100 | step: 319/422 | loss: 0.044944725930690765\n",
      "Epoch: 82/100 | step: 320/422 | loss: 0.01002694945782423\n",
      "Epoch: 82/100 | step: 321/422 | loss: 0.02705385349690914\n",
      "Epoch: 82/100 | step: 322/422 | loss: 0.219201922416687\n",
      "Epoch: 82/100 | step: 323/422 | loss: 0.44995182752609253\n",
      "Epoch: 82/100 | step: 324/422 | loss: 1.625901222229004\n",
      "Epoch: 82/100 | step: 325/422 | loss: 1.6566728353500366\n",
      "Epoch: 82/100 | step: 326/422 | loss: 1.757951259613037\n",
      "Epoch: 82/100 | step: 327/422 | loss: 0.29678457975387573\n",
      "Epoch: 82/100 | step: 328/422 | loss: 0.07469106465578079\n",
      "Epoch: 82/100 | step: 329/422 | loss: 0.05768553540110588\n",
      "Epoch: 82/100 | step: 330/422 | loss: 0.15370987355709076\n",
      "Epoch: 82/100 | step: 331/422 | loss: 0.05966104939579964\n",
      "Epoch: 82/100 | step: 332/422 | loss: 0.04595333710312843\n",
      "Epoch: 82/100 | step: 333/422 | loss: 0.6240094304084778\n",
      "Epoch: 82/100 | step: 334/422 | loss: 0.025127243250608444\n",
      "Epoch: 82/100 | step: 335/422 | loss: 0.16050001978874207\n",
      "Epoch: 82/100 | step: 336/422 | loss: 0.05895193666219711\n",
      "Epoch: 82/100 | step: 337/422 | loss: 0.12123345583677292\n",
      "Epoch: 82/100 | step: 338/422 | loss: 0.21240641176700592\n",
      "Epoch: 82/100 | step: 339/422 | loss: 0.46276092529296875\n",
      "Epoch: 82/100 | step: 340/422 | loss: 0.10309305042028427\n",
      "Epoch: 82/100 | step: 341/422 | loss: 0.3940640687942505\n",
      "Epoch: 82/100 | step: 342/422 | loss: 0.06247605010867119\n",
      "Epoch: 82/100 | step: 343/422 | loss: 0.04832712933421135\n",
      "Epoch: 82/100 | step: 344/422 | loss: 0.052385177463293076\n",
      "Epoch: 82/100 | step: 345/422 | loss: 0.02091033384203911\n",
      "Epoch: 82/100 | step: 346/422 | loss: 0.1105327233672142\n",
      "Epoch: 82/100 | step: 347/422 | loss: 0.04518866911530495\n",
      "Epoch: 82/100 | step: 348/422 | loss: 0.048760946840047836\n",
      "Epoch: 82/100 | step: 349/422 | loss: 0.04018912464380264\n",
      "Epoch: 82/100 | step: 350/422 | loss: 0.07820334285497665\n",
      "Epoch: 82/100 | step: 351/422 | loss: 0.031401604413986206\n",
      "Epoch: 82/100 | step: 352/422 | loss: 0.18896064162254333\n",
      "Epoch: 82/100 | step: 353/422 | loss: 0.14536455273628235\n",
      "Epoch: 82/100 | step: 354/422 | loss: 0.028504328802227974\n",
      "Epoch: 82/100 | step: 355/422 | loss: 0.06387130916118622\n",
      "Epoch: 82/100 | step: 356/422 | loss: 0.05278635770082474\n",
      "Epoch: 82/100 | step: 357/422 | loss: 0.17376279830932617\n",
      "Epoch: 82/100 | step: 358/422 | loss: 0.08035466074943542\n",
      "Epoch: 82/100 | step: 359/422 | loss: 0.07955930382013321\n",
      "Epoch: 82/100 | step: 360/422 | loss: 0.062216199934482574\n",
      "Epoch: 82/100 | step: 361/422 | loss: 0.15029145777225494\n",
      "Epoch: 82/100 | step: 362/422 | loss: 0.03477161005139351\n",
      "Epoch: 82/100 | step: 363/422 | loss: 0.027782343327999115\n",
      "Epoch: 82/100 | step: 364/422 | loss: 0.07452724874019623\n",
      "Epoch: 82/100 | step: 365/422 | loss: 0.05020473524928093\n",
      "Epoch: 82/100 | step: 366/422 | loss: 0.02350890077650547\n",
      "Epoch: 82/100 | step: 367/422 | loss: 0.0911586806178093\n",
      "Epoch: 82/100 | step: 368/422 | loss: 0.05589023604989052\n",
      "Epoch: 82/100 | step: 369/422 | loss: 0.1049238070845604\n",
      "Epoch: 82/100 | step: 370/422 | loss: 0.10298541188240051\n",
      "Epoch: 82/100 | step: 371/422 | loss: 0.03989553824067116\n",
      "Epoch: 82/100 | step: 372/422 | loss: 0.029023077338933945\n",
      "Epoch: 82/100 | step: 373/422 | loss: 0.06252593547105789\n",
      "Epoch: 82/100 | step: 374/422 | loss: 0.025770802050828934\n",
      "Epoch: 82/100 | step: 375/422 | loss: 0.02583063766360283\n",
      "Epoch: 82/100 | step: 376/422 | loss: 0.06674180179834366\n",
      "Epoch: 82/100 | step: 377/422 | loss: 0.0202096626162529\n",
      "Epoch: 82/100 | step: 378/422 | loss: 0.022511934861540794\n",
      "Epoch: 82/100 | step: 379/422 | loss: 0.0871528685092926\n",
      "Epoch: 82/100 | step: 380/422 | loss: 0.07983800768852234\n",
      "Epoch: 82/100 | step: 381/422 | loss: 0.05813773721456528\n",
      "Epoch: 82/100 | step: 382/422 | loss: 0.03493790328502655\n",
      "Epoch: 82/100 | step: 383/422 | loss: 0.013536183163523674\n",
      "Epoch: 82/100 | step: 384/422 | loss: 0.030863082036376\n",
      "Epoch: 82/100 | step: 385/422 | loss: 0.021268585696816444\n",
      "Epoch: 82/100 | step: 386/422 | loss: 0.04725669324398041\n",
      "Epoch: 82/100 | step: 387/422 | loss: 0.11776361614465714\n",
      "Epoch: 82/100 | step: 388/422 | loss: 0.07480722665786743\n",
      "Epoch: 82/100 | step: 389/422 | loss: 0.051889024674892426\n",
      "Epoch: 82/100 | step: 390/422 | loss: 0.12892454862594604\n",
      "Epoch: 82/100 | step: 391/422 | loss: 0.13666637241840363\n",
      "Epoch: 82/100 | step: 392/422 | loss: 0.20515666902065277\n",
      "Epoch: 82/100 | step: 393/422 | loss: 0.21293187141418457\n",
      "Epoch: 82/100 | step: 394/422 | loss: 0.04173611104488373\n",
      "Epoch: 82/100 | step: 395/422 | loss: 0.0672968402504921\n",
      "Epoch: 82/100 | step: 396/422 | loss: 0.4253120422363281\n",
      "Epoch: 82/100 | step: 397/422 | loss: 0.2546367347240448\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 83/100 | step: 1/422 | loss: 0.13086453080177307\n",
      "Epoch: 83/100 | step: 2/422 | loss: 0.06587322801351547\n",
      "Epoch: 83/100 | step: 3/422 | loss: 0.35729527473449707\n",
      "Epoch: 83/100 | step: 4/422 | loss: 0.03724993020296097\n",
      "Epoch: 83/100 | step: 5/422 | loss: 0.10385452955961227\n",
      "Epoch: 83/100 | step: 6/422 | loss: 0.05806165561079979\n",
      "Epoch: 83/100 | step: 7/422 | loss: 0.18405427038669586\n",
      "Epoch: 83/100 | step: 8/422 | loss: 0.14862683415412903\n",
      "Epoch: 83/100 | step: 9/422 | loss: 0.18491369485855103\n",
      "Epoch: 83/100 | step: 10/422 | loss: 0.031051557511091232\n",
      "Epoch: 83/100 | step: 11/422 | loss: 0.03428514301776886\n",
      "Epoch: 83/100 | step: 12/422 | loss: 0.056259799748659134\n",
      "Epoch: 83/100 | step: 13/422 | loss: 0.08537063002586365\n",
      "Epoch: 83/100 | step: 14/422 | loss: 0.016729706898331642\n",
      "Epoch: 83/100 | step: 15/422 | loss: 0.08257938176393509\n",
      "Epoch: 83/100 | step: 16/422 | loss: 0.020198117941617966\n",
      "Epoch: 83/100 | step: 17/422 | loss: 0.019828470423817635\n",
      "Epoch: 83/100 | step: 18/422 | loss: 0.024571020156145096\n",
      "Epoch: 83/100 | step: 19/422 | loss: 0.0998554676771164\n",
      "Epoch: 83/100 | step: 20/422 | loss: 0.04864479973912239\n",
      "Epoch: 83/100 | step: 21/422 | loss: 0.03431866317987442\n",
      "Epoch: 83/100 | step: 22/422 | loss: 0.02098553255200386\n",
      "Epoch: 83/100 | step: 23/422 | loss: 0.026757340878248215\n",
      "Epoch: 83/100 | step: 24/422 | loss: 0.04352715238928795\n",
      "Epoch: 83/100 | step: 25/422 | loss: 0.01052862498909235\n",
      "Epoch: 83/100 | step: 26/422 | loss: 0.020551469177007675\n",
      "Epoch: 83/100 | step: 27/422 | loss: 0.01647588051855564\n",
      "Epoch: 83/100 | step: 28/422 | loss: 0.021504908800125122\n",
      "Epoch: 83/100 | step: 29/422 | loss: 0.012463807128369808\n",
      "Epoch: 83/100 | step: 30/422 | loss: 0.025980502367019653\n",
      "Epoch: 83/100 | step: 31/422 | loss: 0.018334658816456795\n",
      "Epoch: 83/100 | step: 32/422 | loss: 0.025516292080283165\n",
      "Epoch: 83/100 | step: 33/422 | loss: 0.04210265725851059\n",
      "Epoch: 83/100 | step: 34/422 | loss: 0.04803364351391792\n",
      "Epoch: 83/100 | step: 35/422 | loss: 0.018507415428757668\n",
      "Epoch: 83/100 | step: 36/422 | loss: 0.020547103136777878\n",
      "Epoch: 83/100 | step: 37/422 | loss: 0.011922897771000862\n",
      "Epoch: 83/100 | step: 38/422 | loss: 0.028956938534975052\n",
      "Epoch: 83/100 | step: 39/422 | loss: 0.009169913828372955\n",
      "Epoch: 83/100 | step: 40/422 | loss: 0.036613523960113525\n",
      "Epoch: 83/100 | step: 41/422 | loss: 0.031740494072437286\n",
      "Epoch: 83/100 | step: 42/422 | loss: 0.019125428050756454\n",
      "Epoch: 83/100 | step: 43/422 | loss: 0.02828335389494896\n",
      "Epoch: 83/100 | step: 44/422 | loss: 0.029448455199599266\n",
      "Epoch: 83/100 | step: 45/422 | loss: 0.053378500044345856\n",
      "Epoch: 83/100 | step: 46/422 | loss: 0.020230473950505257\n",
      "Epoch: 83/100 | step: 47/422 | loss: 0.0412173829972744\n",
      "Epoch: 83/100 | step: 48/422 | loss: 0.24674367904663086\n",
      "Epoch: 83/100 | step: 49/422 | loss: 0.02809463068842888\n",
      "Epoch: 83/100 | step: 50/422 | loss: 0.0743459090590477\n",
      "Epoch: 83/100 | step: 51/422 | loss: 0.15451280772686005\n",
      "Epoch: 83/100 | step: 52/422 | loss: 0.02233211323618889\n",
      "Epoch: 83/100 | step: 53/422 | loss: 0.2743034362792969\n",
      "Epoch: 83/100 | step: 54/422 | loss: 0.05268679931759834\n",
      "Epoch: 83/100 | step: 55/422 | loss: 0.037805669009685516\n",
      "Epoch: 83/100 | step: 56/422 | loss: 0.019447213038802147\n",
      "Epoch: 83/100 | step: 57/422 | loss: 0.014752302318811417\n",
      "Epoch: 83/100 | step: 58/422 | loss: 0.015410101041197777\n",
      "Epoch: 83/100 | step: 59/422 | loss: 0.03689110651612282\n",
      "Epoch: 83/100 | step: 60/422 | loss: 0.03704967349767685\n",
      "Epoch: 83/100 | step: 61/422 | loss: 0.01858503557741642\n",
      "Epoch: 83/100 | step: 62/422 | loss: 0.025098875164985657\n",
      "Epoch: 83/100 | step: 63/422 | loss: 0.03427538275718689\n",
      "Epoch: 83/100 | step: 64/422 | loss: 0.01973779872059822\n",
      "Epoch: 83/100 | step: 65/422 | loss: 0.02015729248523712\n",
      "Epoch: 83/100 | step: 66/422 | loss: 0.021228622645139694\n",
      "Epoch: 83/100 | step: 67/422 | loss: 0.05924642086029053\n",
      "Epoch: 83/100 | step: 68/422 | loss: 0.03649471327662468\n",
      "Epoch: 83/100 | step: 69/422 | loss: 0.022109080106019974\n",
      "Epoch: 83/100 | step: 70/422 | loss: 0.024263938888907433\n",
      "Epoch: 83/100 | step: 71/422 | loss: 0.017214324325323105\n",
      "Epoch: 83/100 | step: 72/422 | loss: 0.031515300273895264\n",
      "Epoch: 83/100 | step: 73/422 | loss: 0.017025019973516464\n",
      "Epoch: 83/100 | step: 74/422 | loss: 0.022372225299477577\n",
      "Epoch: 83/100 | step: 75/422 | loss: 0.021426420658826828\n",
      "Epoch: 83/100 | step: 76/422 | loss: 0.03260407596826553\n",
      "Epoch: 83/100 | step: 77/422 | loss: 0.020420972257852554\n",
      "Epoch: 83/100 | step: 78/422 | loss: 0.020169667899608612\n",
      "Epoch: 83/100 | step: 79/422 | loss: 0.020207243040204048\n",
      "Epoch: 83/100 | step: 80/422 | loss: 0.014813992194831371\n",
      "Epoch: 83/100 | step: 81/422 | loss: 0.014786780811846256\n",
      "Epoch: 83/100 | step: 82/422 | loss: 0.012582640163600445\n",
      "Epoch: 83/100 | step: 83/422 | loss: 0.08495695888996124\n",
      "Epoch: 83/100 | step: 84/422 | loss: 0.010738587006926537\n",
      "Epoch: 83/100 | step: 85/422 | loss: 0.018983077257871628\n",
      "Epoch: 83/100 | step: 86/422 | loss: 0.01680382713675499\n",
      "Epoch: 83/100 | step: 87/422 | loss: 0.17183059453964233\n",
      "Epoch: 83/100 | step: 88/422 | loss: 0.03457890823483467\n",
      "Epoch: 83/100 | step: 89/422 | loss: 0.06123897433280945\n",
      "Epoch: 83/100 | step: 90/422 | loss: 0.023219678550958633\n",
      "Epoch: 83/100 | step: 91/422 | loss: 0.03259143605828285\n",
      "Epoch: 83/100 | step: 92/422 | loss: 0.05795176327228546\n",
      "Epoch: 83/100 | step: 93/422 | loss: 0.07363345474004745\n",
      "Epoch: 83/100 | step: 94/422 | loss: 0.04648970812559128\n",
      "Epoch: 83/100 | step: 95/422 | loss: 0.020895829424262047\n",
      "Epoch: 83/100 | step: 96/422 | loss: 0.042403049767017365\n",
      "Epoch: 83/100 | step: 97/422 | loss: 0.014060497283935547\n",
      "Epoch: 83/100 | step: 98/422 | loss: 0.061326764523983\n",
      "Epoch: 83/100 | step: 99/422 | loss: 0.03977561369538307\n",
      "Epoch: 83/100 | step: 100/422 | loss: 0.01787300407886505\n",
      "Epoch: 83/100 | step: 101/422 | loss: 0.020368706434965134\n",
      "Epoch: 83/100 | step: 102/422 | loss: 0.062214016914367676\n",
      "Epoch: 83/100 | step: 103/422 | loss: 0.015595999546349049\n",
      "Epoch: 83/100 | step: 104/422 | loss: 0.04628380015492439\n",
      "Epoch: 83/100 | step: 105/422 | loss: 0.018494421616196632\n",
      "Epoch: 83/100 | step: 106/422 | loss: 0.02568233013153076\n",
      "Epoch: 83/100 | step: 107/422 | loss: 0.024806994944810867\n",
      "Epoch: 83/100 | step: 108/422 | loss: 0.00978308729827404\n",
      "Epoch: 83/100 | step: 109/422 | loss: 0.03565400466322899\n",
      "Epoch: 83/100 | step: 110/422 | loss: 0.019920766353607178\n",
      "Epoch: 83/100 | step: 111/422 | loss: 0.014725660905241966\n",
      "Epoch: 83/100 | step: 112/422 | loss: 0.013748444616794586\n",
      "Epoch: 83/100 | step: 113/422 | loss: 0.00827458780258894\n",
      "Epoch: 83/100 | step: 114/422 | loss: 0.01052901055663824\n",
      "Epoch: 83/100 | step: 115/422 | loss: 0.03153252229094505\n",
      "Epoch: 83/100 | step: 116/422 | loss: 0.031528182327747345\n",
      "Epoch: 83/100 | step: 117/422 | loss: 0.01638578251004219\n",
      "Epoch: 83/100 | step: 118/422 | loss: 0.05513190105557442\n",
      "Epoch: 83/100 | step: 119/422 | loss: 0.03424648195505142\n",
      "Epoch: 83/100 | step: 120/422 | loss: 0.039997246116399765\n",
      "Epoch: 83/100 | step: 121/422 | loss: 0.028945231810212135\n",
      "Epoch: 83/100 | step: 122/422 | loss: 0.01917555369436741\n",
      "Epoch: 83/100 | step: 123/422 | loss: 0.01843060366809368\n",
      "Epoch: 83/100 | step: 124/422 | loss: 0.02660347707569599\n",
      "Epoch: 83/100 | step: 125/422 | loss: 0.0705905556678772\n",
      "Epoch: 83/100 | step: 126/422 | loss: 0.03008926287293434\n",
      "Epoch: 83/100 | step: 127/422 | loss: 0.02452276088297367\n",
      "Epoch: 83/100 | step: 128/422 | loss: 0.04988626018166542\n",
      "Epoch: 83/100 | step: 129/422 | loss: 0.040395453572273254\n",
      "Epoch: 83/100 | step: 130/422 | loss: 0.016954932361841202\n",
      "Epoch: 83/100 | step: 131/422 | loss: 0.02452077902853489\n",
      "Epoch: 83/100 | step: 132/422 | loss: 0.013677369803190231\n",
      "Epoch: 83/100 | step: 133/422 | loss: 0.030812537297606468\n",
      "Epoch: 83/100 | step: 134/422 | loss: 0.01288225594907999\n",
      "Epoch: 83/100 | step: 135/422 | loss: 0.01955362968146801\n",
      "Epoch: 83/100 | step: 136/422 | loss: 0.016344483941793442\n",
      "Epoch: 83/100 | step: 137/422 | loss: 0.019180545583367348\n",
      "Epoch: 83/100 | step: 138/422 | loss: 0.02712293341755867\n",
      "Epoch: 83/100 | step: 139/422 | loss: 0.052992936223745346\n",
      "Epoch: 83/100 | step: 140/422 | loss: 0.019681503996253014\n",
      "Epoch: 83/100 | step: 141/422 | loss: 0.0353630967438221\n",
      "Epoch: 83/100 | step: 142/422 | loss: 0.008756592869758606\n",
      "Epoch: 83/100 | step: 143/422 | loss: 0.018801162019371986\n",
      "Epoch: 83/100 | step: 144/422 | loss: 0.020471852272748947\n",
      "Epoch: 83/100 | step: 145/422 | loss: 0.09342174232006073\n",
      "Epoch: 83/100 | step: 146/422 | loss: 0.15467865765094757\n",
      "Epoch: 83/100 | step: 147/422 | loss: 0.17948240041732788\n",
      "Epoch: 83/100 | step: 148/422 | loss: 0.05337073281407356\n",
      "Epoch: 83/100 | step: 149/422 | loss: 0.04202134907245636\n",
      "Epoch: 83/100 | step: 150/422 | loss: 0.05580626428127289\n",
      "Epoch: 83/100 | step: 151/422 | loss: 0.06507059931755066\n",
      "Epoch: 83/100 | step: 152/422 | loss: 0.02604403905570507\n",
      "Epoch: 83/100 | step: 153/422 | loss: 0.044444624334573746\n",
      "Epoch: 83/100 | step: 154/422 | loss: 0.048726651817560196\n",
      "Epoch: 83/100 | step: 155/422 | loss: 0.04689763858914375\n",
      "Epoch: 83/100 | step: 156/422 | loss: 0.032785605639219284\n",
      "Epoch: 83/100 | step: 157/422 | loss: 0.029408836737275124\n",
      "Epoch: 83/100 | step: 158/422 | loss: 0.024996645748615265\n",
      "Epoch: 83/100 | step: 159/422 | loss: 0.025977615267038345\n",
      "Epoch: 83/100 | step: 160/422 | loss: 0.03371858224272728\n",
      "Epoch: 83/100 | step: 161/422 | loss: 0.12941975891590118\n",
      "Epoch: 83/100 | step: 162/422 | loss: 0.0244528129696846\n",
      "Epoch: 83/100 | step: 163/422 | loss: 0.06037629023194313\n",
      "Epoch: 83/100 | step: 164/422 | loss: 0.03618477284908295\n",
      "Epoch: 83/100 | step: 165/422 | loss: 0.03130609542131424\n",
      "Epoch: 83/100 | step: 166/422 | loss: 0.021417517215013504\n",
      "Epoch: 83/100 | step: 167/422 | loss: 0.07619666308164597\n",
      "Epoch: 83/100 | step: 168/422 | loss: 0.02021620236337185\n",
      "Epoch: 83/100 | step: 169/422 | loss: 0.025450557470321655\n",
      "Epoch: 83/100 | step: 170/422 | loss: 0.032508645206689835\n",
      "Epoch: 83/100 | step: 171/422 | loss: 0.02791442722082138\n",
      "Epoch: 83/100 | step: 172/422 | loss: 0.024790437892079353\n",
      "Epoch: 83/100 | step: 173/422 | loss: 0.02046966180205345\n",
      "Epoch: 83/100 | step: 174/422 | loss: 0.01215555053204298\n",
      "Epoch: 83/100 | step: 175/422 | loss: 0.009225649759173393\n",
      "Epoch: 83/100 | step: 176/422 | loss: 0.020543884485960007\n",
      "Epoch: 83/100 | step: 177/422 | loss: 0.03141274303197861\n",
      "Epoch: 83/100 | step: 178/422 | loss: 0.027187608182430267\n",
      "Epoch: 83/100 | step: 179/422 | loss: 0.030295897275209427\n",
      "Epoch: 83/100 | step: 180/422 | loss: 0.015938980504870415\n",
      "Epoch: 83/100 | step: 181/422 | loss: 0.033739540725946426\n",
      "Epoch: 83/100 | step: 182/422 | loss: 0.02493787743151188\n",
      "Epoch: 83/100 | step: 183/422 | loss: 0.02820713445544243\n",
      "Epoch: 83/100 | step: 184/422 | loss: 0.013764418661594391\n",
      "Epoch: 83/100 | step: 185/422 | loss: 0.022188551723957062\n",
      "Epoch: 83/100 | step: 186/422 | loss: 0.01322508417069912\n",
      "Epoch: 83/100 | step: 187/422 | loss: 0.02250574715435505\n",
      "Epoch: 83/100 | step: 188/422 | loss: 0.02670455165207386\n",
      "Epoch: 83/100 | step: 189/422 | loss: 0.012190730310976505\n",
      "Epoch: 83/100 | step: 190/422 | loss: 0.029959965497255325\n",
      "Epoch: 83/100 | step: 191/422 | loss: 0.03328506276011467\n",
      "Epoch: 83/100 | step: 192/422 | loss: 0.01716582104563713\n",
      "Epoch: 83/100 | step: 193/422 | loss: 0.01626572199165821\n",
      "Epoch: 83/100 | step: 194/422 | loss: 0.01817343384027481\n",
      "Epoch: 83/100 | step: 195/422 | loss: 0.044666510075330734\n",
      "Epoch: 83/100 | step: 196/422 | loss: 0.007126921322196722\n",
      "Epoch: 83/100 | step: 197/422 | loss: 0.01938360556960106\n",
      "Epoch: 83/100 | step: 198/422 | loss: 0.022739987820386887\n",
      "Epoch: 83/100 | step: 199/422 | loss: 0.015455486252903938\n",
      "Epoch: 83/100 | step: 200/422 | loss: 0.023001085966825485\n",
      "Epoch: 83/100 | step: 201/422 | loss: 0.017355412244796753\n",
      "Epoch: 83/100 | step: 202/422 | loss: 0.020794915035367012\n",
      "Epoch: 83/100 | step: 203/422 | loss: 0.014942592009902\n",
      "Epoch: 83/100 | step: 204/422 | loss: 0.019392317160964012\n",
      "Epoch: 83/100 | step: 205/422 | loss: 0.02555958181619644\n",
      "Epoch: 83/100 | step: 206/422 | loss: 0.015295140445232391\n",
      "Epoch: 83/100 | step: 207/422 | loss: 0.02821652963757515\n",
      "Epoch: 83/100 | step: 208/422 | loss: 0.015545041300356388\n",
      "Epoch: 83/100 | step: 209/422 | loss: 0.011252759955823421\n",
      "Epoch: 83/100 | step: 210/422 | loss: 0.008801599964499474\n",
      "Epoch: 83/100 | step: 211/422 | loss: 0.026927735656499863\n",
      "Epoch: 83/100 | step: 212/422 | loss: 0.021926967427134514\n",
      "Epoch: 83/100 | step: 213/422 | loss: 0.01680596172809601\n",
      "Epoch: 83/100 | step: 214/422 | loss: 0.023931872099637985\n",
      "Epoch: 83/100 | step: 215/422 | loss: 0.04136645793914795\n",
      "Epoch: 83/100 | step: 216/422 | loss: 0.06882498413324356\n",
      "Epoch: 83/100 | step: 217/422 | loss: 0.08152062445878983\n",
      "Epoch: 83/100 | step: 218/422 | loss: 0.019878197461366653\n",
      "Epoch: 83/100 | step: 219/422 | loss: 0.15599575638771057\n",
      "Epoch: 83/100 | step: 220/422 | loss: 0.044761739671230316\n",
      "Epoch: 83/100 | step: 221/422 | loss: 0.02100791037082672\n",
      "Epoch: 83/100 | step: 222/422 | loss: 0.011020028963685036\n",
      "Epoch: 83/100 | step: 223/422 | loss: 0.02882695570588112\n",
      "Epoch: 83/100 | step: 224/422 | loss: 0.09789199382066727\n",
      "Epoch: 83/100 | step: 225/422 | loss: 0.11679934710264206\n",
      "Epoch: 83/100 | step: 226/422 | loss: 0.19535762071609497\n",
      "Epoch: 83/100 | step: 227/422 | loss: 0.151086688041687\n",
      "Epoch: 83/100 | step: 228/422 | loss: 0.2076520174741745\n",
      "Epoch: 83/100 | step: 229/422 | loss: 0.10754488408565521\n",
      "Epoch: 83/100 | step: 230/422 | loss: 0.08795025199651718\n",
      "Epoch: 83/100 | step: 231/422 | loss: 0.029390955343842506\n",
      "Epoch: 83/100 | step: 232/422 | loss: 0.025371979922056198\n",
      "Epoch: 83/100 | step: 233/422 | loss: 0.0348486602306366\n",
      "Epoch: 83/100 | step: 234/422 | loss: 0.03748887777328491\n",
      "Epoch: 83/100 | step: 235/422 | loss: 0.025974148884415627\n",
      "Epoch: 83/100 | step: 236/422 | loss: 0.03309587016701698\n",
      "Epoch: 83/100 | step: 237/422 | loss: 0.04436194896697998\n",
      "Epoch: 83/100 | step: 238/422 | loss: 0.029846297577023506\n",
      "Epoch: 83/100 | step: 239/422 | loss: 0.03318474069237709\n",
      "Epoch: 83/100 | step: 240/422 | loss: 0.06454963237047195\n",
      "Epoch: 83/100 | step: 241/422 | loss: 0.01498198788613081\n",
      "Epoch: 83/100 | step: 242/422 | loss: 0.022808942943811417\n",
      "Epoch: 83/100 | step: 243/422 | loss: 0.02601306512951851\n",
      "Epoch: 83/100 | step: 244/422 | loss: 0.018098045140504837\n",
      "Epoch: 83/100 | step: 245/422 | loss: 0.022561747580766678\n",
      "Epoch: 83/100 | step: 246/422 | loss: 0.024222273379564285\n",
      "Epoch: 83/100 | step: 247/422 | loss: 0.020386584103107452\n",
      "Epoch: 83/100 | step: 248/422 | loss: 0.05227278172969818\n",
      "Epoch: 83/100 | step: 249/422 | loss: 0.047100793570280075\n",
      "Epoch: 83/100 | step: 250/422 | loss: 0.009022882208228111\n",
      "Epoch: 83/100 | step: 251/422 | loss: 0.023254862055182457\n",
      "Epoch: 83/100 | step: 252/422 | loss: 0.015547032468020916\n",
      "Epoch: 83/100 | step: 253/422 | loss: 0.06335018575191498\n",
      "Epoch: 83/100 | step: 254/422 | loss: 0.02856225147843361\n",
      "Epoch: 83/100 | step: 255/422 | loss: 0.0289161317050457\n",
      "Epoch: 83/100 | step: 256/422 | loss: 0.0203973650932312\n",
      "Epoch: 83/100 | step: 257/422 | loss: 0.026992354542016983\n",
      "Epoch: 83/100 | step: 258/422 | loss: 0.018507668748497963\n",
      "Epoch: 83/100 | step: 259/422 | loss: 0.02965095266699791\n",
      "Epoch: 83/100 | step: 260/422 | loss: 0.03092024475336075\n",
      "Epoch: 83/100 | step: 261/422 | loss: 0.010235524736344814\n",
      "Epoch: 83/100 | step: 262/422 | loss: 0.022291822358965874\n",
      "Epoch: 83/100 | step: 263/422 | loss: 0.022132424637675285\n",
      "Epoch: 83/100 | step: 264/422 | loss: 0.020656947046518326\n",
      "Epoch: 83/100 | step: 265/422 | loss: 0.028072189539670944\n",
      "Epoch: 83/100 | step: 266/422 | loss: 0.05327524244785309\n",
      "Epoch: 83/100 | step: 267/422 | loss: 0.053791530430316925\n",
      "Epoch: 83/100 | step: 268/422 | loss: 0.06731349974870682\n",
      "Epoch: 83/100 | step: 269/422 | loss: 0.01757892407476902\n",
      "Epoch: 83/100 | step: 270/422 | loss: 0.01852879673242569\n",
      "Epoch: 83/100 | step: 271/422 | loss: 0.02547881007194519\n",
      "Epoch: 83/100 | step: 272/422 | loss: 0.013449194841086864\n",
      "Epoch: 83/100 | step: 273/422 | loss: 0.024925461038947105\n",
      "Epoch: 83/100 | step: 274/422 | loss: 0.011684600263834\n",
      "Epoch: 83/100 | step: 275/422 | loss: 0.01837126724421978\n",
      "Epoch: 83/100 | step: 276/422 | loss: 0.010960862971842289\n",
      "Epoch: 83/100 | step: 277/422 | loss: 0.013308146968483925\n",
      "Epoch: 83/100 | step: 278/422 | loss: 0.018945567309856415\n",
      "Epoch: 83/100 | step: 279/422 | loss: 0.027826009318232536\n",
      "Epoch: 83/100 | step: 280/422 | loss: 0.015137386508286\n",
      "Epoch: 83/100 | step: 281/422 | loss: 0.016893314197659492\n",
      "Epoch: 83/100 | step: 282/422 | loss: 0.018279442563652992\n",
      "Epoch: 83/100 | step: 283/422 | loss: 0.01365889236330986\n",
      "Epoch: 83/100 | step: 284/422 | loss: 0.01821671612560749\n",
      "Epoch: 83/100 | step: 285/422 | loss: 0.02096051163971424\n",
      "Epoch: 83/100 | step: 286/422 | loss: 0.041903309524059296\n",
      "Epoch: 83/100 | step: 287/422 | loss: 0.036614757031202316\n",
      "Epoch: 83/100 | step: 288/422 | loss: 0.017553241923451424\n",
      "Epoch: 83/100 | step: 289/422 | loss: 0.040699247270822525\n",
      "Epoch: 83/100 | step: 290/422 | loss: 0.053395334631204605\n",
      "Epoch: 83/100 | step: 291/422 | loss: 0.052225109189748764\n",
      "Epoch: 83/100 | step: 292/422 | loss: 0.16222813725471497\n",
      "Epoch: 83/100 | step: 293/422 | loss: 0.08654393255710602\n",
      "Epoch: 83/100 | step: 294/422 | loss: 0.14219199120998383\n",
      "Epoch: 83/100 | step: 295/422 | loss: 0.3241174817085266\n",
      "Epoch: 83/100 | step: 296/422 | loss: 0.07855240255594254\n",
      "Epoch: 83/100 | step: 297/422 | loss: 0.11493506282567978\n",
      "Epoch: 83/100 | step: 298/422 | loss: 0.25758978724479675\n",
      "Epoch: 83/100 | step: 299/422 | loss: 0.024091869592666626\n",
      "Epoch: 83/100 | step: 300/422 | loss: 0.12062924355268478\n",
      "Epoch: 83/100 | step: 301/422 | loss: 0.11254586279392242\n",
      "Epoch: 83/100 | step: 302/422 | loss: 0.1485256552696228\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 84/100 | step: 1/422 | loss: 0.13092544674873352\n",
      "Epoch: 84/100 | step: 2/422 | loss: 0.03498401865363121\n",
      "Epoch: 84/100 | step: 3/422 | loss: 0.02491440810263157\n",
      "Epoch: 84/100 | step: 4/422 | loss: 0.016878357157111168\n",
      "Epoch: 84/100 | step: 5/422 | loss: 0.10730160772800446\n",
      "Epoch: 84/100 | step: 6/422 | loss: 0.01168933603912592\n",
      "Epoch: 84/100 | step: 7/422 | loss: 0.07763393223285675\n",
      "Epoch: 84/100 | step: 8/422 | loss: 0.10295762121677399\n",
      "Epoch: 84/100 | step: 9/422 | loss: 0.021788014099001884\n",
      "Epoch: 84/100 | step: 10/422 | loss: 0.014506244100630283\n",
      "Epoch: 84/100 | step: 11/422 | loss: 0.0331173874437809\n",
      "Epoch: 84/100 | step: 12/422 | loss: 0.04042314738035202\n",
      "Epoch: 84/100 | step: 13/422 | loss: 0.014571037143468857\n",
      "Epoch: 84/100 | step: 14/422 | loss: 0.030543122440576553\n",
      "Epoch: 84/100 | step: 15/422 | loss: 0.10996822267770767\n",
      "Epoch: 84/100 | step: 16/422 | loss: 0.04864882305264473\n",
      "Epoch: 84/100 | step: 17/422 | loss: 0.03479193150997162\n",
      "Epoch: 84/100 | step: 18/422 | loss: 0.02854781784117222\n",
      "Epoch: 84/100 | step: 19/422 | loss: 0.031637370586395264\n",
      "Epoch: 84/100 | step: 20/422 | loss: 0.02403687871992588\n",
      "Epoch: 84/100 | step: 21/422 | loss: 0.020290857180953026\n",
      "Epoch: 84/100 | step: 22/422 | loss: 0.04719500243663788\n",
      "Epoch: 84/100 | step: 23/422 | loss: 0.02717086300253868\n",
      "Epoch: 84/100 | step: 24/422 | loss: 0.018540773540735245\n",
      "Epoch: 84/100 | step: 25/422 | loss: 0.019034652039408684\n",
      "Epoch: 84/100 | step: 26/422 | loss: 0.02234126813709736\n",
      "Epoch: 84/100 | step: 27/422 | loss: 0.03933299705386162\n",
      "Epoch: 84/100 | step: 28/422 | loss: 0.023729346692562103\n",
      "Epoch: 84/100 | step: 29/422 | loss: 0.018234683200716972\n",
      "Epoch: 84/100 | step: 30/422 | loss: 0.02268519625067711\n",
      "Epoch: 84/100 | step: 31/422 | loss: 0.032873209565877914\n",
      "Epoch: 84/100 | step: 32/422 | loss: 0.015653176233172417\n",
      "Epoch: 84/100 | step: 33/422 | loss: 0.01413620263338089\n",
      "Epoch: 84/100 | step: 34/422 | loss: 0.014613199979066849\n",
      "Epoch: 84/100 | step: 35/422 | loss: 0.016954563558101654\n",
      "Epoch: 84/100 | step: 36/422 | loss: 0.014312363229691982\n",
      "Epoch: 84/100 | step: 37/422 | loss: 0.018921492621302605\n",
      "Epoch: 84/100 | step: 38/422 | loss: 0.019977884367108345\n",
      "Epoch: 84/100 | step: 39/422 | loss: 0.025014027953147888\n",
      "Epoch: 84/100 | step: 40/422 | loss: 0.0250802431255579\n",
      "Epoch: 84/100 | step: 41/422 | loss: 0.018033094704151154\n",
      "Epoch: 84/100 | step: 42/422 | loss: 0.017502134665846825\n",
      "Epoch: 84/100 | step: 43/422 | loss: 0.012299088761210442\n",
      "Epoch: 84/100 | step: 44/422 | loss: 0.016679532825946808\n",
      "Epoch: 84/100 | step: 45/422 | loss: 0.03260777145624161\n",
      "Epoch: 84/100 | step: 46/422 | loss: 0.022368784993886948\n",
      "Epoch: 84/100 | step: 47/422 | loss: 0.034357089549303055\n",
      "Epoch: 84/100 | step: 48/422 | loss: 0.015007132664322853\n",
      "Epoch: 84/100 | step: 49/422 | loss: 0.0145633015781641\n",
      "Epoch: 84/100 | step: 50/422 | loss: 0.01285640336573124\n",
      "Epoch: 84/100 | step: 51/422 | loss: 0.014175528660416603\n",
      "Epoch: 84/100 | step: 52/422 | loss: 0.009464044123888016\n",
      "Epoch: 84/100 | step: 53/422 | loss: 0.017160778865218163\n",
      "Epoch: 84/100 | step: 54/422 | loss: 0.01917443983256817\n",
      "Epoch: 84/100 | step: 55/422 | loss: 0.033139511942863464\n",
      "Epoch: 84/100 | step: 56/422 | loss: 0.026071647182106972\n",
      "Epoch: 84/100 | step: 57/422 | loss: 0.03156108409166336\n",
      "Epoch: 84/100 | step: 58/422 | loss: 0.022548021748661995\n",
      "Epoch: 84/100 | step: 59/422 | loss: 0.014023630879819393\n",
      "Epoch: 84/100 | step: 60/422 | loss: 0.012563113123178482\n",
      "Epoch: 84/100 | step: 61/422 | loss: 0.012388592585921288\n",
      "Epoch: 84/100 | step: 62/422 | loss: 0.015976687893271446\n",
      "Epoch: 84/100 | step: 63/422 | loss: 0.011915738694369793\n",
      "Epoch: 84/100 | step: 64/422 | loss: 0.013596740551292896\n",
      "Epoch: 84/100 | step: 65/422 | loss: 0.02656734175980091\n",
      "Epoch: 84/100 | step: 66/422 | loss: 0.00828869640827179\n",
      "Epoch: 84/100 | step: 67/422 | loss: 0.013676990754902363\n",
      "Epoch: 84/100 | step: 68/422 | loss: 0.009850947186350822\n",
      "Epoch: 84/100 | step: 69/422 | loss: 0.010833904147148132\n",
      "Epoch: 84/100 | step: 70/422 | loss: 0.016500625759363174\n",
      "Epoch: 84/100 | step: 71/422 | loss: 0.015978511422872543\n",
      "Epoch: 84/100 | step: 72/422 | loss: 0.010067391209304333\n",
      "Epoch: 84/100 | step: 73/422 | loss: 0.018129508942365646\n",
      "Epoch: 84/100 | step: 74/422 | loss: 0.01681525819003582\n",
      "Epoch: 84/100 | step: 75/422 | loss: 0.01794438809156418\n",
      "Epoch: 84/100 | step: 76/422 | loss: 0.02994200587272644\n",
      "Epoch: 84/100 | step: 77/422 | loss: 0.010051939636468887\n",
      "Epoch: 84/100 | step: 78/422 | loss: 0.012171399779617786\n",
      "Epoch: 84/100 | step: 79/422 | loss: 0.008439915254712105\n",
      "Epoch: 84/100 | step: 80/422 | loss: 0.011518356390297413\n",
      "Epoch: 84/100 | step: 81/422 | loss: 0.011899457313120365\n",
      "Epoch: 84/100 | step: 82/422 | loss: 0.010714446194469929\n",
      "Epoch: 84/100 | step: 83/422 | loss: 0.018902543932199478\n",
      "Epoch: 84/100 | step: 84/422 | loss: 0.016382696107029915\n",
      "Epoch: 84/100 | step: 85/422 | loss: 0.019123448058962822\n",
      "Epoch: 84/100 | step: 86/422 | loss: 0.01346578635275364\n",
      "Epoch: 84/100 | step: 87/422 | loss: 0.06081496179103851\n",
      "Epoch: 84/100 | step: 88/422 | loss: 0.03620162606239319\n",
      "Epoch: 84/100 | step: 89/422 | loss: 0.022687066346406937\n",
      "Epoch: 84/100 | step: 90/422 | loss: 0.017104309052228928\n",
      "Epoch: 84/100 | step: 91/422 | loss: 0.02202528342604637\n",
      "Epoch: 84/100 | step: 92/422 | loss: 0.016959412023425102\n",
      "Epoch: 84/100 | step: 93/422 | loss: 0.018381696194410324\n",
      "Epoch: 84/100 | step: 94/422 | loss: 0.024404840543866158\n",
      "Epoch: 84/100 | step: 95/422 | loss: 0.012881898321211338\n",
      "Epoch: 84/100 | step: 96/422 | loss: 0.021197805181145668\n",
      "Epoch: 84/100 | step: 97/422 | loss: 0.010576844215393066\n",
      "Epoch: 84/100 | step: 98/422 | loss: 0.012669781222939491\n",
      "Epoch: 84/100 | step: 99/422 | loss: 0.012235918082296848\n",
      "Epoch: 84/100 | step: 100/422 | loss: 0.009660761803388596\n",
      "Epoch: 84/100 | step: 101/422 | loss: 0.012597326189279556\n",
      "Epoch: 84/100 | step: 102/422 | loss: 0.037431392818689346\n",
      "Epoch: 84/100 | step: 103/422 | loss: 0.02941606380045414\n",
      "Epoch: 84/100 | step: 104/422 | loss: 0.01187001820653677\n",
      "Epoch: 84/100 | step: 105/422 | loss: 0.02349625714123249\n",
      "Epoch: 84/100 | step: 106/422 | loss: 0.017073750495910645\n",
      "Epoch: 84/100 | step: 107/422 | loss: 0.01374085247516632\n",
      "Epoch: 84/100 | step: 108/422 | loss: 0.013200138695538044\n",
      "Epoch: 84/100 | step: 109/422 | loss: 0.012073508463799953\n",
      "Epoch: 84/100 | step: 110/422 | loss: 0.020358409732580185\n",
      "Epoch: 84/100 | step: 111/422 | loss: 0.015028276480734348\n",
      "Epoch: 84/100 | step: 112/422 | loss: 0.013862572610378265\n",
      "Epoch: 84/100 | step: 113/422 | loss: 0.011369424872100353\n",
      "Epoch: 84/100 | step: 114/422 | loss: 0.018345940858125687\n",
      "Epoch: 84/100 | step: 115/422 | loss: 0.008019923232495785\n",
      "Epoch: 84/100 | step: 116/422 | loss: 0.014972902834415436\n",
      "Epoch: 84/100 | step: 117/422 | loss: 0.010235539637506008\n",
      "Epoch: 84/100 | step: 118/422 | loss: 0.01032059732824564\n",
      "Epoch: 84/100 | step: 119/422 | loss: 0.013422425836324692\n",
      "Epoch: 84/100 | step: 120/422 | loss: 0.037305157631635666\n",
      "Epoch: 84/100 | step: 121/422 | loss: 0.011841529048979282\n",
      "Epoch: 84/100 | step: 122/422 | loss: 0.01568068005144596\n",
      "Epoch: 84/100 | step: 123/422 | loss: 0.02057785727083683\n",
      "Epoch: 84/100 | step: 124/422 | loss: 0.01754285953938961\n",
      "Epoch: 84/100 | step: 125/422 | loss: 0.018778586760163307\n",
      "Epoch: 84/100 | step: 126/422 | loss: 0.0067495000548660755\n",
      "Epoch: 84/100 | step: 127/422 | loss: 0.009921805933117867\n",
      "Epoch: 84/100 | step: 128/422 | loss: 0.016362391412258148\n",
      "Epoch: 84/100 | step: 129/422 | loss: 0.02040560357272625\n",
      "Epoch: 84/100 | step: 130/422 | loss: 0.009283597581088543\n",
      "Epoch: 84/100 | step: 131/422 | loss: 0.010992820374667645\n",
      "Epoch: 84/100 | step: 132/422 | loss: 0.022871237248182297\n",
      "Epoch: 84/100 | step: 133/422 | loss: 0.012004111893475056\n",
      "Epoch: 84/100 | step: 134/422 | loss: 0.008650651201605797\n",
      "Epoch: 84/100 | step: 135/422 | loss: 0.009488129988312721\n",
      "Epoch: 84/100 | step: 136/422 | loss: 0.017815012484788895\n",
      "Epoch: 84/100 | step: 137/422 | loss: 0.03126126155257225\n",
      "Epoch: 84/100 | step: 138/422 | loss: 0.019042761996388435\n",
      "Epoch: 84/100 | step: 139/422 | loss: 0.01179492101073265\n",
      "Epoch: 84/100 | step: 140/422 | loss: 0.013382592238485813\n",
      "Epoch: 84/100 | step: 141/422 | loss: 0.01557684876024723\n",
      "Epoch: 84/100 | step: 142/422 | loss: 0.012091774493455887\n",
      "Epoch: 84/100 | step: 143/422 | loss: 0.016257215291261673\n",
      "Epoch: 84/100 | step: 144/422 | loss: 0.020046798512339592\n",
      "Epoch: 84/100 | step: 145/422 | loss: 0.009819078259170055\n",
      "Epoch: 84/100 | step: 146/422 | loss: 0.018666286021471024\n",
      "Epoch: 84/100 | step: 147/422 | loss: 0.028804689645767212\n",
      "Epoch: 84/100 | step: 148/422 | loss: 0.01875552162528038\n",
      "Epoch: 84/100 | step: 149/422 | loss: 0.019099527969956398\n",
      "Epoch: 84/100 | step: 150/422 | loss: 0.029349390417337418\n",
      "Epoch: 84/100 | step: 151/422 | loss: 0.010099540464580059\n",
      "Epoch: 84/100 | step: 152/422 | loss: 0.01235467940568924\n",
      "Epoch: 84/100 | step: 153/422 | loss: 0.010559970512986183\n",
      "Epoch: 84/100 | step: 154/422 | loss: 0.010051561519503593\n",
      "Epoch: 84/100 | step: 155/422 | loss: 0.01338838879019022\n",
      "Epoch: 84/100 | step: 156/422 | loss: 0.017461948096752167\n",
      "Epoch: 84/100 | step: 157/422 | loss: 0.02841319516301155\n",
      "Epoch: 84/100 | step: 158/422 | loss: 0.014356682077050209\n",
      "Epoch: 84/100 | step: 159/422 | loss: 0.012039017863571644\n",
      "Epoch: 84/100 | step: 160/422 | loss: 0.01506753358989954\n",
      "Epoch: 84/100 | step: 161/422 | loss: 0.013180349953472614\n",
      "Epoch: 84/100 | step: 162/422 | loss: 0.009280471131205559\n",
      "Epoch: 84/100 | step: 163/422 | loss: 0.028601281344890594\n",
      "Epoch: 84/100 | step: 164/422 | loss: 0.014153692871332169\n",
      "Epoch: 84/100 | step: 165/422 | loss: 0.013433830812573433\n",
      "Epoch: 84/100 | step: 166/422 | loss: 0.010220538824796677\n",
      "Epoch: 84/100 | step: 167/422 | loss: 0.012058385647833347\n",
      "Epoch: 84/100 | step: 168/422 | loss: 0.016493359580636024\n",
      "Epoch: 84/100 | step: 169/422 | loss: 0.008764324709773064\n",
      "Epoch: 84/100 | step: 170/422 | loss: 0.02678639255464077\n",
      "Epoch: 84/100 | step: 171/422 | loss: 0.007901754230260849\n",
      "Epoch: 84/100 | step: 172/422 | loss: 0.012806115671992302\n",
      "Epoch: 84/100 | step: 173/422 | loss: 0.009619480930268764\n",
      "Epoch: 84/100 | step: 174/422 | loss: 0.01617291383445263\n",
      "Epoch: 84/100 | step: 175/422 | loss: 0.018816040828824043\n",
      "Epoch: 84/100 | step: 176/422 | loss: 0.036924097687006\n",
      "Epoch: 84/100 | step: 177/422 | loss: 0.10164948552846909\n",
      "Epoch: 84/100 | step: 178/422 | loss: 0.14952519536018372\n",
      "Epoch: 84/100 | step: 179/422 | loss: 0.040906090289354324\n",
      "Epoch: 84/100 | step: 180/422 | loss: 0.010999479331076145\n",
      "Epoch: 84/100 | step: 181/422 | loss: 0.006797002628445625\n",
      "Epoch: 84/100 | step: 182/422 | loss: 0.03977827727794647\n",
      "Epoch: 84/100 | step: 183/422 | loss: 0.04254649579524994\n",
      "Epoch: 84/100 | step: 184/422 | loss: 0.05138477310538292\n",
      "Epoch: 84/100 | step: 185/422 | loss: 0.04107118770480156\n",
      "Epoch: 84/100 | step: 186/422 | loss: 0.022063829004764557\n",
      "Epoch: 84/100 | step: 187/422 | loss: 0.02298879250884056\n",
      "Epoch: 84/100 | step: 188/422 | loss: 0.028530921787023544\n",
      "Epoch: 84/100 | step: 189/422 | loss: 0.015514508821070194\n",
      "Epoch: 84/100 | step: 190/422 | loss: 0.023446504026651382\n",
      "Epoch: 84/100 | step: 191/422 | loss: 0.041174307465553284\n",
      "Epoch: 84/100 | step: 192/422 | loss: 0.08913303911685944\n",
      "Epoch: 84/100 | step: 193/422 | loss: 0.022561408579349518\n",
      "Epoch: 84/100 | step: 194/422 | loss: 0.014949426986277103\n",
      "Epoch: 84/100 | step: 195/422 | loss: 0.06317389756441116\n",
      "Epoch: 84/100 | step: 196/422 | loss: 0.021596990525722504\n",
      "Epoch: 84/100 | step: 197/422 | loss: 0.009958045557141304\n",
      "Epoch: 84/100 | step: 198/422 | loss: 0.027993712574243546\n",
      "Epoch: 84/100 | step: 199/422 | loss: 0.00899231806397438\n",
      "Epoch: 84/100 | step: 200/422 | loss: 0.020013218745589256\n",
      "Epoch: 84/100 | step: 201/422 | loss: 0.020383698865771294\n",
      "Epoch: 84/100 | step: 202/422 | loss: 0.012166334316134453\n",
      "Epoch: 84/100 | step: 203/422 | loss: 0.011795451864600182\n",
      "Epoch: 84/100 | step: 204/422 | loss: 0.017510727047920227\n",
      "Epoch: 84/100 | step: 205/422 | loss: 0.01797533594071865\n",
      "Epoch: 84/100 | step: 206/422 | loss: 0.009733743034303188\n",
      "Epoch: 84/100 | step: 207/422 | loss: 0.014333982020616531\n",
      "Epoch: 84/100 | step: 208/422 | loss: 0.0088196387514472\n",
      "Epoch: 84/100 | step: 209/422 | loss: 0.035410426557064056\n",
      "Epoch: 84/100 | step: 210/422 | loss: 0.022324495017528534\n",
      "Epoch: 84/100 | step: 211/422 | loss: 0.023380368947982788\n",
      "Epoch: 84/100 | step: 212/422 | loss: 0.03573373332619667\n",
      "Epoch: 84/100 | step: 213/422 | loss: 0.012419624254107475\n",
      "Epoch: 84/100 | step: 214/422 | loss: 0.008290019817650318\n",
      "Epoch: 84/100 | step: 215/422 | loss: 0.021782200783491135\n",
      "Epoch: 84/100 | step: 216/422 | loss: 0.021300382912158966\n",
      "Epoch: 84/100 | step: 217/422 | loss: 0.014078988693654537\n",
      "Epoch: 84/100 | step: 218/422 | loss: 0.012412667274475098\n",
      "Epoch: 84/100 | step: 219/422 | loss: 0.02424195408821106\n",
      "Epoch: 84/100 | step: 220/422 | loss: 0.010216542519629002\n",
      "Epoch: 84/100 | step: 221/422 | loss: 0.009129755198955536\n",
      "Epoch: 84/100 | step: 222/422 | loss: 0.021758466958999634\n",
      "Epoch: 84/100 | step: 223/422 | loss: 0.012130428105592728\n",
      "Epoch: 84/100 | step: 224/422 | loss: 0.016714870929718018\n",
      "Epoch: 84/100 | step: 225/422 | loss: 0.03149087354540825\n",
      "Epoch: 84/100 | step: 226/422 | loss: 0.00782005861401558\n",
      "Epoch: 84/100 | step: 227/422 | loss: 0.017235469073057175\n",
      "Epoch: 84/100 | step: 228/422 | loss: 0.012507886625826359\n",
      "Epoch: 84/100 | step: 229/422 | loss: 0.019305018708109856\n",
      "Epoch: 84/100 | step: 230/422 | loss: 0.023021290078759193\n",
      "Epoch: 84/100 | step: 231/422 | loss: 0.011178819462656975\n",
      "Epoch: 84/100 | step: 232/422 | loss: 0.01859191246330738\n",
      "Epoch: 84/100 | step: 233/422 | loss: 0.012693958356976509\n",
      "Epoch: 84/100 | step: 234/422 | loss: 0.012158483266830444\n",
      "Epoch: 84/100 | step: 235/422 | loss: 0.011315930634737015\n",
      "Epoch: 84/100 | step: 236/422 | loss: 0.007666026242077351\n",
      "Epoch: 84/100 | step: 237/422 | loss: 0.01463779155164957\n",
      "Epoch: 84/100 | step: 238/422 | loss: 0.008895151317119598\n",
      "Epoch: 84/100 | step: 239/422 | loss: 0.01210695132613182\n",
      "Epoch: 84/100 | step: 240/422 | loss: 0.013978874310851097\n",
      "Epoch: 84/100 | step: 241/422 | loss: 0.01526288129389286\n",
      "Epoch: 84/100 | step: 242/422 | loss: 0.018833251670002937\n",
      "Epoch: 84/100 | step: 243/422 | loss: 0.009232300333678722\n",
      "Epoch: 84/100 | step: 244/422 | loss: 0.016985518857836723\n",
      "Epoch: 84/100 | step: 245/422 | loss: 0.034250661730766296\n",
      "Epoch: 84/100 | step: 246/422 | loss: 0.02355913445353508\n",
      "Epoch: 84/100 | step: 247/422 | loss: 0.011499432846903801\n",
      "Epoch: 84/100 | step: 248/422 | loss: 0.008861277252435684\n",
      "Epoch: 84/100 | step: 249/422 | loss: 0.007561827544122934\n",
      "Error occurred during training: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple\n",
      "Epoch: 85/100 | step: 1/422 | loss: 0.011259485967457294\n",
      "Epoch: 85/100 | step: 2/422 | loss: 0.04429308697581291\n",
      "Epoch: 85/100 | step: 3/422 | loss: 0.01225607842206955\n",
      "Epoch: 85/100 | step: 4/422 | loss: 0.01515202783048153\n",
      "Epoch: 85/100 | step: 5/422 | loss: 0.01240801066160202\n",
      "Epoch: 85/100 | step: 6/422 | loss: 0.059233322739601135\n",
      "Epoch: 85/100 | step: 7/422 | loss: 0.2224639505147934\n",
      "Epoch: 85/100 | step: 8/422 | loss: 0.011144566349685192\n",
      "Epoch: 85/100 | step: 9/422 | loss: 0.013480639085173607\n",
      "Epoch: 85/100 | step: 10/422 | loss: 0.08464053273200989\n",
      "Epoch: 85/100 | step: 11/422 | loss: 0.11942790448665619\n",
      "Epoch: 85/100 | step: 12/422 | loss: 0.07948140054941177\n",
      "Epoch: 85/100 | step: 13/422 | loss: 0.014582408592104912\n",
      "Epoch: 85/100 | step: 14/422 | loss: 0.16308405995368958\n",
      "Epoch: 85/100 | step: 15/422 | loss: 0.09358128905296326\n",
      "Epoch: 85/100 | step: 16/422 | loss: 0.08447033166885376\n",
      "Epoch: 85/100 | step: 17/422 | loss: 0.012966914102435112\n",
      "Epoch: 85/100 | step: 18/422 | loss: 0.02306430973112583\n",
      "Epoch: 85/100 | step: 19/422 | loss: 0.009185348637402058\n",
      "Epoch: 85/100 | step: 20/422 | loss: 0.023628273978829384\n",
      "Epoch: 85/100 | step: 21/422 | loss: 0.014015642926096916\n",
      "Epoch: 85/100 | step: 22/422 | loss: 0.010395893827080727\n",
      "Epoch: 85/100 | step: 23/422 | loss: 0.0459517240524292\n",
      "Epoch: 85/100 | step: 24/422 | loss: 0.0238533653318882\n",
      "Epoch: 85/100 | step: 25/422 | loss: 0.036299847066402435\n",
      "Epoch: 85/100 | step: 26/422 | loss: 0.03479227051138878\n",
      "Epoch: 85/100 | step: 27/422 | loss: 0.0191032737493515\n",
      "Epoch: 85/100 | step: 28/422 | loss: 0.03974970057606697\n",
      "Epoch: 85/100 | step: 29/422 | loss: 0.012646815739572048\n",
      "Epoch: 85/100 | step: 30/422 | loss: 0.020317191258072853\n",
      "Epoch: 85/100 | step: 31/422 | loss: 0.026865486055612564\n",
      "Epoch: 85/100 | step: 32/422 | loss: 0.010291234590113163\n",
      "Epoch: 85/100 | step: 33/422 | loss: 0.014768557623028755\n",
      "Epoch: 85/100 | step: 34/422 | loss: 0.020787576213479042\n",
      "Epoch: 85/100 | step: 35/422 | loss: 0.02449721284210682\n",
      "Epoch: 85/100 | step: 36/422 | loss: 0.016140596941113472\n",
      "Epoch: 85/100 | step: 37/422 | loss: 0.010500872507691383\n",
      "Epoch: 85/100 | step: 38/422 | loss: 0.028738118708133698\n",
      "Epoch: 85/100 | step: 39/422 | loss: 0.011053470894694328\n",
      "Epoch: 85/100 | step: 40/422 | loss: 0.03177972137928009\n",
      "Epoch: 85/100 | step: 41/422 | loss: 0.018287280574440956\n",
      "Epoch: 85/100 | step: 42/422 | loss: 0.01443081721663475\n",
      "Epoch: 85/100 | step: 43/422 | loss: 0.02587714046239853\n",
      "Epoch: 85/100 | step: 44/422 | loss: 0.023271124809980392\n",
      "Epoch: 85/100 | step: 45/422 | loss: 0.00975861307233572\n",
      "Epoch: 85/100 | step: 46/422 | loss: 0.03494821488857269\n",
      "Epoch: 85/100 | step: 47/422 | loss: 0.022670581936836243\n",
      "Epoch: 85/100 | step: 48/422 | loss: 0.010112795047461987\n",
      "Epoch: 85/100 | step: 49/422 | loss: 0.009401499293744564\n",
      "Epoch: 85/100 | step: 50/422 | loss: 0.020288845524191856\n",
      "Epoch: 85/100 | step: 51/422 | loss: 0.009722850285470486\n",
      "Epoch: 85/100 | step: 52/422 | loss: 0.017387310042977333\n",
      "Epoch: 85/100 | step: 53/422 | loss: 0.022006647661328316\n",
      "Epoch: 85/100 | step: 54/422 | loss: 0.029363151639699936\n",
      "Epoch: 85/100 | step: 55/422 | loss: 0.011671838350594044\n",
      "Epoch: 85/100 | step: 56/422 | loss: 0.007314927410334349\n",
      "Epoch: 85/100 | step: 57/422 | loss: 0.010247791185975075\n",
      "Epoch: 85/100 | step: 58/422 | loss: 0.021814517676830292\n",
      "Epoch: 85/100 | step: 59/422 | loss: 0.019122475758194923\n",
      "Epoch: 85/100 | step: 60/422 | loss: 0.01233100425451994\n",
      "Epoch: 85/100 | step: 61/422 | loss: 0.014328965917229652\n",
      "Epoch: 85/100 | step: 62/422 | loss: 0.006979845464229584\n",
      "Epoch: 85/100 | step: 63/422 | loss: 0.035032372921705246\n",
      "Epoch: 85/100 | step: 64/422 | loss: 0.1103331670165062\n",
      "Epoch: 85/100 | step: 65/422 | loss: 0.01581750437617302\n",
      "Epoch: 85/100 | step: 66/422 | loss: 0.07266677916049957\n",
      "Epoch: 85/100 | step: 67/422 | loss: 0.07537738233804703\n",
      "Epoch: 85/100 | step: 68/422 | loss: 0.01585986837744713\n",
      "Epoch: 85/100 | step: 69/422 | loss: 0.11092769354581833\n",
      "Epoch: 85/100 | step: 70/422 | loss: 0.18057607114315033\n",
      "Epoch: 85/100 | step: 71/422 | loss: 0.014169699512422085\n",
      "Epoch: 85/100 | step: 72/422 | loss: 0.086463563144207\n",
      "Epoch: 85/100 | step: 73/422 | loss: 0.025958450511097908\n",
      "Epoch: 85/100 | step: 74/422 | loss: 0.014155909419059753\n",
      "Epoch: 85/100 | step: 75/422 | loss: 0.08105865120887756\n",
      "Epoch: 85/100 | step: 76/422 | loss: 0.023803774267435074\n",
      "Epoch: 85/100 | step: 77/422 | loss: 0.011688298545777798\n",
      "Epoch: 85/100 | step: 78/422 | loss: 0.12153342366218567\n",
      "Epoch: 85/100 | step: 79/422 | loss: 0.15729832649230957\n",
      "Epoch: 85/100 | step: 80/422 | loss: 0.015505889430642128\n",
      "Epoch: 85/100 | step: 81/422 | loss: 0.08989471197128296\n",
      "Epoch: 85/100 | step: 82/422 | loss: 0.0153480414301157\n",
      "Epoch: 85/100 | step: 83/422 | loss: 0.011040671728551388\n",
      "Epoch: 85/100 | step: 84/422 | loss: 0.016135958954691887\n",
      "Epoch: 85/100 | step: 85/422 | loss: 0.01435506995767355\n",
      "Epoch: 85/100 | step: 86/422 | loss: 0.0245075523853302\n",
      "Epoch: 85/100 | step: 87/422 | loss: 0.013641895726323128\n",
      "Epoch: 85/100 | step: 88/422 | loss: 0.0832303985953331\n",
      "Epoch: 85/100 | step: 89/422 | loss: 0.014390211552381516\n",
      "Epoch: 85/100 | step: 90/422 | loss: 0.019157465547323227\n",
      "Epoch: 85/100 | step: 91/422 | loss: 0.041324518620967865\n",
      "Epoch: 85/100 | step: 92/422 | loss: 0.018601741641759872\n",
      "Epoch: 85/100 | step: 93/422 | loss: 0.019502367824316025\n",
      "Epoch: 85/100 | step: 94/422 | loss: 0.018418634310364723\n",
      "Epoch: 85/100 | step: 95/422 | loss: 0.015273379161953926\n",
      "Epoch: 85/100 | step: 96/422 | loss: 0.012664493173360825\n",
      "Epoch: 85/100 | step: 97/422 | loss: 0.008131764829158783\n",
      "Epoch: 85/100 | step: 98/422 | loss: 0.01208530180156231\n",
      "Epoch: 85/100 | step: 99/422 | loss: 0.08351772278547287\n",
      "Epoch: 85/100 | step: 100/422 | loss: 0.014873847365379333\n",
      "Epoch: 85/100 | step: 101/422 | loss: 0.014488646760582924\n",
      "Epoch: 85/100 | step: 102/422 | loss: 0.01294371485710144\n",
      "Epoch: 85/100 | step: 103/422 | loss: 0.015284989029169083\n",
      "Epoch: 85/100 | step: 104/422 | loss: 0.008923452347517014\n",
      "Epoch: 85/100 | step: 105/422 | loss: 0.016831103712320328\n",
      "Epoch: 85/100 | step: 106/422 | loss: 0.021260801702737808\n",
      "Epoch: 85/100 | step: 107/422 | loss: 0.016760459169745445\n",
      "Epoch: 85/100 | step: 108/422 | loss: 0.06775812804698944\n",
      "Epoch: 85/100 | step: 109/422 | loss: 0.15384916961193085\n",
      "Epoch: 85/100 | step: 110/422 | loss: 0.014634368941187859\n",
      "Epoch: 85/100 | step: 111/422 | loss: 0.014778196811676025\n",
      "Epoch: 85/100 | step: 112/422 | loss: 0.22264854609966278\n",
      "Epoch: 85/100 | step: 113/422 | loss: 0.36929914355278015\n",
      "Epoch: 85/100 | step: 114/422 | loss: 0.01533343456685543\n",
      "Epoch: 85/100 | step: 115/422 | loss: 0.021535327658057213\n",
      "Epoch: 85/100 | step: 116/422 | loss: 0.12970305979251862\n",
      "Epoch: 85/100 | step: 117/422 | loss: 0.012290650978684425\n",
      "Epoch: 85/100 | step: 118/422 | loss: 0.16724300384521484\n",
      "Epoch: 85/100 | step: 119/422 | loss: 0.013994362205266953\n",
      "Epoch: 85/100 | step: 120/422 | loss: 0.024164073169231415\n",
      "Epoch: 85/100 | step: 121/422 | loss: 0.024747606366872787\n",
      "Epoch: 85/100 | step: 122/422 | loss: 0.016962844878435135\n",
      "Epoch: 85/100 | step: 123/422 | loss: 0.013879252597689629\n",
      "Epoch: 85/100 | step: 124/422 | loss: 0.027222713455557823\n",
      "Epoch: 85/100 | step: 125/422 | loss: 0.016979428008198738\n",
      "Epoch: 85/100 | step: 126/422 | loss: 0.01216521393507719\n",
      "Epoch: 85/100 | step: 127/422 | loss: 0.04339030012488365\n",
      "Epoch: 85/100 | step: 128/422 | loss: 0.032686226069927216\n",
      "Epoch: 85/100 | step: 129/422 | loss: 0.058887943625450134\n",
      "Epoch: 85/100 | step: 130/422 | loss: 0.012974027544260025\n",
      "Epoch: 85/100 | step: 131/422 | loss: 0.02148417942225933\n",
      "Epoch: 85/100 | step: 132/422 | loss: 0.023455604910850525\n",
      "Epoch: 85/100 | step: 133/422 | loss: 0.06628326326608658\n",
      "Epoch: 85/100 | step: 134/422 | loss: 0.01516892109066248\n",
      "Epoch: 85/100 | step: 135/422 | loss: 0.09067237377166748\n",
      "Epoch: 85/100 | step: 136/422 | loss: 0.13266108930110931\n",
      "Epoch: 85/100 | step: 137/422 | loss: 0.23845267295837402\n",
      "Epoch: 85/100 | step: 138/422 | loss: 0.1985570192337036\n",
      "Epoch: 85/100 | step: 139/422 | loss: 0.26946592330932617\n",
      "Epoch: 85/100 | step: 140/422 | loss: 0.06626700609922409\n",
      "Epoch: 85/100 | step: 141/422 | loss: 0.040107470005750656\n",
      "Epoch: 85/100 | step: 142/422 | loss: 0.040078386664390564\n",
      "Epoch: 85/100 | step: 143/422 | loss: 0.05738929286599159\n",
      "Epoch: 85/100 | step: 144/422 | loss: 0.03219188004732132\n",
      "Epoch: 85/100 | step: 145/422 | loss: 0.09944406151771545\n",
      "Epoch: 85/100 | step: 146/422 | loss: 0.1737567037343979\n",
      "Epoch: 85/100 | step: 147/422 | loss: 0.015256478451192379\n",
      "Epoch: 85/100 | step: 148/422 | loss: 0.02798491343855858\n",
      "Epoch: 85/100 | step: 149/422 | loss: 0.019858453422784805\n",
      "Epoch: 85/100 | step: 150/422 | loss: 0.023408381268382072\n",
      "Epoch: 85/100 | step: 151/422 | loss: 0.021937312558293343\n",
      "Epoch: 85/100 | step: 152/422 | loss: 0.012935350649058819\n",
      "Epoch: 85/100 | step: 153/422 | loss: 0.025558756664395332\n",
      "Epoch: 85/100 | step: 154/422 | loss: 0.04475155472755432\n",
      "Epoch: 85/100 | step: 155/422 | loss: 0.02140079438686371\n",
      "Epoch: 85/100 | step: 156/422 | loss: 0.021396923810243607\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 86/100 | step: 1/422 | loss: 0.014709495939314365\n",
      "Epoch: 86/100 | step: 2/422 | loss: 0.02087097428739071\n",
      "Epoch: 86/100 | step: 3/422 | loss: 0.02545403130352497\n",
      "Epoch: 86/100 | step: 4/422 | loss: 0.018705368041992188\n",
      "Epoch: 86/100 | step: 5/422 | loss: 0.013797048479318619\n",
      "Epoch: 86/100 | step: 6/422 | loss: 0.011367990635335445\n",
      "Epoch: 86/100 | step: 7/422 | loss: 0.009057029150426388\n",
      "Epoch: 86/100 | step: 8/422 | loss: 0.007275229319930077\n",
      "Epoch: 86/100 | step: 9/422 | loss: 0.011718942783772945\n",
      "Epoch: 86/100 | step: 10/422 | loss: 0.009655670262873173\n",
      "Epoch: 86/100 | step: 11/422 | loss: 0.018015164881944656\n",
      "Epoch: 86/100 | step: 12/422 | loss: 0.007153598126024008\n",
      "Epoch: 86/100 | step: 13/422 | loss: 0.010859736241400242\n",
      "Epoch: 86/100 | step: 14/422 | loss: 0.0178587194532156\n",
      "Epoch: 86/100 | step: 15/422 | loss: 0.009798947721719742\n",
      "Epoch: 86/100 | step: 16/422 | loss: 0.014371609315276146\n",
      "Epoch: 86/100 | step: 17/422 | loss: 0.021366577595472336\n",
      "Epoch: 86/100 | step: 18/422 | loss: 0.010981684550642967\n",
      "Epoch: 86/100 | step: 19/422 | loss: 0.013685433194041252\n",
      "Epoch: 86/100 | step: 20/422 | loss: 0.01192200742661953\n",
      "Epoch: 86/100 | step: 21/422 | loss: 0.012292623519897461\n",
      "Epoch: 86/100 | step: 22/422 | loss: 0.012212812900543213\n",
      "Epoch: 86/100 | step: 23/422 | loss: 0.015114162117242813\n",
      "Epoch: 86/100 | step: 24/422 | loss: 0.010094774886965752\n",
      "Epoch: 86/100 | step: 25/422 | loss: 0.01273298915475607\n",
      "Epoch: 86/100 | step: 26/422 | loss: 0.008097512647509575\n",
      "Epoch: 86/100 | step: 27/422 | loss: 0.010091634467244148\n",
      "Epoch: 86/100 | step: 28/422 | loss: 0.009498335421085358\n",
      "Epoch: 86/100 | step: 29/422 | loss: 0.07503554224967957\n",
      "Epoch: 86/100 | step: 30/422 | loss: 0.014764076098799706\n",
      "Epoch: 86/100 | step: 31/422 | loss: 0.008165586739778519\n",
      "Epoch: 86/100 | step: 32/422 | loss: 0.010288277640938759\n",
      "Epoch: 86/100 | step: 33/422 | loss: 0.05152912810444832\n",
      "Epoch: 86/100 | step: 34/422 | loss: 0.06575468927621841\n",
      "Epoch: 86/100 | step: 35/422 | loss: 0.0207907035946846\n",
      "Epoch: 86/100 | step: 36/422 | loss: 0.03642977401614189\n",
      "Epoch: 86/100 | step: 37/422 | loss: 0.04137932136654854\n",
      "Epoch: 86/100 | step: 38/422 | loss: 0.010943340137600899\n",
      "Epoch: 86/100 | step: 39/422 | loss: 0.018175123259425163\n",
      "Epoch: 86/100 | step: 40/422 | loss: 0.013654464855790138\n",
      "Epoch: 86/100 | step: 41/422 | loss: 0.055791616439819336\n",
      "Epoch: 86/100 | step: 42/422 | loss: 0.022559119388461113\n",
      "Epoch: 86/100 | step: 43/422 | loss: 0.0249054953455925\n",
      "Epoch: 86/100 | step: 44/422 | loss: 0.030520886182785034\n",
      "Epoch: 86/100 | step: 45/422 | loss: 0.006596464663743973\n",
      "Epoch: 86/100 | step: 46/422 | loss: 0.01013280265033245\n",
      "Epoch: 86/100 | step: 47/422 | loss: 0.016148395836353302\n",
      "Epoch: 86/100 | step: 48/422 | loss: 0.010998478159308434\n",
      "Epoch: 86/100 | step: 49/422 | loss: 0.011359542608261108\n",
      "Epoch: 86/100 | step: 50/422 | loss: 0.006871038116514683\n",
      "Epoch: 86/100 | step: 51/422 | loss: 0.012840216979384422\n",
      "Epoch: 86/100 | step: 52/422 | loss: 0.028905510902404785\n",
      "Epoch: 86/100 | step: 53/422 | loss: 0.02373264729976654\n",
      "Epoch: 86/100 | step: 54/422 | loss: 0.019494526088237762\n",
      "Epoch: 86/100 | step: 55/422 | loss: 0.014768789522349834\n",
      "Epoch: 86/100 | step: 56/422 | loss: 0.020130380988121033\n",
      "Epoch: 86/100 | step: 57/422 | loss: 0.00964186992496252\n",
      "Epoch: 86/100 | step: 58/422 | loss: 0.016672365367412567\n",
      "Epoch: 86/100 | step: 59/422 | loss: 0.009709903039038181\n",
      "Epoch: 86/100 | step: 60/422 | loss: 0.009619586169719696\n",
      "Epoch: 86/100 | step: 61/422 | loss: 0.018932269886136055\n",
      "Epoch: 86/100 | step: 62/422 | loss: 0.017653822898864746\n",
      "Epoch: 86/100 | step: 63/422 | loss: 0.013867639936506748\n",
      "Epoch: 86/100 | step: 64/422 | loss: 0.009714655578136444\n",
      "Epoch: 86/100 | step: 65/422 | loss: 0.05206794664263725\n",
      "Epoch: 86/100 | step: 66/422 | loss: 0.010592608712613583\n",
      "Epoch: 86/100 | step: 67/422 | loss: 0.025427520275115967\n",
      "Epoch: 86/100 | step: 68/422 | loss: 0.01453754398971796\n",
      "Epoch: 86/100 | step: 69/422 | loss: 0.047342125326395035\n",
      "Epoch: 86/100 | step: 70/422 | loss: 0.09725753217935562\n",
      "Epoch: 86/100 | step: 71/422 | loss: 0.022959057241678238\n",
      "Epoch: 86/100 | step: 72/422 | loss: 0.030019298195838928\n",
      "Epoch: 86/100 | step: 73/422 | loss: 0.014700767584145069\n",
      "Epoch: 86/100 | step: 74/422 | loss: 0.011996819637715816\n",
      "Epoch: 86/100 | step: 75/422 | loss: 0.021606238558888435\n",
      "Epoch: 86/100 | step: 76/422 | loss: 0.12571313977241516\n",
      "Epoch: 86/100 | step: 77/422 | loss: 0.213352233171463\n",
      "Epoch: 86/100 | step: 78/422 | loss: 0.05599120631814003\n",
      "Epoch: 86/100 | step: 79/422 | loss: 0.08181482553482056\n",
      "Epoch: 86/100 | step: 80/422 | loss: 0.027746763080358505\n",
      "Epoch: 86/100 | step: 81/422 | loss: 0.07989129424095154\n",
      "Epoch: 86/100 | step: 82/422 | loss: 0.01693856716156006\n",
      "Epoch: 86/100 | step: 83/422 | loss: 0.031236348673701286\n",
      "Epoch: 86/100 | step: 84/422 | loss: 0.024218151345849037\n",
      "Epoch: 86/100 | step: 85/422 | loss: 0.022138701751828194\n",
      "Epoch: 86/100 | step: 86/422 | loss: 0.026500733569264412\n",
      "Epoch: 86/100 | step: 87/422 | loss: 0.01620236225426197\n",
      "Epoch: 86/100 | step: 88/422 | loss: 0.008627403527498245\n",
      "Epoch: 86/100 | step: 89/422 | loss: 0.02943422831594944\n",
      "Epoch: 86/100 | step: 90/422 | loss: 0.031666774302721024\n",
      "Epoch: 86/100 | step: 91/422 | loss: 0.019209126010537148\n",
      "Epoch: 86/100 | step: 92/422 | loss: 0.021202685311436653\n",
      "Epoch: 86/100 | step: 93/422 | loss: 0.01276966743171215\n",
      "Epoch: 86/100 | step: 94/422 | loss: 0.01876980811357498\n",
      "Epoch: 86/100 | step: 95/422 | loss: 0.03201844543218613\n",
      "Epoch: 86/100 | step: 96/422 | loss: 0.01332516223192215\n",
      "Epoch: 86/100 | step: 97/422 | loss: 0.011040889658033848\n",
      "Epoch: 86/100 | step: 98/422 | loss: 0.013601606711745262\n",
      "Epoch: 86/100 | step: 99/422 | loss: 0.014418447390198708\n",
      "Epoch: 86/100 | step: 100/422 | loss: 0.013404885306954384\n",
      "Epoch: 86/100 | step: 101/422 | loss: 0.014957991428673267\n",
      "Epoch: 86/100 | step: 102/422 | loss: 0.013418485410511494\n",
      "Epoch: 86/100 | step: 103/422 | loss: 0.04012329876422882\n",
      "Epoch: 86/100 | step: 104/422 | loss: 0.00903528556227684\n",
      "Epoch: 86/100 | step: 105/422 | loss: 0.006465488113462925\n",
      "Epoch: 86/100 | step: 106/422 | loss: 0.011708914302289486\n",
      "Epoch: 86/100 | step: 107/422 | loss: 0.02192147821187973\n",
      "Epoch: 86/100 | step: 108/422 | loss: 0.011875050142407417\n",
      "Epoch: 86/100 | step: 109/422 | loss: 0.00804313737899065\n",
      "Epoch: 86/100 | step: 110/422 | loss: 0.006148985121399164\n",
      "Epoch: 86/100 | step: 111/422 | loss: 0.008470914326608181\n",
      "Epoch: 86/100 | step: 112/422 | loss: 0.014955615624785423\n",
      "Epoch: 86/100 | step: 113/422 | loss: 0.012457320466637611\n",
      "Epoch: 86/100 | step: 114/422 | loss: 0.027232429012656212\n",
      "Epoch: 86/100 | step: 115/422 | loss: 0.01847606524825096\n",
      "Epoch: 86/100 | step: 116/422 | loss: 0.017216460779309273\n",
      "Epoch: 86/100 | step: 117/422 | loss: 0.006295941304415464\n",
      "Epoch: 86/100 | step: 118/422 | loss: 0.016642484813928604\n",
      "Epoch: 86/100 | step: 119/422 | loss: 0.008651651442050934\n",
      "Epoch: 86/100 | step: 120/422 | loss: 0.01308471243828535\n",
      "Epoch: 86/100 | step: 121/422 | loss: 0.013829901814460754\n",
      "Epoch: 86/100 | step: 122/422 | loss: 0.013758002780377865\n",
      "Epoch: 86/100 | step: 123/422 | loss: 0.03420212119817734\n",
      "Epoch: 86/100 | step: 124/422 | loss: 0.02123281918466091\n",
      "Epoch: 86/100 | step: 125/422 | loss: 0.029817543923854828\n",
      "Epoch: 86/100 | step: 126/422 | loss: 0.024571526795625687\n",
      "Epoch: 86/100 | step: 127/422 | loss: 0.016669193282723427\n",
      "Epoch: 86/100 | step: 128/422 | loss: 0.013995293527841568\n",
      "Epoch: 86/100 | step: 129/422 | loss: 0.012146159075200558\n",
      "Epoch: 86/100 | step: 130/422 | loss: 0.014977606944739819\n",
      "Epoch: 86/100 | step: 131/422 | loss: 0.048472363501787186\n",
      "Epoch: 86/100 | step: 132/422 | loss: 0.015213184989988804\n",
      "Epoch: 86/100 | step: 133/422 | loss: 0.01937534101307392\n",
      "Epoch: 86/100 | step: 134/422 | loss: 0.013000967912375927\n",
      "Epoch: 86/100 | step: 135/422 | loss: 0.015214971266686916\n",
      "Epoch: 86/100 | step: 136/422 | loss: 0.01395046804100275\n",
      "Epoch: 86/100 | step: 137/422 | loss: 0.00923508033156395\n",
      "Epoch: 86/100 | step: 138/422 | loss: 0.00699212308973074\n",
      "Epoch: 86/100 | step: 139/422 | loss: 0.014963587746024132\n",
      "Epoch: 86/100 | step: 140/422 | loss: 0.008627110160887241\n",
      "Epoch: 86/100 | step: 141/422 | loss: 0.007342762779444456\n",
      "Epoch: 86/100 | step: 142/422 | loss: 0.0077546341344714165\n",
      "Epoch: 86/100 | step: 143/422 | loss: 0.016855057328939438\n",
      "Epoch: 86/100 | step: 144/422 | loss: 0.0095496391877532\n",
      "Epoch: 86/100 | step: 145/422 | loss: 0.016088401898741722\n",
      "Epoch: 86/100 | step: 146/422 | loss: 0.01065913587808609\n",
      "Epoch: 86/100 | step: 147/422 | loss: 0.012416660785675049\n",
      "Epoch: 86/100 | step: 148/422 | loss: 0.015771610662341118\n",
      "Epoch: 86/100 | step: 149/422 | loss: 0.01120035257190466\n",
      "Epoch: 86/100 | step: 150/422 | loss: 0.02840062417089939\n",
      "Epoch: 86/100 | step: 151/422 | loss: 0.02435668744146824\n",
      "Epoch: 86/100 | step: 152/422 | loss: 0.007734074257314205\n",
      "Epoch: 86/100 | step: 153/422 | loss: 0.015953868627548218\n",
      "Epoch: 86/100 | step: 154/422 | loss: 0.012148978188633919\n",
      "Epoch: 86/100 | step: 155/422 | loss: 0.014480650424957275\n",
      "Epoch: 86/100 | step: 156/422 | loss: 0.010751482099294662\n",
      "Epoch: 86/100 | step: 157/422 | loss: 0.011273432523012161\n",
      "Epoch: 86/100 | step: 158/422 | loss: 0.03235659375786781\n",
      "Epoch: 86/100 | step: 159/422 | loss: 0.033262480050325394\n",
      "Epoch: 86/100 | step: 160/422 | loss: 0.016582367941737175\n",
      "Epoch: 86/100 | step: 161/422 | loss: 0.018337728455662727\n",
      "Epoch: 86/100 | step: 162/422 | loss: 0.01731302961707115\n",
      "Epoch: 86/100 | step: 163/422 | loss: 0.005779891740530729\n",
      "Epoch: 86/100 | step: 164/422 | loss: 0.00653145182877779\n",
      "Epoch: 86/100 | step: 165/422 | loss: 0.015384828671813011\n",
      "Epoch: 86/100 | step: 166/422 | loss: 0.008597565814852715\n",
      "Epoch: 86/100 | step: 167/422 | loss: 0.015679532662034035\n",
      "Epoch: 86/100 | step: 168/422 | loss: 0.012456245720386505\n",
      "Epoch: 86/100 | step: 169/422 | loss: 0.017323851585388184\n",
      "Epoch: 86/100 | step: 170/422 | loss: 0.0859917402267456\n",
      "Epoch: 86/100 | step: 171/422 | loss: 0.022861571982502937\n",
      "Epoch: 86/100 | step: 172/422 | loss: 0.056578345596790314\n",
      "Epoch: 86/100 | step: 173/422 | loss: 0.007144067902117968\n",
      "Epoch: 86/100 | step: 174/422 | loss: 0.02394757978618145\n",
      "Epoch: 86/100 | step: 175/422 | loss: 0.01869135908782482\n",
      "Epoch: 86/100 | step: 176/422 | loss: 0.010872854851186275\n",
      "Epoch: 86/100 | step: 177/422 | loss: 0.01864531636238098\n",
      "Epoch: 86/100 | step: 178/422 | loss: 0.08786484599113464\n",
      "Epoch: 86/100 | step: 179/422 | loss: 0.03508548066020012\n",
      "Epoch: 86/100 | step: 180/422 | loss: 0.04766082391142845\n",
      "Epoch: 86/100 | step: 181/422 | loss: 0.011322570964694023\n",
      "Epoch: 86/100 | step: 182/422 | loss: 0.019080750644207\n",
      "Epoch: 86/100 | step: 183/422 | loss: 0.012200044468045235\n",
      "Epoch: 86/100 | step: 184/422 | loss: 0.022863902151584625\n",
      "Epoch: 86/100 | step: 185/422 | loss: 0.01577562838792801\n",
      "Epoch: 86/100 | step: 186/422 | loss: 0.01643090322613716\n",
      "Epoch: 86/100 | step: 187/422 | loss: 0.012060745619237423\n",
      "Epoch: 86/100 | step: 188/422 | loss: 0.027192547917366028\n",
      "Epoch: 86/100 | step: 189/422 | loss: 0.015705248340964317\n",
      "Epoch: 86/100 | step: 190/422 | loss: 0.012994730845093727\n",
      "Epoch: 86/100 | step: 191/422 | loss: 0.01042582280933857\n",
      "Epoch: 86/100 | step: 192/422 | loss: 0.0321626253426075\n",
      "Epoch: 86/100 | step: 193/422 | loss: 0.10718365013599396\n",
      "Epoch: 86/100 | step: 194/422 | loss: 0.13122783601284027\n",
      "Epoch: 86/100 | step: 195/422 | loss: 0.06783822178840637\n",
      "Epoch: 86/100 | step: 196/422 | loss: 0.019660916179418564\n",
      "Epoch: 86/100 | step: 197/422 | loss: 0.04694139584898949\n",
      "Epoch: 86/100 | step: 198/422 | loss: 0.018075941130518913\n",
      "Epoch: 86/100 | step: 199/422 | loss: 0.02331675961613655\n",
      "Epoch: 86/100 | step: 200/422 | loss: 0.03593394160270691\n",
      "Epoch: 86/100 | step: 201/422 | loss: 0.13930828869342804\n",
      "Epoch: 86/100 | step: 202/422 | loss: 0.36333954334259033\n",
      "Epoch: 86/100 | step: 203/422 | loss: 0.08173862099647522\n",
      "Epoch: 86/100 | step: 204/422 | loss: 0.04779252037405968\n",
      "Epoch: 86/100 | step: 205/422 | loss: 0.04868524894118309\n",
      "Epoch: 86/100 | step: 206/422 | loss: 0.03451170399785042\n",
      "Epoch: 86/100 | step: 207/422 | loss: 0.049412064254283905\n",
      "Epoch: 86/100 | step: 208/422 | loss: 0.04830637201666832\n",
      "Epoch: 86/100 | step: 209/422 | loss: 0.03493938595056534\n",
      "Epoch: 86/100 | step: 210/422 | loss: 0.015676897019147873\n",
      "Epoch: 86/100 | step: 211/422 | loss: 0.020197361707687378\n",
      "Epoch: 86/100 | step: 212/422 | loss: 0.024599282070994377\n",
      "Epoch: 86/100 | step: 213/422 | loss: 0.013339189812541008\n",
      "Epoch: 86/100 | step: 214/422 | loss: 0.013648212887346745\n",
      "Epoch: 86/100 | step: 215/422 | loss: 0.020902620628476143\n",
      "Epoch: 86/100 | step: 216/422 | loss: 0.020954472944140434\n",
      "Epoch: 86/100 | step: 217/422 | loss: 0.008830385282635689\n",
      "Epoch: 86/100 | step: 218/422 | loss: 0.014966707676649094\n",
      "Epoch: 86/100 | step: 219/422 | loss: 0.009689541533589363\n",
      "Epoch: 86/100 | step: 220/422 | loss: 0.017827119678258896\n",
      "Epoch: 86/100 | step: 221/422 | loss: 0.01035180315375328\n",
      "Epoch: 86/100 | step: 222/422 | loss: 0.0213298499584198\n",
      "Epoch: 86/100 | step: 223/422 | loss: 0.02582625113427639\n",
      "Epoch: 86/100 | step: 224/422 | loss: 0.009450819343328476\n",
      "Epoch: 86/100 | step: 225/422 | loss: 0.01557197980582714\n",
      "Epoch: 86/100 | step: 226/422 | loss: 0.016718992963433266\n",
      "Epoch: 86/100 | step: 227/422 | loss: 0.008507267571985722\n",
      "Epoch: 86/100 | step: 228/422 | loss: 0.010018735192716122\n",
      "Epoch: 86/100 | step: 229/422 | loss: 0.022164970636367798\n",
      "Epoch: 86/100 | step: 230/422 | loss: 0.0111551433801651\n",
      "Epoch: 86/100 | step: 231/422 | loss: 0.006027529016137123\n",
      "Epoch: 86/100 | step: 232/422 | loss: 0.013990083709359169\n",
      "Epoch: 86/100 | step: 233/422 | loss: 0.036524660885334015\n",
      "Epoch: 86/100 | step: 234/422 | loss: 0.029460016638040543\n",
      "Epoch: 86/100 | step: 235/422 | loss: 0.013830962590873241\n",
      "Epoch: 86/100 | step: 236/422 | loss: 0.010341980494558811\n",
      "Epoch: 86/100 | step: 237/422 | loss: 0.01603073813021183\n",
      "Epoch: 86/100 | step: 238/422 | loss: 0.026302559301257133\n",
      "Epoch: 86/100 | step: 239/422 | loss: 0.017023157328367233\n",
      "Epoch: 86/100 | step: 240/422 | loss: 0.04792948439717293\n",
      "Epoch: 86/100 | step: 241/422 | loss: 0.02234880067408085\n",
      "Epoch: 86/100 | step: 242/422 | loss: 0.02848590724170208\n",
      "Epoch: 86/100 | step: 243/422 | loss: 0.008875529281795025\n",
      "Epoch: 86/100 | step: 244/422 | loss: 0.08499496430158615\n",
      "Epoch: 86/100 | step: 245/422 | loss: 0.01821683533489704\n",
      "Epoch: 86/100 | step: 246/422 | loss: 0.015059544704854488\n",
      "Epoch: 86/100 | step: 247/422 | loss: 0.02086218260228634\n",
      "Epoch: 86/100 | step: 248/422 | loss: 0.03252924606204033\n",
      "Epoch: 86/100 | step: 249/422 | loss: 0.01931646279990673\n",
      "Epoch: 86/100 | step: 250/422 | loss: 0.011505210772156715\n",
      "Epoch: 86/100 | step: 251/422 | loss: 0.010177298448979855\n",
      "Epoch: 86/100 | step: 252/422 | loss: 0.01385426614433527\n",
      "Epoch: 86/100 | step: 253/422 | loss: 0.012738853693008423\n",
      "Epoch: 86/100 | step: 254/422 | loss: 0.013258950784802437\n",
      "Epoch: 86/100 | step: 255/422 | loss: 0.020662304013967514\n",
      "Epoch: 86/100 | step: 256/422 | loss: 0.01568436063826084\n",
      "Epoch: 86/100 | step: 257/422 | loss: 0.013549499213695526\n",
      "Epoch: 86/100 | step: 258/422 | loss: 0.015194726176559925\n",
      "Epoch: 86/100 | step: 259/422 | loss: 0.022722726687788963\n",
      "Epoch: 86/100 | step: 260/422 | loss: 0.020706776529550552\n",
      "Epoch: 86/100 | step: 261/422 | loss: 0.018431447446346283\n",
      "Epoch: 86/100 | step: 262/422 | loss: 0.013492089696228504\n",
      "Epoch: 86/100 | step: 263/422 | loss: 0.013198225758969784\n",
      "Epoch: 86/100 | step: 264/422 | loss: 0.0106202382594347\n",
      "Epoch: 86/100 | step: 265/422 | loss: 0.054746583104133606\n",
      "Epoch: 86/100 | step: 266/422 | loss: 0.01094471663236618\n",
      "Epoch: 86/100 | step: 267/422 | loss: 0.06129596754908562\n",
      "Epoch: 86/100 | step: 268/422 | loss: 0.034898508340120316\n",
      "Epoch: 86/100 | step: 269/422 | loss: 0.01460657361894846\n",
      "Epoch: 86/100 | step: 270/422 | loss: 0.008000561967492104\n",
      "Epoch: 86/100 | step: 271/422 | loss: 0.011593327857553959\n",
      "Epoch: 86/100 | step: 272/422 | loss: 0.023692317306995392\n",
      "Epoch: 86/100 | step: 273/422 | loss: 0.008630646392703056\n",
      "Epoch: 86/100 | step: 274/422 | loss: 0.037458956241607666\n",
      "Epoch: 86/100 | step: 275/422 | loss: 0.014045901596546173\n",
      "Epoch: 86/100 | step: 276/422 | loss: 0.045091766864061356\n",
      "Epoch: 86/100 | step: 277/422 | loss: 0.014623272232711315\n",
      "Epoch: 86/100 | step: 278/422 | loss: 0.0714515745639801\n",
      "Epoch: 86/100 | step: 279/422 | loss: 0.012673139572143555\n",
      "Epoch: 86/100 | step: 280/422 | loss: 0.025597965344786644\n",
      "Epoch: 86/100 | step: 281/422 | loss: 0.021231643855571747\n",
      "Epoch: 86/100 | step: 282/422 | loss: 0.017080925405025482\n",
      "Epoch: 86/100 | step: 283/422 | loss: 0.020600588992238045\n",
      "Epoch: 86/100 | step: 284/422 | loss: 0.011363746598362923\n",
      "Epoch: 86/100 | step: 285/422 | loss: 0.010755226016044617\n",
      "Epoch: 86/100 | step: 286/422 | loss: 0.018210235983133316\n",
      "Epoch: 86/100 | step: 287/422 | loss: 0.009979123249650002\n",
      "Epoch: 86/100 | step: 288/422 | loss: 0.011172402650117874\n",
      "Epoch: 86/100 | step: 289/422 | loss: 0.01617606170475483\n",
      "Epoch: 86/100 | step: 290/422 | loss: 0.030754612758755684\n",
      "Epoch: 86/100 | step: 291/422 | loss: 0.02097870223224163\n",
      "Epoch: 86/100 | step: 292/422 | loss: 0.014003323391079903\n",
      "Epoch: 86/100 | step: 293/422 | loss: 0.013277623802423477\n",
      "Epoch: 86/100 | step: 294/422 | loss: 0.015518793836236\n",
      "Epoch: 86/100 | step: 295/422 | loss: 0.011269665323197842\n",
      "Epoch: 86/100 | step: 296/422 | loss: 0.011909658089280128\n",
      "Epoch: 86/100 | step: 297/422 | loss: 0.026110295206308365\n",
      "Epoch: 86/100 | step: 298/422 | loss: 0.03178461641073227\n",
      "Epoch: 86/100 | step: 299/422 | loss: 0.02088051475584507\n",
      "Epoch: 86/100 | step: 300/422 | loss: 0.0116230184212327\n",
      "Epoch: 86/100 | step: 301/422 | loss: 0.02516406960785389\n",
      "Epoch: 86/100 | step: 302/422 | loss: 0.07043670117855072\n",
      "Epoch: 86/100 | step: 303/422 | loss: 0.047924287617206573\n",
      "Epoch: 86/100 | step: 304/422 | loss: 0.1338479220867157\n",
      "Epoch: 86/100 | step: 305/422 | loss: 0.13064566254615784\n",
      "Epoch: 86/100 | step: 306/422 | loss: 0.3571206033229828\n",
      "Epoch: 86/100 | step: 307/422 | loss: 0.017047015950083733\n",
      "Epoch: 86/100 | step: 308/422 | loss: 0.055613256990909576\n",
      "Epoch: 86/100 | step: 309/422 | loss: 0.010588825680315495\n",
      "Epoch: 86/100 | step: 310/422 | loss: 0.04092102497816086\n",
      "Epoch: 86/100 | step: 311/422 | loss: 0.024503866210579872\n",
      "Epoch: 86/100 | step: 312/422 | loss: 0.053277820348739624\n",
      "Epoch: 86/100 | step: 313/422 | loss: 0.0480782650411129\n",
      "Epoch: 86/100 | step: 314/422 | loss: 0.07894221693277359\n",
      "Epoch: 86/100 | step: 315/422 | loss: 0.017178118228912354\n",
      "Epoch: 86/100 | step: 316/422 | loss: 0.10954512655735016\n",
      "Epoch: 86/100 | step: 317/422 | loss: 0.01481759361922741\n",
      "Epoch: 86/100 | step: 318/422 | loss: 0.04366861283779144\n",
      "Epoch: 86/100 | step: 319/422 | loss: 0.013435415923595428\n",
      "Epoch: 86/100 | step: 320/422 | loss: 0.016312191262841225\n",
      "Epoch: 86/100 | step: 321/422 | loss: 0.02284727245569229\n",
      "Epoch: 86/100 | step: 322/422 | loss: 0.00996908824890852\n",
      "Epoch: 86/100 | step: 323/422 | loss: 0.031720828264951706\n",
      "Epoch: 86/100 | step: 324/422 | loss: 0.028508545830845833\n",
      "Epoch: 86/100 | step: 325/422 | loss: 0.013960759155452251\n",
      "Epoch: 86/100 | step: 326/422 | loss: 0.024168916046619415\n",
      "Epoch: 86/100 | step: 327/422 | loss: 0.018240980803966522\n",
      "Epoch: 86/100 | step: 328/422 | loss: 0.01990385539829731\n",
      "Epoch: 86/100 | step: 329/422 | loss: 0.009358185343444347\n",
      "Epoch: 86/100 | step: 330/422 | loss: 0.008280704729259014\n",
      "Epoch: 86/100 | step: 331/422 | loss: 0.014256742782890797\n",
      "Epoch: 86/100 | step: 332/422 | loss: 0.011073542758822441\n",
      "Epoch: 86/100 | step: 333/422 | loss: 0.011224543675780296\n",
      "Epoch: 86/100 | step: 334/422 | loss: 0.020063046365976334\n",
      "Epoch: 86/100 | step: 335/422 | loss: 0.022061925381422043\n",
      "Epoch: 86/100 | step: 336/422 | loss: 0.029051141813397408\n",
      "Epoch: 86/100 | step: 337/422 | loss: 0.01443162839859724\n",
      "Epoch: 86/100 | step: 338/422 | loss: 0.01024208776652813\n",
      "Epoch: 86/100 | step: 339/422 | loss: 0.013939991593360901\n",
      "Epoch: 86/100 | step: 340/422 | loss: 0.01738174445927143\n",
      "Epoch: 86/100 | step: 341/422 | loss: 0.01251719519495964\n",
      "Epoch: 86/100 | step: 342/422 | loss: 0.00897674448788166\n",
      "Epoch: 86/100 | step: 343/422 | loss: 0.010863320901989937\n",
      "Epoch: 86/100 | step: 344/422 | loss: 0.013850686140358448\n",
      "Epoch: 86/100 | step: 345/422 | loss: 0.014857706613838673\n",
      "Epoch: 86/100 | step: 346/422 | loss: 0.011155887506902218\n",
      "Epoch: 86/100 | step: 347/422 | loss: 0.005638443864881992\n",
      "Epoch: 86/100 | step: 348/422 | loss: 0.01296969037503004\n",
      "Epoch: 86/100 | step: 349/422 | loss: 0.013390185311436653\n",
      "Epoch: 86/100 | step: 350/422 | loss: 0.019981544464826584\n",
      "Epoch: 86/100 | step: 351/422 | loss: 0.01728574000298977\n",
      "Epoch: 86/100 | step: 352/422 | loss: 0.018558431416749954\n",
      "Epoch: 86/100 | step: 353/422 | loss: 0.08086360991001129\n",
      "Epoch: 86/100 | step: 354/422 | loss: 0.1181594505906105\n",
      "Epoch: 86/100 | step: 355/422 | loss: 0.03264099359512329\n",
      "Epoch: 86/100 | step: 356/422 | loss: 0.02572295255959034\n",
      "Epoch: 86/100 | step: 357/422 | loss: 0.033999424427747726\n",
      "Epoch: 86/100 | step: 358/422 | loss: 0.03286786377429962\n",
      "Epoch: 86/100 | step: 359/422 | loss: 0.03495153412222862\n",
      "Epoch: 86/100 | step: 360/422 | loss: 0.011823213659226894\n",
      "Epoch: 86/100 | step: 361/422 | loss: 0.04090609773993492\n",
      "Epoch: 86/100 | step: 362/422 | loss: 0.02919408120214939\n",
      "Epoch: 86/100 | step: 363/422 | loss: 0.01675625517964363\n",
      "Epoch: 86/100 | step: 364/422 | loss: 0.050492577254772186\n",
      "Epoch: 86/100 | step: 365/422 | loss: 0.16982890665531158\n",
      "Epoch: 86/100 | step: 366/422 | loss: 0.08987833559513092\n",
      "Epoch: 86/100 | step: 367/422 | loss: 0.04103507474064827\n",
      "Epoch: 86/100 | step: 368/422 | loss: 0.08995059877634048\n",
      "Epoch: 86/100 | step: 369/422 | loss: 0.32984521985054016\n",
      "Epoch: 86/100 | step: 370/422 | loss: 0.44506579637527466\n",
      "Epoch: 86/100 | step: 371/422 | loss: 0.4690691828727722\n",
      "Epoch: 86/100 | step: 372/422 | loss: 0.45595046877861023\n",
      "Epoch: 86/100 | step: 373/422 | loss: 1.409854769706726\n",
      "Epoch: 86/100 | step: 374/422 | loss: 0.2690296173095703\n",
      "Epoch: 86/100 | step: 375/422 | loss: 0.04971607029438019\n",
      "Epoch: 86/100 | step: 376/422 | loss: 0.7787221670150757\n",
      "Epoch: 86/100 | step: 377/422 | loss: 0.7514265775680542\n",
      "Epoch: 86/100 | step: 378/422 | loss: 0.4257374703884125\n",
      "Epoch: 86/100 | step: 379/422 | loss: 0.1572289764881134\n",
      "Epoch: 86/100 | step: 380/422 | loss: 0.05652179569005966\n",
      "Epoch: 86/100 | step: 381/422 | loss: 0.09366952627897263\n",
      "Epoch: 86/100 | step: 382/422 | loss: 0.051887236535549164\n",
      "Epoch: 86/100 | step: 383/422 | loss: 0.0614062063395977\n",
      "Epoch: 86/100 | step: 384/422 | loss: 0.3375011086463928\n",
      "Epoch: 86/100 | step: 385/422 | loss: 0.09548326581716537\n",
      "Epoch: 86/100 | step: 386/422 | loss: 0.027346467599272728\n",
      "Epoch: 86/100 | step: 387/422 | loss: 0.04637511447072029\n",
      "Epoch: 86/100 | step: 388/422 | loss: 0.05024009197950363\n",
      "Epoch: 86/100 | step: 389/422 | loss: 0.059040579944849014\n",
      "Epoch: 86/100 | step: 390/422 | loss: 0.02456449344754219\n",
      "Epoch: 86/100 | step: 391/422 | loss: 0.08451715111732483\n",
      "Epoch: 86/100 | step: 392/422 | loss: 0.037919044494628906\n",
      "Epoch: 86/100 | step: 393/422 | loss: 0.12709608674049377\n",
      "Epoch: 86/100 | step: 394/422 | loss: 0.08438578248023987\n",
      "Epoch: 86/100 | step: 395/422 | loss: 0.015584311448037624\n",
      "Epoch: 86/100 | step: 396/422 | loss: 0.05557357519865036\n",
      "Epoch: 86/100 | step: 397/422 | loss: 0.02643970400094986\n",
      "Epoch: 86/100 | step: 398/422 | loss: 0.0721672773361206\n",
      "Epoch: 86/100 | step: 399/422 | loss: 0.15240222215652466\n",
      "Epoch: 86/100 | step: 400/422 | loss: 0.014546127058565617\n",
      "Epoch: 86/100 | step: 401/422 | loss: 0.03583882004022598\n",
      "Epoch: 86/100 | step: 402/422 | loss: 0.07034061849117279\n",
      "Epoch: 86/100 | step: 403/422 | loss: 0.0294154305011034\n",
      "Epoch: 86/100 | step: 404/422 | loss: 0.033172182738780975\n",
      "Epoch: 86/100 | step: 405/422 | loss: 0.10322847217321396\n",
      "Epoch: 86/100 | step: 406/422 | loss: 0.04584937542676926\n",
      "Epoch: 86/100 | step: 407/422 | loss: 0.036715153604745865\n",
      "Epoch: 86/100 | step: 408/422 | loss: 0.03590191528201103\n",
      "Epoch: 86/100 | step: 409/422 | loss: 0.0317981094121933\n",
      "Epoch: 86/100 | step: 410/422 | loss: 0.022481262683868408\n",
      "Epoch: 86/100 | step: 411/422 | loss: 0.012224031612277031\n",
      "Epoch: 86/100 | step: 412/422 | loss: 0.03820129483938217\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 87/100 | step: 1/422 | loss: 0.07144312560558319\n",
      "Epoch: 87/100 | step: 2/422 | loss: 0.014974584802985191\n",
      "Epoch: 87/100 | step: 3/422 | loss: 0.010236394591629505\n",
      "Epoch: 87/100 | step: 4/422 | loss: 0.1608731895685196\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 88/100 | step: 1/422 | loss: 0.02102113701403141\n",
      "Epoch: 88/100 | step: 2/422 | loss: 0.05533216521143913\n",
      "Epoch: 88/100 | step: 3/422 | loss: 0.01992165483534336\n",
      "Epoch: 88/100 | step: 4/422 | loss: 0.024233724921941757\n",
      "Epoch: 88/100 | step: 5/422 | loss: 0.02826564945280552\n",
      "Epoch: 88/100 | step: 6/422 | loss: 0.036971356719732285\n",
      "Epoch: 88/100 | step: 7/422 | loss: 0.029796313494443893\n",
      "Epoch: 88/100 | step: 8/422 | loss: 0.02427379973232746\n",
      "Epoch: 88/100 | step: 9/422 | loss: 0.022651344537734985\n",
      "Epoch: 88/100 | step: 10/422 | loss: 0.008401710540056229\n",
      "Epoch: 88/100 | step: 11/422 | loss: 0.03375052660703659\n",
      "Epoch: 88/100 | step: 12/422 | loss: 0.01667814329266548\n",
      "Epoch: 88/100 | step: 13/422 | loss: 0.043141793459653854\n",
      "Epoch: 88/100 | step: 14/422 | loss: 0.018542608246207237\n",
      "Epoch: 88/100 | step: 15/422 | loss: 0.015551986172795296\n",
      "Epoch: 88/100 | step: 16/422 | loss: 0.020324181765317917\n",
      "Epoch: 88/100 | step: 17/422 | loss: 0.009974400512874126\n",
      "Epoch: 88/100 | step: 18/422 | loss: 0.028641454875469208\n",
      "Epoch: 88/100 | step: 19/422 | loss: 0.01391738560050726\n",
      "Epoch: 88/100 | step: 20/422 | loss: 0.013387936167418957\n",
      "Epoch: 88/100 | step: 21/422 | loss: 0.017028549686074257\n",
      "Epoch: 88/100 | step: 22/422 | loss: 0.02220901846885681\n",
      "Epoch: 88/100 | step: 23/422 | loss: 0.01733505353331566\n",
      "Epoch: 88/100 | step: 24/422 | loss: 0.021169444546103477\n",
      "Epoch: 88/100 | step: 25/422 | loss: 0.0233380775898695\n",
      "Epoch: 88/100 | step: 26/422 | loss: 0.03166528046131134\n",
      "Epoch: 88/100 | step: 27/422 | loss: 0.00985392089933157\n",
      "Epoch: 88/100 | step: 28/422 | loss: 0.046028077602386475\n",
      "Epoch: 88/100 | step: 29/422 | loss: 0.014950469136238098\n",
      "Epoch: 88/100 | step: 30/422 | loss: 0.029928872361779213\n",
      "Epoch: 88/100 | step: 31/422 | loss: 0.01268061064183712\n",
      "Epoch: 88/100 | step: 32/422 | loss: 0.012954151257872581\n",
      "Epoch: 88/100 | step: 33/422 | loss: 0.023025602102279663\n",
      "Epoch: 88/100 | step: 34/422 | loss: 0.010565581731498241\n",
      "Epoch: 88/100 | step: 35/422 | loss: 0.00808534026145935\n",
      "Epoch: 88/100 | step: 36/422 | loss: 0.012986931018531322\n",
      "Epoch: 88/100 | step: 37/422 | loss: 0.014714611694216728\n",
      "Epoch: 88/100 | step: 38/422 | loss: 0.01767118088901043\n",
      "Epoch: 88/100 | step: 39/422 | loss: 0.014058435335755348\n",
      "Epoch: 88/100 | step: 40/422 | loss: 0.020898474380373955\n",
      "Epoch: 88/100 | step: 41/422 | loss: 0.01880798302590847\n",
      "Epoch: 88/100 | step: 42/422 | loss: 0.027128763496875763\n",
      "Epoch: 88/100 | step: 43/422 | loss: 0.013896036893129349\n",
      "Epoch: 88/100 | step: 44/422 | loss: 0.016111265867948532\n",
      "Epoch: 88/100 | step: 45/422 | loss: 0.021447235718369484\n",
      "Epoch: 88/100 | step: 46/422 | loss: 0.01338098756968975\n",
      "Epoch: 88/100 | step: 47/422 | loss: 0.011601095087826252\n",
      "Epoch: 88/100 | step: 48/422 | loss: 0.010225209407508373\n",
      "Epoch: 88/100 | step: 49/422 | loss: 0.008705832064151764\n",
      "Epoch: 88/100 | step: 50/422 | loss: 0.009722861461341381\n",
      "Epoch: 88/100 | step: 51/422 | loss: 0.017016855999827385\n",
      "Epoch: 88/100 | step: 52/422 | loss: 0.0257834792137146\n",
      "Epoch: 88/100 | step: 53/422 | loss: 0.07710946351289749\n",
      "Epoch: 88/100 | step: 54/422 | loss: 0.016757797449827194\n",
      "Epoch: 88/100 | step: 55/422 | loss: 0.012422665022313595\n",
      "Epoch: 88/100 | step: 56/422 | loss: 0.04897432029247284\n",
      "Epoch: 88/100 | step: 57/422 | loss: 0.1059790700674057\n",
      "Epoch: 88/100 | step: 58/422 | loss: 0.12157876789569855\n",
      "Epoch: 88/100 | step: 59/422 | loss: 0.014430825598537922\n",
      "Epoch: 88/100 | step: 60/422 | loss: 0.030514122918248177\n",
      "Epoch: 88/100 | step: 61/422 | loss: 0.07716885209083557\n",
      "Epoch: 88/100 | step: 62/422 | loss: 0.046565085649490356\n",
      "Epoch: 88/100 | step: 63/422 | loss: 0.014306298457086086\n",
      "Epoch: 88/100 | step: 64/422 | loss: 0.020480167120695114\n",
      "Epoch: 88/100 | step: 65/422 | loss: 0.022727422416210175\n",
      "Epoch: 88/100 | step: 66/422 | loss: 0.01994975283741951\n",
      "Epoch: 88/100 | step: 67/422 | loss: 0.017015470191836357\n",
      "Epoch: 88/100 | step: 68/422 | loss: 0.024582823738455772\n",
      "Epoch: 88/100 | step: 69/422 | loss: 0.01580752618610859\n",
      "Epoch: 88/100 | step: 70/422 | loss: 0.040669020265340805\n",
      "Epoch: 88/100 | step: 71/422 | loss: 0.02619071491062641\n",
      "Epoch: 88/100 | step: 72/422 | loss: 0.012323792092502117\n",
      "Epoch: 88/100 | step: 73/422 | loss: 0.05617080256342888\n",
      "Epoch: 88/100 | step: 74/422 | loss: 0.018525000661611557\n",
      "Epoch: 88/100 | step: 75/422 | loss: 0.0642058253288269\n",
      "Epoch: 88/100 | step: 76/422 | loss: 0.014642313122749329\n",
      "Epoch: 88/100 | step: 77/422 | loss: 0.011483368463814259\n",
      "Epoch: 88/100 | step: 78/422 | loss: 0.01055337954312563\n",
      "Epoch: 88/100 | step: 79/422 | loss: 0.06957264244556427\n",
      "Epoch: 88/100 | step: 80/422 | loss: 0.08322762697935104\n",
      "Epoch: 88/100 | step: 81/422 | loss: 0.033834539353847504\n",
      "Epoch: 88/100 | step: 82/422 | loss: 0.01970810256898403\n",
      "Epoch: 88/100 | step: 83/422 | loss: 0.09087995439767838\n",
      "Epoch: 88/100 | step: 84/422 | loss: 0.018971404060721397\n",
      "Epoch: 88/100 | step: 85/422 | loss: 0.01359526440501213\n",
      "Epoch: 88/100 | step: 86/422 | loss: 0.017648402601480484\n",
      "Epoch: 88/100 | step: 87/422 | loss: 0.017886262387037277\n",
      "Epoch: 88/100 | step: 88/422 | loss: 0.014843665063381195\n",
      "Epoch: 88/100 | step: 89/422 | loss: 0.019106512889266014\n",
      "Epoch: 88/100 | step: 90/422 | loss: 0.013783996924757957\n",
      "Epoch: 88/100 | step: 91/422 | loss: 0.043573811650276184\n",
      "Epoch: 88/100 | step: 92/422 | loss: 0.010509463027119637\n",
      "Epoch: 88/100 | step: 93/422 | loss: 0.017108898609876633\n",
      "Epoch: 88/100 | step: 94/422 | loss: 0.011984775774180889\n",
      "Epoch: 88/100 | step: 95/422 | loss: 0.030056878924369812\n",
      "Epoch: 88/100 | step: 96/422 | loss: 0.017171159386634827\n",
      "Epoch: 88/100 | step: 97/422 | loss: 0.007214177865535021\n",
      "Epoch: 88/100 | step: 98/422 | loss: 0.009344940073788166\n",
      "Epoch: 88/100 | step: 99/422 | loss: 0.01258935034275055\n",
      "Epoch: 88/100 | step: 100/422 | loss: 0.008272243663668633\n",
      "Epoch: 88/100 | step: 101/422 | loss: 0.0101186353713274\n",
      "Epoch: 88/100 | step: 102/422 | loss: 0.022068142890930176\n",
      "Epoch: 88/100 | step: 103/422 | loss: 0.014541801065206528\n",
      "Epoch: 88/100 | step: 104/422 | loss: 0.013508758507668972\n",
      "Epoch: 88/100 | step: 105/422 | loss: 0.00722696864977479\n",
      "Epoch: 88/100 | step: 106/422 | loss: 0.009919398464262486\n",
      "Epoch: 88/100 | step: 107/422 | loss: 0.017754610627889633\n",
      "Epoch: 88/100 | step: 108/422 | loss: 0.033797748386859894\n",
      "Epoch: 88/100 | step: 109/422 | loss: 0.01657632552087307\n",
      "Epoch: 88/100 | step: 110/422 | loss: 0.02027107961475849\n",
      "Epoch: 88/100 | step: 111/422 | loss: 0.009968661703169346\n",
      "Epoch: 88/100 | step: 112/422 | loss: 0.009741936810314655\n",
      "Epoch: 88/100 | step: 113/422 | loss: 0.009753969497978687\n",
      "Epoch: 88/100 | step: 114/422 | loss: 0.02770652249455452\n",
      "Epoch: 88/100 | step: 115/422 | loss: 0.014426954090595245\n",
      "Epoch: 88/100 | step: 116/422 | loss: 0.00795748271048069\n",
      "Epoch: 88/100 | step: 117/422 | loss: 0.012151923961937428\n",
      "Epoch: 88/100 | step: 118/422 | loss: 0.05169832706451416\n",
      "Epoch: 88/100 | step: 119/422 | loss: 0.24006150662899017\n",
      "Epoch: 88/100 | step: 120/422 | loss: 0.04333062097430229\n",
      "Epoch: 88/100 | step: 121/422 | loss: 0.042817097157239914\n",
      "Epoch: 88/100 | step: 122/422 | loss: 0.03831295296549797\n",
      "Epoch: 88/100 | step: 123/422 | loss: 0.013056674972176552\n",
      "Epoch: 88/100 | step: 124/422 | loss: 0.012580330483615398\n",
      "Epoch: 88/100 | step: 125/422 | loss: 0.05489363521337509\n",
      "Epoch: 88/100 | step: 126/422 | loss: 0.01572367735207081\n",
      "Epoch: 88/100 | step: 127/422 | loss: 0.012070722877979279\n",
      "Epoch: 88/100 | step: 128/422 | loss: 0.1654808521270752\n",
      "Epoch: 88/100 | step: 129/422 | loss: 0.01925165206193924\n",
      "Epoch: 88/100 | step: 130/422 | loss: 0.031137138605117798\n",
      "Epoch: 88/100 | step: 131/422 | loss: 0.03135938197374344\n",
      "Epoch: 88/100 | step: 132/422 | loss: 0.05219525098800659\n",
      "Epoch: 88/100 | step: 133/422 | loss: 0.007528835907578468\n",
      "Epoch: 88/100 | step: 134/422 | loss: 0.02700737677514553\n",
      "Epoch: 88/100 | step: 135/422 | loss: 0.017741769552230835\n",
      "Epoch: 88/100 | step: 136/422 | loss: 0.02633778750896454\n",
      "Epoch: 88/100 | step: 137/422 | loss: 0.010208492167294025\n",
      "Epoch: 88/100 | step: 138/422 | loss: 0.012200777418911457\n",
      "Epoch: 88/100 | step: 139/422 | loss: 0.008248576894402504\n",
      "Epoch: 88/100 | step: 140/422 | loss: 0.014453415758907795\n",
      "Epoch: 88/100 | step: 141/422 | loss: 0.020973579958081245\n",
      "Epoch: 88/100 | step: 142/422 | loss: 0.012183153070509434\n",
      "Epoch: 88/100 | step: 143/422 | loss: 0.0118049131706357\n",
      "Epoch: 88/100 | step: 144/422 | loss: 0.02428271621465683\n",
      "Epoch: 88/100 | step: 145/422 | loss: 0.011497105471789837\n",
      "Epoch: 88/100 | step: 146/422 | loss: 0.008933324366807938\n",
      "Epoch: 88/100 | step: 147/422 | loss: 0.015613550320267677\n",
      "Epoch: 88/100 | step: 148/422 | loss: 0.009841674007475376\n",
      "Epoch: 88/100 | step: 149/422 | loss: 0.010437156073749065\n",
      "Epoch: 88/100 | step: 150/422 | loss: 0.016652490943670273\n",
      "Epoch: 88/100 | step: 151/422 | loss: 0.0220455601811409\n",
      "Epoch: 88/100 | step: 152/422 | loss: 0.02472110278904438\n",
      "Epoch: 88/100 | step: 153/422 | loss: 0.020643265917897224\n",
      "Epoch: 88/100 | step: 154/422 | loss: 0.03335237875580788\n",
      "Epoch: 88/100 | step: 155/422 | loss: 0.008884494192898273\n",
      "Epoch: 88/100 | step: 156/422 | loss: 0.02318946272134781\n",
      "Epoch: 88/100 | step: 157/422 | loss: 0.03567131608724594\n",
      "Epoch: 88/100 | step: 158/422 | loss: 0.010671313852071762\n",
      "Epoch: 88/100 | step: 159/422 | loss: 0.012408573180437088\n",
      "Epoch: 88/100 | step: 160/422 | loss: 0.011636120267212391\n",
      "Epoch: 88/100 | step: 161/422 | loss: 0.01729675382375717\n",
      "Epoch: 88/100 | step: 162/422 | loss: 0.05505313724279404\n",
      "Epoch: 88/100 | step: 163/422 | loss: 0.023620961233973503\n",
      "Epoch: 88/100 | step: 164/422 | loss: 0.03463791310787201\n",
      "Epoch: 88/100 | step: 165/422 | loss: 0.015523411333560944\n",
      "Epoch: 88/100 | step: 166/422 | loss: 0.018005957826972008\n",
      "Epoch: 88/100 | step: 167/422 | loss: 0.009944694116711617\n",
      "Epoch: 88/100 | step: 168/422 | loss: 0.015430890023708344\n",
      "Epoch: 88/100 | step: 169/422 | loss: 0.012140888720750809\n",
      "Epoch: 88/100 | step: 170/422 | loss: 0.00902143307030201\n",
      "Epoch: 88/100 | step: 171/422 | loss: 0.010768141597509384\n",
      "Epoch: 88/100 | step: 172/422 | loss: 0.009427914395928383\n",
      "Epoch: 88/100 | step: 173/422 | loss: 0.021782826632261276\n",
      "Epoch: 88/100 | step: 174/422 | loss: 0.015301976352930069\n",
      "Epoch: 88/100 | step: 175/422 | loss: 0.02214968577027321\n",
      "Epoch: 88/100 | step: 176/422 | loss: 0.010385801084339619\n",
      "Epoch: 88/100 | step: 177/422 | loss: 0.0065916795283555984\n",
      "Epoch: 88/100 | step: 178/422 | loss: 0.018636412918567657\n",
      "Epoch: 88/100 | step: 179/422 | loss: 0.014623107388615608\n",
      "Epoch: 88/100 | step: 180/422 | loss: 0.020925231277942657\n",
      "Epoch: 88/100 | step: 181/422 | loss: 0.012904578819870949\n",
      "Epoch: 88/100 | step: 182/422 | loss: 0.01067292969673872\n",
      "Epoch: 88/100 | step: 183/422 | loss: 0.01005824375897646\n",
      "Epoch: 88/100 | step: 184/422 | loss: 0.017858875915408134\n",
      "Epoch: 88/100 | step: 185/422 | loss: 0.015997271984815598\n",
      "Epoch: 88/100 | step: 186/422 | loss: 0.010252187959849834\n",
      "Epoch: 88/100 | step: 187/422 | loss: 0.011704261414706707\n",
      "Epoch: 88/100 | step: 188/422 | loss: 0.007402854505926371\n",
      "Epoch: 88/100 | step: 189/422 | loss: 0.013414095155894756\n",
      "Epoch: 88/100 | step: 190/422 | loss: 0.01194683089852333\n",
      "Epoch: 88/100 | step: 191/422 | loss: 0.008318168111145496\n",
      "Epoch: 88/100 | step: 192/422 | loss: 0.01701488345861435\n",
      "Epoch: 88/100 | step: 193/422 | loss: 0.011687040328979492\n",
      "Epoch: 88/100 | step: 194/422 | loss: 0.01996651105582714\n",
      "Epoch: 88/100 | step: 195/422 | loss: 0.016020407900214195\n",
      "Epoch: 88/100 | step: 196/422 | loss: 0.01931389421224594\n",
      "Epoch: 88/100 | step: 197/422 | loss: 0.01490150485187769\n",
      "Epoch: 88/100 | step: 198/422 | loss: 0.01308208703994751\n",
      "Epoch: 88/100 | step: 199/422 | loss: 0.018381496891379356\n",
      "Epoch: 88/100 | step: 200/422 | loss: 0.016796614974737167\n",
      "Epoch: 88/100 | step: 201/422 | loss: 0.014431624673306942\n",
      "Epoch: 88/100 | step: 202/422 | loss: 0.015601866878569126\n",
      "Epoch: 88/100 | step: 203/422 | loss: 0.005328773055225611\n",
      "Epoch: 88/100 | step: 204/422 | loss: 0.029875149950385094\n",
      "Epoch: 88/100 | step: 205/422 | loss: 0.1601341962814331\n",
      "Epoch: 88/100 | step: 206/422 | loss: 0.020129792392253876\n",
      "Epoch: 88/100 | step: 207/422 | loss: 0.04232536256313324\n",
      "Epoch: 88/100 | step: 208/422 | loss: 0.010439426638185978\n",
      "Epoch: 88/100 | step: 209/422 | loss: 0.022040214389562607\n",
      "Epoch: 88/100 | step: 210/422 | loss: 0.07032693922519684\n",
      "Epoch: 88/100 | step: 211/422 | loss: 0.06953617185354233\n",
      "Epoch: 88/100 | step: 212/422 | loss: 0.011305093765258789\n",
      "Epoch: 88/100 | step: 213/422 | loss: 0.014455442316830158\n",
      "Epoch: 88/100 | step: 214/422 | loss: 0.052235931158065796\n",
      "Epoch: 88/100 | step: 215/422 | loss: 0.04153768718242645\n",
      "Epoch: 88/100 | step: 216/422 | loss: 0.013830996118485928\n",
      "Epoch: 88/100 | step: 217/422 | loss: 0.018584012985229492\n",
      "Epoch: 88/100 | step: 218/422 | loss: 0.019855676218867302\n",
      "Epoch: 88/100 | step: 219/422 | loss: 0.03071052022278309\n",
      "Epoch: 88/100 | step: 220/422 | loss: 0.031205084174871445\n",
      "Epoch: 88/100 | step: 221/422 | loss: 0.008124832063913345\n",
      "Epoch: 88/100 | step: 222/422 | loss: 0.014358093030750751\n",
      "Epoch: 88/100 | step: 223/422 | loss: 0.019227556884288788\n",
      "Epoch: 88/100 | step: 224/422 | loss: 0.013545159250497818\n",
      "Epoch: 88/100 | step: 225/422 | loss: 0.027717921882867813\n",
      "Epoch: 88/100 | step: 226/422 | loss: 0.044078946113586426\n",
      "Epoch: 88/100 | step: 227/422 | loss: 0.0078045157715678215\n",
      "Epoch: 88/100 | step: 228/422 | loss: 0.012117410078644753\n",
      "Epoch: 88/100 | step: 229/422 | loss: 0.022262588143348694\n",
      "Epoch: 88/100 | step: 230/422 | loss: 0.026785533875226974\n",
      "Epoch: 88/100 | step: 231/422 | loss: 0.02163504995405674\n",
      "Epoch: 88/100 | step: 232/422 | loss: 0.011987925507128239\n",
      "Epoch: 88/100 | step: 233/422 | loss: 0.01125151664018631\n",
      "Epoch: 88/100 | step: 234/422 | loss: 0.013077239505946636\n",
      "Epoch: 88/100 | step: 235/422 | loss: 0.011389528401196003\n",
      "Epoch: 88/100 | step: 236/422 | loss: 0.008746436797082424\n",
      "Epoch: 88/100 | step: 237/422 | loss: 0.005812013056129217\n",
      "Epoch: 88/100 | step: 238/422 | loss: 0.009532928466796875\n",
      "Epoch: 88/100 | step: 239/422 | loss: 0.007587316911667585\n",
      "Epoch: 88/100 | step: 240/422 | loss: 0.012301132082939148\n",
      "Epoch: 88/100 | step: 241/422 | loss: 0.014462927356362343\n",
      "Epoch: 88/100 | step: 242/422 | loss: 0.012394973076879978\n",
      "Epoch: 88/100 | step: 243/422 | loss: 0.008390843868255615\n",
      "Epoch: 88/100 | step: 244/422 | loss: 0.011752561666071415\n",
      "Epoch: 88/100 | step: 245/422 | loss: 0.00925745815038681\n",
      "Epoch: 88/100 | step: 246/422 | loss: 0.006082717329263687\n",
      "Epoch: 88/100 | step: 247/422 | loss: 0.009396186098456383\n",
      "Epoch: 88/100 | step: 248/422 | loss: 0.09983791410923004\n",
      "Epoch: 88/100 | step: 249/422 | loss: 0.027298875153064728\n",
      "Epoch: 88/100 | step: 250/422 | loss: 0.025664014741778374\n",
      "Epoch: 88/100 | step: 251/422 | loss: 0.04050718992948532\n",
      "Epoch: 88/100 | step: 252/422 | loss: 0.011180249974131584\n",
      "Epoch: 88/100 | step: 253/422 | loss: 0.008474411442875862\n",
      "Epoch: 88/100 | step: 254/422 | loss: 0.0363391637802124\n",
      "Epoch: 88/100 | step: 255/422 | loss: 0.010880722664296627\n",
      "Epoch: 88/100 | step: 256/422 | loss: 0.014658575877547264\n",
      "Epoch: 88/100 | step: 257/422 | loss: 0.008541329763829708\n",
      "Epoch: 88/100 | step: 258/422 | loss: 0.0428185760974884\n",
      "Epoch: 88/100 | step: 259/422 | loss: 0.01098529901355505\n",
      "Epoch: 88/100 | step: 260/422 | loss: 0.02976495958864689\n",
      "Epoch: 88/100 | step: 261/422 | loss: 0.01728725992143154\n",
      "Epoch: 88/100 | step: 262/422 | loss: 0.008049809373915195\n",
      "Epoch: 88/100 | step: 263/422 | loss: 0.014433725737035275\n",
      "Epoch: 88/100 | step: 264/422 | loss: 0.01323157362639904\n",
      "Epoch: 88/100 | step: 265/422 | loss: 0.01189832016825676\n",
      "Epoch: 88/100 | step: 266/422 | loss: 0.010493309237062931\n",
      "Epoch: 88/100 | step: 267/422 | loss: 0.011461358517408371\n",
      "Epoch: 88/100 | step: 268/422 | loss: 0.024675538763403893\n",
      "Epoch: 88/100 | step: 269/422 | loss: 0.007968202233314514\n",
      "Epoch: 88/100 | step: 270/422 | loss: 0.013752314262092113\n",
      "Epoch: 88/100 | step: 271/422 | loss: 0.01433870755136013\n",
      "Epoch: 88/100 | step: 272/422 | loss: 0.00976170226931572\n",
      "Epoch: 88/100 | step: 273/422 | loss: 0.00788300484418869\n",
      "Epoch: 88/100 | step: 274/422 | loss: 0.013892135582864285\n",
      "Epoch: 88/100 | step: 275/422 | loss: 0.008644557558000088\n",
      "Epoch: 88/100 | step: 276/422 | loss: 0.015289366245269775\n",
      "Epoch: 88/100 | step: 277/422 | loss: 0.013743260875344276\n",
      "Epoch: 88/100 | step: 278/422 | loss: 0.025801029056310654\n",
      "Epoch: 88/100 | step: 279/422 | loss: 0.012560555711388588\n",
      "Epoch: 88/100 | step: 280/422 | loss: 0.009058688767254353\n",
      "Epoch: 88/100 | step: 281/422 | loss: 0.011238441802561283\n",
      "Epoch: 88/100 | step: 282/422 | loss: 0.01948825642466545\n",
      "Epoch: 88/100 | step: 283/422 | loss: 0.010987093672156334\n",
      "Epoch: 88/100 | step: 284/422 | loss: 0.04517371207475662\n",
      "Epoch: 88/100 | step: 285/422 | loss: 0.013756980188190937\n",
      "Epoch: 88/100 | step: 286/422 | loss: 0.03332003206014633\n",
      "Epoch: 88/100 | step: 287/422 | loss: 0.03168120235204697\n",
      "Epoch: 88/100 | step: 288/422 | loss: 0.014905027113854885\n",
      "Epoch: 88/100 | step: 289/422 | loss: 0.03635663539171219\n",
      "Epoch: 88/100 | step: 290/422 | loss: 0.10118311643600464\n",
      "Epoch: 88/100 | step: 291/422 | loss: 0.009375385008752346\n",
      "Epoch: 88/100 | step: 292/422 | loss: 0.06426979601383209\n",
      "Epoch: 88/100 | step: 293/422 | loss: 0.01435154490172863\n",
      "Epoch: 88/100 | step: 294/422 | loss: 0.016138577833771706\n",
      "Epoch: 88/100 | step: 295/422 | loss: 0.12404822558164597\n",
      "Epoch: 88/100 | step: 296/422 | loss: 0.007489086594432592\n",
      "Epoch: 88/100 | step: 297/422 | loss: 0.01671108976006508\n",
      "Epoch: 88/100 | step: 298/422 | loss: 0.0226561538875103\n",
      "Epoch: 88/100 | step: 299/422 | loss: 0.030846625566482544\n",
      "Epoch: 88/100 | step: 300/422 | loss: 0.03974490985274315\n",
      "Epoch: 88/100 | step: 301/422 | loss: 0.02553323283791542\n",
      "Epoch: 88/100 | step: 302/422 | loss: 0.017287911847233772\n",
      "Epoch: 88/100 | step: 303/422 | loss: 0.01029236614704132\n",
      "Epoch: 88/100 | step: 304/422 | loss: 0.029454579576849937\n",
      "Epoch: 88/100 | step: 305/422 | loss: 0.015136070549488068\n",
      "Epoch: 88/100 | step: 306/422 | loss: 0.014305906370282173\n",
      "Epoch: 88/100 | step: 307/422 | loss: 0.013874543830752373\n",
      "Epoch: 88/100 | step: 308/422 | loss: 0.10706975311040878\n",
      "Epoch: 88/100 | step: 309/422 | loss: 0.015505224466323853\n",
      "Epoch: 88/100 | step: 310/422 | loss: 0.013478277251124382\n",
      "Epoch: 88/100 | step: 311/422 | loss: 0.03881671652197838\n",
      "Epoch: 88/100 | step: 312/422 | loss: 0.02389102429151535\n",
      "Epoch: 88/100 | step: 313/422 | loss: 0.011256829835474491\n",
      "Epoch: 88/100 | step: 314/422 | loss: 0.009047985076904297\n",
      "Epoch: 88/100 | step: 315/422 | loss: 0.04627515748143196\n",
      "Epoch: 88/100 | step: 316/422 | loss: 0.03645915910601616\n",
      "Epoch: 88/100 | step: 317/422 | loss: 0.012121575884521008\n",
      "Epoch: 88/100 | step: 318/422 | loss: 0.011931962333619595\n",
      "Epoch: 88/100 | step: 319/422 | loss: 0.017424823716282845\n",
      "Epoch: 88/100 | step: 320/422 | loss: 0.011540156789124012\n",
      "Epoch: 88/100 | step: 321/422 | loss: 0.14544633030891418\n",
      "Epoch: 88/100 | step: 322/422 | loss: 0.021393340080976486\n",
      "Epoch: 88/100 | step: 323/422 | loss: 0.01707494631409645\n",
      "Epoch: 88/100 | step: 324/422 | loss: 0.011907584965229034\n",
      "Epoch: 88/100 | step: 325/422 | loss: 0.01635516621172428\n",
      "Epoch: 88/100 | step: 326/422 | loss: 0.012152825482189655\n",
      "Epoch: 88/100 | step: 327/422 | loss: 0.013195466250181198\n",
      "Epoch: 88/100 | step: 328/422 | loss: 0.01228285487741232\n",
      "Epoch: 88/100 | step: 329/422 | loss: 0.012139568105340004\n",
      "Epoch: 88/100 | step: 330/422 | loss: 0.011947219260036945\n",
      "Epoch: 88/100 | step: 331/422 | loss: 0.011601218953728676\n",
      "Epoch: 88/100 | step: 332/422 | loss: 0.013913213275372982\n",
      "Epoch: 88/100 | step: 333/422 | loss: 0.008700443431735039\n",
      "Epoch: 88/100 | step: 334/422 | loss: 0.01942218281328678\n",
      "Epoch: 88/100 | step: 335/422 | loss: 0.03138399124145508\n",
      "Epoch: 88/100 | step: 336/422 | loss: 0.023169470950961113\n",
      "Epoch: 88/100 | step: 337/422 | loss: 0.01593458093702793\n",
      "Epoch: 88/100 | step: 338/422 | loss: 0.027141980826854706\n",
      "Epoch: 88/100 | step: 339/422 | loss: 0.01229928806424141\n",
      "Epoch: 88/100 | step: 340/422 | loss: 0.011556895449757576\n",
      "Epoch: 88/100 | step: 341/422 | loss: 0.013275853358209133\n",
      "Epoch: 88/100 | step: 342/422 | loss: 0.03741471841931343\n",
      "Epoch: 88/100 | step: 343/422 | loss: 0.04391787573695183\n",
      "Epoch: 88/100 | step: 344/422 | loss: 0.0730971172451973\n",
      "Epoch: 88/100 | step: 345/422 | loss: 0.02049189805984497\n",
      "Epoch: 88/100 | step: 346/422 | loss: 0.06760680675506592\n",
      "Epoch: 88/100 | step: 347/422 | loss: 0.024047447368502617\n",
      "Epoch: 88/100 | step: 348/422 | loss: 0.02488071471452713\n",
      "Epoch: 88/100 | step: 349/422 | loss: 0.02600262686610222\n",
      "Epoch: 88/100 | step: 350/422 | loss: 0.010288874618709087\n",
      "Epoch: 88/100 | step: 351/422 | loss: 0.05745602026581764\n",
      "Epoch: 88/100 | step: 352/422 | loss: 0.12643320858478546\n",
      "Epoch: 88/100 | step: 353/422 | loss: 0.03400648385286331\n",
      "Epoch: 88/100 | step: 354/422 | loss: 0.011351997032761574\n",
      "Epoch: 88/100 | step: 355/422 | loss: 0.029444023966789246\n",
      "Epoch: 88/100 | step: 356/422 | loss: 0.019925666972994804\n",
      "Epoch: 88/100 | step: 357/422 | loss: 0.012461874634027481\n",
      "Epoch: 88/100 | step: 358/422 | loss: 0.008495043031871319\n",
      "Epoch: 88/100 | step: 359/422 | loss: 0.08731625974178314\n",
      "Epoch: 88/100 | step: 360/422 | loss: 0.037636592984199524\n",
      "Epoch: 88/100 | step: 361/422 | loss: 0.01217110175639391\n",
      "Epoch: 88/100 | step: 362/422 | loss: 0.01934110000729561\n",
      "Epoch: 88/100 | step: 363/422 | loss: 0.010521391406655312\n",
      "Epoch: 88/100 | step: 364/422 | loss: 0.014091890305280685\n",
      "Epoch: 88/100 | step: 365/422 | loss: 0.06844586133956909\n",
      "Epoch: 88/100 | step: 366/422 | loss: 0.02430301532149315\n",
      "Epoch: 88/100 | step: 367/422 | loss: 0.03647281602025032\n",
      "Epoch: 88/100 | step: 368/422 | loss: 0.023444298654794693\n",
      "Epoch: 88/100 | step: 369/422 | loss: 0.0178692527115345\n",
      "Epoch: 88/100 | step: 370/422 | loss: 0.035012200474739075\n",
      "Epoch: 88/100 | step: 371/422 | loss: 0.033765971660614014\n",
      "Epoch: 88/100 | step: 372/422 | loss: 0.006895664148032665\n",
      "Epoch: 88/100 | step: 373/422 | loss: 0.03368401154875755\n",
      "Epoch: 88/100 | step: 374/422 | loss: 0.007935753092169762\n",
      "Epoch: 88/100 | step: 375/422 | loss: 0.014039968140423298\n",
      "Epoch: 88/100 | step: 376/422 | loss: 0.010900643654167652\n",
      "Epoch: 88/100 | step: 377/422 | loss: 0.019433991983532906\n",
      "Epoch: 88/100 | step: 378/422 | loss: 0.011178692802786827\n",
      "Epoch: 88/100 | step: 379/422 | loss: 0.010486708022654057\n",
      "Epoch: 88/100 | step: 380/422 | loss: 0.006404655519872904\n",
      "Epoch: 88/100 | step: 381/422 | loss: 0.0100588733330369\n",
      "Epoch: 88/100 | step: 382/422 | loss: 0.012819190509617329\n",
      "Epoch: 88/100 | step: 383/422 | loss: 0.01710715889930725\n",
      "Epoch: 88/100 | step: 384/422 | loss: 0.009587770327925682\n",
      "Epoch: 88/100 | step: 385/422 | loss: 0.008241923525929451\n",
      "Epoch: 88/100 | step: 386/422 | loss: 0.014451800845563412\n",
      "Epoch: 88/100 | step: 387/422 | loss: 0.016569657251238823\n",
      "Epoch: 88/100 | step: 388/422 | loss: 0.02001531422138214\n",
      "Epoch: 88/100 | step: 389/422 | loss: 0.015315799973905087\n",
      "Epoch: 88/100 | step: 390/422 | loss: 0.010304276831448078\n",
      "Epoch: 88/100 | step: 391/422 | loss: 0.013636079616844654\n",
      "Epoch: 88/100 | step: 392/422 | loss: 0.018545206636190414\n",
      "Epoch: 88/100 | step: 393/422 | loss: 0.012194924056529999\n",
      "Epoch: 88/100 | step: 394/422 | loss: 0.011845079250633717\n",
      "Epoch: 88/100 | step: 395/422 | loss: 0.01570647396147251\n",
      "Epoch: 88/100 | step: 396/422 | loss: 0.014070125296711922\n",
      "Epoch: 88/100 | step: 397/422 | loss: 0.00819256342947483\n",
      "Epoch: 88/100 | step: 398/422 | loss: 0.009909807704389095\n",
      "Epoch: 88/100 | step: 399/422 | loss: 0.012135624885559082\n",
      "Epoch: 88/100 | step: 400/422 | loss: 0.01098367478698492\n",
      "Epoch: 88/100 | step: 401/422 | loss: 0.013262275606393814\n",
      "Epoch: 88/100 | step: 402/422 | loss: 0.010431659407913685\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 89/100 | step: 1/422 | loss: 0.007651044055819511\n",
      "Epoch: 89/100 | step: 2/422 | loss: 0.00783504731953144\n",
      "Epoch: 89/100 | step: 3/422 | loss: 0.006914983037859201\n",
      "Epoch: 89/100 | step: 4/422 | loss: 0.005559199023991823\n",
      "Epoch: 89/100 | step: 5/422 | loss: 0.0058311279863119125\n",
      "Epoch: 89/100 | step: 6/422 | loss: 0.00962906051427126\n",
      "Epoch: 89/100 | step: 7/422 | loss: 0.009338369593024254\n",
      "Epoch: 89/100 | step: 8/422 | loss: 0.010214152745902538\n",
      "Epoch: 89/100 | step: 9/422 | loss: 0.010719366371631622\n",
      "Epoch: 89/100 | step: 10/422 | loss: 0.006505279336124659\n",
      "Epoch: 89/100 | step: 11/422 | loss: 0.011805763468146324\n",
      "Epoch: 89/100 | step: 12/422 | loss: 0.007465129252523184\n",
      "Epoch: 89/100 | step: 13/422 | loss: 0.011116264387965202\n",
      "Epoch: 89/100 | step: 14/422 | loss: 0.008479331620037556\n",
      "Epoch: 89/100 | step: 15/422 | loss: 0.006934098433703184\n",
      "Epoch: 89/100 | step: 16/422 | loss: 0.009792180731892586\n",
      "Epoch: 89/100 | step: 17/422 | loss: 0.00804910995066166\n",
      "Epoch: 89/100 | step: 18/422 | loss: 0.009246131405234337\n",
      "Epoch: 89/100 | step: 19/422 | loss: 0.015006378293037415\n",
      "Epoch: 89/100 | step: 20/422 | loss: 0.015177246183156967\n",
      "Epoch: 89/100 | step: 21/422 | loss: 0.005151216872036457\n",
      "Epoch: 89/100 | step: 22/422 | loss: 0.00679729413241148\n",
      "Epoch: 89/100 | step: 23/422 | loss: 0.007630628068000078\n",
      "Epoch: 89/100 | step: 24/422 | loss: 0.016100140288472176\n",
      "Epoch: 89/100 | step: 25/422 | loss: 0.006152736023068428\n",
      "Epoch: 89/100 | step: 26/422 | loss: 0.00767617067322135\n",
      "Epoch: 89/100 | step: 27/422 | loss: 0.0078795300796628\n",
      "Epoch: 89/100 | step: 28/422 | loss: 0.012887662276625633\n",
      "Epoch: 89/100 | step: 29/422 | loss: 0.007116388063877821\n",
      "Epoch: 89/100 | step: 30/422 | loss: 0.006403645034879446\n",
      "Epoch: 89/100 | step: 31/422 | loss: 0.008216727524995804\n",
      "Epoch: 89/100 | step: 32/422 | loss: 0.011451934464275837\n",
      "Epoch: 89/100 | step: 33/422 | loss: 0.010783679783344269\n",
      "Epoch: 89/100 | step: 34/422 | loss: 0.009599351324141026\n",
      "Epoch: 89/100 | step: 35/422 | loss: 0.010458225384354591\n",
      "Epoch: 89/100 | step: 36/422 | loss: 0.0050604818388819695\n",
      "Epoch: 89/100 | step: 37/422 | loss: 0.005946016870439053\n",
      "Epoch: 89/100 | step: 38/422 | loss: 0.008647910319268703\n",
      "Epoch: 89/100 | step: 39/422 | loss: 0.008819697424769402\n",
      "Epoch: 89/100 | step: 40/422 | loss: 0.012235254980623722\n",
      "Epoch: 89/100 | step: 41/422 | loss: 0.00893597211688757\n",
      "Epoch: 89/100 | step: 42/422 | loss: 0.008099942468106747\n",
      "Epoch: 89/100 | step: 43/422 | loss: 0.015863336622714996\n",
      "Epoch: 89/100 | step: 44/422 | loss: 0.0091565465554595\n",
      "Epoch: 89/100 | step: 45/422 | loss: 0.012132111005485058\n",
      "Epoch: 89/100 | step: 46/422 | loss: 0.020253833383321762\n",
      "Epoch: 89/100 | step: 47/422 | loss: 0.010673933662474155\n",
      "Epoch: 89/100 | step: 48/422 | loss: 0.010549130849540234\n",
      "Epoch: 89/100 | step: 49/422 | loss: 0.014801504090428352\n",
      "Epoch: 89/100 | step: 50/422 | loss: 0.012766917236149311\n",
      "Epoch: 89/100 | step: 51/422 | loss: 0.017847271636128426\n",
      "Epoch: 89/100 | step: 52/422 | loss: 0.01625722460448742\n",
      "Epoch: 89/100 | step: 53/422 | loss: 0.008026853203773499\n",
      "Epoch: 89/100 | step: 54/422 | loss: 0.008537155576050282\n",
      "Epoch: 89/100 | step: 55/422 | loss: 0.015490233898162842\n",
      "Epoch: 89/100 | step: 56/422 | loss: 0.01582297310233116\n",
      "Epoch: 89/100 | step: 57/422 | loss: 0.00599156878888607\n",
      "Epoch: 89/100 | step: 58/422 | loss: 0.005186809692531824\n",
      "Epoch: 89/100 | step: 59/422 | loss: 0.016342004761099815\n",
      "Epoch: 89/100 | step: 60/422 | loss: 0.006783343385905027\n",
      "Epoch: 89/100 | step: 61/422 | loss: 0.0076220971532166\n",
      "Epoch: 89/100 | step: 62/422 | loss: 0.011319448240101337\n",
      "Epoch: 89/100 | step: 63/422 | loss: 0.007512382231652737\n",
      "Epoch: 89/100 | step: 64/422 | loss: 0.010590419173240662\n",
      "Epoch: 89/100 | step: 65/422 | loss: 0.005537805147469044\n",
      "Epoch: 89/100 | step: 66/422 | loss: 0.007683177012950182\n",
      "Epoch: 89/100 | step: 67/422 | loss: 0.009669998660683632\n",
      "Epoch: 89/100 | step: 68/422 | loss: 0.006998861208558083\n",
      "Epoch: 89/100 | step: 69/422 | loss: 0.006255311891436577\n",
      "Epoch: 89/100 | step: 70/422 | loss: 0.009856765158474445\n",
      "Epoch: 89/100 | step: 71/422 | loss: 0.006445888429880142\n",
      "Epoch: 89/100 | step: 72/422 | loss: 0.01429281197488308\n",
      "Epoch: 89/100 | step: 73/422 | loss: 0.007874412462115288\n",
      "Epoch: 89/100 | step: 74/422 | loss: 0.00844279769808054\n",
      "Epoch: 89/100 | step: 75/422 | loss: 0.010748210363090038\n",
      "Epoch: 89/100 | step: 76/422 | loss: 0.008718074299395084\n",
      "Epoch: 89/100 | step: 77/422 | loss: 0.00679378118366003\n",
      "Epoch: 89/100 | step: 78/422 | loss: 0.01162227988243103\n",
      "Epoch: 89/100 | step: 79/422 | loss: 0.007833432406187057\n",
      "Epoch: 89/100 | step: 80/422 | loss: 0.005848300643265247\n",
      "Epoch: 89/100 | step: 81/422 | loss: 0.011663158424198627\n",
      "Epoch: 89/100 | step: 82/422 | loss: 0.012983519583940506\n",
      "Epoch: 89/100 | step: 83/422 | loss: 0.00793566182255745\n",
      "Epoch: 89/100 | step: 84/422 | loss: 0.008063146844506264\n",
      "Epoch: 89/100 | step: 85/422 | loss: 0.009067765437066555\n",
      "Epoch: 89/100 | step: 86/422 | loss: 0.020434455946087837\n",
      "Epoch: 89/100 | step: 87/422 | loss: 0.014475610107183456\n",
      "Epoch: 89/100 | step: 88/422 | loss: 0.009128416888415813\n",
      "Epoch: 89/100 | step: 89/422 | loss: 0.005766693037003279\n",
      "Epoch: 89/100 | step: 90/422 | loss: 0.007782935630530119\n",
      "Epoch: 89/100 | step: 91/422 | loss: 0.008388152346014977\n",
      "Epoch: 89/100 | step: 92/422 | loss: 0.0139306066557765\n",
      "Epoch: 89/100 | step: 93/422 | loss: 0.009401578456163406\n",
      "Epoch: 89/100 | step: 94/422 | loss: 0.00833790935575962\n",
      "Epoch: 89/100 | step: 95/422 | loss: 0.007362182717770338\n",
      "Epoch: 89/100 | step: 96/422 | loss: 0.013775632716715336\n",
      "Epoch: 89/100 | step: 97/422 | loss: 0.007314098533242941\n",
      "Epoch: 89/100 | step: 98/422 | loss: 0.013158345595002174\n",
      "Epoch: 89/100 | step: 99/422 | loss: 0.015069561079144478\n",
      "Epoch: 89/100 | step: 100/422 | loss: 0.01083516888320446\n",
      "Epoch: 89/100 | step: 101/422 | loss: 0.007643998600542545\n",
      "Epoch: 89/100 | step: 102/422 | loss: 0.008162508718669415\n",
      "Epoch: 89/100 | step: 103/422 | loss: 0.009695268236100674\n",
      "Epoch: 89/100 | step: 104/422 | loss: 0.0118337944149971\n",
      "Epoch: 89/100 | step: 105/422 | loss: 0.009099436923861504\n",
      "Epoch: 89/100 | step: 106/422 | loss: 0.029113372787833214\n",
      "Epoch: 89/100 | step: 107/422 | loss: 0.00797148235142231\n",
      "Epoch: 89/100 | step: 108/422 | loss: 0.011351259425282478\n",
      "Epoch: 89/100 | step: 109/422 | loss: 0.024462338536977768\n",
      "Epoch: 89/100 | step: 110/422 | loss: 0.01661328785121441\n",
      "Epoch: 89/100 | step: 111/422 | loss: 0.007109630387276411\n",
      "Epoch: 89/100 | step: 112/422 | loss: 0.007081381976604462\n",
      "Epoch: 89/100 | step: 113/422 | loss: 0.012635589577257633\n",
      "Epoch: 89/100 | step: 114/422 | loss: 0.009722797200083733\n",
      "Epoch: 89/100 | step: 115/422 | loss: 0.009208330884575844\n",
      "Epoch: 89/100 | step: 116/422 | loss: 0.009914171881973743\n",
      "Epoch: 89/100 | step: 117/422 | loss: 0.007602191064506769\n",
      "Epoch: 89/100 | step: 118/422 | loss: 0.008392333984375\n",
      "Epoch: 89/100 | step: 119/422 | loss: 0.0074883499182760715\n",
      "Epoch: 89/100 | step: 120/422 | loss: 0.00501204514876008\n",
      "Epoch: 89/100 | step: 121/422 | loss: 0.0061246189288794994\n",
      "Epoch: 89/100 | step: 122/422 | loss: 0.0074056098237633705\n",
      "Epoch: 89/100 | step: 123/422 | loss: 0.007366580422967672\n",
      "Epoch: 89/100 | step: 124/422 | loss: 0.0327274426817894\n",
      "Epoch: 89/100 | step: 125/422 | loss: 0.03170345351099968\n",
      "Epoch: 89/100 | step: 126/422 | loss: 0.00841447152197361\n",
      "Epoch: 89/100 | step: 127/422 | loss: 0.014729678630828857\n",
      "Epoch: 89/100 | step: 128/422 | loss: 0.008939801715314388\n",
      "Epoch: 89/100 | step: 129/422 | loss: 0.013554774224758148\n",
      "Epoch: 89/100 | step: 130/422 | loss: 0.008508558385074139\n",
      "Epoch: 89/100 | step: 131/422 | loss: 0.010534538887441158\n",
      "Epoch: 89/100 | step: 132/422 | loss: 0.008979089558124542\n",
      "Epoch: 89/100 | step: 133/422 | loss: 0.03619254380464554\n",
      "Epoch: 89/100 | step: 134/422 | loss: 0.01299058273434639\n",
      "Epoch: 89/100 | step: 135/422 | loss: 0.009562816470861435\n",
      "Epoch: 89/100 | step: 136/422 | loss: 0.015960823744535446\n",
      "Epoch: 89/100 | step: 137/422 | loss: 0.020291365683078766\n",
      "Epoch: 89/100 | step: 138/422 | loss: 0.009074456989765167\n",
      "Epoch: 89/100 | step: 139/422 | loss: 0.008322284556925297\n",
      "Epoch: 89/100 | step: 140/422 | loss: 0.01814885623753071\n",
      "Epoch: 89/100 | step: 141/422 | loss: 0.00839150045067072\n",
      "Epoch: 89/100 | step: 142/422 | loss: 0.009293410927057266\n",
      "Epoch: 89/100 | step: 143/422 | loss: 0.006588505115360022\n",
      "Epoch: 89/100 | step: 144/422 | loss: 0.01475617941468954\n",
      "Epoch: 89/100 | step: 145/422 | loss: 0.006933515425771475\n",
      "Epoch: 89/100 | step: 146/422 | loss: 0.014397777616977692\n",
      "Epoch: 89/100 | step: 147/422 | loss: 0.012898078188300133\n",
      "Epoch: 89/100 | step: 148/422 | loss: 0.003398204455152154\n",
      "Epoch: 89/100 | step: 149/422 | loss: 0.009143080562353134\n",
      "Epoch: 89/100 | step: 150/422 | loss: 0.00597363943234086\n",
      "Epoch: 89/100 | step: 151/422 | loss: 0.012484515085816383\n",
      "Epoch: 89/100 | step: 152/422 | loss: 0.008048911578953266\n",
      "Epoch: 89/100 | step: 153/422 | loss: 0.011615077964961529\n",
      "Epoch: 89/100 | step: 154/422 | loss: 0.00829507876187563\n",
      "Epoch: 89/100 | step: 155/422 | loss: 0.013013755902647972\n",
      "Epoch: 89/100 | step: 156/422 | loss: 0.06285630166530609\n",
      "Epoch: 89/100 | step: 157/422 | loss: 0.08859631419181824\n",
      "Epoch: 89/100 | step: 158/422 | loss: 0.05996016785502434\n",
      "Epoch: 89/100 | step: 159/422 | loss: 0.0084789814427495\n",
      "Epoch: 89/100 | step: 160/422 | loss: 0.009022753685712814\n",
      "Epoch: 89/100 | step: 161/422 | loss: 0.01631951332092285\n",
      "Epoch: 89/100 | step: 162/422 | loss: 0.008526274934411049\n",
      "Epoch: 89/100 | step: 163/422 | loss: 0.008317407220602036\n",
      "Epoch: 89/100 | step: 164/422 | loss: 0.11927006393671036\n",
      "Epoch: 89/100 | step: 165/422 | loss: 0.04200664162635803\n",
      "Epoch: 89/100 | step: 166/422 | loss: 0.011815106496214867\n",
      "Epoch: 89/100 | step: 167/422 | loss: 0.016269009560346603\n",
      "Epoch: 89/100 | step: 168/422 | loss: 0.02273428812623024\n",
      "Epoch: 89/100 | step: 169/422 | loss: 0.009063832461833954\n",
      "Epoch: 89/100 | step: 170/422 | loss: 0.010544794611632824\n",
      "Epoch: 89/100 | step: 171/422 | loss: 0.014110282063484192\n",
      "Epoch: 89/100 | step: 172/422 | loss: 0.009067348204553127\n",
      "Epoch: 89/100 | step: 173/422 | loss: 0.008211195468902588\n",
      "Epoch: 89/100 | step: 174/422 | loss: 0.008946137502789497\n",
      "Epoch: 89/100 | step: 175/422 | loss: 0.01727530173957348\n",
      "Epoch: 89/100 | step: 176/422 | loss: 0.01618489995598793\n",
      "Epoch: 89/100 | step: 177/422 | loss: 0.010706196539103985\n",
      "Epoch: 89/100 | step: 178/422 | loss: 0.00884171575307846\n",
      "Epoch: 89/100 | step: 179/422 | loss: 0.03345407545566559\n",
      "Epoch: 89/100 | step: 180/422 | loss: 0.012211969122290611\n",
      "Epoch: 89/100 | step: 181/422 | loss: 0.010846525430679321\n",
      "Epoch: 89/100 | step: 182/422 | loss: 0.019557775929570198\n",
      "Epoch: 89/100 | step: 183/422 | loss: 0.015524311922490597\n",
      "Epoch: 89/100 | step: 184/422 | loss: 0.013923409394919872\n",
      "Epoch: 89/100 | step: 185/422 | loss: 0.00788600742816925\n",
      "Epoch: 89/100 | step: 186/422 | loss: 0.0095417071133852\n",
      "Epoch: 89/100 | step: 187/422 | loss: 0.011925842612981796\n",
      "Epoch: 89/100 | step: 188/422 | loss: 0.010678594000637531\n",
      "Epoch: 89/100 | step: 189/422 | loss: 0.010173667222261429\n",
      "Epoch: 89/100 | step: 190/422 | loss: 0.006916650105267763\n",
      "Epoch: 89/100 | step: 191/422 | loss: 0.01341976411640644\n",
      "Epoch: 89/100 | step: 192/422 | loss: 0.007820168510079384\n",
      "Epoch: 89/100 | step: 193/422 | loss: 0.011978963389992714\n",
      "Epoch: 89/100 | step: 194/422 | loss: 0.010595403611660004\n",
      "Epoch: 89/100 | step: 195/422 | loss: 0.00654086098074913\n",
      "Epoch: 89/100 | step: 196/422 | loss: 0.010034019127488136\n",
      "Epoch: 89/100 | step: 197/422 | loss: 0.005219453014433384\n",
      "Epoch: 89/100 | step: 198/422 | loss: 0.010944857262074947\n",
      "Epoch: 89/100 | step: 199/422 | loss: 0.012144486419856548\n",
      "Epoch: 89/100 | step: 200/422 | loss: 0.007670356426388025\n",
      "Epoch: 89/100 | step: 201/422 | loss: 0.009755618870258331\n",
      "Epoch: 89/100 | step: 202/422 | loss: 0.010834405198693275\n",
      "Epoch: 89/100 | step: 203/422 | loss: 0.005979480221867561\n",
      "Epoch: 89/100 | step: 204/422 | loss: 0.010100097395479679\n",
      "Epoch: 89/100 | step: 205/422 | loss: 0.007232243660837412\n",
      "Epoch: 89/100 | step: 206/422 | loss: 0.015345095656812191\n",
      "Epoch: 89/100 | step: 207/422 | loss: 0.00794469378888607\n",
      "Epoch: 89/100 | step: 208/422 | loss: 0.01080127153545618\n",
      "Epoch: 89/100 | step: 209/422 | loss: 0.019258780404925346\n",
      "Epoch: 89/100 | step: 210/422 | loss: 0.01903383620083332\n",
      "Epoch: 89/100 | step: 211/422 | loss: 0.013714087195694447\n",
      "Epoch: 89/100 | step: 212/422 | loss: 0.016097573563456535\n",
      "Epoch: 89/100 | step: 213/422 | loss: 0.010058393701910973\n",
      "Epoch: 89/100 | step: 214/422 | loss: 0.007775567937642336\n",
      "Epoch: 89/100 | step: 215/422 | loss: 0.008853618986904621\n",
      "Epoch: 89/100 | step: 216/422 | loss: 0.013184207491576672\n",
      "Epoch: 89/100 | step: 217/422 | loss: 0.008390218950808048\n",
      "Epoch: 89/100 | step: 218/422 | loss: 0.007811012677848339\n",
      "Epoch: 89/100 | step: 219/422 | loss: 0.017849601805210114\n",
      "Epoch: 89/100 | step: 220/422 | loss: 0.010134287178516388\n",
      "Epoch: 89/100 | step: 221/422 | loss: 0.009348668158054352\n",
      "Epoch: 89/100 | step: 222/422 | loss: 0.009197495877742767\n",
      "Epoch: 89/100 | step: 223/422 | loss: 0.008100798353552818\n",
      "Epoch: 89/100 | step: 224/422 | loss: 0.01026029884815216\n",
      "Epoch: 89/100 | step: 225/422 | loss: 0.007722330279648304\n",
      "Epoch: 89/100 | step: 226/422 | loss: 0.0061540305614471436\n",
      "Epoch: 89/100 | step: 227/422 | loss: 0.007453100755810738\n",
      "Epoch: 89/100 | step: 228/422 | loss: 0.012714897282421589\n",
      "Epoch: 89/100 | step: 229/422 | loss: 0.008690288290381432\n",
      "Epoch: 89/100 | step: 230/422 | loss: 0.009277179837226868\n",
      "Epoch: 89/100 | step: 231/422 | loss: 0.013376045972108841\n",
      "Epoch: 89/100 | step: 232/422 | loss: 0.010076972655951977\n",
      "Epoch: 89/100 | step: 233/422 | loss: 0.019193217158317566\n",
      "Epoch: 89/100 | step: 234/422 | loss: 0.011279906146228313\n",
      "Epoch: 89/100 | step: 235/422 | loss: 0.006797393783926964\n",
      "Epoch: 89/100 | step: 236/422 | loss: 0.007405343931168318\n",
      "Epoch: 89/100 | step: 237/422 | loss: 0.0075322845950722694\n",
      "Epoch: 89/100 | step: 238/422 | loss: 0.005610866472125053\n",
      "Epoch: 89/100 | step: 239/422 | loss: 0.005626371130347252\n",
      "Epoch: 89/100 | step: 240/422 | loss: 0.00967655424028635\n",
      "Epoch: 89/100 | step: 241/422 | loss: 0.00858735665678978\n",
      "Epoch: 89/100 | step: 242/422 | loss: 0.005426214542239904\n",
      "Epoch: 89/100 | step: 243/422 | loss: 0.00931552704423666\n",
      "Epoch: 89/100 | step: 244/422 | loss: 0.015126405283808708\n",
      "Epoch: 89/100 | step: 245/422 | loss: 0.01857541874051094\n",
      "Epoch: 89/100 | step: 246/422 | loss: 0.008567381650209427\n",
      "Epoch: 89/100 | step: 247/422 | loss: 0.007590440101921558\n",
      "Epoch: 89/100 | step: 248/422 | loss: 0.01060253381729126\n",
      "Epoch: 89/100 | step: 249/422 | loss: 0.010282658971846104\n",
      "Epoch: 89/100 | step: 250/422 | loss: 0.0063925073482096195\n",
      "Epoch: 89/100 | step: 251/422 | loss: 0.009817454032599926\n",
      "Epoch: 89/100 | step: 252/422 | loss: 0.010575965978205204\n",
      "Epoch: 89/100 | step: 253/422 | loss: 0.0061156558804214\n",
      "Epoch: 89/100 | step: 254/422 | loss: 0.006824775133281946\n",
      "Epoch: 89/100 | step: 255/422 | loss: 0.005931129213422537\n",
      "Epoch: 89/100 | step: 256/422 | loss: 0.009806118905544281\n",
      "Epoch: 89/100 | step: 257/422 | loss: 0.005543891340494156\n",
      "Epoch: 89/100 | step: 258/422 | loss: 0.016531087458133698\n",
      "Epoch: 89/100 | step: 259/422 | loss: 0.010194675996899605\n",
      "Epoch: 89/100 | step: 260/422 | loss: 0.0076250615529716015\n",
      "Epoch: 89/100 | step: 261/422 | loss: 0.007692885119467974\n",
      "Epoch: 89/100 | step: 262/422 | loss: 0.006606846582144499\n",
      "Epoch: 89/100 | step: 263/422 | loss: 0.009282340295612812\n",
      "Epoch: 89/100 | step: 264/422 | loss: 0.008201557211577892\n",
      "Epoch: 89/100 | step: 265/422 | loss: 0.0060012973845005035\n",
      "Epoch: 89/100 | step: 266/422 | loss: 0.011835125274956226\n",
      "Epoch: 89/100 | step: 267/422 | loss: 0.005023852456361055\n",
      "Epoch: 89/100 | step: 268/422 | loss: 0.013283362612128258\n",
      "Epoch: 89/100 | step: 269/422 | loss: 0.007009655702859163\n",
      "Epoch: 89/100 | step: 270/422 | loss: 0.01143903099000454\n",
      "Epoch: 89/100 | step: 271/422 | loss: 0.01079403143376112\n",
      "Epoch: 89/100 | step: 272/422 | loss: 0.004934431053698063\n",
      "Epoch: 89/100 | step: 273/422 | loss: 0.009369159117341042\n",
      "Epoch: 89/100 | step: 274/422 | loss: 0.016615256667137146\n",
      "Epoch: 89/100 | step: 275/422 | loss: 0.013043461367487907\n",
      "Epoch: 89/100 | step: 276/422 | loss: 0.009759853594005108\n",
      "Epoch: 89/100 | step: 277/422 | loss: 0.006639362778514624\n",
      "Epoch: 89/100 | step: 278/422 | loss: 0.009040707722306252\n",
      "Epoch: 89/100 | step: 279/422 | loss: 0.010752025991678238\n",
      "Epoch: 89/100 | step: 280/422 | loss: 0.007155342027544975\n",
      "Epoch: 89/100 | step: 281/422 | loss: 0.012468620203435421\n",
      "Epoch: 89/100 | step: 282/422 | loss: 0.006611737888306379\n",
      "Epoch: 89/100 | step: 283/422 | loss: 0.008367065340280533\n",
      "Epoch: 89/100 | step: 284/422 | loss: 0.012172085233032703\n",
      "Epoch: 89/100 | step: 285/422 | loss: 0.006978730671107769\n",
      "Epoch: 89/100 | step: 286/422 | loss: 0.011128799989819527\n",
      "Epoch: 89/100 | step: 287/422 | loss: 0.008064515888690948\n",
      "Epoch: 89/100 | step: 288/422 | loss: 0.009602979756891727\n",
      "Epoch: 89/100 | step: 289/422 | loss: 0.007914041168987751\n",
      "Epoch: 89/100 | step: 290/422 | loss: 0.016265496611595154\n",
      "Epoch: 89/100 | step: 291/422 | loss: 0.010695239529013634\n",
      "Epoch: 89/100 | step: 292/422 | loss: 0.007451858837157488\n",
      "Epoch: 89/100 | step: 293/422 | loss: 0.018502039834856987\n",
      "Epoch: 89/100 | step: 294/422 | loss: 0.010121769271790981\n",
      "Epoch: 89/100 | step: 295/422 | loss: 0.007756428327411413\n",
      "Epoch: 89/100 | step: 296/422 | loss: 0.023684777319431305\n",
      "Epoch: 89/100 | step: 297/422 | loss: 0.012521793134510517\n",
      "Epoch: 89/100 | step: 298/422 | loss: 0.008670119568705559\n",
      "Epoch: 89/100 | step: 299/422 | loss: 0.004813063889741898\n",
      "Epoch: 89/100 | step: 300/422 | loss: 0.015516544692218304\n",
      "Epoch: 89/100 | step: 301/422 | loss: 0.009241428226232529\n",
      "Epoch: 89/100 | step: 302/422 | loss: 0.0182806346565485\n",
      "Epoch: 89/100 | step: 303/422 | loss: 0.03266863524913788\n",
      "Epoch: 89/100 | step: 304/422 | loss: 0.005024717189371586\n",
      "Epoch: 89/100 | step: 305/422 | loss: 0.014352290891110897\n",
      "Epoch: 89/100 | step: 306/422 | loss: 0.021459046751260757\n",
      "Epoch: 89/100 | step: 307/422 | loss: 0.008494032546877861\n",
      "Epoch: 89/100 | step: 308/422 | loss: 0.0150905204936862\n",
      "Epoch: 89/100 | step: 309/422 | loss: 0.025254391133785248\n",
      "Epoch: 89/100 | step: 310/422 | loss: 0.02205238677561283\n",
      "Epoch: 89/100 | step: 311/422 | loss: 0.013997279107570648\n",
      "Epoch: 89/100 | step: 312/422 | loss: 0.006578437983989716\n",
      "Epoch: 89/100 | step: 313/422 | loss: 0.012415038421750069\n",
      "Epoch: 89/100 | step: 314/422 | loss: 0.005492693278938532\n",
      "Epoch: 89/100 | step: 315/422 | loss: 0.011595122516155243\n",
      "Epoch: 89/100 | step: 316/422 | loss: 0.011410201899707317\n",
      "Epoch: 89/100 | step: 317/422 | loss: 0.008859619498252869\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 90/100 | step: 1/422 | loss: 0.00798837374895811\n",
      "Epoch: 90/100 | step: 2/422 | loss: 0.011753876693546772\n",
      "Epoch: 90/100 | step: 3/422 | loss: 0.009272843599319458\n",
      "Epoch: 90/100 | step: 4/422 | loss: 0.008453167043626308\n",
      "Epoch: 90/100 | step: 5/422 | loss: 0.008631708100438118\n",
      "Epoch: 90/100 | step: 6/422 | loss: 0.008164629340171814\n",
      "Epoch: 90/100 | step: 7/422 | loss: 0.023964710533618927\n",
      "Epoch: 90/100 | step: 8/422 | loss: 0.0057703349739313126\n",
      "Epoch: 90/100 | step: 9/422 | loss: 0.008760137483477592\n",
      "Epoch: 90/100 | step: 10/422 | loss: 0.01893056370317936\n",
      "Epoch: 90/100 | step: 11/422 | loss: 0.012268872000277042\n",
      "Epoch: 90/100 | step: 12/422 | loss: 0.006343943532556295\n",
      "Epoch: 90/100 | step: 13/422 | loss: 0.008255662396550179\n",
      "Epoch: 90/100 | step: 14/422 | loss: 0.00973527692258358\n",
      "Epoch: 90/100 | step: 15/422 | loss: 0.010107878595590591\n",
      "Epoch: 90/100 | step: 16/422 | loss: 0.006883024238049984\n",
      "Epoch: 90/100 | step: 17/422 | loss: 0.007211877964437008\n",
      "Epoch: 90/100 | step: 18/422 | loss: 0.009064269252121449\n",
      "Epoch: 90/100 | step: 19/422 | loss: 0.005350877530872822\n",
      "Epoch: 90/100 | step: 20/422 | loss: 0.0063940975815057755\n",
      "Epoch: 90/100 | step: 21/422 | loss: 0.009185012429952621\n",
      "Epoch: 90/100 | step: 22/422 | loss: 0.01311655156314373\n",
      "Epoch: 90/100 | step: 23/422 | loss: 0.009471584111452103\n",
      "Epoch: 90/100 | step: 24/422 | loss: 0.004336889833211899\n",
      "Epoch: 90/100 | step: 25/422 | loss: 0.009249584749341011\n",
      "Epoch: 90/100 | step: 26/422 | loss: 0.009169758297502995\n",
      "Epoch: 90/100 | step: 27/422 | loss: 0.004933569580316544\n",
      "Epoch: 90/100 | step: 28/422 | loss: 0.01473124511539936\n",
      "Epoch: 90/100 | step: 29/422 | loss: 0.012586717493832111\n",
      "Epoch: 90/100 | step: 30/422 | loss: 0.0058097136206924915\n",
      "Epoch: 90/100 | step: 31/422 | loss: 0.010783286765217781\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 91/100 | step: 1/422 | loss: 0.007137370761483908\n",
      "Epoch: 91/100 | step: 2/422 | loss: 0.00864902138710022\n",
      "Epoch: 91/100 | step: 3/422 | loss: 0.011799981817603111\n",
      "Epoch: 91/100 | step: 4/422 | loss: 0.0043295565992593765\n",
      "Epoch: 91/100 | step: 5/422 | loss: 0.009084518998861313\n",
      "Epoch: 91/100 | step: 6/422 | loss: 0.005881450604647398\n",
      "Epoch: 91/100 | step: 7/422 | loss: 0.008017155341804028\n",
      "Epoch: 91/100 | step: 8/422 | loss: 0.009036418050527573\n",
      "Epoch: 91/100 | step: 9/422 | loss: 0.007034413982182741\n",
      "Epoch: 91/100 | step: 10/422 | loss: 0.008572169579565525\n",
      "Epoch: 91/100 | step: 11/422 | loss: 0.005445196758955717\n",
      "Epoch: 91/100 | step: 12/422 | loss: 0.007850207388401031\n",
      "Epoch: 91/100 | step: 13/422 | loss: 0.007416896987706423\n",
      "Epoch: 91/100 | step: 14/422 | loss: 0.0046469722874462605\n",
      "Epoch: 91/100 | step: 15/422 | loss: 0.006528900470584631\n",
      "Epoch: 91/100 | step: 16/422 | loss: 0.01495025772601366\n",
      "Epoch: 91/100 | step: 17/422 | loss: 0.007234248332679272\n",
      "Epoch: 91/100 | step: 18/422 | loss: 0.005389366764575243\n",
      "Epoch: 91/100 | step: 19/422 | loss: 0.005052205640822649\n",
      "Epoch: 91/100 | step: 20/422 | loss: 0.0071347178891301155\n",
      "Epoch: 91/100 | step: 21/422 | loss: 0.008016500622034073\n",
      "Epoch: 91/100 | step: 22/422 | loss: 0.01494852639734745\n",
      "Epoch: 91/100 | step: 23/422 | loss: 0.009262985549867153\n",
      "Epoch: 91/100 | step: 24/422 | loss: 0.006729337386786938\n",
      "Epoch: 91/100 | step: 25/422 | loss: 0.010848095640540123\n",
      "Epoch: 91/100 | step: 26/422 | loss: 0.004851776175200939\n",
      "Epoch: 91/100 | step: 27/422 | loss: 0.007137970067560673\n",
      "Epoch: 91/100 | step: 28/422 | loss: 0.01330473367124796\n",
      "Epoch: 91/100 | step: 29/422 | loss: 0.0072578852996230125\n",
      "Epoch: 91/100 | step: 30/422 | loss: 0.012723068706691265\n",
      "Epoch: 91/100 | step: 31/422 | loss: 0.013793389312922955\n",
      "Epoch: 91/100 | step: 32/422 | loss: 0.005635764915496111\n",
      "Epoch: 91/100 | step: 33/422 | loss: 0.008523995988070965\n",
      "Epoch: 91/100 | step: 34/422 | loss: 0.00820466224104166\n",
      "Epoch: 91/100 | step: 35/422 | loss: 0.009372491389513016\n",
      "Epoch: 91/100 | step: 36/422 | loss: 0.005297642666846514\n",
      "Epoch: 91/100 | step: 37/422 | loss: 0.009207181632518768\n",
      "Epoch: 91/100 | step: 38/422 | loss: 0.01153707318007946\n",
      "Epoch: 91/100 | step: 39/422 | loss: 0.007117971312254667\n",
      "Epoch: 91/100 | step: 40/422 | loss: 0.010539526119828224\n",
      "Epoch: 91/100 | step: 41/422 | loss: 0.009067188948392868\n",
      "Epoch: 91/100 | step: 42/422 | loss: 0.004509408492594957\n",
      "Epoch: 91/100 | step: 43/422 | loss: 0.00633865175768733\n",
      "Epoch: 91/100 | step: 44/422 | loss: 0.00874724704772234\n",
      "Epoch: 91/100 | step: 45/422 | loss: 0.0057084527797997\n",
      "Epoch: 91/100 | step: 46/422 | loss: 0.0072449930012226105\n",
      "Epoch: 91/100 | step: 47/422 | loss: 0.00567068625241518\n",
      "Epoch: 91/100 | step: 48/422 | loss: 0.005013818386942148\n",
      "Epoch: 91/100 | step: 49/422 | loss: 0.004107599146664143\n",
      "Epoch: 91/100 | step: 50/422 | loss: 0.006069664377719164\n",
      "Epoch: 91/100 | step: 51/422 | loss: 0.005221943370997906\n",
      "Epoch: 91/100 | step: 52/422 | loss: 0.008688618429005146\n",
      "Epoch: 91/100 | step: 53/422 | loss: 0.006795993074774742\n",
      "Epoch: 91/100 | step: 54/422 | loss: 0.004170621279627085\n",
      "Epoch: 91/100 | step: 55/422 | loss: 0.005723265931010246\n",
      "Epoch: 91/100 | step: 56/422 | loss: 0.0094065610319376\n",
      "Epoch: 91/100 | step: 57/422 | loss: 0.0064996047876775265\n",
      "Epoch: 91/100 | step: 58/422 | loss: 0.006001266185194254\n",
      "Epoch: 91/100 | step: 59/422 | loss: 0.015886474400758743\n",
      "Epoch: 91/100 | step: 60/422 | loss: 0.012769579887390137\n",
      "Epoch: 91/100 | step: 61/422 | loss: 0.005928242579102516\n",
      "Epoch: 91/100 | step: 62/422 | loss: 0.005988446995615959\n",
      "Epoch: 91/100 | step: 63/422 | loss: 0.007097381167113781\n",
      "Epoch: 91/100 | step: 64/422 | loss: 0.005214310251176357\n",
      "Epoch: 91/100 | step: 65/422 | loss: 0.00879152212291956\n",
      "Epoch: 91/100 | step: 66/422 | loss: 0.00732921389862895\n",
      "Epoch: 91/100 | step: 67/422 | loss: 0.01235190685838461\n",
      "Epoch: 91/100 | step: 68/422 | loss: 0.008162267506122589\n",
      "Epoch: 91/100 | step: 69/422 | loss: 0.0066628423519432545\n",
      "Epoch: 91/100 | step: 70/422 | loss: 0.009827686473727226\n",
      "Epoch: 91/100 | step: 71/422 | loss: 0.007689701858907938\n",
      "Epoch: 91/100 | step: 72/422 | loss: 0.008014742285013199\n",
      "Epoch: 91/100 | step: 73/422 | loss: 0.0075347996316850185\n",
      "Epoch: 91/100 | step: 74/422 | loss: 0.008618330582976341\n",
      "Epoch: 91/100 | step: 75/422 | loss: 0.008236457593739033\n",
      "Epoch: 91/100 | step: 76/422 | loss: 0.03432946652173996\n",
      "Epoch: 91/100 | step: 77/422 | loss: 0.008455988019704819\n",
      "Epoch: 91/100 | step: 78/422 | loss: 0.03779188171029091\n",
      "Epoch: 91/100 | step: 79/422 | loss: 0.03343702480196953\n",
      "Epoch: 91/100 | step: 80/422 | loss: 0.008403029292821884\n",
      "Epoch: 91/100 | step: 81/422 | loss: 0.006146397441625595\n",
      "Epoch: 91/100 | step: 82/422 | loss: 0.007487073540687561\n",
      "Epoch: 91/100 | step: 83/422 | loss: 0.03484760969877243\n",
      "Epoch: 91/100 | step: 84/422 | loss: 0.035399798303842545\n",
      "Epoch: 91/100 | step: 85/422 | loss: 0.010902552865445614\n",
      "Epoch: 91/100 | step: 86/422 | loss: 0.028762351721525192\n",
      "Epoch: 91/100 | step: 87/422 | loss: 0.008679192513227463\n",
      "Epoch: 91/100 | step: 88/422 | loss: 0.009407631121575832\n",
      "Epoch: 91/100 | step: 89/422 | loss: 0.029481440782546997\n",
      "Epoch: 91/100 | step: 90/422 | loss: 0.05726703256368637\n",
      "Epoch: 91/100 | step: 91/422 | loss: 0.00916704535484314\n",
      "Epoch: 91/100 | step: 92/422 | loss: 0.01095654908567667\n",
      "Epoch: 91/100 | step: 93/422 | loss: 0.011966710910201073\n",
      "Epoch: 91/100 | step: 94/422 | loss: 0.008183890022337437\n",
      "Epoch: 91/100 | step: 95/422 | loss: 0.008139031939208508\n",
      "Epoch: 91/100 | step: 96/422 | loss: 0.061576277017593384\n",
      "Epoch: 91/100 | step: 97/422 | loss: 0.03854266554117203\n",
      "Epoch: 91/100 | step: 98/422 | loss: 0.007358555682003498\n",
      "Epoch: 91/100 | step: 99/422 | loss: 0.024164199829101562\n",
      "Epoch: 91/100 | step: 100/422 | loss: 0.008234468288719654\n",
      "Epoch: 91/100 | step: 101/422 | loss: 0.017587047070264816\n",
      "Epoch: 91/100 | step: 102/422 | loss: 0.009885289706289768\n",
      "Epoch: 91/100 | step: 103/422 | loss: 0.007438264787197113\n",
      "Epoch: 91/100 | step: 104/422 | loss: 0.038066308945417404\n",
      "Epoch: 91/100 | step: 105/422 | loss: 0.006109646055847406\n",
      "Epoch: 91/100 | step: 106/422 | loss: 0.02180999144911766\n",
      "Epoch: 91/100 | step: 107/422 | loss: 0.009144390933215618\n",
      "Epoch: 91/100 | step: 108/422 | loss: 0.01811317168176174\n",
      "Epoch: 91/100 | step: 109/422 | loss: 0.011667194776237011\n",
      "Epoch: 91/100 | step: 110/422 | loss: 0.018794026225805283\n",
      "Epoch: 91/100 | step: 111/422 | loss: 0.007641305215656757\n",
      "Epoch: 91/100 | step: 112/422 | loss: 0.00619896687567234\n",
      "Epoch: 91/100 | step: 113/422 | loss: 0.007094963453710079\n",
      "Epoch: 91/100 | step: 114/422 | loss: 0.008435975760221481\n",
      "Epoch: 91/100 | step: 115/422 | loss: 0.009225753135979176\n",
      "Epoch: 91/100 | step: 116/422 | loss: 0.0098252072930336\n",
      "Epoch: 91/100 | step: 117/422 | loss: 0.00989394262433052\n",
      "Epoch: 91/100 | step: 118/422 | loss: 0.01051554549485445\n",
      "Epoch: 91/100 | step: 119/422 | loss: 0.010553214699029922\n",
      "Epoch: 91/100 | step: 120/422 | loss: 0.012525581754744053\n",
      "Epoch: 91/100 | step: 121/422 | loss: 0.0089261494576931\n",
      "Epoch: 91/100 | step: 122/422 | loss: 0.007108279503881931\n",
      "Epoch: 91/100 | step: 123/422 | loss: 0.013177214190363884\n",
      "Epoch: 91/100 | step: 124/422 | loss: 0.005511268507689238\n",
      "Epoch: 91/100 | step: 125/422 | loss: 0.006404431536793709\n",
      "Epoch: 91/100 | step: 126/422 | loss: 0.006469763349741697\n",
      "Epoch: 91/100 | step: 127/422 | loss: 0.007071101572364569\n",
      "Epoch: 91/100 | step: 128/422 | loss: 0.005437234416604042\n",
      "Epoch: 91/100 | step: 129/422 | loss: 0.006575141102075577\n",
      "Epoch: 91/100 | step: 130/422 | loss: 0.004994483198970556\n",
      "Epoch: 91/100 | step: 131/422 | loss: 0.008482205681502819\n",
      "Epoch: 91/100 | step: 132/422 | loss: 0.007684112526476383\n",
      "Epoch: 91/100 | step: 133/422 | loss: 0.008089333772659302\n",
      "Epoch: 91/100 | step: 134/422 | loss: 0.015413080342113972\n",
      "Epoch: 91/100 | step: 135/422 | loss: 0.005718335509300232\n",
      "Epoch: 91/100 | step: 136/422 | loss: 0.00933937355875969\n",
      "Epoch: 91/100 | step: 137/422 | loss: 0.00990215316414833\n",
      "Epoch: 91/100 | step: 138/422 | loss: 0.007345494348555803\n",
      "Epoch: 91/100 | step: 139/422 | loss: 0.005042041186243296\n",
      "Epoch: 91/100 | step: 140/422 | loss: 0.007870021276175976\n",
      "Epoch: 91/100 | step: 141/422 | loss: 0.007290851790457964\n",
      "Epoch: 91/100 | step: 142/422 | loss: 0.005770150106400251\n",
      "Epoch: 91/100 | step: 143/422 | loss: 0.007488667964935303\n",
      "Epoch: 91/100 | step: 144/422 | loss: 0.0056870547123253345\n",
      "Epoch: 91/100 | step: 145/422 | loss: 0.00975276343524456\n",
      "Epoch: 91/100 | step: 146/422 | loss: 0.009751793928444386\n",
      "Epoch: 91/100 | step: 147/422 | loss: 0.007863921113312244\n",
      "Epoch: 91/100 | step: 148/422 | loss: 0.010950419120490551\n",
      "Epoch: 91/100 | step: 149/422 | loss: 0.007933137007057667\n",
      "Epoch: 91/100 | step: 150/422 | loss: 0.007661405019462109\n",
      "Epoch: 91/100 | step: 151/422 | loss: 0.0054682716727256775\n",
      "Epoch: 91/100 | step: 152/422 | loss: 0.006683963816612959\n",
      "Epoch: 91/100 | step: 153/422 | loss: 0.010162370279431343\n",
      "Epoch: 91/100 | step: 154/422 | loss: 0.009599446319043636\n",
      "Epoch: 91/100 | step: 155/422 | loss: 0.0072659640572965145\n",
      "Epoch: 91/100 | step: 156/422 | loss: 0.004808214493095875\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 92/100 | step: 1/422 | loss: 0.006517807953059673\n",
      "Epoch: 92/100 | step: 2/422 | loss: 0.009325156919658184\n",
      "Epoch: 92/100 | step: 3/422 | loss: 0.006876397877931595\n",
      "Epoch: 92/100 | step: 4/422 | loss: 0.011540849693119526\n",
      "Epoch: 92/100 | step: 5/422 | loss: 0.005081271752715111\n",
      "Epoch: 92/100 | step: 6/422 | loss: 0.005869941785931587\n",
      "Epoch: 92/100 | step: 7/422 | loss: 0.005360342562198639\n",
      "Epoch: 92/100 | step: 8/422 | loss: 0.008023033849895\n",
      "Epoch: 92/100 | step: 9/422 | loss: 0.0127177145332098\n",
      "Epoch: 92/100 | step: 10/422 | loss: 0.007522665895521641\n",
      "Epoch: 92/100 | step: 11/422 | loss: 0.005133570171892643\n",
      "Epoch: 92/100 | step: 12/422 | loss: 0.005248307716101408\n",
      "Epoch: 92/100 | step: 13/422 | loss: 0.006126741413027048\n",
      "Epoch: 92/100 | step: 14/422 | loss: 0.004579308908432722\n",
      "Epoch: 92/100 | step: 15/422 | loss: 0.008524071425199509\n",
      "Epoch: 92/100 | step: 16/422 | loss: 0.008058113045990467\n",
      "Epoch: 92/100 | step: 17/422 | loss: 0.008267591707408428\n",
      "Epoch: 92/100 | step: 18/422 | loss: 0.006321326829493046\n",
      "Epoch: 92/100 | step: 19/422 | loss: 0.0069243283942341805\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 93/100 | step: 1/422 | loss: 0.005043077282607555\n",
      "Epoch: 93/100 | step: 2/422 | loss: 0.012369934469461441\n",
      "Epoch: 93/100 | step: 3/422 | loss: 0.005866379011422396\n",
      "Epoch: 93/100 | step: 4/422 | loss: 0.004544076509773731\n",
      "Epoch: 93/100 | step: 5/422 | loss: 0.007840149104595184\n",
      "Epoch: 93/100 | step: 6/422 | loss: 0.0066624414175748825\n",
      "Epoch: 93/100 | step: 7/422 | loss: 0.008330141194164753\n",
      "Epoch: 93/100 | step: 8/422 | loss: 0.004710442386567593\n",
      "Epoch: 93/100 | step: 9/422 | loss: 0.006447164807468653\n",
      "Epoch: 93/100 | step: 10/422 | loss: 0.007469221018254757\n",
      "Epoch: 93/100 | step: 11/422 | loss: 0.006899280473589897\n",
      "Epoch: 93/100 | step: 12/422 | loss: 0.020564798265695572\n",
      "Epoch: 93/100 | step: 13/422 | loss: 0.004379026126116514\n",
      "Epoch: 93/100 | step: 14/422 | loss: 0.0057134004309773445\n",
      "Epoch: 93/100 | step: 15/422 | loss: 0.008590283803641796\n",
      "Epoch: 93/100 | step: 16/422 | loss: 0.005803688894957304\n",
      "Epoch: 93/100 | step: 17/422 | loss: 0.011594795621931553\n",
      "Epoch: 93/100 | step: 18/422 | loss: 0.00796227902173996\n",
      "Epoch: 93/100 | step: 19/422 | loss: 0.007175955921411514\n",
      "Epoch: 93/100 | step: 20/422 | loss: 0.010048428550362587\n",
      "Epoch: 93/100 | step: 21/422 | loss: 0.007785747293382883\n",
      "Epoch: 93/100 | step: 22/422 | loss: 0.010502099990844727\n",
      "Epoch: 93/100 | step: 23/422 | loss: 0.005610284861177206\n",
      "Epoch: 93/100 | step: 24/422 | loss: 0.008390380069613457\n",
      "Epoch: 93/100 | step: 25/422 | loss: 0.009734589606523514\n",
      "Epoch: 93/100 | step: 26/422 | loss: 0.008658071979880333\n",
      "Epoch: 93/100 | step: 27/422 | loss: 0.0068519096821546555\n",
      "Epoch: 93/100 | step: 28/422 | loss: 0.006898656487464905\n",
      "Epoch: 93/100 | step: 29/422 | loss: 0.010123909451067448\n",
      "Epoch: 93/100 | step: 30/422 | loss: 0.008979774080216885\n",
      "Epoch: 93/100 | step: 31/422 | loss: 0.02208269014954567\n",
      "Epoch: 93/100 | step: 32/422 | loss: 0.006511329673230648\n",
      "Epoch: 93/100 | step: 33/422 | loss: 0.007101707626134157\n",
      "Epoch: 93/100 | step: 34/422 | loss: 0.010097077116370201\n",
      "Epoch: 93/100 | step: 35/422 | loss: 0.021872874349355698\n",
      "Epoch: 93/100 | step: 36/422 | loss: 0.006943264044821262\n",
      "Epoch: 93/100 | step: 37/422 | loss: 0.0052531021647155285\n",
      "Epoch: 93/100 | step: 38/422 | loss: 0.010006148368120193\n",
      "Epoch: 93/100 | step: 39/422 | loss: 0.007634617853909731\n",
      "Epoch: 93/100 | step: 40/422 | loss: 0.010996519587934017\n",
      "Epoch: 93/100 | step: 41/422 | loss: 0.004135808441787958\n",
      "Epoch: 93/100 | step: 42/422 | loss: 0.009839778766036034\n",
      "Epoch: 93/100 | step: 43/422 | loss: 0.007162623573094606\n",
      "Epoch: 93/100 | step: 44/422 | loss: 0.003772356314584613\n",
      "Epoch: 93/100 | step: 45/422 | loss: 0.008442320860922337\n",
      "Epoch: 93/100 | step: 46/422 | loss: 0.011707601137459278\n",
      "Epoch: 93/100 | step: 47/422 | loss: 0.008774830028414726\n",
      "Epoch: 93/100 | step: 48/422 | loss: 0.00826955959200859\n",
      "Epoch: 93/100 | step: 49/422 | loss: 0.007091950625181198\n",
      "Epoch: 93/100 | step: 50/422 | loss: 0.009254368022084236\n",
      "Epoch: 93/100 | step: 51/422 | loss: 0.0047509013675153255\n",
      "Epoch: 93/100 | step: 52/422 | loss: 0.008103338070213795\n",
      "Epoch: 93/100 | step: 53/422 | loss: 0.008477592840790749\n",
      "Epoch: 93/100 | step: 54/422 | loss: 0.008020663633942604\n",
      "Epoch: 93/100 | step: 55/422 | loss: 0.006940746679902077\n",
      "Epoch: 93/100 | step: 56/422 | loss: 0.006498414557427168\n",
      "Epoch: 93/100 | step: 57/422 | loss: 0.0061480398289859295\n",
      "Epoch: 93/100 | step: 58/422 | loss: 0.0067903390154242516\n",
      "Epoch: 93/100 | step: 59/422 | loss: 0.0039667715318500996\n",
      "Epoch: 93/100 | step: 60/422 | loss: 0.007682631257921457\n",
      "Epoch: 93/100 | step: 61/422 | loss: 0.04386031627655029\n",
      "Epoch: 93/100 | step: 62/422 | loss: 0.007421555463224649\n",
      "Epoch: 93/100 | step: 63/422 | loss: 0.010794585570693016\n",
      "Epoch: 93/100 | step: 64/422 | loss: 0.01831037923693657\n",
      "Epoch: 93/100 | step: 65/422 | loss: 0.004601213149726391\n",
      "Epoch: 93/100 | step: 66/422 | loss: 0.003149001393467188\n",
      "Epoch: 93/100 | step: 67/422 | loss: 0.005559550132602453\n",
      "Epoch: 93/100 | step: 68/422 | loss: 0.006017519626766443\n",
      "Epoch: 93/100 | step: 69/422 | loss: 0.010907398536801338\n",
      "Epoch: 93/100 | step: 70/422 | loss: 0.006468163803219795\n",
      "Epoch: 93/100 | step: 71/422 | loss: 0.005763422232121229\n",
      "Epoch: 93/100 | step: 72/422 | loss: 0.013358653523027897\n",
      "Epoch: 93/100 | step: 73/422 | loss: 0.01543917041271925\n",
      "Epoch: 93/100 | step: 74/422 | loss: 0.010690275579690933\n",
      "Epoch: 93/100 | step: 75/422 | loss: 0.02064422518014908\n",
      "Epoch: 93/100 | step: 76/422 | loss: 0.006173896603286266\n",
      "Epoch: 93/100 | step: 77/422 | loss: 0.009829702787101269\n",
      "Epoch: 93/100 | step: 78/422 | loss: 0.005834551993757486\n",
      "Epoch: 93/100 | step: 79/422 | loss: 0.007955874316394329\n",
      "Epoch: 93/100 | step: 80/422 | loss: 0.007785536348819733\n",
      "Epoch: 93/100 | step: 81/422 | loss: 0.026077348738908768\n",
      "Epoch: 93/100 | step: 82/422 | loss: 0.00500081991776824\n",
      "Epoch: 93/100 | step: 83/422 | loss: 0.006900066044181585\n",
      "Epoch: 93/100 | step: 84/422 | loss: 0.008138267323374748\n",
      "Epoch: 93/100 | step: 85/422 | loss: 0.0053698779083788395\n",
      "Epoch: 93/100 | step: 86/422 | loss: 0.020774515345692635\n",
      "Epoch: 93/100 | step: 87/422 | loss: 0.008793945424258709\n",
      "Epoch: 93/100 | step: 88/422 | loss: 0.0068763066083192825\n",
      "Epoch: 93/100 | step: 89/422 | loss: 0.007242665160447359\n",
      "Epoch: 93/100 | step: 90/422 | loss: 0.009254551492631435\n",
      "Epoch: 93/100 | step: 91/422 | loss: 0.01528722420334816\n",
      "Epoch: 93/100 | step: 92/422 | loss: 0.005403190851211548\n",
      "Epoch: 93/100 | step: 93/422 | loss: 0.006262514274567366\n",
      "Epoch: 93/100 | step: 94/422 | loss: 0.006306031718850136\n",
      "Epoch: 93/100 | step: 95/422 | loss: 0.006551762577146292\n",
      "Epoch: 93/100 | step: 96/422 | loss: 0.007790637668222189\n",
      "Epoch: 93/100 | step: 97/422 | loss: 0.00969475507736206\n",
      "Epoch: 93/100 | step: 98/422 | loss: 0.010606775060296059\n",
      "Epoch: 93/100 | step: 99/422 | loss: 0.005777172278612852\n",
      "Epoch: 93/100 | step: 100/422 | loss: 0.009694548323750496\n",
      "Epoch: 93/100 | step: 101/422 | loss: 0.006579413544386625\n",
      "Epoch: 93/100 | step: 102/422 | loss: 0.012895135208964348\n",
      "Epoch: 93/100 | step: 103/422 | loss: 0.008547109551727772\n",
      "Epoch: 93/100 | step: 104/422 | loss: 0.003751833690330386\n",
      "Epoch: 93/100 | step: 105/422 | loss: 0.007833828218281269\n",
      "Epoch: 93/100 | step: 106/422 | loss: 0.01057540811598301\n",
      "Epoch: 93/100 | step: 107/422 | loss: 0.006412207148969173\n",
      "Epoch: 93/100 | step: 108/422 | loss: 0.005073270760476589\n",
      "Epoch: 93/100 | step: 109/422 | loss: 0.0049245888367295265\n",
      "Epoch: 93/100 | step: 110/422 | loss: 0.006125838495790958\n",
      "Epoch: 93/100 | step: 111/422 | loss: 0.007272239774465561\n",
      "Epoch: 93/100 | step: 112/422 | loss: 0.01004616729915142\n",
      "Epoch: 93/100 | step: 113/422 | loss: 0.007584363222122192\n",
      "Epoch: 93/100 | step: 114/422 | loss: 0.007412461098283529\n",
      "Epoch: 93/100 | step: 115/422 | loss: 0.004852920770645142\n",
      "Epoch: 93/100 | step: 116/422 | loss: 0.0037992538418620825\n",
      "Epoch: 93/100 | step: 117/422 | loss: 0.006116134580224752\n",
      "Epoch: 93/100 | step: 118/422 | loss: 0.007442421745508909\n",
      "Epoch: 93/100 | step: 119/422 | loss: 0.008314103819429874\n",
      "Epoch: 93/100 | step: 120/422 | loss: 0.017320815473794937\n",
      "Epoch: 93/100 | step: 121/422 | loss: 0.007537351921200752\n",
      "Epoch: 93/100 | step: 122/422 | loss: 0.006532065104693174\n",
      "Epoch: 93/100 | step: 123/422 | loss: 0.004872466903179884\n",
      "Epoch: 93/100 | step: 124/422 | loss: 0.006533649284392595\n",
      "Epoch: 93/100 | step: 125/422 | loss: 0.010054494254291058\n",
      "Epoch: 93/100 | step: 126/422 | loss: 0.003307734150439501\n",
      "Epoch: 93/100 | step: 127/422 | loss: 0.009806857444345951\n",
      "Epoch: 93/100 | step: 128/422 | loss: 0.006604078691452742\n",
      "Epoch: 93/100 | step: 129/422 | loss: 0.011038349941372871\n",
      "Epoch: 93/100 | step: 130/422 | loss: 0.013152013532817364\n",
      "Epoch: 93/100 | step: 131/422 | loss: 0.007510599214583635\n",
      "Epoch: 93/100 | step: 132/422 | loss: 0.008374309167265892\n",
      "Epoch: 93/100 | step: 133/422 | loss: 0.004361322149634361\n",
      "Epoch: 93/100 | step: 134/422 | loss: 0.004569893702864647\n",
      "Epoch: 93/100 | step: 135/422 | loss: 0.005674395710229874\n",
      "Epoch: 93/100 | step: 136/422 | loss: 0.004427960608154535\n",
      "Epoch: 93/100 | step: 137/422 | loss: 0.005896105896681547\n",
      "Epoch: 93/100 | step: 138/422 | loss: 0.004991231020539999\n",
      "Epoch: 93/100 | step: 139/422 | loss: 0.00493998359888792\n",
      "Epoch: 93/100 | step: 140/422 | loss: 0.00612660963088274\n",
      "Epoch: 93/100 | step: 141/422 | loss: 0.028322789818048477\n",
      "Epoch: 93/100 | step: 142/422 | loss: 0.005252818111330271\n",
      "Epoch: 93/100 | step: 143/422 | loss: 0.006576648913323879\n",
      "Epoch: 93/100 | step: 144/422 | loss: 0.004075575154274702\n",
      "Epoch: 93/100 | step: 145/422 | loss: 0.004538052715361118\n",
      "Epoch: 93/100 | step: 146/422 | loss: 0.005177908577024937\n",
      "Epoch: 93/100 | step: 147/422 | loss: 0.050750982016325\n",
      "Epoch: 93/100 | step: 148/422 | loss: 0.009933896362781525\n",
      "Epoch: 93/100 | step: 149/422 | loss: 0.011293831281363964\n",
      "Epoch: 93/100 | step: 150/422 | loss: 0.20399342477321625\n",
      "Epoch: 93/100 | step: 151/422 | loss: 0.325961709022522\n",
      "Epoch: 93/100 | step: 152/422 | loss: 0.04147873446345329\n",
      "Epoch: 93/100 | step: 153/422 | loss: 0.011033515445888042\n",
      "Epoch: 93/100 | step: 154/422 | loss: 0.009655099362134933\n",
      "Epoch: 93/100 | step: 155/422 | loss: 0.008414668962359428\n",
      "Epoch: 93/100 | step: 156/422 | loss: 0.010480091907083988\n",
      "Epoch: 93/100 | step: 157/422 | loss: 0.011642050929367542\n",
      "Epoch: 93/100 | step: 158/422 | loss: 0.006253260187804699\n",
      "Epoch: 93/100 | step: 159/422 | loss: 0.022987883538007736\n",
      "Epoch: 93/100 | step: 160/422 | loss: 0.013675260357558727\n",
      "Epoch: 93/100 | step: 161/422 | loss: 0.008606377989053726\n",
      "Epoch: 93/100 | step: 162/422 | loss: 0.007081822957843542\n",
      "Epoch: 93/100 | step: 163/422 | loss: 0.009689394384622574\n",
      "Epoch: 93/100 | step: 164/422 | loss: 0.008858184330165386\n",
      "Epoch: 93/100 | step: 165/422 | loss: 0.007176692131906748\n",
      "Epoch: 93/100 | step: 166/422 | loss: 0.005991178564727306\n",
      "Epoch: 93/100 | step: 167/422 | loss: 0.012862395495176315\n",
      "Epoch: 93/100 | step: 168/422 | loss: 0.00899549014866352\n",
      "Epoch: 93/100 | step: 169/422 | loss: 0.014631635509431362\n",
      "Epoch: 93/100 | step: 170/422 | loss: 0.017622092738747597\n",
      "Epoch: 93/100 | step: 171/422 | loss: 0.013810829259455204\n",
      "Epoch: 93/100 | step: 172/422 | loss: 0.00823264755308628\n",
      "Epoch: 93/100 | step: 173/422 | loss: 0.00796036422252655\n",
      "Epoch: 93/100 | step: 174/422 | loss: 0.0127969179302454\n",
      "Epoch: 93/100 | step: 175/422 | loss: 0.021375255659222603\n",
      "Epoch: 93/100 | step: 176/422 | loss: 0.014564416371285915\n",
      "Epoch: 93/100 | step: 177/422 | loss: 0.005823252256959677\n",
      "Epoch: 93/100 | step: 178/422 | loss: 0.005277771037071943\n",
      "Epoch: 93/100 | step: 179/422 | loss: 0.005869660526514053\n",
      "Epoch: 93/100 | step: 180/422 | loss: 0.007798582315444946\n",
      "Epoch: 93/100 | step: 181/422 | loss: 0.007528983987867832\n",
      "Epoch: 93/100 | step: 182/422 | loss: 0.009764818474650383\n",
      "Epoch: 93/100 | step: 183/422 | loss: 0.006594874430447817\n",
      "Epoch: 93/100 | step: 184/422 | loss: 0.005519543308764696\n",
      "Epoch: 93/100 | step: 185/422 | loss: 0.004813142120838165\n",
      "Epoch: 93/100 | step: 186/422 | loss: 0.007277581840753555\n",
      "Epoch: 93/100 | step: 187/422 | loss: 0.006473086308687925\n",
      "Epoch: 93/100 | step: 188/422 | loss: 0.0072965724393725395\n",
      "Epoch: 93/100 | step: 189/422 | loss: 0.005958703812211752\n",
      "Epoch: 93/100 | step: 190/422 | loss: 0.007197317201644182\n",
      "Epoch: 93/100 | step: 191/422 | loss: 0.007766858674585819\n",
      "Epoch: 93/100 | step: 192/422 | loss: 0.007269333582371473\n",
      "Epoch: 93/100 | step: 193/422 | loss: 0.006469036918133497\n",
      "Epoch: 93/100 | step: 194/422 | loss: 0.017227567732334137\n",
      "Epoch: 93/100 | step: 195/422 | loss: 0.016400795429944992\n",
      "Epoch: 93/100 | step: 196/422 | loss: 0.00888956431299448\n",
      "Epoch: 93/100 | step: 197/422 | loss: 0.015870707109570503\n",
      "Epoch: 93/100 | step: 198/422 | loss: 0.006974645424634218\n",
      "Epoch: 93/100 | step: 199/422 | loss: 0.009387518279254436\n",
      "Epoch: 93/100 | step: 200/422 | loss: 0.008324968628585339\n",
      "Epoch: 93/100 | step: 201/422 | loss: 0.006242964882403612\n",
      "Epoch: 93/100 | step: 202/422 | loss: 0.007224784232676029\n",
      "Epoch: 93/100 | step: 203/422 | loss: 0.007345434278249741\n",
      "Epoch: 93/100 | step: 204/422 | loss: 0.013752707280218601\n",
      "Epoch: 93/100 | step: 205/422 | loss: 0.005568037275224924\n",
      "Epoch: 93/100 | step: 206/422 | loss: 0.007219450548291206\n",
      "Epoch: 93/100 | step: 207/422 | loss: 0.006875297054648399\n",
      "Epoch: 93/100 | step: 208/422 | loss: 0.004029945004731417\n",
      "Epoch: 93/100 | step: 209/422 | loss: 0.008272367529571056\n",
      "Epoch: 93/100 | step: 210/422 | loss: 0.006455602124333382\n",
      "Epoch: 93/100 | step: 211/422 | loss: 0.005265507847070694\n",
      "Epoch: 93/100 | step: 212/422 | loss: 0.006424273829907179\n",
      "Epoch: 93/100 | step: 213/422 | loss: 0.012365833856165409\n",
      "Epoch: 93/100 | step: 214/422 | loss: 0.007391473278403282\n",
      "Epoch: 93/100 | step: 215/422 | loss: 0.009525456465780735\n",
      "Epoch: 93/100 | step: 216/422 | loss: 0.007306298241019249\n",
      "Epoch: 93/100 | step: 217/422 | loss: 0.008227907121181488\n",
      "Epoch: 93/100 | step: 218/422 | loss: 0.026687361299991608\n",
      "Epoch: 93/100 | step: 219/422 | loss: 0.07724906504154205\n",
      "Epoch: 93/100 | step: 220/422 | loss: 0.022485703229904175\n",
      "Epoch: 93/100 | step: 221/422 | loss: 0.27795860171318054\n",
      "Epoch: 93/100 | step: 222/422 | loss: 0.28282085061073303\n",
      "Epoch: 93/100 | step: 223/422 | loss: 1.5540494918823242\n",
      "Epoch: 93/100 | step: 224/422 | loss: 0.1243869960308075\n",
      "Epoch: 93/100 | step: 225/422 | loss: 0.053870853036642075\n",
      "Epoch: 93/100 | step: 226/422 | loss: 0.167240172624588\n",
      "Epoch: 93/100 | step: 227/422 | loss: 0.023058710619807243\n",
      "Epoch: 93/100 | step: 228/422 | loss: 0.32418498396873474\n",
      "Epoch: 93/100 | step: 229/422 | loss: 0.2741422653198242\n",
      "Epoch: 93/100 | step: 230/422 | loss: 0.1551334708929062\n",
      "Epoch: 93/100 | step: 231/422 | loss: 0.03854949772357941\n",
      "Epoch: 93/100 | step: 232/422 | loss: 0.013946272432804108\n",
      "Epoch: 93/100 | step: 233/422 | loss: 0.019906168803572655\n",
      "Epoch: 93/100 | step: 234/422 | loss: 0.033077508211135864\n",
      "Epoch: 93/100 | step: 235/422 | loss: 0.03572744131088257\n",
      "Epoch: 93/100 | step: 236/422 | loss: 0.016750480979681015\n",
      "Epoch: 93/100 | step: 237/422 | loss: 0.012614511884748936\n",
      "Epoch: 93/100 | step: 238/422 | loss: 0.014655529521405697\n",
      "Epoch: 93/100 | step: 239/422 | loss: 0.02448919229209423\n",
      "Epoch: 93/100 | step: 240/422 | loss: 0.010705705732107162\n",
      "Epoch: 93/100 | step: 241/422 | loss: 0.011501206085085869\n",
      "Epoch: 93/100 | step: 242/422 | loss: 0.017135905101895332\n",
      "Epoch: 93/100 | step: 243/422 | loss: 0.022798851132392883\n",
      "Epoch: 93/100 | step: 244/422 | loss: 0.010460488498210907\n",
      "Epoch: 93/100 | step: 245/422 | loss: 0.014276549220085144\n",
      "Epoch: 93/100 | step: 246/422 | loss: 0.012343978509306908\n",
      "Epoch: 93/100 | step: 247/422 | loss: 0.02486167475581169\n",
      "Epoch: 93/100 | step: 248/422 | loss: 0.016590310260653496\n",
      "Epoch: 93/100 | step: 249/422 | loss: 0.00836446788161993\n",
      "Epoch: 93/100 | step: 250/422 | loss: 0.008960128761827946\n",
      "Epoch: 93/100 | step: 251/422 | loss: 0.009687784127891064\n",
      "Epoch: 93/100 | step: 252/422 | loss: 0.11891782283782959\n",
      "Epoch: 93/100 | step: 253/422 | loss: 0.011518599465489388\n",
      "Epoch: 93/100 | step: 254/422 | loss: 0.00852360762655735\n",
      "Epoch: 93/100 | step: 255/422 | loss: 0.015243738889694214\n",
      "Epoch: 93/100 | step: 256/422 | loss: 0.010114830918610096\n",
      "Epoch: 93/100 | step: 257/422 | loss: 0.026955263689160347\n",
      "Epoch: 93/100 | step: 258/422 | loss: 0.021251976490020752\n",
      "Epoch: 93/100 | step: 259/422 | loss: 0.015379674732685089\n",
      "Epoch: 93/100 | step: 260/422 | loss: 0.01082367729395628\n",
      "Epoch: 93/100 | step: 261/422 | loss: 0.007915807887911797\n",
      "Epoch: 93/100 | step: 262/422 | loss: 0.018729861825704575\n",
      "Epoch: 93/100 | step: 263/422 | loss: 0.010368063114583492\n",
      "Epoch: 93/100 | step: 264/422 | loss: 0.016317099332809448\n",
      "Epoch: 93/100 | step: 265/422 | loss: 0.025039300322532654\n",
      "Epoch: 93/100 | step: 266/422 | loss: 0.010072387754917145\n",
      "Epoch: 93/100 | step: 267/422 | loss: 0.006703255232423544\n",
      "Epoch: 93/100 | step: 268/422 | loss: 0.01576237753033638\n",
      "Epoch: 93/100 | step: 269/422 | loss: 0.01439814642071724\n",
      "Epoch: 93/100 | step: 270/422 | loss: 0.006190922111272812\n",
      "Epoch: 93/100 | step: 271/422 | loss: 0.030865659937262535\n",
      "Epoch: 93/100 | step: 272/422 | loss: 0.012104456312954426\n",
      "Epoch: 93/100 | step: 273/422 | loss: 0.01266541238874197\n",
      "Epoch: 93/100 | step: 274/422 | loss: 0.013971494510769844\n",
      "Epoch: 93/100 | step: 275/422 | loss: 0.011042733676731586\n",
      "Epoch: 93/100 | step: 276/422 | loss: 0.006265630014240742\n",
      "Epoch: 93/100 | step: 277/422 | loss: 0.012690837495028973\n",
      "Epoch: 93/100 | step: 278/422 | loss: 0.009239035658538342\n",
      "Epoch: 93/100 | step: 279/422 | loss: 0.009416033513844013\n",
      "Epoch: 93/100 | step: 280/422 | loss: 0.01098333578556776\n",
      "Epoch: 93/100 | step: 281/422 | loss: 0.011963597498834133\n",
      "Epoch: 93/100 | step: 282/422 | loss: 0.02475937455892563\n",
      "Epoch: 93/100 | step: 283/422 | loss: 0.009717116132378578\n",
      "Epoch: 93/100 | step: 284/422 | loss: 0.015032505616545677\n",
      "Epoch: 93/100 | step: 285/422 | loss: 0.01574443094432354\n",
      "Epoch: 93/100 | step: 286/422 | loss: 0.01241143699735403\n",
      "Epoch: 93/100 | step: 287/422 | loss: 0.007676438428461552\n",
      "Epoch: 93/100 | step: 288/422 | loss: 0.006263562012463808\n",
      "Epoch: 93/100 | step: 289/422 | loss: 0.007562458980828524\n",
      "Epoch: 93/100 | step: 290/422 | loss: 0.011621926911175251\n",
      "Epoch: 93/100 | step: 291/422 | loss: 0.019647035747766495\n",
      "Epoch: 93/100 | step: 292/422 | loss: 0.011664839461445808\n",
      "Epoch: 93/100 | step: 293/422 | loss: 0.00851848442107439\n",
      "Epoch: 93/100 | step: 294/422 | loss: 0.01535818725824356\n",
      "Epoch: 93/100 | step: 295/422 | loss: 0.012533215805888176\n",
      "Epoch: 93/100 | step: 296/422 | loss: 0.01583099737763405\n",
      "Epoch: 93/100 | step: 297/422 | loss: 0.012613298371434212\n",
      "Epoch: 93/100 | step: 298/422 | loss: 0.040803227573633194\n",
      "Epoch: 93/100 | step: 299/422 | loss: 0.010166607797145844\n",
      "Epoch: 93/100 | step: 300/422 | loss: 0.010850954800844193\n",
      "Epoch: 93/100 | step: 301/422 | loss: 0.02827909216284752\n",
      "Epoch: 93/100 | step: 302/422 | loss: 0.04146800562739372\n",
      "Epoch: 93/100 | step: 303/422 | loss: 0.0080984802916646\n",
      "Epoch: 93/100 | step: 304/422 | loss: 0.02745613269507885\n",
      "Epoch: 93/100 | step: 305/422 | loss: 0.008506347425282001\n",
      "Epoch: 93/100 | step: 306/422 | loss: 0.007153336890041828\n",
      "Epoch: 93/100 | step: 307/422 | loss: 0.015558364801108837\n",
      "Epoch: 93/100 | step: 308/422 | loss: 0.018189238384366035\n",
      "Epoch: 93/100 | step: 309/422 | loss: 0.007292401511222124\n",
      "Epoch: 93/100 | step: 310/422 | loss: 0.009400214068591595\n",
      "Epoch: 93/100 | step: 311/422 | loss: 0.008647420443594456\n",
      "Epoch: 93/100 | step: 312/422 | loss: 0.0524032823741436\n",
      "Epoch: 93/100 | step: 313/422 | loss: 0.06355895102024078\n",
      "Epoch: 93/100 | step: 314/422 | loss: 0.022885367274284363\n",
      "Epoch: 93/100 | step: 315/422 | loss: 0.05787089467048645\n",
      "Epoch: 93/100 | step: 316/422 | loss: 0.10831989347934723\n",
      "Epoch: 93/100 | step: 317/422 | loss: 0.023157482966780663\n",
      "Epoch: 93/100 | step: 318/422 | loss: 0.012634438462555408\n",
      "Epoch: 93/100 | step: 319/422 | loss: 0.009951446205377579\n",
      "Epoch: 93/100 | step: 320/422 | loss: 0.044692493975162506\n",
      "Epoch: 93/100 | step: 321/422 | loss: 0.01894179731607437\n",
      "Epoch: 93/100 | step: 322/422 | loss: 0.012351804412901402\n",
      "Epoch: 93/100 | step: 323/422 | loss: 0.009014279581606388\n",
      "Epoch: 93/100 | step: 324/422 | loss: 0.018001511693000793\n",
      "Epoch: 93/100 | step: 325/422 | loss: 0.00946231558918953\n",
      "Epoch: 93/100 | step: 326/422 | loss: 0.11315500736236572\n",
      "Epoch: 93/100 | step: 327/422 | loss: 0.016683783382177353\n",
      "Epoch: 93/100 | step: 328/422 | loss: 0.009565407410264015\n",
      "Epoch: 93/100 | step: 329/422 | loss: 0.07196968793869019\n",
      "Epoch: 93/100 | step: 330/422 | loss: 0.03059757687151432\n",
      "Epoch: 93/100 | step: 331/422 | loss: 0.015142296440899372\n",
      "Epoch: 93/100 | step: 332/422 | loss: 0.009269158355891705\n",
      "Epoch: 93/100 | step: 333/422 | loss: 0.02261936292052269\n",
      "Epoch: 93/100 | step: 334/422 | loss: 0.02992374449968338\n",
      "Epoch: 93/100 | step: 335/422 | loss: 0.00741430651396513\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 94/100 | step: 1/422 | loss: 0.010498188436031342\n",
      "Epoch: 94/100 | step: 2/422 | loss: 0.009081712923943996\n",
      "Epoch: 94/100 | step: 3/422 | loss: 0.017959440127015114\n",
      "Epoch: 94/100 | step: 4/422 | loss: 0.009534957818686962\n",
      "Epoch: 94/100 | step: 5/422 | loss: 0.00959082879126072\n",
      "Epoch: 94/100 | step: 6/422 | loss: 0.010396516881883144\n",
      "Epoch: 94/100 | step: 7/422 | loss: 0.008393540978431702\n",
      "Epoch: 94/100 | step: 8/422 | loss: 0.006306874565780163\n",
      "Epoch: 94/100 | step: 9/422 | loss: 0.01677894778549671\n",
      "Epoch: 94/100 | step: 10/422 | loss: 0.007239374797791243\n",
      "Epoch: 94/100 | step: 11/422 | loss: 0.007934244349598885\n",
      "Epoch: 94/100 | step: 12/422 | loss: 0.005332853179425001\n",
      "Epoch: 94/100 | step: 13/422 | loss: 0.009778277017176151\n",
      "Epoch: 94/100 | step: 14/422 | loss: 0.005751502234488726\n",
      "Epoch: 94/100 | step: 15/422 | loss: 0.00927271880209446\n",
      "Epoch: 94/100 | step: 16/422 | loss: 0.004000805784016848\n",
      "Epoch: 94/100 | step: 17/422 | loss: 0.008630096912384033\n",
      "Epoch: 94/100 | step: 18/422 | loss: 0.002893011085689068\n",
      "Epoch: 94/100 | step: 19/422 | loss: 0.008740706369280815\n",
      "Epoch: 94/100 | step: 20/422 | loss: 0.006309177726507187\n",
      "Epoch: 94/100 | step: 21/422 | loss: 0.00724583538249135\n",
      "Epoch: 94/100 | step: 22/422 | loss: 0.024933477863669395\n",
      "Epoch: 94/100 | step: 23/422 | loss: 0.00837891548871994\n",
      "Epoch: 94/100 | step: 24/422 | loss: 0.005659532267600298\n",
      "Epoch: 94/100 | step: 25/422 | loss: 0.009351656772196293\n",
      "Epoch: 94/100 | step: 26/422 | loss: 0.007791480980813503\n",
      "Epoch: 94/100 | step: 27/422 | loss: 0.014272721484303474\n",
      "Epoch: 94/100 | step: 28/422 | loss: 0.008179283700883389\n",
      "Epoch: 94/100 | step: 29/422 | loss: 0.009989353828132153\n",
      "Epoch: 94/100 | step: 30/422 | loss: 0.007331348955631256\n",
      "Epoch: 94/100 | step: 31/422 | loss: 0.005689963698387146\n",
      "Epoch: 94/100 | step: 32/422 | loss: 0.005607725586742163\n",
      "Epoch: 94/100 | step: 33/422 | loss: 0.010645641945302486\n",
      "Epoch: 94/100 | step: 34/422 | loss: 0.0053309001959860325\n",
      "Epoch: 94/100 | step: 35/422 | loss: 0.009458933025598526\n",
      "Epoch: 94/100 | step: 36/422 | loss: 0.006854405626654625\n",
      "Epoch: 94/100 | step: 37/422 | loss: 0.006474936380982399\n",
      "Epoch: 94/100 | step: 38/422 | loss: 0.007376232650130987\n",
      "Epoch: 94/100 | step: 39/422 | loss: 0.006372199393808842\n",
      "Epoch: 94/100 | step: 40/422 | loss: 0.0063823251985013485\n",
      "Epoch: 94/100 | step: 41/422 | loss: 0.011170484125614166\n",
      "Epoch: 94/100 | step: 42/422 | loss: 0.007518822327256203\n",
      "Epoch: 94/100 | step: 43/422 | loss: 0.0060049910098314285\n",
      "Epoch: 94/100 | step: 44/422 | loss: 0.006625832989811897\n",
      "Epoch: 94/100 | step: 45/422 | loss: 0.00788052286952734\n",
      "Epoch: 94/100 | step: 46/422 | loss: 0.01421435084193945\n",
      "Epoch: 94/100 | step: 47/422 | loss: 0.008898775093257427\n",
      "Epoch: 94/100 | step: 48/422 | loss: 0.009775768034160137\n",
      "Epoch: 94/100 | step: 49/422 | loss: 0.008159950375556946\n",
      "Epoch: 94/100 | step: 50/422 | loss: 0.003544835140928626\n",
      "Epoch: 94/100 | step: 51/422 | loss: 0.0041150888428092\n",
      "Epoch: 94/100 | step: 52/422 | loss: 0.009047421626746655\n",
      "Epoch: 94/100 | step: 53/422 | loss: 0.005544336512684822\n",
      "Epoch: 94/100 | step: 54/422 | loss: 0.009337450377643108\n",
      "Epoch: 94/100 | step: 55/422 | loss: 0.004900485277175903\n",
      "Epoch: 94/100 | step: 56/422 | loss: 0.008737461641430855\n",
      "Epoch: 94/100 | step: 57/422 | loss: 0.013741275295615196\n",
      "Epoch: 94/100 | step: 58/422 | loss: 0.007840236648917198\n",
      "Epoch: 94/100 | step: 59/422 | loss: 0.005666779819875956\n",
      "Epoch: 94/100 | step: 60/422 | loss: 0.0066057248041033745\n",
      "Epoch: 94/100 | step: 61/422 | loss: 0.008463975973427296\n",
      "Epoch: 94/100 | step: 62/422 | loss: 0.006844762247055769\n",
      "Epoch: 94/100 | step: 63/422 | loss: 0.004903919063508511\n",
      "Epoch: 94/100 | step: 64/422 | loss: 0.007036587223410606\n",
      "Epoch: 94/100 | step: 65/422 | loss: 0.009577739052474499\n",
      "Epoch: 94/100 | step: 66/422 | loss: 0.0067643821239471436\n",
      "Epoch: 94/100 | step: 67/422 | loss: 0.011426894925534725\n",
      "Epoch: 94/100 | step: 68/422 | loss: 0.005318630952388048\n",
      "Epoch: 94/100 | step: 69/422 | loss: 0.005106639117002487\n",
      "Epoch: 94/100 | step: 70/422 | loss: 0.009601470082998276\n",
      "Epoch: 94/100 | step: 71/422 | loss: 0.005385720171034336\n",
      "Epoch: 94/100 | step: 72/422 | loss: 0.005505705252289772\n",
      "Epoch: 94/100 | step: 73/422 | loss: 0.007076780777424574\n",
      "Epoch: 94/100 | step: 74/422 | loss: 0.00912398286163807\n",
      "Epoch: 94/100 | step: 75/422 | loss: 0.008571834303438663\n",
      "Epoch: 94/100 | step: 76/422 | loss: 0.0075543224811553955\n",
      "Epoch: 94/100 | step: 77/422 | loss: 0.011027885600924492\n",
      "Epoch: 94/100 | step: 78/422 | loss: 0.009873730130493641\n",
      "Epoch: 94/100 | step: 79/422 | loss: 0.007071955129504204\n",
      "Epoch: 94/100 | step: 80/422 | loss: 0.01591203175485134\n",
      "Epoch: 94/100 | step: 81/422 | loss: 0.0047380635514855385\n",
      "Epoch: 94/100 | step: 82/422 | loss: 0.004229468293488026\n",
      "Epoch: 94/100 | step: 83/422 | loss: 0.006072796881198883\n",
      "Epoch: 94/100 | step: 84/422 | loss: 0.008480674587190151\n",
      "Epoch: 94/100 | step: 85/422 | loss: 0.005183861590921879\n",
      "Epoch: 94/100 | step: 86/422 | loss: 0.0077337659895420074\n",
      "Epoch: 94/100 | step: 87/422 | loss: 0.004687047563493252\n",
      "Epoch: 94/100 | step: 88/422 | loss: 0.006543118506669998\n",
      "Epoch: 94/100 | step: 89/422 | loss: 0.00911223515868187\n",
      "Epoch: 94/100 | step: 90/422 | loss: 0.01786184310913086\n",
      "Epoch: 94/100 | step: 91/422 | loss: 0.011493781581521034\n",
      "Epoch: 94/100 | step: 92/422 | loss: 0.010384202934801579\n",
      "Epoch: 94/100 | step: 93/422 | loss: 0.008814031258225441\n",
      "Epoch: 94/100 | step: 94/422 | loss: 0.012126392684876919\n",
      "Epoch: 94/100 | step: 95/422 | loss: 0.008890808559954166\n",
      "Epoch: 94/100 | step: 96/422 | loss: 0.004785250872373581\n",
      "Epoch: 94/100 | step: 97/422 | loss: 0.00479921605437994\n",
      "Epoch: 94/100 | step: 98/422 | loss: 0.006349030416458845\n",
      "Epoch: 94/100 | step: 99/422 | loss: 0.009084396995604038\n",
      "Epoch: 94/100 | step: 100/422 | loss: 0.005000803619623184\n",
      "Epoch: 94/100 | step: 101/422 | loss: 0.008990342728793621\n",
      "Epoch: 94/100 | step: 102/422 | loss: 0.005513269919902086\n",
      "Epoch: 94/100 | step: 103/422 | loss: 0.006921387743204832\n",
      "Epoch: 94/100 | step: 104/422 | loss: 0.006579006090760231\n",
      "Epoch: 94/100 | step: 105/422 | loss: 0.008678818121552467\n",
      "Epoch: 94/100 | step: 106/422 | loss: 0.008771372027695179\n",
      "Epoch: 94/100 | step: 107/422 | loss: 0.014291085302829742\n",
      "Epoch: 94/100 | step: 108/422 | loss: 0.013993020169436932\n",
      "Epoch: 94/100 | step: 109/422 | loss: 0.01911320351064205\n",
      "Epoch: 94/100 | step: 110/422 | loss: 0.009651555679738522\n",
      "Epoch: 94/100 | step: 111/422 | loss: 0.010922014713287354\n",
      "Epoch: 94/100 | step: 112/422 | loss: 0.009556672535836697\n",
      "Epoch: 94/100 | step: 113/422 | loss: 0.003840184072032571\n",
      "Epoch: 94/100 | step: 114/422 | loss: 0.011590501293540001\n",
      "Epoch: 94/100 | step: 115/422 | loss: 0.007026263512670994\n",
      "Epoch: 94/100 | step: 116/422 | loss: 0.006759582087397575\n",
      "Epoch: 94/100 | step: 117/422 | loss: 0.006102351937443018\n",
      "Epoch: 94/100 | step: 118/422 | loss: 0.00447385199368\n",
      "Epoch: 94/100 | step: 119/422 | loss: 0.007246762979775667\n",
      "Epoch: 94/100 | step: 120/422 | loss: 0.006613906007260084\n",
      "Epoch: 94/100 | step: 121/422 | loss: 0.007540079299360514\n",
      "Epoch: 94/100 | step: 122/422 | loss: 0.011222559958696365\n",
      "Epoch: 94/100 | step: 123/422 | loss: 0.012572424486279488\n",
      "Epoch: 94/100 | step: 124/422 | loss: 0.005237154196947813\n",
      "Epoch: 94/100 | step: 125/422 | loss: 0.008206257596611977\n",
      "Epoch: 94/100 | step: 126/422 | loss: 0.008016958832740784\n",
      "Epoch: 94/100 | step: 127/422 | loss: 0.007833017967641354\n",
      "Epoch: 94/100 | step: 128/422 | loss: 0.02002803236246109\n",
      "Epoch: 94/100 | step: 129/422 | loss: 0.006283450406044722\n",
      "Epoch: 94/100 | step: 130/422 | loss: 0.007457432337105274\n",
      "Epoch: 94/100 | step: 131/422 | loss: 0.008291862905025482\n",
      "Epoch: 94/100 | step: 132/422 | loss: 0.008239592425525188\n",
      "Epoch: 94/100 | step: 133/422 | loss: 0.01043295580893755\n",
      "Epoch: 94/100 | step: 134/422 | loss: 0.008442564867436886\n",
      "Epoch: 94/100 | step: 135/422 | loss: 0.012489518150687218\n",
      "Epoch: 94/100 | step: 136/422 | loss: 0.0076742409728467464\n",
      "Epoch: 94/100 | step: 137/422 | loss: 0.0043840473517775536\n",
      "Epoch: 94/100 | step: 138/422 | loss: 0.012381475418806076\n",
      "Epoch: 94/100 | step: 139/422 | loss: 0.005485305096954107\n",
      "Epoch: 94/100 | step: 140/422 | loss: 0.006421142257750034\n",
      "Epoch: 94/100 | step: 141/422 | loss: 0.014274580404162407\n",
      "Epoch: 94/100 | step: 142/422 | loss: 0.012005385011434555\n",
      "Epoch: 94/100 | step: 143/422 | loss: 0.016410937532782555\n",
      "Epoch: 94/100 | step: 144/422 | loss: 0.0071923197247087955\n",
      "Epoch: 94/100 | step: 145/422 | loss: 0.007313738111406565\n",
      "Epoch: 94/100 | step: 146/422 | loss: 0.005720939487218857\n",
      "Epoch: 94/100 | step: 147/422 | loss: 0.008894890546798706\n",
      "Epoch: 94/100 | step: 148/422 | loss: 0.00932309590280056\n",
      "Epoch: 94/100 | step: 149/422 | loss: 0.004639471415430307\n",
      "Epoch: 94/100 | step: 150/422 | loss: 0.005447084084153175\n",
      "Epoch: 94/100 | step: 151/422 | loss: 0.009793324396014214\n",
      "Epoch: 94/100 | step: 152/422 | loss: 0.006762240082025528\n",
      "Epoch: 94/100 | step: 153/422 | loss: 0.006700276397168636\n",
      "Epoch: 94/100 | step: 154/422 | loss: 0.00618794746696949\n",
      "Epoch: 94/100 | step: 155/422 | loss: 0.005976066458970308\n",
      "Epoch: 94/100 | step: 156/422 | loss: 0.006800307892262936\n",
      "Epoch: 94/100 | step: 157/422 | loss: 0.008740308694541454\n",
      "Epoch: 94/100 | step: 158/422 | loss: 0.010421989485621452\n",
      "Epoch: 94/100 | step: 159/422 | loss: 0.008424066938459873\n",
      "Epoch: 94/100 | step: 160/422 | loss: 0.009891805239021778\n",
      "Epoch: 94/100 | step: 161/422 | loss: 0.0058222184889018536\n",
      "Epoch: 94/100 | step: 162/422 | loss: 0.008198097348213196\n",
      "Epoch: 94/100 | step: 163/422 | loss: 0.011431037448346615\n",
      "Epoch: 94/100 | step: 164/422 | loss: 0.004843780770897865\n",
      "Epoch: 94/100 | step: 165/422 | loss: 0.01533979270607233\n",
      "Epoch: 94/100 | step: 166/422 | loss: 0.004010732285678387\n",
      "Epoch: 94/100 | step: 167/422 | loss: 0.007751832716166973\n",
      "Epoch: 94/100 | step: 168/422 | loss: 0.005830617155879736\n",
      "Epoch: 94/100 | step: 169/422 | loss: 0.004915006924420595\n",
      "Epoch: 94/100 | step: 170/422 | loss: 0.009268736466765404\n",
      "Epoch: 94/100 | step: 171/422 | loss: 0.01204385980963707\n",
      "Epoch: 94/100 | step: 172/422 | loss: 0.007130822166800499\n",
      "Epoch: 94/100 | step: 173/422 | loss: 0.008526531048119068\n",
      "Epoch: 94/100 | step: 174/422 | loss: 0.004845699295401573\n",
      "Epoch: 94/100 | step: 175/422 | loss: 0.004941592924296856\n",
      "Epoch: 94/100 | step: 176/422 | loss: 0.007098384667187929\n",
      "Epoch: 94/100 | step: 177/422 | loss: 0.008122267201542854\n",
      "Epoch: 94/100 | step: 178/422 | loss: 0.004631322808563709\n",
      "Epoch: 94/100 | step: 179/422 | loss: 0.006908292416483164\n",
      "Epoch: 94/100 | step: 180/422 | loss: 0.010532519780099392\n",
      "Epoch: 94/100 | step: 181/422 | loss: 0.007146479096263647\n",
      "Epoch: 94/100 | step: 182/422 | loss: 0.006642448250204325\n",
      "Epoch: 94/100 | step: 183/422 | loss: 0.007425899151712656\n",
      "Epoch: 94/100 | step: 184/422 | loss: 0.011668846942484379\n",
      "Epoch: 94/100 | step: 185/422 | loss: 0.006223671138286591\n",
      "Epoch: 94/100 | step: 186/422 | loss: 0.00992241594940424\n",
      "Epoch: 94/100 | step: 187/422 | loss: 0.00849140528589487\n",
      "Epoch: 94/100 | step: 188/422 | loss: 0.011393283493816853\n",
      "Epoch: 94/100 | step: 189/422 | loss: 0.007526726461946964\n",
      "Epoch: 94/100 | step: 190/422 | loss: 0.005881941877305508\n",
      "Epoch: 94/100 | step: 191/422 | loss: 0.007080626208335161\n",
      "Epoch: 94/100 | step: 192/422 | loss: 0.004674206487834454\n",
      "Epoch: 94/100 | step: 193/422 | loss: 0.007876326330006123\n",
      "Epoch: 94/100 | step: 194/422 | loss: 0.0060844821855425835\n",
      "Epoch: 94/100 | step: 195/422 | loss: 0.005322299897670746\n",
      "Epoch: 94/100 | step: 196/422 | loss: 0.02109338343143463\n",
      "Epoch: 94/100 | step: 197/422 | loss: 0.005386128555983305\n",
      "Epoch: 94/100 | step: 198/422 | loss: 0.011115459725260735\n",
      "Epoch: 94/100 | step: 199/422 | loss: 0.015047420747578144\n",
      "Epoch: 94/100 | step: 200/422 | loss: 0.0072753774002194405\n",
      "Epoch: 94/100 | step: 201/422 | loss: 0.014038537628948689\n",
      "Epoch: 94/100 | step: 202/422 | loss: 0.005096400156617165\n",
      "Epoch: 94/100 | step: 203/422 | loss: 0.007978450506925583\n",
      "Epoch: 94/100 | step: 204/422 | loss: 0.009956946596503258\n",
      "Epoch: 94/100 | step: 205/422 | loss: 0.00811950396746397\n",
      "Epoch: 94/100 | step: 206/422 | loss: 0.006071461830288172\n",
      "Epoch: 94/100 | step: 207/422 | loss: 0.006863294169306755\n",
      "Epoch: 94/100 | step: 208/422 | loss: 0.009389449842274189\n",
      "Epoch: 94/100 | step: 209/422 | loss: 0.009985333308577538\n",
      "Epoch: 94/100 | step: 210/422 | loss: 0.011003118008375168\n",
      "Epoch: 94/100 | step: 211/422 | loss: 0.007395231630653143\n",
      "Epoch: 94/100 | step: 212/422 | loss: 0.010477394796907902\n",
      "Epoch: 94/100 | step: 213/422 | loss: 0.011568485759198666\n",
      "Epoch: 94/100 | step: 214/422 | loss: 0.005971955601125956\n",
      "Epoch: 94/100 | step: 215/422 | loss: 0.0071655442006886005\n",
      "Epoch: 94/100 | step: 216/422 | loss: 0.010851622559130192\n",
      "Epoch: 94/100 | step: 217/422 | loss: 0.010142392478883266\n",
      "Epoch: 94/100 | step: 218/422 | loss: 0.005486435256898403\n",
      "Epoch: 94/100 | step: 219/422 | loss: 0.007090480532497168\n",
      "Epoch: 94/100 | step: 220/422 | loss: 0.007710996549576521\n",
      "Epoch: 94/100 | step: 221/422 | loss: 0.003615845926105976\n",
      "Epoch: 94/100 | step: 222/422 | loss: 0.00851451139897108\n",
      "Epoch: 94/100 | step: 223/422 | loss: 0.006703608203679323\n",
      "Epoch: 94/100 | step: 224/422 | loss: 0.007394789252430201\n",
      "Epoch: 94/100 | step: 225/422 | loss: 0.005027344450354576\n",
      "Epoch: 94/100 | step: 226/422 | loss: 0.00893780030310154\n",
      "Epoch: 94/100 | step: 227/422 | loss: 0.006304455455392599\n",
      "Epoch: 94/100 | step: 228/422 | loss: 0.08676284551620483\n",
      "Epoch: 94/100 | step: 229/422 | loss: 0.02321542054414749\n",
      "Epoch: 94/100 | step: 230/422 | loss: 0.041165806353092194\n",
      "Epoch: 94/100 | step: 231/422 | loss: 0.06611372530460358\n",
      "Epoch: 94/100 | step: 232/422 | loss: 0.008951415307819843\n",
      "Epoch: 94/100 | step: 233/422 | loss: 0.0102285947650671\n",
      "Epoch: 94/100 | step: 234/422 | loss: 0.009106435813009739\n",
      "Epoch: 94/100 | step: 235/422 | loss: 0.04485417902469635\n",
      "Epoch: 94/100 | step: 236/422 | loss: 0.011115177534520626\n",
      "Epoch: 94/100 | step: 237/422 | loss: 0.009378585033118725\n",
      "Epoch: 94/100 | step: 238/422 | loss: 0.009427757002413273\n",
      "Epoch: 94/100 | step: 239/422 | loss: 0.008837022818624973\n",
      "Epoch: 94/100 | step: 240/422 | loss: 0.015895232558250427\n",
      "Epoch: 94/100 | step: 241/422 | loss: 0.02052968554198742\n",
      "Epoch: 94/100 | step: 242/422 | loss: 0.006043412256985903\n",
      "Epoch: 94/100 | step: 243/422 | loss: 0.008928141556680202\n",
      "Epoch: 94/100 | step: 244/422 | loss: 0.005974901374429464\n",
      "Epoch: 94/100 | step: 245/422 | loss: 0.015200678259134293\n",
      "Epoch: 94/100 | step: 246/422 | loss: 0.018233610317111015\n",
      "Epoch: 94/100 | step: 247/422 | loss: 0.01977941393852234\n",
      "Epoch: 94/100 | step: 248/422 | loss: 0.011262822896242142\n",
      "Epoch: 94/100 | step: 249/422 | loss: 0.005045442841947079\n",
      "Epoch: 94/100 | step: 250/422 | loss: 0.007032318506389856\n",
      "Epoch: 94/100 | step: 251/422 | loss: 0.016316169872879982\n",
      "Epoch: 94/100 | step: 252/422 | loss: 0.005623125936836004\n",
      "Epoch: 94/100 | step: 253/422 | loss: 0.004501814488321543\n",
      "Epoch: 94/100 | step: 254/422 | loss: 0.01779766194522381\n",
      "Epoch: 94/100 | step: 255/422 | loss: 0.00849410891532898\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 95/100 | step: 1/422 | loss: 0.008472029119729996\n",
      "Epoch: 95/100 | step: 2/422 | loss: 0.008337544277310371\n",
      "Epoch: 95/100 | step: 3/422 | loss: 0.00672039994969964\n",
      "Epoch: 95/100 | step: 4/422 | loss: 0.006494910456240177\n",
      "Epoch: 95/100 | step: 5/422 | loss: 0.00509692681953311\n",
      "Epoch: 95/100 | step: 6/422 | loss: 0.0046335370279848576\n",
      "Epoch: 95/100 | step: 7/422 | loss: 0.006231227889657021\n",
      "Epoch: 95/100 | step: 8/422 | loss: 0.014809214510023594\n",
      "Epoch: 95/100 | step: 9/422 | loss: 0.008452709764242172\n",
      "Epoch: 95/100 | step: 10/422 | loss: 0.007691456004977226\n",
      "Epoch: 95/100 | step: 11/422 | loss: 0.00590424332767725\n",
      "Epoch: 95/100 | step: 12/422 | loss: 0.005645411554723978\n",
      "Epoch: 95/100 | step: 13/422 | loss: 0.006199811119586229\n",
      "Epoch: 95/100 | step: 14/422 | loss: 0.006570412311702967\n",
      "Epoch: 95/100 | step: 15/422 | loss: 0.007747297175228596\n",
      "Epoch: 95/100 | step: 16/422 | loss: 0.011702528223395348\n",
      "Epoch: 95/100 | step: 17/422 | loss: 0.009486101567745209\n",
      "Epoch: 95/100 | step: 18/422 | loss: 0.005536733195185661\n",
      "Epoch: 95/100 | step: 19/422 | loss: 0.010782786644995213\n",
      "Epoch: 95/100 | step: 20/422 | loss: 0.0046933721750974655\n",
      "Epoch: 95/100 | step: 21/422 | loss: 0.0067517636343836784\n",
      "Epoch: 95/100 | step: 22/422 | loss: 0.0048501803539693356\n",
      "Epoch: 95/100 | step: 23/422 | loss: 0.010989952832460403\n",
      "Epoch: 95/100 | step: 24/422 | loss: 0.00715568196028471\n",
      "Epoch: 95/100 | step: 25/422 | loss: 0.007374809589236975\n",
      "Epoch: 95/100 | step: 26/422 | loss: 0.0058798156678676605\n",
      "Epoch: 95/100 | step: 27/422 | loss: 0.004625602159649134\n",
      "Epoch: 95/100 | step: 28/422 | loss: 0.006456507369875908\n",
      "Epoch: 95/100 | step: 29/422 | loss: 0.00755245890468359\n",
      "Epoch: 95/100 | step: 30/422 | loss: 0.006865968462079763\n",
      "Epoch: 95/100 | step: 31/422 | loss: 0.005512821953743696\n",
      "Epoch: 95/100 | step: 32/422 | loss: 0.007424424402415752\n",
      "Epoch: 95/100 | step: 33/422 | loss: 0.00977051630616188\n",
      "Epoch: 95/100 | step: 34/422 | loss: 0.005938442423939705\n",
      "Epoch: 95/100 | step: 35/422 | loss: 0.007403227966278791\n",
      "Epoch: 95/100 | step: 36/422 | loss: 0.0060536181554198265\n",
      "Epoch: 95/100 | step: 37/422 | loss: 0.0029649739153683186\n",
      "Epoch: 95/100 | step: 38/422 | loss: 0.007507507689297199\n",
      "Epoch: 95/100 | step: 39/422 | loss: 0.006505063269287348\n",
      "Epoch: 95/100 | step: 40/422 | loss: 0.005164509639143944\n",
      "Epoch: 95/100 | step: 41/422 | loss: 0.007234062533825636\n",
      "Epoch: 95/100 | step: 42/422 | loss: 0.012876966968178749\n",
      "Epoch: 95/100 | step: 43/422 | loss: 0.0044743153266608715\n",
      "Epoch: 95/100 | step: 44/422 | loss: 0.007998963817954063\n",
      "Epoch: 95/100 | step: 45/422 | loss: 0.005432409234344959\n",
      "Epoch: 95/100 | step: 46/422 | loss: 0.006327865179628134\n",
      "Epoch: 95/100 | step: 47/422 | loss: 0.006125916261225939\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 96/100 | step: 1/422 | loss: 0.005825466942042112\n",
      "Epoch: 96/100 | step: 2/422 | loss: 0.004635096061974764\n",
      "Epoch: 96/100 | step: 3/422 | loss: 0.004443800542503595\n",
      "Epoch: 96/100 | step: 4/422 | loss: 0.009425286203622818\n",
      "Epoch: 96/100 | step: 5/422 | loss: 0.004179227165877819\n",
      "Epoch: 96/100 | step: 6/422 | loss: 0.0075232721865177155\n",
      "Epoch: 96/100 | step: 7/422 | loss: 0.0036837609950453043\n",
      "Epoch: 96/100 | step: 8/422 | loss: 0.008515714667737484\n",
      "Epoch: 96/100 | step: 9/422 | loss: 0.006123482249677181\n",
      "Epoch: 96/100 | step: 10/422 | loss: 0.005973535589873791\n",
      "Epoch: 96/100 | step: 11/422 | loss: 0.003906514495611191\n",
      "Epoch: 96/100 | step: 12/422 | loss: 0.015366942621767521\n",
      "Epoch: 96/100 | step: 13/422 | loss: 0.009367771446704865\n",
      "Epoch: 96/100 | step: 14/422 | loss: 0.011407535523176193\n",
      "Epoch: 96/100 | step: 15/422 | loss: 0.0047439467161893845\n",
      "Epoch: 96/100 | step: 16/422 | loss: 0.005753686185926199\n",
      "Epoch: 96/100 | step: 17/422 | loss: 0.005398901179432869\n",
      "Epoch: 96/100 | step: 18/422 | loss: 0.005604819860309362\n",
      "Epoch: 96/100 | step: 19/422 | loss: 0.007203593384474516\n",
      "Epoch: 96/100 | step: 20/422 | loss: 0.0061373719945549965\n",
      "Epoch: 96/100 | step: 21/422 | loss: 0.005598124582320452\n",
      "Epoch: 96/100 | step: 22/422 | loss: 0.00645084772258997\n",
      "Epoch: 96/100 | step: 23/422 | loss: 0.003372948383912444\n",
      "Epoch: 96/100 | step: 24/422 | loss: 0.0065123517997562885\n",
      "Epoch: 96/100 | step: 25/422 | loss: 0.00660001439973712\n",
      "Epoch: 96/100 | step: 26/422 | loss: 0.0077032288536429405\n",
      "Epoch: 96/100 | step: 27/422 | loss: 0.00858970545232296\n",
      "Epoch: 96/100 | step: 28/422 | loss: 0.005076709669083357\n",
      "Epoch: 96/100 | step: 29/422 | loss: 0.0065681845881044865\n",
      "Epoch: 96/100 | step: 30/422 | loss: 0.006869558710604906\n",
      "Epoch: 96/100 | step: 31/422 | loss: 0.006678369361907244\n",
      "Epoch: 96/100 | step: 32/422 | loss: 0.005941242445260286\n",
      "Epoch: 96/100 | step: 33/422 | loss: 0.0069921743124723434\n",
      "Epoch: 96/100 | step: 34/422 | loss: 0.007322643417865038\n",
      "Epoch: 96/100 | step: 35/422 | loss: 0.004347184672951698\n",
      "Epoch: 96/100 | step: 36/422 | loss: 0.008358159102499485\n",
      "Epoch: 96/100 | step: 37/422 | loss: 0.016797363758087158\n",
      "Epoch: 96/100 | step: 38/422 | loss: 0.006413234397768974\n",
      "Epoch: 96/100 | step: 39/422 | loss: 0.005136496853083372\n",
      "Epoch: 96/100 | step: 40/422 | loss: 0.011379159986972809\n",
      "Epoch: 96/100 | step: 41/422 | loss: 0.010035558603703976\n",
      "Epoch: 96/100 | step: 42/422 | loss: 0.005969678051769733\n",
      "Epoch: 96/100 | step: 43/422 | loss: 0.006134158466011286\n",
      "Epoch: 96/100 | step: 44/422 | loss: 0.005163467489182949\n",
      "Epoch: 96/100 | step: 45/422 | loss: 0.005191744305193424\n",
      "Epoch: 96/100 | step: 46/422 | loss: 0.006140413228422403\n",
      "Epoch: 96/100 | step: 47/422 | loss: 0.006281699985265732\n",
      "Epoch: 96/100 | step: 48/422 | loss: 0.010686499066650867\n",
      "Epoch: 96/100 | step: 49/422 | loss: 0.004309457261115313\n",
      "Epoch: 96/100 | step: 50/422 | loss: 0.005921492353081703\n",
      "Epoch: 96/100 | step: 51/422 | loss: 0.0062317312695086\n",
      "Epoch: 96/100 | step: 52/422 | loss: 0.009518317878246307\n",
      "Epoch: 96/100 | step: 53/422 | loss: 0.0031674543861299753\n",
      "Epoch: 96/100 | step: 54/422 | loss: 0.006561859976500273\n",
      "Epoch: 96/100 | step: 55/422 | loss: 0.009019200690090656\n",
      "Epoch: 96/100 | step: 56/422 | loss: 0.0040734633803367615\n",
      "Epoch: 96/100 | step: 57/422 | loss: 0.005500331521034241\n",
      "Epoch: 96/100 | step: 58/422 | loss: 0.003396922955289483\n",
      "Epoch: 96/100 | step: 59/422 | loss: 0.005189605988562107\n",
      "Epoch: 96/100 | step: 60/422 | loss: 0.008009533397853374\n",
      "Epoch: 96/100 | step: 61/422 | loss: 0.0072989696636796\n",
      "Epoch: 96/100 | step: 62/422 | loss: 0.005677629262208939\n",
      "Epoch: 96/100 | step: 63/422 | loss: 0.007313427049666643\n",
      "Epoch: 96/100 | step: 64/422 | loss: 0.005263655446469784\n",
      "Epoch: 96/100 | step: 65/422 | loss: 0.005265354178845882\n",
      "Epoch: 96/100 | step: 66/422 | loss: 0.012446691282093525\n",
      "Epoch: 96/100 | step: 67/422 | loss: 0.00710423244163394\n",
      "Epoch: 96/100 | step: 68/422 | loss: 0.005825189873576164\n",
      "Epoch: 96/100 | step: 69/422 | loss: 0.005073072388768196\n",
      "Epoch: 96/100 | step: 70/422 | loss: 0.0048141698352992535\n",
      "Epoch: 96/100 | step: 71/422 | loss: 0.00527789955958724\n",
      "Epoch: 96/100 | step: 72/422 | loss: 0.008777143433690071\n",
      "Epoch: 96/100 | step: 73/422 | loss: 0.007017175666987896\n",
      "Epoch: 96/100 | step: 74/422 | loss: 0.005042864475399256\n",
      "Epoch: 96/100 | step: 75/422 | loss: 0.007363225799053907\n",
      "Epoch: 96/100 | step: 76/422 | loss: 0.006776758935302496\n",
      "Epoch: 96/100 | step: 77/422 | loss: 0.007532140240073204\n",
      "Epoch: 96/100 | step: 78/422 | loss: 0.0072991433553397655\n",
      "Epoch: 96/100 | step: 79/422 | loss: 0.007703832350671291\n",
      "Epoch: 96/100 | step: 80/422 | loss: 0.012106896378099918\n",
      "Epoch: 96/100 | step: 81/422 | loss: 0.005988179240375757\n",
      "Epoch: 96/100 | step: 82/422 | loss: 0.008084289729595184\n",
      "Epoch: 96/100 | step: 83/422 | loss: 0.0058458177372813225\n",
      "Epoch: 96/100 | step: 84/422 | loss: 0.00591119471937418\n",
      "Epoch: 96/100 | step: 85/422 | loss: 0.006564487237483263\n",
      "Epoch: 96/100 | step: 86/422 | loss: 0.00928475335240364\n",
      "Epoch: 96/100 | step: 87/422 | loss: 0.005487028043717146\n",
      "Epoch: 96/100 | step: 88/422 | loss: 0.007268534507602453\n",
      "Epoch: 96/100 | step: 89/422 | loss: 0.005127470474690199\n",
      "Epoch: 96/100 | step: 90/422 | loss: 0.004631066229194403\n",
      "Epoch: 96/100 | step: 91/422 | loss: 0.007950210943818092\n",
      "Epoch: 96/100 | step: 92/422 | loss: 0.00772508792579174\n",
      "Epoch: 96/100 | step: 93/422 | loss: 0.017023716121912003\n",
      "Epoch: 96/100 | step: 94/422 | loss: 0.005850477609783411\n",
      "Epoch: 96/100 | step: 95/422 | loss: 0.00858624093234539\n",
      "Epoch: 96/100 | step: 96/422 | loss: 0.005330412182956934\n",
      "Epoch: 96/100 | step: 97/422 | loss: 0.006218471098691225\n",
      "Epoch: 96/100 | step: 98/422 | loss: 0.005000462289899588\n",
      "Epoch: 96/100 | step: 99/422 | loss: 0.005496614146977663\n",
      "Epoch: 96/100 | step: 100/422 | loss: 0.010387127287685871\n",
      "Epoch: 96/100 | step: 101/422 | loss: 0.006808140315115452\n",
      "Epoch: 96/100 | step: 102/422 | loss: 0.0053167762234807014\n",
      "Epoch: 96/100 | step: 103/422 | loss: 0.02772374451160431\n",
      "Epoch: 96/100 | step: 104/422 | loss: 0.00963462982326746\n",
      "Epoch: 96/100 | step: 105/422 | loss: 0.007237585261464119\n",
      "Epoch: 96/100 | step: 106/422 | loss: 0.06619786471128464\n",
      "Epoch: 96/100 | step: 107/422 | loss: 0.0075186509639024734\n",
      "Epoch: 96/100 | step: 108/422 | loss: 0.006240398623049259\n",
      "Epoch: 96/100 | step: 109/422 | loss: 0.009275518357753754\n",
      "Epoch: 96/100 | step: 110/422 | loss: 0.008995722979307175\n",
      "Epoch: 96/100 | step: 111/422 | loss: 0.007516662590205669\n",
      "Epoch: 96/100 | step: 112/422 | loss: 0.00680094538256526\n",
      "Epoch: 96/100 | step: 113/422 | loss: 0.005080556031316519\n",
      "Epoch: 96/100 | step: 114/422 | loss: 0.006582127884030342\n",
      "Epoch: 96/100 | step: 115/422 | loss: 0.00512267928570509\n",
      "Epoch: 96/100 | step: 116/422 | loss: 0.006113207899034023\n",
      "Epoch: 96/100 | step: 117/422 | loss: 0.007514103781431913\n",
      "Epoch: 96/100 | step: 118/422 | loss: 0.0030986310448497534\n",
      "Epoch: 96/100 | step: 119/422 | loss: 0.012790356762707233\n",
      "Epoch: 96/100 | step: 120/422 | loss: 0.0037055741995573044\n",
      "Epoch: 96/100 | step: 121/422 | loss: 0.009939312934875488\n",
      "Epoch: 96/100 | step: 122/422 | loss: 0.005356818437576294\n",
      "Epoch: 96/100 | step: 123/422 | loss: 0.004672206472605467\n",
      "Epoch: 96/100 | step: 124/422 | loss: 0.005876544862985611\n",
      "Epoch: 96/100 | step: 125/422 | loss: 0.005560759920626879\n",
      "Epoch: 96/100 | step: 126/422 | loss: 0.004134606570005417\n",
      "Epoch: 96/100 | step: 127/422 | loss: 0.008500665426254272\n",
      "Epoch: 96/100 | step: 128/422 | loss: 0.008855626918375492\n",
      "Epoch: 96/100 | step: 129/422 | loss: 0.008096965029835701\n",
      "Epoch: 96/100 | step: 130/422 | loss: 0.007441586349159479\n",
      "Epoch: 96/100 | step: 131/422 | loss: 0.007247230503708124\n",
      "Epoch: 96/100 | step: 132/422 | loss: 0.007399437483400106\n",
      "Epoch: 96/100 | step: 133/422 | loss: 0.003936437424272299\n",
      "Epoch: 96/100 | step: 134/422 | loss: 0.006968495436012745\n",
      "Epoch: 96/100 | step: 135/422 | loss: 0.0033856204245239496\n",
      "Epoch: 96/100 | step: 136/422 | loss: 0.005019196309149265\n",
      "Epoch: 96/100 | step: 137/422 | loss: 0.0073873731307685375\n",
      "Epoch: 96/100 | step: 138/422 | loss: 0.0072731878608465195\n",
      "Epoch: 96/100 | step: 139/422 | loss: 0.011909122578799725\n",
      "Epoch: 96/100 | step: 140/422 | loss: 0.003681659698486328\n",
      "Epoch: 96/100 | step: 141/422 | loss: 0.0077786692418158054\n",
      "Epoch: 96/100 | step: 142/422 | loss: 0.004691219422966242\n",
      "Epoch: 96/100 | step: 143/422 | loss: 0.003408597782254219\n",
      "Epoch: 96/100 | step: 144/422 | loss: 0.005203158129006624\n",
      "Epoch: 96/100 | step: 145/422 | loss: 0.0043780989944934845\n",
      "Epoch: 96/100 | step: 146/422 | loss: 0.005715199746191502\n",
      "Epoch: 96/100 | step: 147/422 | loss: 0.005195154342800379\n",
      "Epoch: 96/100 | step: 148/422 | loss: 0.006865126546472311\n",
      "Epoch: 96/100 | step: 149/422 | loss: 0.005985396448522806\n",
      "Epoch: 96/100 | step: 150/422 | loss: 0.00474401842802763\n",
      "Epoch: 96/100 | step: 151/422 | loss: 0.004873496014624834\n",
      "Epoch: 96/100 | step: 152/422 | loss: 0.0065192957408726215\n",
      "Epoch: 96/100 | step: 153/422 | loss: 0.005744584370404482\n",
      "Epoch: 96/100 | step: 154/422 | loss: 0.004825165495276451\n",
      "Epoch: 96/100 | step: 155/422 | loss: 0.007803549990057945\n",
      "Epoch: 96/100 | step: 156/422 | loss: 0.01020889077335596\n",
      "Epoch: 96/100 | step: 157/422 | loss: 0.006776879075914621\n",
      "Epoch: 96/100 | step: 158/422 | loss: 0.00510699488222599\n",
      "Epoch: 96/100 | step: 159/422 | loss: 0.004858332220464945\n",
      "Epoch: 96/100 | step: 160/422 | loss: 0.0073646013624966145\n",
      "Epoch: 96/100 | step: 161/422 | loss: 0.006777208298444748\n",
      "Epoch: 96/100 | step: 162/422 | loss: 0.004072456620633602\n",
      "Epoch: 96/100 | step: 163/422 | loss: 0.0029052153695374727\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 97/100 | step: 1/422 | loss: 0.005540595389902592\n",
      "Epoch: 97/100 | step: 2/422 | loss: 0.005034418776631355\n",
      "Epoch: 97/100 | step: 3/422 | loss: 0.005644797347486019\n",
      "Epoch: 97/100 | step: 4/422 | loss: 0.006285401992499828\n",
      "Epoch: 97/100 | step: 5/422 | loss: 0.00874887965619564\n",
      "Epoch: 97/100 | step: 6/422 | loss: 0.005737678147852421\n",
      "Epoch: 97/100 | step: 7/422 | loss: 0.017491934821009636\n",
      "Epoch: 97/100 | step: 8/422 | loss: 0.016461946070194244\n",
      "Epoch: 97/100 | step: 9/422 | loss: 0.00588957080617547\n",
      "Epoch: 97/100 | step: 10/422 | loss: 0.0046334825456142426\n",
      "Epoch: 97/100 | step: 11/422 | loss: 0.004634495358914137\n",
      "Epoch: 97/100 | step: 12/422 | loss: 0.004987286403775215\n",
      "Epoch: 97/100 | step: 13/422 | loss: 0.004958163481205702\n",
      "Epoch: 97/100 | step: 14/422 | loss: 0.007779677398502827\n",
      "Epoch: 97/100 | step: 15/422 | loss: 0.006407477892935276\n",
      "Epoch: 97/100 | step: 16/422 | loss: 0.0037222295068204403\n",
      "Epoch: 97/100 | step: 17/422 | loss: 0.004787430167198181\n",
      "Epoch: 97/100 | step: 18/422 | loss: 0.006188401486724615\n",
      "Epoch: 97/100 | step: 19/422 | loss: 0.004166098777204752\n",
      "Epoch: 97/100 | step: 20/422 | loss: 0.0049489401280879974\n",
      "Epoch: 97/100 | step: 21/422 | loss: 0.008231191895902157\n",
      "Epoch: 97/100 | step: 22/422 | loss: 0.0052970414981245995\n",
      "Epoch: 97/100 | step: 23/422 | loss: 0.005610923748463392\n",
      "Epoch: 97/100 | step: 24/422 | loss: 0.005038255359977484\n",
      "Epoch: 97/100 | step: 25/422 | loss: 0.006400867365300655\n",
      "Epoch: 97/100 | step: 26/422 | loss: 0.007533443626016378\n",
      "Epoch: 97/100 | step: 27/422 | loss: 0.0046620918437838554\n",
      "Epoch: 97/100 | step: 28/422 | loss: 0.004208801314234734\n",
      "Epoch: 97/100 | step: 29/422 | loss: 0.005596226546913385\n",
      "Epoch: 97/100 | step: 30/422 | loss: 0.004791600629687309\n",
      "Epoch: 97/100 | step: 31/422 | loss: 0.006196904927492142\n",
      "Epoch: 97/100 | step: 32/422 | loss: 0.00491400295868516\n",
      "Epoch: 97/100 | step: 33/422 | loss: 0.010434622876346111\n",
      "Epoch: 97/100 | step: 34/422 | loss: 0.008713679388165474\n",
      "Epoch: 97/100 | step: 35/422 | loss: 0.0058366782031953335\n",
      "Epoch: 97/100 | step: 36/422 | loss: 0.01014029048383236\n",
      "Epoch: 97/100 | step: 37/422 | loss: 0.00840978417545557\n",
      "Epoch: 97/100 | step: 38/422 | loss: 0.004315031226724386\n",
      "Epoch: 97/100 | step: 39/422 | loss: 0.00333516881801188\n",
      "Epoch: 97/100 | step: 40/422 | loss: 0.005610517226159573\n",
      "Epoch: 97/100 | step: 41/422 | loss: 0.004323567729443312\n",
      "Epoch: 97/100 | step: 42/422 | loss: 0.004007990472018719\n",
      "Epoch: 97/100 | step: 43/422 | loss: 0.005916423164308071\n",
      "Epoch: 97/100 | step: 44/422 | loss: 0.0032444593962281942\n",
      "Epoch: 97/100 | step: 45/422 | loss: 0.005514376796782017\n",
      "Epoch: 97/100 | step: 46/422 | loss: 0.006204638630151749\n",
      "Epoch: 97/100 | step: 47/422 | loss: 0.007475713733583689\n",
      "Epoch: 97/100 | step: 48/422 | loss: 0.013585072942078114\n",
      "Epoch: 97/100 | step: 49/422 | loss: 0.005332347471266985\n",
      "Epoch: 97/100 | step: 50/422 | loss: 0.006515226326882839\n",
      "Epoch: 97/100 | step: 51/422 | loss: 0.005518479738384485\n",
      "Epoch: 97/100 | step: 52/422 | loss: 0.0032509611919522285\n",
      "Epoch: 97/100 | step: 53/422 | loss: 0.01083169225603342\n",
      "Epoch: 97/100 | step: 54/422 | loss: 0.01414128951728344\n",
      "Epoch: 97/100 | step: 55/422 | loss: 0.007430819794535637\n",
      "Epoch: 97/100 | step: 56/422 | loss: 0.006685471627861261\n",
      "Epoch: 97/100 | step: 57/422 | loss: 0.00663354666903615\n",
      "Epoch: 97/100 | step: 58/422 | loss: 0.019090110436081886\n",
      "Epoch: 97/100 | step: 59/422 | loss: 0.006840159650892019\n",
      "Epoch: 97/100 | step: 60/422 | loss: 0.005136793479323387\n",
      "Epoch: 97/100 | step: 61/422 | loss: 0.0031980331987142563\n",
      "Epoch: 97/100 | step: 62/422 | loss: 0.008579988963901997\n",
      "Epoch: 97/100 | step: 63/422 | loss: 0.006832765880972147\n",
      "Epoch: 97/100 | step: 64/422 | loss: 0.016527950763702393\n",
      "Epoch: 97/100 | step: 65/422 | loss: 0.007092984393239021\n",
      "Epoch: 97/100 | step: 66/422 | loss: 0.01062325295060873\n",
      "Epoch: 97/100 | step: 67/422 | loss: 0.00787778664380312\n",
      "Epoch: 97/100 | step: 68/422 | loss: 0.005821740720421076\n",
      "Epoch: 97/100 | step: 69/422 | loss: 0.005023083183914423\n",
      "Epoch: 97/100 | step: 70/422 | loss: 0.006044850219041109\n",
      "Epoch: 97/100 | step: 71/422 | loss: 0.0062032826244831085\n",
      "Epoch: 97/100 | step: 72/422 | loss: 0.005680336616933346\n",
      "Epoch: 97/100 | step: 73/422 | loss: 0.0077317748218774796\n",
      "Epoch: 97/100 | step: 74/422 | loss: 0.0048147523775696754\n",
      "Epoch: 97/100 | step: 75/422 | loss: 0.00961847510188818\n",
      "Epoch: 97/100 | step: 76/422 | loss: 0.0030982878524810076\n",
      "Epoch: 97/100 | step: 77/422 | loss: 0.006815673783421516\n",
      "Epoch: 97/100 | step: 78/422 | loss: 0.009078963659703732\n",
      "Epoch: 97/100 | step: 79/422 | loss: 0.01505052950233221\n",
      "Epoch: 97/100 | step: 80/422 | loss: 0.005600848235189915\n",
      "Epoch: 97/100 | step: 81/422 | loss: 0.005081693176180124\n",
      "Epoch: 97/100 | step: 82/422 | loss: 0.005963754840195179\n",
      "Epoch: 97/100 | step: 83/422 | loss: 0.010049525648355484\n",
      "Epoch: 97/100 | step: 84/422 | loss: 0.007850647903978825\n",
      "Epoch: 97/100 | step: 85/422 | loss: 0.009163852781057358\n",
      "Epoch: 97/100 | step: 86/422 | loss: 0.0072793359868228436\n",
      "Epoch: 97/100 | step: 87/422 | loss: 0.007203925866633654\n",
      "Epoch: 97/100 | step: 88/422 | loss: 0.005499052349478006\n",
      "Epoch: 97/100 | step: 89/422 | loss: 0.0042841630056500435\n",
      "Epoch: 97/100 | step: 90/422 | loss: 0.007765313610434532\n",
      "Epoch: 97/100 | step: 91/422 | loss: 0.004927025176584721\n",
      "Epoch: 97/100 | step: 92/422 | loss: 0.004673783667385578\n",
      "Epoch: 97/100 | step: 93/422 | loss: 0.0056081428192555904\n",
      "Epoch: 97/100 | step: 94/422 | loss: 0.008237899281084538\n",
      "Epoch: 97/100 | step: 95/422 | loss: 0.008248482830822468\n",
      "Epoch: 97/100 | step: 96/422 | loss: 0.005503975786268711\n",
      "Epoch: 97/100 | step: 97/422 | loss: 0.004179574549198151\n",
      "Epoch: 97/100 | step: 98/422 | loss: 0.004158643539994955\n",
      "Epoch: 97/100 | step: 99/422 | loss: 0.004756274167448282\n",
      "Epoch: 97/100 | step: 100/422 | loss: 0.005815624725073576\n",
      "Epoch: 97/100 | step: 101/422 | loss: 0.01032266765832901\n",
      "Epoch: 97/100 | step: 102/422 | loss: 0.009851567447185516\n",
      "Epoch: 97/100 | step: 103/422 | loss: 0.00510676484555006\n",
      "Epoch: 97/100 | step: 104/422 | loss: 0.008574295789003372\n",
      "Epoch: 97/100 | step: 105/422 | loss: 0.005268569570034742\n",
      "Epoch: 97/100 | step: 106/422 | loss: 0.004399060737341642\n",
      "Epoch: 97/100 | step: 107/422 | loss: 0.004634513985365629\n",
      "Epoch: 97/100 | step: 108/422 | loss: 0.004694376606494188\n",
      "Epoch: 97/100 | step: 109/422 | loss: 0.006419001612812281\n",
      "Epoch: 97/100 | step: 110/422 | loss: 0.007620432414114475\n",
      "Epoch: 97/100 | step: 111/422 | loss: 0.004457900766283274\n",
      "Epoch: 97/100 | step: 112/422 | loss: 0.0037805105093866587\n",
      "Epoch: 97/100 | step: 113/422 | loss: 0.00971301831305027\n",
      "Epoch: 97/100 | step: 114/422 | loss: 0.005807206500321627\n",
      "Epoch: 97/100 | step: 115/422 | loss: 0.005085098557174206\n",
      "Epoch: 97/100 | step: 116/422 | loss: 0.005409831181168556\n",
      "Epoch: 97/100 | step: 117/422 | loss: 0.00366784306243062\n",
      "Epoch: 97/100 | step: 118/422 | loss: 0.0043199933134019375\n",
      "Epoch: 97/100 | step: 119/422 | loss: 0.004455723334103823\n",
      "Epoch: 97/100 | step: 120/422 | loss: 0.0040178257040679455\n",
      "Epoch: 97/100 | step: 121/422 | loss: 0.0048627774231135845\n",
      "Epoch: 97/100 | step: 122/422 | loss: 0.00427405396476388\n",
      "Epoch: 97/100 | step: 123/422 | loss: 0.004897109232842922\n",
      "Epoch: 97/100 | step: 124/422 | loss: 0.0059747579507529736\n",
      "Epoch: 97/100 | step: 125/422 | loss: 0.010141432285308838\n",
      "Epoch: 97/100 | step: 126/422 | loss: 0.008066210895776749\n",
      "Epoch: 97/100 | step: 127/422 | loss: 0.0038796851877123117\n",
      "Epoch: 97/100 | step: 128/422 | loss: 0.004705579020082951\n",
      "Epoch: 97/100 | step: 129/422 | loss: 0.005053566303104162\n",
      "Epoch: 97/100 | step: 130/422 | loss: 0.004225659649819136\n",
      "Epoch: 97/100 | step: 131/422 | loss: 0.0045124744065105915\n",
      "Epoch: 97/100 | step: 132/422 | loss: 0.024699820205569267\n",
      "Epoch: 97/100 | step: 133/422 | loss: 0.007917631417512894\n",
      "Epoch: 97/100 | step: 134/422 | loss: 0.008352326229214668\n",
      "Epoch: 97/100 | step: 135/422 | loss: 0.008567488752305508\n",
      "Epoch: 97/100 | step: 136/422 | loss: 0.005665392614901066\n",
      "Epoch: 97/100 | step: 137/422 | loss: 0.007036568131297827\n",
      "Epoch: 97/100 | step: 138/422 | loss: 0.007672982290387154\n",
      "Epoch: 97/100 | step: 139/422 | loss: 0.00854249857366085\n",
      "Epoch: 97/100 | step: 140/422 | loss: 0.011862724088132381\n",
      "Epoch: 97/100 | step: 141/422 | loss: 0.0048779333010315895\n",
      "Epoch: 97/100 | step: 142/422 | loss: 0.007124017458409071\n",
      "Epoch: 97/100 | step: 143/422 | loss: 0.004930362105369568\n",
      "Epoch: 97/100 | step: 144/422 | loss: 0.004907156340777874\n",
      "Epoch: 97/100 | step: 145/422 | loss: 0.0050597889348864555\n",
      "Epoch: 97/100 | step: 146/422 | loss: 0.01279938779771328\n",
      "Epoch: 97/100 | step: 147/422 | loss: 0.005257684737443924\n",
      "Epoch: 97/100 | step: 148/422 | loss: 0.00828864797949791\n",
      "Epoch: 97/100 | step: 149/422 | loss: 0.012455591931939125\n",
      "Epoch: 97/100 | step: 150/422 | loss: 0.008342718705534935\n",
      "Epoch: 97/100 | step: 151/422 | loss: 0.0053983042016625404\n",
      "Epoch: 97/100 | step: 152/422 | loss: 0.00451567443087697\n",
      "Epoch: 97/100 | step: 153/422 | loss: 0.014503350481390953\n",
      "Epoch: 97/100 | step: 154/422 | loss: 0.00821918249130249\n",
      "Epoch: 97/100 | step: 155/422 | loss: 0.006916602607816458\n",
      "Epoch: 97/100 | step: 156/422 | loss: 0.007342163007706404\n",
      "Epoch: 97/100 | step: 157/422 | loss: 0.004820631816983223\n",
      "Epoch: 97/100 | step: 158/422 | loss: 0.059253234416246414\n",
      "Epoch: 97/100 | step: 159/422 | loss: 0.005630284547805786\n",
      "Epoch: 97/100 | step: 160/422 | loss: 0.010779649950563908\n",
      "Epoch: 97/100 | step: 161/422 | loss: 0.01693359762430191\n",
      "Epoch: 97/100 | step: 162/422 | loss: 0.008512451313436031\n",
      "Epoch: 97/100 | step: 163/422 | loss: 0.09207813441753387\n",
      "Epoch: 97/100 | step: 164/422 | loss: 0.008528107777237892\n",
      "Epoch: 97/100 | step: 165/422 | loss: 0.007415581960231066\n",
      "Epoch: 97/100 | step: 166/422 | loss: 0.007991839200258255\n",
      "Epoch: 97/100 | step: 167/422 | loss: 0.017854386940598488\n",
      "Epoch: 97/100 | step: 168/422 | loss: 0.004766526166349649\n",
      "Epoch: 97/100 | step: 169/422 | loss: 0.01254433486610651\n",
      "Epoch: 97/100 | step: 170/422 | loss: 0.005048510618507862\n",
      "Epoch: 97/100 | step: 171/422 | loss: 0.006049029063433409\n",
      "Epoch: 97/100 | step: 172/422 | loss: 0.00859036110341549\n",
      "Epoch: 97/100 | step: 173/422 | loss: 0.0075968471355736256\n",
      "Epoch: 97/100 | step: 174/422 | loss: 0.004366138018667698\n",
      "Epoch: 97/100 | step: 175/422 | loss: 0.008955754339694977\n",
      "Epoch: 97/100 | step: 176/422 | loss: 0.010571926832199097\n",
      "Epoch: 97/100 | step: 177/422 | loss: 0.005941666662693024\n",
      "Epoch: 97/100 | step: 178/422 | loss: 0.006807388737797737\n",
      "Epoch: 97/100 | step: 179/422 | loss: 0.004647939465939999\n",
      "Epoch: 97/100 | step: 180/422 | loss: 0.007005123421549797\n",
      "Epoch: 97/100 | step: 181/422 | loss: 0.007115660235285759\n",
      "Epoch: 97/100 | step: 182/422 | loss: 0.007893070578575134\n",
      "Epoch: 97/100 | step: 183/422 | loss: 0.012556517496705055\n",
      "Epoch: 97/100 | step: 184/422 | loss: 0.0048222956247627735\n",
      "Epoch: 97/100 | step: 185/422 | loss: 0.006747761741280556\n",
      "Epoch: 97/100 | step: 186/422 | loss: 0.007848501205444336\n",
      "Epoch: 97/100 | step: 187/422 | loss: 0.00941713061183691\n",
      "Epoch: 97/100 | step: 188/422 | loss: 0.010185605846345425\n",
      "Epoch: 97/100 | step: 189/422 | loss: 0.005740714725106955\n",
      "Epoch: 97/100 | step: 190/422 | loss: 0.00678839860484004\n",
      "Epoch: 97/100 | step: 191/422 | loss: 0.006943365093320608\n",
      "Epoch: 97/100 | step: 192/422 | loss: 0.007892233319580555\n",
      "Epoch: 97/100 | step: 193/422 | loss: 0.0067654503509402275\n",
      "Epoch: 97/100 | step: 194/422 | loss: 0.005167867057025433\n",
      "Epoch: 97/100 | step: 195/422 | loss: 0.005507844965904951\n",
      "Epoch: 97/100 | step: 196/422 | loss: 0.008269946090877056\n",
      "Epoch: 97/100 | step: 197/422 | loss: 0.005424866918474436\n",
      "Epoch: 97/100 | step: 198/422 | loss: 0.008421388454735279\n",
      "Epoch: 97/100 | step: 199/422 | loss: 0.009535366669297218\n",
      "Epoch: 97/100 | step: 200/422 | loss: 0.004361455794423819\n",
      "Epoch: 97/100 | step: 201/422 | loss: 0.009515400975942612\n",
      "Epoch: 97/100 | step: 202/422 | loss: 0.006650502793490887\n",
      "Epoch: 97/100 | step: 203/422 | loss: 0.009349056519567966\n",
      "Epoch: 97/100 | step: 204/422 | loss: 0.004132822155952454\n",
      "Epoch: 97/100 | step: 205/422 | loss: 0.005319424904882908\n",
      "Epoch: 97/100 | step: 206/422 | loss: 0.004701757337898016\n",
      "Epoch: 97/100 | step: 207/422 | loss: 0.006677977275103331\n",
      "Epoch: 97/100 | step: 208/422 | loss: 0.00557795399799943\n",
      "Epoch: 97/100 | step: 209/422 | loss: 0.008250957354903221\n",
      "Epoch: 97/100 | step: 210/422 | loss: 0.005289201624691486\n",
      "Epoch: 97/100 | step: 211/422 | loss: 0.006290217861533165\n",
      "Epoch: 97/100 | step: 212/422 | loss: 0.003859364427626133\n",
      "Epoch: 97/100 | step: 213/422 | loss: 0.006800119299441576\n",
      "Epoch: 97/100 | step: 214/422 | loss: 0.005596623755991459\n",
      "Epoch: 97/100 | step: 215/422 | loss: 0.005712485406547785\n",
      "Epoch: 97/100 | step: 216/422 | loss: 0.007716360501945019\n",
      "Epoch: 97/100 | step: 217/422 | loss: 0.005038613453507423\n",
      "Epoch: 97/100 | step: 218/422 | loss: 0.005663798190653324\n",
      "Epoch: 97/100 | step: 219/422 | loss: 0.008128205314278603\n",
      "Epoch: 97/100 | step: 220/422 | loss: 0.0036277889739722013\n",
      "Epoch: 97/100 | step: 221/422 | loss: 0.005820451769977808\n",
      "Epoch: 97/100 | step: 222/422 | loss: 0.008068742230534554\n",
      "Epoch: 97/100 | step: 223/422 | loss: 0.008917057886719704\n",
      "Epoch: 97/100 | step: 224/422 | loss: 0.003619944443926215\n",
      "Epoch: 97/100 | step: 225/422 | loss: 0.0073963068425655365\n",
      "Epoch: 97/100 | step: 226/422 | loss: 0.0060082487761974335\n",
      "Epoch: 97/100 | step: 227/422 | loss: 0.004364893771708012\n",
      "Epoch: 97/100 | step: 228/422 | loss: 0.006434274837374687\n",
      "Epoch: 97/100 | step: 229/422 | loss: 0.004980846773833036\n",
      "Epoch: 97/100 | step: 230/422 | loss: 0.00424575200304389\n",
      "Epoch: 97/100 | step: 231/422 | loss: 0.00748178455978632\n",
      "Epoch: 97/100 | step: 232/422 | loss: 0.004383854568004608\n",
      "Epoch: 97/100 | step: 233/422 | loss: 0.09416646510362625\n",
      "Epoch: 97/100 | step: 234/422 | loss: 0.01954483985900879\n",
      "Epoch: 97/100 | step: 235/422 | loss: 0.14947889745235443\n",
      "Epoch: 97/100 | step: 236/422 | loss: 0.030389219522476196\n",
      "Epoch: 97/100 | step: 237/422 | loss: 0.16044361889362335\n",
      "Epoch: 97/100 | step: 238/422 | loss: 0.028693851083517075\n",
      "Epoch: 97/100 | step: 239/422 | loss: 0.07192998379468918\n",
      "Epoch: 97/100 | step: 240/422 | loss: 0.026441378518939018\n",
      "Epoch: 97/100 | step: 241/422 | loss: 0.010691552422940731\n",
      "Epoch: 97/100 | step: 242/422 | loss: 0.008478948846459389\n",
      "Epoch: 97/100 | step: 243/422 | loss: 0.022754134610295296\n",
      "Epoch: 97/100 | step: 244/422 | loss: 0.006107876542955637\n",
      "Epoch: 97/100 | step: 245/422 | loss: 0.003688247874379158\n",
      "Epoch: 97/100 | step: 246/422 | loss: 0.02226453274488449\n",
      "Epoch: 97/100 | step: 247/422 | loss: 0.02507256157696247\n",
      "Epoch: 97/100 | step: 248/422 | loss: 0.02026129513978958\n",
      "Epoch: 97/100 | step: 249/422 | loss: 0.004380800761282444\n",
      "Epoch: 97/100 | step: 250/422 | loss: 0.08668124675750732\n",
      "Epoch: 97/100 | step: 251/422 | loss: 0.008965595625340939\n",
      "Epoch: 97/100 | step: 252/422 | loss: 0.010032625868916512\n",
      "Epoch: 97/100 | step: 253/422 | loss: 0.019562549889087677\n",
      "Epoch: 97/100 | step: 254/422 | loss: 0.014648396521806717\n",
      "Epoch: 97/100 | step: 255/422 | loss: 0.005846107844263315\n",
      "Epoch: 97/100 | step: 256/422 | loss: 0.01164435874670744\n",
      "Epoch: 97/100 | step: 257/422 | loss: 0.004381626844406128\n",
      "Epoch: 97/100 | step: 258/422 | loss: 0.007717807311564684\n",
      "Epoch: 97/100 | step: 259/422 | loss: 0.006082896143198013\n",
      "Epoch: 97/100 | step: 260/422 | loss: 0.008558115921914577\n",
      "Epoch: 97/100 | step: 261/422 | loss: 0.012599931098520756\n",
      "Epoch: 97/100 | step: 262/422 | loss: 0.032669756561517715\n",
      "Epoch: 97/100 | step: 263/422 | loss: 0.014687826856970787\n",
      "Epoch: 97/100 | step: 264/422 | loss: 0.12803836166858673\n",
      "Epoch: 97/100 | step: 265/422 | loss: 0.040082432329654694\n",
      "Epoch: 97/100 | step: 266/422 | loss: 0.014708957634866238\n",
      "Epoch: 97/100 | step: 267/422 | loss: 0.008529304526746273\n",
      "Epoch: 97/100 | step: 268/422 | loss: 0.0061964960768818855\n",
      "Epoch: 97/100 | step: 269/422 | loss: 0.0070108347572386265\n",
      "Epoch: 97/100 | step: 270/422 | loss: 0.008123229257762432\n",
      "Epoch: 97/100 | step: 271/422 | loss: 0.01632896438241005\n",
      "Epoch: 97/100 | step: 272/422 | loss: 0.004641540348529816\n",
      "Epoch: 97/100 | step: 273/422 | loss: 0.010517040267586708\n",
      "Epoch: 97/100 | step: 274/422 | loss: 0.037297263741493225\n",
      "Epoch: 97/100 | step: 275/422 | loss: 0.008113745599985123\n",
      "Epoch: 97/100 | step: 276/422 | loss: 0.08753804117441177\n",
      "Epoch: 97/100 | step: 277/422 | loss: 0.029735572636127472\n",
      "Epoch: 97/100 | step: 278/422 | loss: 0.009699530899524689\n",
      "Epoch: 97/100 | step: 279/422 | loss: 0.004289894364774227\n",
      "Epoch: 97/100 | step: 280/422 | loss: 0.012714034877717495\n",
      "Epoch: 97/100 | step: 281/422 | loss: 0.007419085130095482\n",
      "Epoch: 97/100 | step: 282/422 | loss: 0.03982371836900711\n",
      "Epoch: 97/100 | step: 283/422 | loss: 0.03544619306921959\n",
      "Epoch: 97/100 | step: 284/422 | loss: 0.06578828394412994\n",
      "Epoch: 97/100 | step: 285/422 | loss: 0.023183563724160194\n",
      "Epoch: 97/100 | step: 286/422 | loss: 0.0157404113560915\n",
      "Epoch: 97/100 | step: 287/422 | loss: 0.013925831764936447\n",
      "Epoch: 97/100 | step: 288/422 | loss: 0.0179283544421196\n",
      "Epoch: 97/100 | step: 289/422 | loss: 0.008286052383482456\n",
      "Epoch: 97/100 | step: 290/422 | loss: 0.007498702500015497\n",
      "Epoch: 97/100 | step: 291/422 | loss: 0.005054399371147156\n",
      "Epoch: 97/100 | step: 292/422 | loss: 0.005453069694340229\n",
      "Epoch: 97/100 | step: 293/422 | loss: 0.004386989865452051\n",
      "Epoch: 97/100 | step: 294/422 | loss: 0.008109158836305141\n",
      "Epoch: 97/100 | step: 295/422 | loss: 0.005254951771348715\n",
      "Epoch: 97/100 | step: 296/422 | loss: 0.00767438905313611\n",
      "Epoch: 97/100 | step: 297/422 | loss: 0.007444337476044893\n",
      "Epoch: 97/100 | step: 298/422 | loss: 0.008436882868409157\n",
      "Epoch: 97/100 | step: 299/422 | loss: 0.016377106308937073\n",
      "Epoch: 97/100 | step: 300/422 | loss: 0.009822765365242958\n",
      "Epoch: 97/100 | step: 301/422 | loss: 0.01995990052819252\n",
      "Epoch: 97/100 | step: 302/422 | loss: 0.006110009271651506\n",
      "Epoch: 97/100 | step: 303/422 | loss: 0.0068261525593698025\n",
      "Epoch: 97/100 | step: 304/422 | loss: 0.00484298262745142\n",
      "Epoch: 97/100 | step: 305/422 | loss: 0.004953849129378796\n",
      "Epoch: 97/100 | step: 306/422 | loss: 0.009914041496813297\n",
      "Epoch: 97/100 | step: 307/422 | loss: 0.005472093354910612\n",
      "Epoch: 97/100 | step: 308/422 | loss: 0.008260142058134079\n",
      "Epoch: 97/100 | step: 309/422 | loss: 0.00599085446447134\n",
      "Epoch: 97/100 | step: 310/422 | loss: 0.004832545295357704\n",
      "Epoch: 97/100 | step: 311/422 | loss: 0.09108368307352066\n",
      "Epoch: 97/100 | step: 312/422 | loss: 0.017828475683927536\n",
      "Epoch: 97/100 | step: 313/422 | loss: 0.5053001046180725\n",
      "Epoch: 97/100 | step: 314/422 | loss: 0.35646530985832214\n",
      "Epoch: 97/100 | step: 315/422 | loss: 0.4735220670700073\n",
      "Epoch: 97/100 | step: 316/422 | loss: 0.0780802071094513\n",
      "Epoch: 97/100 | step: 317/422 | loss: 0.021293437108397484\n",
      "Epoch: 97/100 | step: 318/422 | loss: 0.4233231544494629\n",
      "Epoch: 97/100 | step: 319/422 | loss: 0.4593698978424072\n",
      "Epoch: 97/100 | step: 320/422 | loss: 0.2039966732263565\n",
      "Epoch: 97/100 | step: 321/422 | loss: 0.0668194368481636\n",
      "Epoch: 97/100 | step: 322/422 | loss: 0.07973125576972961\n",
      "Epoch: 97/100 | step: 323/422 | loss: 0.7779421806335449\n",
      "Epoch: 97/100 | step: 324/422 | loss: 0.31766456365585327\n",
      "Epoch: 97/100 | step: 325/422 | loss: 0.007750616874545813\n",
      "Epoch: 97/100 | step: 326/422 | loss: 0.06628367304801941\n",
      "Epoch: 97/100 | step: 327/422 | loss: 0.014142396859824657\n",
      "Epoch: 97/100 | step: 328/422 | loss: 0.01896963082253933\n",
      "Epoch: 97/100 | step: 329/422 | loss: 0.01088810060173273\n",
      "Epoch: 97/100 | step: 330/422 | loss: 0.013218164443969727\n",
      "Epoch: 97/100 | step: 331/422 | loss: 0.03766052797436714\n",
      "Epoch: 97/100 | step: 332/422 | loss: 0.019946390762925148\n",
      "Epoch: 97/100 | step: 333/422 | loss: 0.008800157345831394\n",
      "Epoch: 97/100 | step: 334/422 | loss: 0.021182777360081673\n",
      "Epoch: 97/100 | step: 335/422 | loss: 0.01587488129734993\n",
      "Epoch: 97/100 | step: 336/422 | loss: 0.006628567352890968\n",
      "Epoch: 97/100 | step: 337/422 | loss: 0.008614642545580864\n",
      "Epoch: 97/100 | step: 338/422 | loss: 0.012630891054868698\n",
      "Epoch: 97/100 | step: 339/422 | loss: 0.007871956564486027\n",
      "Epoch: 97/100 | step: 340/422 | loss: 0.011062748730182648\n",
      "Epoch: 97/100 | step: 341/422 | loss: 0.012400918640196323\n",
      "Epoch: 97/100 | step: 342/422 | loss: 0.00584785221144557\n",
      "Epoch: 97/100 | step: 343/422 | loss: 0.04163869097828865\n",
      "Epoch: 97/100 | step: 344/422 | loss: 0.005841088946908712\n",
      "Epoch: 97/100 | step: 345/422 | loss: 0.009650005027651787\n",
      "Epoch: 97/100 | step: 346/422 | loss: 0.012889863923192024\n",
      "Epoch: 97/100 | step: 347/422 | loss: 0.009599450044333935\n",
      "Epoch: 97/100 | step: 348/422 | loss: 0.013827307149767876\n",
      "Epoch: 97/100 | step: 349/422 | loss: 0.011551182717084885\n",
      "Epoch: 97/100 | step: 350/422 | loss: 0.012743349187076092\n",
      "Epoch: 97/100 | step: 351/422 | loss: 0.006016583181917667\n",
      "Epoch: 97/100 | step: 352/422 | loss: 0.019574163481593132\n",
      "Epoch: 97/100 | step: 353/422 | loss: 0.007693585939705372\n",
      "Epoch: 97/100 | step: 354/422 | loss: 0.017670052126049995\n",
      "Epoch: 97/100 | step: 355/422 | loss: 0.04252120107412338\n",
      "Epoch: 97/100 | step: 356/422 | loss: 0.04312962666153908\n",
      "Epoch: 97/100 | step: 357/422 | loss: 0.013099854812026024\n",
      "Epoch: 97/100 | step: 358/422 | loss: 0.013803348876535892\n",
      "Epoch: 97/100 | step: 359/422 | loss: 0.0034606000408530235\n",
      "Epoch: 97/100 | step: 360/422 | loss: 0.05047985538840294\n",
      "Epoch: 97/100 | step: 361/422 | loss: 0.009542763233184814\n",
      "Epoch: 97/100 | step: 362/422 | loss: 0.011061749421060085\n",
      "Epoch: 97/100 | step: 363/422 | loss: 0.02573241852223873\n",
      "Epoch: 97/100 | step: 364/422 | loss: 0.007228020112961531\n",
      "Epoch: 97/100 | step: 365/422 | loss: 0.021203240379691124\n",
      "Epoch: 97/100 | step: 366/422 | loss: 0.007046202663332224\n",
      "Epoch: 97/100 | step: 367/422 | loss: 0.013909339904785156\n",
      "Epoch: 97/100 | step: 368/422 | loss: 0.010601554065942764\n",
      "Epoch: 97/100 | step: 369/422 | loss: 0.017026478424668312\n",
      "Epoch: 97/100 | step: 370/422 | loss: 0.004793863743543625\n",
      "Epoch: 97/100 | step: 371/422 | loss: 0.01943008229136467\n",
      "Epoch: 97/100 | step: 372/422 | loss: 0.012717962265014648\n",
      "Epoch: 97/100 | step: 373/422 | loss: 0.0070189060643315315\n",
      "Epoch: 97/100 | step: 374/422 | loss: 0.014258855022490025\n",
      "Epoch: 97/100 | step: 375/422 | loss: 0.012053790502250195\n",
      "Epoch: 97/100 | step: 376/422 | loss: 0.006750829517841339\n",
      "Epoch: 97/100 | step: 377/422 | loss: 0.0065275817178189754\n",
      "Epoch: 97/100 | step: 378/422 | loss: 0.010917454026639462\n",
      "Epoch: 97/100 | step: 379/422 | loss: 0.011803995817899704\n",
      "Epoch: 97/100 | step: 380/422 | loss: 0.00815340131521225\n",
      "Epoch: 97/100 | step: 381/422 | loss: 0.012580852955579758\n",
      "Epoch: 97/100 | step: 382/422 | loss: 0.005829296074807644\n",
      "Epoch: 97/100 | step: 383/422 | loss: 0.006741195917129517\n",
      "Epoch: 97/100 | step: 384/422 | loss: 0.02631019987165928\n",
      "Epoch: 97/100 | step: 385/422 | loss: 0.006502055563032627\n",
      "Epoch: 97/100 | step: 386/422 | loss: 0.009341622702777386\n",
      "Epoch: 97/100 | step: 387/422 | loss: 0.009827972389757633\n",
      "Epoch: 97/100 | step: 388/422 | loss: 0.020220035687088966\n",
      "Epoch: 97/100 | step: 389/422 | loss: 0.012512631714344025\n",
      "Epoch: 97/100 | step: 390/422 | loss: 0.00644270982593298\n",
      "Epoch: 97/100 | step: 391/422 | loss: 0.005653955042362213\n",
      "Epoch: 97/100 | step: 392/422 | loss: 0.004268110729753971\n",
      "Epoch: 97/100 | step: 393/422 | loss: 0.00992265623062849\n",
      "Epoch: 97/100 | step: 394/422 | loss: 0.008112899959087372\n",
      "Epoch: 97/100 | step: 395/422 | loss: 0.02406650222837925\n",
      "Epoch: 97/100 | step: 396/422 | loss: 0.007307613734155893\n",
      "Epoch: 97/100 | step: 397/422 | loss: 0.01930917799472809\n",
      "Epoch: 97/100 | step: 398/422 | loss: 0.024615857750177383\n",
      "Epoch: 97/100 | step: 399/422 | loss: 0.007034921552985907\n",
      "Epoch: 97/100 | step: 400/422 | loss: 0.006824582349509001\n",
      "Epoch: 97/100 | step: 401/422 | loss: 0.008422777988016605\n",
      "Epoch: 97/100 | step: 402/422 | loss: 0.006634696386754513\n",
      "Epoch: 97/100 | step: 403/422 | loss: 0.00741067249327898\n",
      "Epoch: 97/100 | step: 404/422 | loss: 0.00515923323109746\n",
      "Epoch: 97/100 | step: 405/422 | loss: 0.013278641737997532\n",
      "Epoch: 97/100 | step: 406/422 | loss: 0.015306234359741211\n",
      "Epoch: 97/100 | step: 407/422 | loss: 0.009392834268510342\n",
      "Epoch: 97/100 | step: 408/422 | loss: 0.00880691222846508\n",
      "Epoch: 97/100 | step: 409/422 | loss: 0.006512044463306665\n",
      "Epoch: 97/100 | step: 410/422 | loss: 0.005510637536644936\n",
      "Epoch: 97/100 | step: 411/422 | loss: 0.0065795183181762695\n",
      "Epoch: 97/100 | step: 412/422 | loss: 0.00930156372487545\n",
      "Epoch: 97/100 | step: 413/422 | loss: 0.0061397491954267025\n",
      "Epoch: 97/100 | step: 414/422 | loss: 0.005285255610942841\n",
      "Epoch: 97/100 | step: 415/422 | loss: 0.013264461420476437\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 98/100 | step: 1/422 | loss: 0.005696593318134546\n",
      "Epoch: 98/100 | step: 2/422 | loss: 0.005766851827502251\n",
      "Epoch: 98/100 | step: 3/422 | loss: 0.006442782003432512\n",
      "Epoch: 98/100 | step: 4/422 | loss: 0.006091722287237644\n",
      "Epoch: 98/100 | step: 5/422 | loss: 0.013102708384394646\n",
      "Epoch: 98/100 | step: 6/422 | loss: 0.005111207254230976\n",
      "Epoch: 98/100 | step: 7/422 | loss: 0.006570310331881046\n",
      "Epoch: 98/100 | step: 8/422 | loss: 0.018503786996006966\n",
      "Epoch: 98/100 | step: 9/422 | loss: 0.00466492772102356\n",
      "Epoch: 98/100 | step: 10/422 | loss: 0.015368240885436535\n",
      "Epoch: 98/100 | step: 11/422 | loss: 0.004338132217526436\n",
      "Epoch: 98/100 | step: 12/422 | loss: 0.009868541732430458\n",
      "Epoch: 98/100 | step: 13/422 | loss: 0.01176517829298973\n",
      "Epoch: 98/100 | step: 14/422 | loss: 0.010562208481132984\n",
      "Epoch: 98/100 | step: 15/422 | loss: 0.005028929561376572\n",
      "Epoch: 98/100 | step: 16/422 | loss: 0.00773249939084053\n",
      "Epoch: 98/100 | step: 17/422 | loss: 0.004946202505379915\n",
      "Epoch: 98/100 | step: 18/422 | loss: 0.009894189424812794\n",
      "Epoch: 98/100 | step: 19/422 | loss: 0.007562866900116205\n",
      "Epoch: 98/100 | step: 20/422 | loss: 0.05133349448442459\n",
      "Epoch: 98/100 | step: 21/422 | loss: 0.363224059343338\n",
      "Epoch: 98/100 | step: 22/422 | loss: 1.3643099069595337\n",
      "Epoch: 98/100 | step: 23/422 | loss: 0.007150927092880011\n",
      "Epoch: 98/100 | step: 24/422 | loss: 0.026916898787021637\n",
      "Epoch: 98/100 | step: 25/422 | loss: 0.0495113767683506\n",
      "Epoch: 98/100 | step: 26/422 | loss: 0.015304049476981163\n",
      "Epoch: 98/100 | step: 27/422 | loss: 0.05215972661972046\n",
      "Epoch: 98/100 | step: 28/422 | loss: 0.017410144209861755\n",
      "Epoch: 98/100 | step: 29/422 | loss: 0.009365980513393879\n",
      "Epoch: 98/100 | step: 30/422 | loss: 0.013127343729138374\n",
      "Epoch: 98/100 | step: 31/422 | loss: 0.05988399311900139\n",
      "Epoch: 98/100 | step: 32/422 | loss: 0.22031845152378082\n",
      "Epoch: 98/100 | step: 33/422 | loss: 0.009373696520924568\n",
      "Epoch: 98/100 | step: 34/422 | loss: 0.007640172261744738\n",
      "Epoch: 98/100 | step: 35/422 | loss: 0.015164262615144253\n",
      "Epoch: 98/100 | step: 36/422 | loss: 0.11662312597036362\n",
      "Epoch: 98/100 | step: 37/422 | loss: 0.14493902027606964\n",
      "Epoch: 98/100 | step: 38/422 | loss: 0.01410645991563797\n",
      "Epoch: 98/100 | step: 39/422 | loss: 0.017477691173553467\n",
      "Epoch: 98/100 | step: 40/422 | loss: 0.012420780956745148\n",
      "Epoch: 98/100 | step: 41/422 | loss: 0.01601768098771572\n",
      "Epoch: 98/100 | step: 42/422 | loss: 0.02391822263598442\n",
      "Epoch: 98/100 | step: 43/422 | loss: 0.012162559665739536\n",
      "Epoch: 98/100 | step: 44/422 | loss: 0.008568335324525833\n",
      "Epoch: 98/100 | step: 45/422 | loss: 0.04552879557013512\n",
      "Epoch: 98/100 | step: 46/422 | loss: 0.01443710271269083\n",
      "Epoch: 98/100 | step: 47/422 | loss: 0.06929709762334824\n",
      "Epoch: 98/100 | step: 48/422 | loss: 0.009116447530686855\n",
      "Epoch: 98/100 | step: 49/422 | loss: 0.011993219144642353\n",
      "Epoch: 98/100 | step: 50/422 | loss: 0.015093750320374966\n",
      "Epoch: 98/100 | step: 51/422 | loss: 0.016005532816052437\n",
      "Epoch: 98/100 | step: 52/422 | loss: 0.01684698835015297\n",
      "Epoch: 98/100 | step: 53/422 | loss: 0.012501354329288006\n",
      "Epoch: 98/100 | step: 54/422 | loss: 0.01789497584104538\n",
      "Epoch: 98/100 | step: 55/422 | loss: 0.01909070834517479\n",
      "Epoch: 98/100 | step: 56/422 | loss: 0.008766804821789265\n",
      "Epoch: 98/100 | step: 57/422 | loss: 0.0085565485060215\n",
      "Epoch: 98/100 | step: 58/422 | loss: 0.00750163197517395\n",
      "Epoch: 98/100 | step: 59/422 | loss: 0.060897551476955414\n",
      "Epoch: 98/100 | step: 60/422 | loss: 0.011832628399133682\n",
      "Epoch: 98/100 | step: 61/422 | loss: 0.009420972317457199\n",
      "Epoch: 98/100 | step: 62/422 | loss: 0.00959537923336029\n",
      "Epoch: 98/100 | step: 63/422 | loss: 0.025344667956233025\n",
      "Epoch: 98/100 | step: 64/422 | loss: 0.012323277071118355\n",
      "Epoch: 98/100 | step: 65/422 | loss: 0.007249733433127403\n",
      "Epoch: 98/100 | step: 66/422 | loss: 0.005715177394449711\n",
      "Epoch: 98/100 | step: 67/422 | loss: 0.007815956138074398\n",
      "Epoch: 98/100 | step: 68/422 | loss: 0.009147470816969872\n",
      "Epoch: 98/100 | step: 69/422 | loss: 0.012559476308524609\n",
      "Epoch: 98/100 | step: 70/422 | loss: 0.011197386309504509\n",
      "Epoch: 98/100 | step: 71/422 | loss: 0.01178042497485876\n",
      "Epoch: 98/100 | step: 72/422 | loss: 0.011699149385094643\n",
      "Epoch: 98/100 | step: 73/422 | loss: 0.0060973213985562325\n",
      "Epoch: 98/100 | step: 74/422 | loss: 0.008039789274334908\n",
      "Epoch: 98/100 | step: 75/422 | loss: 0.006730431225150824\n",
      "Epoch: 98/100 | step: 76/422 | loss: 0.010984419845044613\n",
      "Epoch: 98/100 | step: 77/422 | loss: 0.004231146536767483\n",
      "Epoch: 98/100 | step: 78/422 | loss: 0.008264063857495785\n",
      "Epoch: 98/100 | step: 79/422 | loss: 0.012594575062394142\n",
      "Epoch: 98/100 | step: 80/422 | loss: 0.0091576362028718\n",
      "Epoch: 98/100 | step: 81/422 | loss: 0.00864481832832098\n",
      "Epoch: 98/100 | step: 82/422 | loss: 0.01116622518748045\n",
      "Epoch: 98/100 | step: 83/422 | loss: 0.006901606917381287\n",
      "Epoch: 98/100 | step: 84/422 | loss: 0.007456254214048386\n",
      "Epoch: 98/100 | step: 85/422 | loss: 0.006155211012810469\n",
      "Epoch: 98/100 | step: 86/422 | loss: 0.006766312289983034\n",
      "Epoch: 98/100 | step: 87/422 | loss: 0.011746866628527641\n",
      "Epoch: 98/100 | step: 88/422 | loss: 0.006197105161845684\n",
      "Epoch: 98/100 | step: 89/422 | loss: 0.007049962878227234\n",
      "Epoch: 98/100 | step: 90/422 | loss: 0.005066985730081797\n",
      "Epoch: 98/100 | step: 91/422 | loss: 0.004416119307279587\n",
      "Epoch: 98/100 | step: 92/422 | loss: 0.0052225287072360516\n",
      "Epoch: 98/100 | step: 93/422 | loss: 0.007859698496758938\n",
      "Epoch: 98/100 | step: 94/422 | loss: 0.0073561836034059525\n",
      "Epoch: 98/100 | step: 95/422 | loss: 0.004812613595277071\n",
      "Epoch: 98/100 | step: 96/422 | loss: 0.009290674701333046\n",
      "Epoch: 98/100 | step: 97/422 | loss: 0.008479784242808819\n",
      "Epoch: 98/100 | step: 98/422 | loss: 0.009017030708491802\n",
      "Epoch: 98/100 | step: 99/422 | loss: 0.009256839752197266\n",
      "Epoch: 98/100 | step: 100/422 | loss: 0.00872116256505251\n",
      "Epoch: 98/100 | step: 101/422 | loss: 0.006035980768501759\n",
      "Epoch: 98/100 | step: 102/422 | loss: 0.009042970836162567\n",
      "Epoch: 98/100 | step: 103/422 | loss: 0.016730688512325287\n",
      "Epoch: 98/100 | step: 104/422 | loss: 0.006346218753606081\n",
      "Epoch: 98/100 | step: 105/422 | loss: 0.01100386492908001\n",
      "Epoch: 98/100 | step: 106/422 | loss: 0.011904111132025719\n",
      "Epoch: 98/100 | step: 107/422 | loss: 0.0072329663671553135\n",
      "Epoch: 98/100 | step: 108/422 | loss: 0.005503159947693348\n",
      "Epoch: 98/100 | step: 109/422 | loss: 0.007001972757279873\n",
      "Epoch: 98/100 | step: 110/422 | loss: 0.010274970903992653\n",
      "Epoch: 98/100 | step: 111/422 | loss: 0.006828338373452425\n",
      "Epoch: 98/100 | step: 112/422 | loss: 0.0065450966358184814\n",
      "Epoch: 98/100 | step: 113/422 | loss: 0.003621977288275957\n",
      "Epoch: 98/100 | step: 114/422 | loss: 0.005217472091317177\n",
      "Epoch: 98/100 | step: 115/422 | loss: 0.009045690298080444\n",
      "Epoch: 98/100 | step: 116/422 | loss: 0.006754398345947266\n",
      "Epoch: 98/100 | step: 117/422 | loss: 0.006092410534620285\n",
      "Epoch: 98/100 | step: 118/422 | loss: 0.011237411759793758\n",
      "Epoch: 98/100 | step: 119/422 | loss: 0.006374482065439224\n",
      "Epoch: 98/100 | step: 120/422 | loss: 0.009571884758770466\n",
      "Epoch: 98/100 | step: 121/422 | loss: 0.004988311789929867\n",
      "Epoch: 98/100 | step: 122/422 | loss: 0.006485781166702509\n",
      "Epoch: 98/100 | step: 123/422 | loss: 0.006475240923464298\n",
      "Epoch: 98/100 | step: 124/422 | loss: 0.008694184944033623\n",
      "Epoch: 98/100 | step: 125/422 | loss: 0.005822465755045414\n",
      "Epoch: 98/100 | step: 126/422 | loss: 0.006227061618119478\n",
      "Epoch: 98/100 | step: 127/422 | loss: 0.004574110731482506\n",
      "Epoch: 98/100 | step: 128/422 | loss: 0.003697203239426017\n",
      "Epoch: 98/100 | step: 129/422 | loss: 0.006031382828950882\n",
      "Epoch: 98/100 | step: 130/422 | loss: 0.013013456016778946\n",
      "Epoch: 98/100 | step: 131/422 | loss: 0.007371067069470882\n",
      "Epoch: 98/100 | step: 132/422 | loss: 0.004767484497278929\n",
      "Epoch: 98/100 | step: 133/422 | loss: 0.007235835772007704\n",
      "Epoch: 98/100 | step: 134/422 | loss: 0.008363642729818821\n",
      "Epoch: 98/100 | step: 135/422 | loss: 0.006833983585238457\n",
      "Epoch: 98/100 | step: 136/422 | loss: 0.007192691322416067\n",
      "Epoch: 98/100 | step: 137/422 | loss: 0.011814625933766365\n",
      "Epoch: 98/100 | step: 138/422 | loss: 0.005687405355274677\n",
      "Epoch: 98/100 | step: 139/422 | loss: 0.00526123633608222\n",
      "Epoch: 98/100 | step: 140/422 | loss: 0.004738125018775463\n",
      "Epoch: 98/100 | step: 141/422 | loss: 0.013195021077990532\n",
      "Epoch: 98/100 | step: 142/422 | loss: 0.011784457601606846\n",
      "Epoch: 98/100 | step: 143/422 | loss: 0.009642394259572029\n",
      "Epoch: 98/100 | step: 144/422 | loss: 0.016274454072117805\n",
      "Epoch: 98/100 | step: 145/422 | loss: 0.006394285708665848\n",
      "Epoch: 98/100 | step: 146/422 | loss: 0.006193890236318111\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 99/100 | step: 1/422 | loss: 0.004056550562381744\n",
      "Epoch: 99/100 | step: 2/422 | loss: 0.004471998196095228\n",
      "Epoch: 99/100 | step: 3/422 | loss: 0.010262500494718552\n",
      "Epoch: 99/100 | step: 4/422 | loss: 0.004491455387324095\n",
      "Epoch: 99/100 | step: 5/422 | loss: 0.008861961774528027\n",
      "Epoch: 99/100 | step: 6/422 | loss: 0.006516891531646252\n",
      "Epoch: 99/100 | step: 7/422 | loss: 0.008642296306788921\n",
      "Epoch: 99/100 | step: 8/422 | loss: 0.004953677766025066\n",
      "Epoch: 99/100 | step: 9/422 | loss: 0.005814504809677601\n",
      "Epoch: 99/100 | step: 10/422 | loss: 0.007145356386899948\n",
      "Epoch: 99/100 | step: 11/422 | loss: 0.009888685308396816\n",
      "Epoch: 99/100 | step: 12/422 | loss: 0.01071549579501152\n",
      "Epoch: 99/100 | step: 13/422 | loss: 0.007493472192436457\n",
      "Epoch: 99/100 | step: 14/422 | loss: 0.007192002143710852\n",
      "Epoch: 99/100 | step: 15/422 | loss: 0.006694944575428963\n",
      "Epoch: 99/100 | step: 16/422 | loss: 0.003994599916040897\n",
      "Epoch: 99/100 | step: 17/422 | loss: 0.007869775407016277\n",
      "Epoch: 99/100 | step: 18/422 | loss: 0.0031646532006561756\n",
      "Epoch: 99/100 | step: 19/422 | loss: 0.005634109489619732\n",
      "Epoch: 99/100 | step: 20/422 | loss: 0.003605217207223177\n",
      "Epoch: 99/100 | step: 21/422 | loss: 0.008301823399960995\n",
      "Epoch: 99/100 | step: 22/422 | loss: 0.020121535286307335\n",
      "Epoch: 99/100 | step: 23/422 | loss: 0.00650211563333869\n",
      "Epoch: 99/100 | step: 24/422 | loss: 0.010052982717752457\n",
      "Epoch: 99/100 | step: 25/422 | loss: 0.006738715805113316\n",
      "Epoch: 99/100 | step: 26/422 | loss: 0.00609844084829092\n",
      "Epoch: 99/100 | step: 27/422 | loss: 0.004161844961345196\n",
      "Epoch: 99/100 | step: 28/422 | loss: 0.006825611926615238\n",
      "Epoch: 99/100 | step: 29/422 | loss: 0.0056315534748137\n",
      "Epoch: 99/100 | step: 30/422 | loss: 0.005391733720898628\n",
      "Epoch: 99/100 | step: 31/422 | loss: 0.0056244744919240475\n",
      "Epoch: 99/100 | step: 32/422 | loss: 0.007212050259113312\n",
      "Epoch: 99/100 | step: 33/422 | loss: 0.005806399509310722\n",
      "Epoch: 99/100 | step: 34/422 | loss: 0.005507152993232012\n",
      "Epoch: 99/100 | step: 35/422 | loss: 0.01915968395769596\n",
      "Epoch: 99/100 | step: 36/422 | loss: 0.00765721732750535\n",
      "Epoch: 99/100 | step: 37/422 | loss: 0.005584032274782658\n",
      "Epoch: 99/100 | step: 38/422 | loss: 0.0060920920222997665\n",
      "Epoch: 99/100 | step: 39/422 | loss: 0.008726504631340504\n",
      "Epoch: 99/100 | step: 40/422 | loss: 0.007681757211685181\n",
      "Epoch: 99/100 | step: 41/422 | loss: 0.007246384397149086\n",
      "Epoch: 99/100 | step: 42/422 | loss: 0.00857289507985115\n",
      "Epoch: 99/100 | step: 43/422 | loss: 0.006646900903433561\n",
      "Epoch: 99/100 | step: 44/422 | loss: 0.005740581080317497\n",
      "Epoch: 99/100 | step: 45/422 | loss: 0.007744533475488424\n",
      "Epoch: 99/100 | step: 46/422 | loss: 0.009286833927035332\n",
      "Epoch: 99/100 | step: 47/422 | loss: 0.007414712570607662\n",
      "Epoch: 99/100 | step: 48/422 | loss: 0.006352842785418034\n",
      "Epoch: 99/100 | step: 49/422 | loss: 0.006337818223983049\n",
      "Epoch: 99/100 | step: 50/422 | loss: 0.004742556717246771\n",
      "Epoch: 99/100 | step: 51/422 | loss: 0.006014722399413586\n",
      "Epoch: 99/100 | step: 52/422 | loss: 0.00740561680868268\n",
      "Epoch: 99/100 | step: 53/422 | loss: 0.004604254849255085\n",
      "Epoch: 99/100 | step: 54/422 | loss: 0.005679810419678688\n",
      "Epoch: 99/100 | step: 55/422 | loss: 0.016681618988513947\n",
      "Epoch: 99/100 | step: 56/422 | loss: 0.005125219002366066\n",
      "Epoch: 99/100 | step: 57/422 | loss: 0.008777295239269733\n",
      "Epoch: 99/100 | step: 58/422 | loss: 0.007393872365355492\n",
      "Epoch: 99/100 | step: 59/422 | loss: 0.008050008676946163\n",
      "Epoch: 99/100 | step: 60/422 | loss: 0.005484460387378931\n",
      "Epoch: 99/100 | step: 61/422 | loss: 0.007574429269880056\n",
      "Epoch: 99/100 | step: 62/422 | loss: 0.007125293370336294\n",
      "Epoch: 99/100 | step: 63/422 | loss: 0.0044707246124744415\n",
      "Epoch: 99/100 | step: 64/422 | loss: 0.00988813303411007\n",
      "Epoch: 99/100 | step: 65/422 | loss: 0.002830952638760209\n",
      "Epoch: 99/100 | step: 66/422 | loss: 0.005836903117597103\n",
      "Epoch: 99/100 | step: 67/422 | loss: 0.003546412568539381\n",
      "Epoch: 99/100 | step: 68/422 | loss: 0.00572240399196744\n",
      "Epoch: 99/100 | step: 69/422 | loss: 0.004865964874625206\n",
      "Epoch: 99/100 | step: 70/422 | loss: 0.00810749176889658\n",
      "Epoch: 99/100 | step: 71/422 | loss: 0.0057458169758319855\n",
      "Epoch: 99/100 | step: 72/422 | loss: 0.009857576340436935\n",
      "Epoch: 99/100 | step: 73/422 | loss: 0.006106412969529629\n",
      "Epoch: 99/100 | step: 74/422 | loss: 0.007585578598082066\n",
      "Epoch: 99/100 | step: 75/422 | loss: 0.006757051218301058\n",
      "Epoch: 99/100 | step: 76/422 | loss: 0.009480532258749008\n",
      "Epoch: 99/100 | step: 77/422 | loss: 0.005525752902030945\n",
      "Epoch: 99/100 | step: 78/422 | loss: 0.012390782125294209\n",
      "Epoch: 99/100 | step: 79/422 | loss: 0.02120574750006199\n",
      "Epoch: 99/100 | step: 80/422 | loss: 0.01198330707848072\n",
      "Epoch: 99/100 | step: 81/422 | loss: 0.009502554312348366\n",
      "Epoch: 99/100 | step: 82/422 | loss: 0.012553900480270386\n",
      "Epoch: 99/100 | step: 83/422 | loss: 0.007054678164422512\n",
      "Epoch: 99/100 | step: 84/422 | loss: 0.007081970106810331\n",
      "Epoch: 99/100 | step: 85/422 | loss: 0.009051120840013027\n",
      "Epoch: 99/100 | step: 86/422 | loss: 0.006303399335592985\n",
      "Epoch: 99/100 | step: 87/422 | loss: 0.0049763028509914875\n",
      "Epoch: 99/100 | step: 88/422 | loss: 0.008815284818410873\n",
      "Epoch: 99/100 | step: 89/422 | loss: 0.005927815567702055\n",
      "Epoch: 99/100 | step: 90/422 | loss: 0.007213123608380556\n",
      "Epoch: 99/100 | step: 91/422 | loss: 0.005588202737271786\n",
      "Epoch: 99/100 | step: 92/422 | loss: 0.006833176128566265\n",
      "Epoch: 99/100 | step: 93/422 | loss: 0.008452333509922028\n",
      "Epoch: 99/100 | step: 94/422 | loss: 0.008035287261009216\n",
      "Epoch: 99/100 | step: 95/422 | loss: 0.006320113781839609\n",
      "Epoch: 99/100 | step: 96/422 | loss: 0.0062286690808832645\n",
      "Epoch: 99/100 | step: 97/422 | loss: 0.00808130856603384\n",
      "Epoch: 99/100 | step: 98/422 | loss: 0.005971725098788738\n",
      "Epoch: 99/100 | step: 99/422 | loss: 0.00808473490178585\n",
      "Epoch: 99/100 | step: 100/422 | loss: 0.005225322674959898\n",
      "Epoch: 99/100 | step: 101/422 | loss: 0.003984327893704176\n",
      "Epoch: 99/100 | step: 102/422 | loss: 0.006245624274015427\n",
      "Epoch: 99/100 | step: 103/422 | loss: 0.0032472661696374416\n",
      "Epoch: 99/100 | step: 104/422 | loss: 0.007681850343942642\n",
      "Epoch: 99/100 | step: 105/422 | loss: 0.004860862158238888\n",
      "Epoch: 99/100 | step: 106/422 | loss: 0.0057456642389297485\n",
      "Epoch: 99/100 | step: 107/422 | loss: 0.008036667481064796\n",
      "Epoch: 99/100 | step: 108/422 | loss: 0.0068788412027060986\n",
      "Epoch: 99/100 | step: 109/422 | loss: 0.00694116298109293\n",
      "Epoch: 99/100 | step: 110/422 | loss: 0.008755750954151154\n",
      "Epoch: 99/100 | step: 111/422 | loss: 0.006281841546297073\n",
      "Epoch: 99/100 | step: 112/422 | loss: 0.00646668765693903\n",
      "Epoch: 99/100 | step: 113/422 | loss: 0.009980550967156887\n",
      "Epoch: 99/100 | step: 114/422 | loss: 0.007175643462687731\n",
      "Epoch: 99/100 | step: 115/422 | loss: 0.003708170261234045\n",
      "Epoch: 99/100 | step: 116/422 | loss: 0.006307306699454784\n",
      "Epoch: 99/100 | step: 117/422 | loss: 0.005824693478643894\n",
      "Epoch: 99/100 | step: 118/422 | loss: 0.004892551805824041\n",
      "Epoch: 99/100 | step: 119/422 | loss: 0.005126043688505888\n",
      "Epoch: 99/100 | step: 120/422 | loss: 0.0055136773735284805\n",
      "Epoch: 99/100 | step: 121/422 | loss: 0.008361371234059334\n",
      "Epoch: 99/100 | step: 122/422 | loss: 0.005919208750128746\n",
      "Epoch: 99/100 | step: 123/422 | loss: 0.005932230036705732\n",
      "Epoch: 99/100 | step: 124/422 | loss: 0.009601986035704613\n",
      "Epoch: 99/100 | step: 125/422 | loss: 0.018209414556622505\n",
      "Epoch: 99/100 | step: 126/422 | loss: 0.012241083197295666\n",
      "Epoch: 99/100 | step: 127/422 | loss: 0.006206801626831293\n",
      "Epoch: 99/100 | step: 128/422 | loss: 0.009090403094887733\n",
      "Epoch: 99/100 | step: 129/422 | loss: 0.007206165697425604\n",
      "Epoch: 99/100 | step: 130/422 | loss: 0.007712850347161293\n",
      "Epoch: 99/100 | step: 131/422 | loss: 0.020865464583039284\n",
      "Epoch: 99/100 | step: 132/422 | loss: 0.006042991764843464\n",
      "Epoch: 99/100 | step: 133/422 | loss: 0.02053145505487919\n",
      "Epoch: 99/100 | step: 134/422 | loss: 0.004954725503921509\n",
      "Epoch: 99/100 | step: 135/422 | loss: 0.017343752086162567\n",
      "Epoch: 99/100 | step: 136/422 | loss: 0.007793591823428869\n",
      "Epoch: 99/100 | step: 137/422 | loss: 0.007683113683015108\n",
      "Epoch: 99/100 | step: 138/422 | loss: 0.005138805601745844\n",
      "Epoch: 99/100 | step: 139/422 | loss: 0.006868119351565838\n",
      "Epoch: 99/100 | step: 140/422 | loss: 0.005912385880947113\n",
      "Epoch: 99/100 | step: 141/422 | loss: 0.004585173912346363\n",
      "Epoch: 99/100 | step: 142/422 | loss: 0.004595725331455469\n",
      "Epoch: 99/100 | step: 143/422 | loss: 0.011044600047171116\n",
      "Epoch: 99/100 | step: 144/422 | loss: 0.004582795314490795\n",
      "Epoch: 99/100 | step: 145/422 | loss: 0.01133208442479372\n",
      "Epoch: 99/100 | step: 146/422 | loss: 0.008988886140286922\n",
      "Epoch: 99/100 | step: 147/422 | loss: 0.005969497840851545\n",
      "Epoch: 99/100 | step: 148/422 | loss: 0.007856111973524094\n",
      "Epoch: 99/100 | step: 149/422 | loss: 0.005257006734609604\n",
      "Epoch: 99/100 | step: 150/422 | loss: 0.0050694444216787815\n",
      "Epoch: 99/100 | step: 151/422 | loss: 0.005970960017293692\n",
      "Epoch: 99/100 | step: 152/422 | loss: 0.008904080837965012\n",
      "Epoch: 99/100 | step: 153/422 | loss: 0.005612728651612997\n",
      "Epoch: 99/100 | step: 154/422 | loss: 0.014281186275184155\n",
      "Epoch: 99/100 | step: 155/422 | loss: 0.006232381798326969\n",
      "Epoch: 99/100 | step: 156/422 | loss: 0.008867897093296051\n",
      "Epoch: 99/100 | step: 157/422 | loss: 0.006187077146023512\n",
      "Epoch: 99/100 | step: 158/422 | loss: 0.0045062219724059105\n",
      "Epoch: 99/100 | step: 159/422 | loss: 0.004659574944525957\n",
      "Epoch: 99/100 | step: 160/422 | loss: 0.005594153888523579\n",
      "Epoch: 99/100 | step: 161/422 | loss: 0.006013151258230209\n",
      "Epoch: 99/100 | step: 162/422 | loss: 0.005948271136730909\n",
      "Epoch: 99/100 | step: 163/422 | loss: 0.007779205683618784\n",
      "Epoch: 99/100 | step: 164/422 | loss: 0.007680967450141907\n",
      "Epoch: 99/100 | step: 165/422 | loss: 0.005120141431689262\n",
      "Epoch: 99/100 | step: 166/422 | loss: 0.011134259402751923\n",
      "Epoch: 99/100 | step: 167/422 | loss: 0.010629132390022278\n",
      "Epoch: 99/100 | step: 168/422 | loss: 0.0042068446055054665\n",
      "Epoch: 99/100 | step: 169/422 | loss: 0.00802312046289444\n",
      "Epoch: 99/100 | step: 170/422 | loss: 0.006572065409272909\n",
      "Epoch: 99/100 | step: 171/422 | loss: 0.007695278152823448\n",
      "Epoch: 99/100 | step: 172/422 | loss: 0.0052916849963366985\n",
      "Epoch: 99/100 | step: 173/422 | loss: 0.010793831199407578\n",
      "Epoch: 99/100 | step: 174/422 | loss: 0.00318129756487906\n",
      "Epoch: 99/100 | step: 175/422 | loss: 0.005063805729150772\n",
      "Epoch: 99/100 | step: 176/422 | loss: 0.00832934956997633\n",
      "Epoch: 99/100 | step: 177/422 | loss: 0.00595403928309679\n",
      "Epoch: 99/100 | step: 178/422 | loss: 0.004858763422816992\n",
      "Epoch: 99/100 | step: 179/422 | loss: 0.00910385511815548\n",
      "Epoch: 99/100 | step: 180/422 | loss: 0.0040266253054142\n",
      "Epoch: 99/100 | step: 181/422 | loss: 0.008331319317221642\n",
      "Epoch: 99/100 | step: 182/422 | loss: 0.004172402434051037\n",
      "Epoch: 99/100 | step: 183/422 | loss: 0.0033732163719832897\n",
      "Epoch: 99/100 | step: 184/422 | loss: 0.00824214518070221\n",
      "Epoch: 99/100 | step: 185/422 | loss: 0.006352896802127361\n",
      "Epoch: 99/100 | step: 186/422 | loss: 0.009145732969045639\n",
      "Epoch: 99/100 | step: 187/422 | loss: 0.004911365918815136\n",
      "Epoch: 99/100 | step: 188/422 | loss: 0.01715078577399254\n",
      "Epoch: 99/100 | step: 189/422 | loss: 0.006769123487174511\n",
      "Epoch: 99/100 | step: 190/422 | loss: 0.0029881864320486784\n",
      "Epoch: 99/100 | step: 191/422 | loss: 0.004657234530895948\n",
      "Epoch: 99/100 | step: 192/422 | loss: 0.007742960937321186\n",
      "Epoch: 99/100 | step: 193/422 | loss: 0.005200715269893408\n",
      "Epoch: 99/100 | step: 194/422 | loss: 0.005339620169252157\n",
      "Epoch: 99/100 | step: 195/422 | loss: 0.0025827086064964533\n",
      "Epoch: 99/100 | step: 196/422 | loss: 0.004549281671643257\n",
      "Epoch: 99/100 | step: 197/422 | loss: 0.006898520048707724\n",
      "Epoch: 99/100 | step: 198/422 | loss: 0.00750581081956625\n",
      "Epoch: 99/100 | step: 199/422 | loss: 0.011793588288128376\n",
      "Epoch: 99/100 | step: 200/422 | loss: 0.004926435649394989\n",
      "Epoch: 99/100 | step: 201/422 | loss: 0.014887604862451553\n",
      "Epoch: 99/100 | step: 202/422 | loss: 0.009110658429563046\n",
      "Epoch: 99/100 | step: 203/422 | loss: 0.004693364724516869\n",
      "Epoch: 99/100 | step: 204/422 | loss: 0.006003581918776035\n",
      "Epoch: 99/100 | step: 205/422 | loss: 0.005480511114001274\n",
      "Epoch: 99/100 | step: 206/422 | loss: 0.004259975627064705\n",
      "Epoch: 99/100 | step: 207/422 | loss: 0.007003535982221365\n",
      "Epoch: 99/100 | step: 208/422 | loss: 0.007384830620139837\n",
      "Epoch: 99/100 | step: 209/422 | loss: 0.0075037130154669285\n",
      "Epoch: 99/100 | step: 210/422 | loss: 0.007890821434557438\n",
      "Epoch: 99/100 | step: 211/422 | loss: 0.006831292994320393\n",
      "Epoch: 99/100 | step: 212/422 | loss: 0.0035916033666580915\n",
      "Epoch: 99/100 | step: 213/422 | loss: 0.011486556380987167\n",
      "Epoch: 99/100 | step: 214/422 | loss: 0.005429788958281279\n",
      "Epoch: 99/100 | step: 215/422 | loss: 0.007836446166038513\n",
      "Epoch: 99/100 | step: 216/422 | loss: 0.004325339570641518\n",
      "Epoch: 99/100 | step: 217/422 | loss: 0.005186713300645351\n",
      "Epoch: 99/100 | step: 218/422 | loss: 0.00715672317892313\n",
      "Epoch: 99/100 | step: 219/422 | loss: 0.004901681561022997\n",
      "Epoch: 99/100 | step: 220/422 | loss: 0.009425101801753044\n",
      "Epoch: 99/100 | step: 221/422 | loss: 0.0049084266647696495\n",
      "Epoch: 99/100 | step: 222/422 | loss: 0.005399590823799372\n",
      "Epoch: 99/100 | step: 223/422 | loss: 0.008372793905436993\n",
      "Epoch: 99/100 | step: 224/422 | loss: 0.005715114530175924\n",
      "Epoch: 99/100 | step: 225/422 | loss: 0.004980405792593956\n",
      "Epoch: 99/100 | step: 226/422 | loss: 0.0044364626519382\n",
      "Epoch: 99/100 | step: 227/422 | loss: 0.007917365059256554\n",
      "Epoch: 99/100 | step: 228/422 | loss: 0.013798527419567108\n",
      "Epoch: 99/100 | step: 229/422 | loss: 0.005933270324021578\n",
      "Epoch: 99/100 | step: 230/422 | loss: 0.004983261693269014\n",
      "Epoch: 99/100 | step: 231/422 | loss: 0.006800785195082426\n",
      "Epoch: 99/100 | step: 232/422 | loss: 0.005306366831064224\n",
      "Epoch: 99/100 | step: 233/422 | loss: 0.01999593898653984\n",
      "Epoch: 99/100 | step: 234/422 | loss: 0.005052551627159119\n",
      "Epoch: 99/100 | step: 235/422 | loss: 0.005399130750447512\n",
      "Epoch: 99/100 | step: 236/422 | loss: 0.004822750575840473\n",
      "Epoch: 99/100 | step: 237/422 | loss: 0.006181729957461357\n",
      "Epoch: 99/100 | step: 238/422 | loss: 0.004472195636481047\n",
      "Epoch: 99/100 | step: 239/422 | loss: 0.004657756071537733\n",
      "Epoch: 99/100 | step: 240/422 | loss: 0.006905973423272371\n",
      "Epoch: 99/100 | step: 241/422 | loss: 0.014730753377079964\n",
      "Epoch: 99/100 | step: 242/422 | loss: 0.006273831240832806\n",
      "Epoch: 99/100 | step: 243/422 | loss: 0.006518729962408543\n",
      "Epoch: 99/100 | step: 244/422 | loss: 0.006064702291041613\n",
      "Epoch: 99/100 | step: 245/422 | loss: 0.005707068834453821\n",
      "Epoch: 99/100 | step: 246/422 | loss: 0.0039012269116938114\n",
      "Epoch: 99/100 | step: 247/422 | loss: 0.011473524384200573\n",
      "Epoch: 99/100 | step: 248/422 | loss: 0.013482349924743176\n",
      "Epoch: 99/100 | step: 249/422 | loss: 0.003302895464003086\n",
      "Epoch: 99/100 | step: 250/422 | loss: 0.002882207976654172\n",
      "Epoch: 99/100 | step: 251/422 | loss: 0.0070973592810332775\n",
      "Epoch: 99/100 | step: 252/422 | loss: 0.005460090935230255\n",
      "Epoch: 99/100 | step: 253/422 | loss: 0.006549570243805647\n",
      "Epoch: 99/100 | step: 254/422 | loss: 0.005283467937260866\n",
      "Epoch: 99/100 | step: 255/422 | loss: 0.006225392688065767\n",
      "Epoch: 99/100 | step: 256/422 | loss: 0.004880278836935759\n",
      "Epoch: 99/100 | step: 257/422 | loss: 0.00353635405190289\n",
      "Epoch: 99/100 | step: 258/422 | loss: 0.009242083877325058\n",
      "Epoch: 99/100 | step: 259/422 | loss: 0.006696737837046385\n",
      "Epoch: 99/100 | step: 260/422 | loss: 0.006372623611241579\n",
      "Epoch: 99/100 | step: 261/422 | loss: 0.0038125754799693823\n",
      "Epoch: 99/100 | step: 262/422 | loss: 0.0046918876469135284\n",
      "Epoch: 99/100 | step: 263/422 | loss: 0.006652963813394308\n",
      "Epoch: 99/100 | step: 264/422 | loss: 0.013515127822756767\n",
      "Epoch: 99/100 | step: 265/422 | loss: 0.0051277996972203255\n",
      "Epoch: 99/100 | step: 266/422 | loss: 0.005903087556362152\n",
      "Epoch: 99/100 | step: 267/422 | loss: 0.010037731379270554\n",
      "Epoch: 99/100 | step: 268/422 | loss: 0.0037943313363939524\n",
      "Epoch: 99/100 | step: 269/422 | loss: 0.004080120474100113\n",
      "Epoch: 99/100 | step: 270/422 | loss: 0.007297374773770571\n",
      "Epoch: 99/100 | step: 271/422 | loss: 0.005282559432089329\n",
      "Epoch: 99/100 | step: 272/422 | loss: 0.007853664457798004\n",
      "Epoch: 99/100 | step: 273/422 | loss: 0.005130734294652939\n",
      "Epoch: 99/100 | step: 274/422 | loss: 0.005098108667880297\n",
      "Epoch: 99/100 | step: 275/422 | loss: 0.06682631373405457\n",
      "Epoch: 99/100 | step: 276/422 | loss: 0.007783484645187855\n",
      "Epoch: 99/100 | step: 277/422 | loss: 0.008170259185135365\n",
      "Epoch: 99/100 | step: 278/422 | loss: 0.006021700333803892\n",
      "Epoch: 99/100 | step: 279/422 | loss: 0.2764756679534912\n",
      "Epoch: 99/100 | step: 280/422 | loss: 0.017367934808135033\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Epoch: 100/100 | step: 1/422 | loss: 0.007390682119876146\n",
      "Epoch: 100/100 | step: 2/422 | loss: 0.006203965283930302\n",
      "Epoch: 100/100 | step: 3/422 | loss: 0.003771736053749919\n",
      "Epoch: 100/100 | step: 4/422 | loss: 0.005372225772589445\n",
      "Epoch: 100/100 | step: 5/422 | loss: 0.006081178318709135\n",
      "Epoch: 100/100 | step: 6/422 | loss: 0.014116169884800911\n",
      "Epoch: 100/100 | step: 7/422 | loss: 0.004678602330386639\n",
      "Epoch: 100/100 | step: 8/422 | loss: 0.004864162765443325\n",
      "Epoch: 100/100 | step: 9/422 | loss: 0.005366094876080751\n",
      "Epoch: 100/100 | step: 10/422 | loss: 0.006339624989777803\n",
      "Epoch: 100/100 | step: 11/422 | loss: 0.0060555883683264256\n",
      "Epoch: 100/100 | step: 12/422 | loss: 0.004979630466550589\n",
      "Epoch: 100/100 | step: 13/422 | loss: 0.0065398081205785275\n",
      "Epoch: 100/100 | step: 14/422 | loss: 0.03796225041151047\n",
      "Epoch: 100/100 | step: 15/422 | loss: 0.016923191025853157\n",
      "Epoch: 100/100 | step: 16/422 | loss: 0.0078092715702950954\n",
      "Epoch: 100/100 | step: 17/422 | loss: 0.003725246759131551\n",
      "Epoch: 100/100 | step: 18/422 | loss: 0.01206021849066019\n",
      "Epoch: 100/100 | step: 19/422 | loss: 0.029661688953638077\n",
      "Epoch: 100/100 | step: 20/422 | loss: 0.005712075624614954\n",
      "Epoch: 100/100 | step: 21/422 | loss: 0.011708122678101063\n",
      "Epoch: 100/100 | step: 22/422 | loss: 0.0064611718989908695\n",
      "Epoch: 100/100 | step: 23/422 | loss: 0.0026564502622932196\n",
      "Epoch: 100/100 | step: 24/422 | loss: 0.009820645675063133\n",
      "Epoch: 100/100 | step: 25/422 | loss: 0.0034097249154001474\n",
      "Epoch: 100/100 | step: 26/422 | loss: 0.0038258088752627373\n",
      "Epoch: 100/100 | step: 27/422 | loss: 0.006731042638421059\n",
      "Epoch: 100/100 | step: 28/422 | loss: 0.003235001815482974\n",
      "Epoch: 100/100 | step: 29/422 | loss: 0.007244130130857229\n",
      "Epoch: 100/100 | step: 30/422 | loss: 0.009257768280804157\n",
      "Epoch: 100/100 | step: 31/422 | loss: 0.005152296274900436\n",
      "Epoch: 100/100 | step: 32/422 | loss: 0.010601972229778767\n",
      "Epoch: 100/100 | step: 33/422 | loss: 0.004501753952354193\n",
      "Epoch: 100/100 | step: 34/422 | loss: 0.006968169007450342\n",
      "Epoch: 100/100 | step: 35/422 | loss: 0.003916397225111723\n",
      "Epoch: 100/100 | step: 36/422 | loss: 0.008464093320071697\n",
      "Epoch: 100/100 | step: 37/422 | loss: 0.00957421027123928\n",
      "Epoch: 100/100 | step: 38/422 | loss: 0.006503579672425985\n",
      "Epoch: 100/100 | step: 39/422 | loss: 0.006188566796481609\n",
      "Epoch: 100/100 | step: 40/422 | loss: 0.0258222296833992\n",
      "Epoch: 100/100 | step: 41/422 | loss: 0.009981542825698853\n",
      "Epoch: 100/100 | step: 42/422 | loss: 0.005635409615933895\n",
      "Epoch: 100/100 | step: 43/422 | loss: 0.006656293757259846\n",
      "Epoch: 100/100 | step: 44/422 | loss: 0.007108629681169987\n",
      "Epoch: 100/100 | step: 45/422 | loss: 0.004081470891833305\n",
      "Epoch: 100/100 | step: 46/422 | loss: 0.012292049825191498\n",
      "Epoch: 100/100 | step: 47/422 | loss: 0.006599968299269676\n",
      "Epoch: 100/100 | step: 48/422 | loss: 0.005044878460466862\n",
      "Epoch: 100/100 | step: 49/422 | loss: 0.003239833517000079\n",
      "Epoch: 100/100 | step: 50/422 | loss: 0.003683690447360277\n",
      "Epoch: 100/100 | step: 51/422 | loss: 0.006743354257196188\n",
      "Epoch: 100/100 | step: 52/422 | loss: 0.004433572757989168\n",
      "Epoch: 100/100 | step: 53/422 | loss: 0.006399381905794144\n",
      "Epoch: 100/100 | step: 54/422 | loss: 0.006998012773692608\n",
      "Epoch: 100/100 | step: 55/422 | loss: 0.005157069303095341\n",
      "Epoch: 100/100 | step: 56/422 | loss: 0.012378227896988392\n",
      "Epoch: 100/100 | step: 57/422 | loss: 0.006582633126527071\n",
      "Epoch: 100/100 | step: 58/422 | loss: 0.005229583475738764\n",
      "Epoch: 100/100 | step: 59/422 | loss: 0.006204637233167887\n",
      "Epoch: 100/100 | step: 60/422 | loss: 0.005870021879673004\n",
      "Epoch: 100/100 | step: 61/422 | loss: 0.009113438427448273\n",
      "Epoch: 100/100 | step: 62/422 | loss: 0.004646065644919872\n",
      "Epoch: 100/100 | step: 63/422 | loss: 0.003943465184420347\n",
      "Epoch: 100/100 | step: 64/422 | loss: 0.00513491639867425\n",
      "Error occurred during training: new(): invalid data type 'str'\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    try:\n",
    "        for i, (images, lab) in enumerate(train_dataloader):\n",
    "            # images = images.to(device)\n",
    "            # labels = labels.to(device)\n",
    "\n",
    "            ops = model(images)\n",
    "            loss = criterion(ops, lab)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'Epoch: {epoch + 1}/{epochs} | step: {i + 1}/{n_total_steps} | loss: {loss.item()}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred during training: {e}')\n",
    "        # Optionally, you can break the loop or continue to the next epoch\n",
    "        continue#`continue` depending on your needs\n",
    "\n",
    "print('Finished training!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fine->0\n",
      "data fine->1\n",
      "data fine->2\n",
      "data fine->3\n",
      "data fine->4\n",
      "data fine->5\n",
      "data fine->6\n",
      "data fine->7\n",
      "data fine->8\n",
      "data fine->9\n",
      "data fine->10\n",
      "data fine->11\n",
      "data fine->12\n",
      "data fine->13\n",
      "data fine->14\n",
      "data fine->15\n"
     ]
    }
   ],
   "source": [
    "for i, (image, label) in enumerate(test_dataloader):\n",
    "    print(f'data fine->{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched! batch:1/image: 2\n",
      "Matched! batch:1/image: 3\n",
      "Matched! batch:1/image: 4\n",
      "Matched! batch:1/image: 9\n",
      "Matched! batch:1/image: 17\n",
      "Matched! batch:1/image: 20\n",
      "Matched! batch:1/image: 23\n",
      "Matched! batch:1/image: 26\n",
      "Matched! batch:1/image: 27\n",
      "Matched! batch:1/image: 29\n",
      "Matched! batch:1/image: 30\n",
      "Matched! batch:1/image: 31\n",
      "Matched! batch:2/image: 2\n",
      "Matched! batch:2/image: 3\n",
      "Matched! batch:2/image: 11\n",
      "Matched! batch:2/image: 17\n",
      "Matched! batch:2/image: 18\n",
      "Matched! batch:2/image: 21\n",
      "Matched! batch:2/image: 24\n",
      "Matched! batch:2/image: 25\n",
      "Matched! batch:2/image: 27\n",
      "Matched! batch:2/image: 31\n",
      "Matched! batch:3/image: 4\n",
      "Matched! batch:3/image: 6\n",
      "Matched! batch:3/image: 10\n",
      "Matched! batch:3/image: 12\n",
      "Matched! batch:3/image: 13\n",
      "Matched! batch:3/image: 14\n",
      "Matched! batch:3/image: 18\n",
      "Matched! batch:3/image: 20\n",
      "Matched! batch:3/image: 22\n",
      "Matched! batch:3/image: 24\n",
      "Matched! batch:3/image: 29\n",
      "Matched! batch:3/image: 31\n",
      "Matched! batch:4/image: 8\n",
      "Matched! batch:4/image: 13\n",
      "Matched! batch:4/image: 14\n",
      "Matched! batch:4/image: 18\n",
      "Matched! batch:4/image: 19\n",
      "Matched! batch:4/image: 22\n",
      "Matched! batch:4/image: 23\n",
      "Matched! batch:4/image: 25\n",
      "Matched! batch:4/image: 26\n",
      "Matched! batch:4/image: 27\n",
      "Matched! batch:5/image: 5\n",
      "Matched! batch:5/image: 6\n",
      "Matched! batch:5/image: 8\n",
      "Matched! batch:5/image: 11\n",
      "Matched! batch:5/image: 12\n",
      "Matched! batch:5/image: 13\n",
      "Matched! batch:5/image: 17\n",
      "Matched! batch:5/image: 18\n",
      "Matched! batch:5/image: 19\n",
      "Matched! batch:5/image: 22\n",
      "Matched! batch:5/image: 25\n",
      "Matched! batch:5/image: 26\n",
      "Matched! batch:5/image: 27\n",
      "Matched! batch:5/image: 28\n",
      "Matched! batch:5/image: 29\n",
      "Matched! batch:6/image: 4\n",
      "Matched! batch:6/image: 13\n",
      "Matched! batch:6/image: 16\n",
      "Matched! batch:6/image: 17\n",
      "Matched! batch:6/image: 20\n",
      "Matched! batch:6/image: 23\n",
      "Matched! batch:6/image: 26\n",
      "Matched! batch:6/image: 28\n",
      "Matched! batch:6/image: 29\n",
      "Matched! batch:6/image: 30\n",
      "Matched! batch:7/image: 1\n",
      "Matched! batch:7/image: 2\n",
      "Matched! batch:7/image: 4\n",
      "Matched! batch:7/image: 6\n",
      "Matched! batch:7/image: 7\n",
      "Matched! batch:7/image: 10\n",
      "Matched! batch:7/image: 11\n",
      "Matched! batch:7/image: 12\n",
      "Matched! batch:7/image: 13\n",
      "Matched! batch:7/image: 14\n",
      "Matched! batch:7/image: 16\n",
      "Matched! batch:7/image: 17\n",
      "Matched! batch:7/image: 18\n",
      "Matched! batch:7/image: 20\n",
      "Matched! batch:7/image: 23\n",
      "Matched! batch:7/image: 27\n",
      "Matched! batch:7/image: 28\n",
      "Matched! batch:8/image: 9\n",
      "Matched! batch:8/image: 10\n",
      "Matched! batch:8/image: 12\n",
      "Matched! batch:8/image: 14\n",
      "Matched! batch:8/image: 15\n",
      "Matched! batch:8/image: 18\n",
      "Matched! batch:8/image: 19\n",
      "Matched! batch:8/image: 20\n",
      "Matched! batch:8/image: 21\n",
      "Matched! batch:8/image: 22\n",
      "Matched! batch:8/image: 26\n",
      "Matched! batch:9/image: 1\n",
      "Matched! batch:9/image: 2\n",
      "Matched! batch:9/image: 5\n",
      "Matched! batch:9/image: 7\n",
      "Matched! batch:9/image: 8\n",
      "Matched! batch:9/image: 9\n",
      "Matched! batch:9/image: 11\n",
      "Matched! batch:9/image: 20\n",
      "Matched! batch:9/image: 21\n",
      "Matched! batch:9/image: 29\n",
      "Matched! batch:9/image: 30\n",
      "Matched! batch:9/image: 32\n",
      "Matched! batch:10/image: 1\n",
      "Matched! batch:10/image: 2\n",
      "Matched! batch:10/image: 6\n",
      "Matched! batch:10/image: 8\n",
      "Matched! batch:10/image: 15\n",
      "Matched! batch:10/image: 18\n",
      "Matched! batch:10/image: 19\n",
      "Matched! batch:10/image: 20\n",
      "Matched! batch:10/image: 22\n",
      "Matched! batch:10/image: 23\n",
      "Matched! batch:11/image: 1\n",
      "Matched! batch:11/image: 3\n",
      "Matched! batch:11/image: 4\n",
      "Matched! batch:11/image: 6\n",
      "Matched! batch:11/image: 8\n",
      "Matched! batch:11/image: 10\n",
      "Matched! batch:11/image: 11\n",
      "Matched! batch:11/image: 12\n",
      "Matched! batch:11/image: 18\n",
      "Matched! batch:11/image: 22\n",
      "Matched! batch:11/image: 27\n",
      "Matched! batch:12/image: 1\n",
      "Matched! batch:12/image: 2\n",
      "Matched! batch:12/image: 5\n",
      "Matched! batch:12/image: 7\n",
      "Matched! batch:12/image: 9\n",
      "Matched! batch:12/image: 10\n",
      "Matched! batch:12/image: 11\n",
      "Matched! batch:12/image: 12\n",
      "Matched! batch:12/image: 13\n",
      "Matched! batch:12/image: 19\n",
      "Matched! batch:12/image: 20\n",
      "Matched! batch:12/image: 21\n",
      "Matched! batch:12/image: 29\n",
      "Matched! batch:13/image: 3\n",
      "Matched! batch:13/image: 4\n",
      "Matched! batch:13/image: 6\n",
      "Matched! batch:13/image: 8\n",
      "Matched! batch:13/image: 9\n",
      "Matched! batch:13/image: 10\n",
      "Matched! batch:13/image: 13\n",
      "Matched! batch:13/image: 20\n",
      "Matched! batch:13/image: 22\n",
      "Matched! batch:13/image: 27\n",
      "Matched! batch:13/image: 32\n",
      "Matched! batch:14/image: 2\n",
      "Matched! batch:14/image: 5\n",
      "Matched! batch:14/image: 9\n",
      "Matched! batch:14/image: 12\n",
      "Matched! batch:14/image: 17\n",
      "Matched! batch:14/image: 23\n",
      "Matched! batch:14/image: 25\n",
      "Matched! batch:14/image: 27\n",
      "Matched! batch:14/image: 29\n",
      "Matched! batch:15/image: 2\n",
      "Matched! batch:15/image: 4\n",
      "Matched! batch:15/image: 13\n",
      "Matched! batch:15/image: 15\n",
      "Matched! batch:15/image: 20\n",
      "Matched! batch:15/image: 21\n",
      "Matched! batch:15/image: 23\n",
      "Matched! batch:15/image: 24\n",
      "Matched! batch:15/image: 27\n",
      "Matched! batch:15/image: 28\n",
      "Matched! batch:15/image: 29\n",
      "Matched! batch:15/image: 30\n",
      "Matched! batch:15/image: 31\n",
      "Matched! batch:16/image: 1\n",
      "Matched! batch:16/image: 3\n",
      "Matched! batch:16/image: 4\n",
      "Matched! batch:16/image: 5\n",
      "Matched! batch:16/image: 8\n",
      "Matched! batch:16/image: 9\n",
      "Matched! batch:16/image: 18\n",
      "Matched! batch:16/image: 20\n",
      "Accuracy achieved: 36.8 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(100)]\n",
    "    n_class_samples = [0 for i in range(100)]\n",
    "    k=0\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples +=labels.size(0)\n",
    "        n_correct +=(predicted==labels).sum().item()\n",
    "\n",
    "        print(len(outputs)==len(labels))\n",
    "\n",
    "        for i in range(len(outputs)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if(label==pred):\n",
    "                n_class_correct[label] += 1\n",
    "                print(f'Matched! batch:{k+1}/image: {i+1}')\n",
    "            n_class_samples[label] += 1\n",
    "        \n",
    "        k+=1\n",
    "        \n",
    "    acc = 100.0 * n_correct/n_samples\n",
    "    print(f'Accuracy achieved: {acc} %')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
